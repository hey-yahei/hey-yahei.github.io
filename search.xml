<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>树莓派也能玩转深度学习——Tengine推断引擎</title>
      <link href="/2018/10/13/RasPi-Tengine/"/>
      <url>/2018/10/13/RasPi-Tengine/</url>
      
        <content type="html"><![CDATA[<p>一直以来，树莓派以其良好的社区生态，广受嵌入式爱好者、创客欢迎。在一些相关的社区上（比如<a href="http://shumeipai.nxez.com/what-raspi-used-for" target="_blank" rel="noopener">树莓派实验室</a>），我们可以看到非常丰富的应用示例及其教程。但在树莓派上的深度学习应用并不常见，这主要是受到树莓派计算力的限制，比如之前看到过有人把yolov2原原本本生硬地部署到树莓派上，结果每一帧检测耗时高达<strong>6分钟</strong>！！作一帧目标检测花费6分钟这实在是无法忍受的！<br><em>如果是用yolov2-tiny的话会快很多，但耗时依旧接近<strong>40秒</strong>，参考<a href="https://blog.csdn.net/wjbwjbwjbwjb/article/details/77688625" target="_blank" rel="noopener">树莓派3B上测试YOLO效果 | CSDN</a></em>     </p><p>那树莓派只能跟深度学习无缘了么？那可未必！    </p><h3 id="Tengine"><a href="#Tengine" class="headerlink" title="Tengine"></a>Tengine</h3><p><a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">OADI/Tengine | github</a>       </p><blockquote><p>Tengine 是OPEN AI LAB为嵌入式设备开发的一个轻量级、高性能并且模块化的引擎。<br>Tengine在嵌入式设备上支持CPU，GPU，DLA/NPU，DSP异构计算的计算框架，实现异构计算的调度器，基于ARM平台的高效的计算库实现，针对特定硬件平台的性能优化，动态规划计算图的内存使用，提供对于网络远端AI计算能力的访问支持，支持多级别并行，整个系统模块可拆卸，基于事件驱动的计算模型，吸取已有AI计算框架的优点，设计全新的计算图表示。    </p></blockquote><h3 id="编译安装开源版Tengine"><a href="#编译安装开源版Tengine" class="headerlink" title="编译安装开源版Tengine"></a>编译安装开源版Tengine</h3><h4 id="安装相关工具"><a href="#安装相关工具" class="headerlink" title="安装相关工具"></a>安装相关工具</h4><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> instal <span class="token function">git</span> cmake</code></pre><ul><li><strong>git</strong> 是一个版本控制系统，稍后将用来从 <a href="http://github.com" target="_blank" rel="noopener">github</a> 网站上下载Tengine的源码     </li><li><strong>cmake</strong> 是一个编译工具，用来产生make过程中所需要的Makefile文件      </li></ul><h4 id="安装支持库"><a href="#安装支持库" class="headerlink" title="安装支持库"></a>安装支持库</h4><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libprotobuf-dev protobuf-compiler libboost-all-dev libgoogle-glog-dev libopencv-dev libopenblas-dev</code></pre><ul><li><strong>protobuf</strong> 是一种轻便高效的数据存储格式，这是caffe各种配置文件所使用的数据格式</li><li><strong>boost</strong> 是一个c++的扩展程序库，稍后Tengine的编译依赖于该库</li><li><strong>google-glog</strong> 是一个google提供的日志系统的程序库</li><li><strong>opencv</strong> 是一个开源的计算机视觉库</li><li><strong>openblas</strong> 是一个开源的基础线性代数子程序库</li></ul><h4 id="下载-amp-编译"><a href="#下载-amp-编译" class="headerlink" title="下载&amp;编译"></a>下载&amp;编译</h4><ol><li>从github上下载最新的开源版Tengine源码     <pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/OAID/Tengine.git</code></pre></li><li>切换工作目录到Tengine      <pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> Tengine</code></pre></li><li>准备好配置文件<br>Tengine目录下提供了配置模板 <code>makefile.config.example</code> 文件        <pre class=" language-bash"><code class="language-bash"><span class="token function">cp</span> makefile.config.example makefile.config</code></pre></li><li>修改配置文件 <code>makefile.config</code><br>由于开源版的Tengine不支持针对armv7的优化，所以需要用openblas替代实现；<br>将 <code>CONFIG_ARCH_ARM64=y</code> 这一行注释掉（行首加井号 <code>#</code>）以关闭ARM64架构的优化实现；<br>解除 <code>CONFIG_ARCH_BLAS=y</code> 这一行解除注释（删除行首的井号 <code>#</code>）以开启BLAS计算库的实现方式      </li><li>编译并安装      <pre class=" language-bash"><code class="language-bash"><span class="token function">make</span> -j4<span class="token function">make</span> <span class="token function">install</span></code></pre>这里的 <code>-j4</code> 表示开启四个线程进行编译       </li></ol><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ol><li>下载mobilenet-ssd模型并放置在 <code>Tengine/models</code> 目录下<br>下载链接（提取码为57vb）：<a href="https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g" target="_blank" rel="noopener">https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g</a>     </li><li>将工作目录切换到mobilenet-ssd示例程序的目录下      <pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> ~/Tengine/examples/mobilenet_ssd</code></pre></li><li>编译示例程序    <pre class=" language-bash"><code class="language-bash">cmake –DTENGINE_DIR<span class="token operator">=</span>/home/pi/Tengine <span class="token keyword">.</span><span class="token function">make</span></code></pre>这里 <code>-DTENGINE_DIR</code>用于为cmake指定环境变量TENGINE_DIR，该变量可以在CMakeLists.txt文件中找到       </li><li>运行示例程序     <pre class=" language-bash"><code class="language-bash">./MSSD</code></pre>可以看到对一张照片进行目标检测，总共耗时1148.32ms<br><img src="/imgs/RasPi-Tengine/mssd-opensource.png" alt="mssd-opensource">     </li></ol><h3 id="树莓派专用教育版Tengine"><a href="#树莓派专用教育版Tengine" class="headerlink" title="树莓派专用教育版Tengine"></a>树莓派专用教育版Tengine</h3><p>最近 <a href="http://www.openailab.com/" target="_blank" rel="noopener">Open AI Lab公司</a> 和 <a href="http://www.cbeis.zju.edu.cn/" target="_blank" rel="noopener">浙江大学生物医学工程与仪器科学学院</a> 在嵌入式人工智能领域上开展了教学合作，公司为学院提供了速度更快的针对armv7优化的Tengine版本用于教学用途，接下来让我们看看这个树莓派专用教育版的Tengine到底有多快吧！     </p><ol><li>用树莓派专用教育版Tengine的动态链接库覆盖掉原先的开源版<br>动态链接库路径为：<code>Tengine/install/lib/libtengine.so</code><br><em>编译时，make会在build目录下产生libtengine.so动态链接库，而make instll将动态链接库、头文件等拷贝到install目录下</em><br><img src="/imgs/RasPi-Tengine/replace.png" alt="replace">      </li><li>重新运行mobilenet-ssd的示例程序<br>可以看到，单帧耗时从1148.32ms下降为<strong>286.136ms</strong>，速度有了非常明显的提升！<br><img src="/imgs/RasPi-Tengine/mssd-education.png" alt="mssd-education"></li></ol><h3 id="小试牛刀"><a href="#小试牛刀" class="headerlink" title="小试牛刀"></a>小试牛刀</h3><p>用上高性能的树莓派专用教育版Tengine，看看mobilenet-ssd在树莓派上能表现如何——     </p><p>为了方便，视频流直接从mp4文件读取，原始视频如下：     </p><video src="/imgs/RasPi-Tengine/mssd-test-origin.mp4" width="800" controls muted></video>       <ol><li>从 <a href="https://github.com/hey-yahei/my_blog/tree/master/RasPi-Tengine/mobilenet_ssd" target="_blank" rel="noopener">hey-yahei/my_blog/RasPi-Tengine/mobilenet-ssd | github</a> 上下载源码，并放置在 <code>Tengine/example</code> 目录下    </li><li>检查 <code>CMakeLists.txt</code> 文件中TENGINE_DIR变量是否正确指向Tengine路径</li><li>执行 <code>cmake .</code> 生成Makefile</li><li>执行 <code>make</code> 编译程序</li><li>执行 <code>./MSSD</code> 运行程序         </li></ol><p>实际效果如下：       </p><p><video src="/imgs/RasPi-Tengine/mssd-test-raspi.mp4" width="800" controls muted></video><br>由于一部分cpu资源被用于视频的解码工作（对于支持硬解码的平台来说不存在这个问题），可以看到单帧耗时有所下降（400ms-700ms），但对于多数应用场景来说这个帧率是绰绰有余的。      </p><hr><p>本文开头我们说道，<br>直接在树莓派上配置darknet部署的yolo网络，yolov2单帧耗时接近<strong>6分钟</strong>，yolov2-tiny单帧耗时接近<strong>40秒</strong>；<br>而在树莓派上配置Tengine部署的yolov2网络，在blas实现下单帧耗时不到<strong>8秒</strong>（参考<a href="https://songrbb.github.io/2018/08/17/%E5%88%A9%E7%94%A8Tengine%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E8%B7%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">利用Tengine在树莓派上跑深度学习网络 | songrbb</a>），在针对armv7优化实现的教育版下单帧耗时甚至不到<strong>2秒</strong>！     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RK3399 </tag>
            
            <tag> RasPi </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于MobileNet-SSD的目标检测Demo（二）</title>
      <link href="/2018/09/10/mssd-try3/"/>
      <url>/2018/09/10/mssd-try3/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=31352588&auto=0&height=32"></iframe>      <p>上一篇文章《<a href="/2018/08/24/mssd-try2/">基于MobileNet-SSD的目标检测Demo（一）</a>》介绍了如何在VOC数据集的基础上削减分类训练出自己的分类器，并且尝试着进一步把SSD改为SSDLite。但作为一个Demo，在RK3399上MobileNet-SSD每秒钟只能检测6-7帧，如果每次检测后再把视频内容展现出来，那么展示的视频也只有6-7帧，这样的展示效果似乎不太好。在本篇文章中，我们将尝试把视频的获取与展示和检测任务分离开来，分别放在两个不同的线程上工作，同时将不同的线程绑定到不同的cpu核上，使得两者的工作不会冲突。         </p><hr><h3 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h3><p>进程和线程是操作系统中的两个重要概念。</p><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>考虑在51单片机或是STM32上开发程序，通常这些程序都是串行结构。打个比方，      </p><ol><li>写个数码管的动态驱动，让四个个数码管持续显示数值<code>1217</code>       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token number">1217</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li>用一个超声波模块进行测距，并且用数码管显示结果       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token function">ultrasonicGetDatum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li>超声波数量少还没关系，数码管还是能正常驱动，如果多来几个呢？（简化一下，数码管只显示求和的结果）       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span><span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">ultrasonicgetDatum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>CPU大部分时间都用去等超声波信号了呀，数码管根本就不能驱动起来，那换种方式——      <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>sum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        sum<span class="token operator">+</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        segment<span class="token operator">=</span>sum<span class="token punctuation">;</span>        i<span class="token operator">=</span>sum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>这样一来，如果超声波数据无效就继续驱动数码管，不会让CPU空等。    </li><li>那如果不是简单的求和运算呢？        <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ultras<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        ultras<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        segment<span class="token operator">=</span><span class="token function">process</span><span class="token punctuation">(</span>ultras<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 变成了其他复杂的运算</span>        i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>万一<code>process</code>函数运算过程很复杂，占用了很多CPU的时间，导致数码管不能及时刷新，那数码管依旧驱动不起来。当然，你可以去推算<code>process</code>运算的复杂程度，人为地去拆解运算，变成<code>process[0]</code>、<code>process[1]</code>、……、<code>process[n]</code>最后再由<code>combine</code>把中间结果整合起来（注意这里要保证每个操作都足够小，不会占用太多运算时间）。     <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i_process<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ultras<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span>tmp<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        ultras<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        tmp<span class="token punctuation">[</span>i_process<span class="token punctuation">]</span><span class="token operator">=</span>process<span class="token punctuation">[</span>i_process<span class="token punctuation">]</span><span class="token punctuation">(</span>ultras<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>i_process<span class="token operator">==</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            segment<span class="token operator">=</span><span class="token function">combine</span><span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">;</span>            i_process<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>            i_process<span class="token operator">++</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>看起来确实可行，但是手工拆解运算，这也太恶心了吧，而且这一点都不优雅，万一我程序开发着开发着，这个运算过程发生改变了怎么办？重新拆解运算？那不得炸毛？！     </li><li>那换个思路，我们让两个任务分时进行吧，每个任务轮流运算10ms，超时就带上你的中间结果滚蛋            <pre class=" language-c"><code class="language-c"><span class="token comment" spellcheck="true">// 设置定时器，每10ms中产生一次中断</span><span class="token function">timer_handler</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// 中断处理</span>    <span class="token function">save_metadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 触发中断后保存数据</span>    <span class="token function">change_task</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 切换到另一个任务</span>    <span class="token function">load_metadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 取出新换入任务的中间数据</span>    <span class="token function">task_run</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">// 继续任务</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// ……省略任务定义</span></code></pre>这样虽然增加了额外的开销（任务的调度），但形成了一个通用化的任务调度功能，无论具体任务怎么改变都能够适用，减少CPU闲置的机会，可以更好的压榨CPU     </li></ol><p>实际应用当中，我们经常都能碰见这种多任务的情况，人为地分配任务给处理器需要大量的推算和分解，费时费力还不易调整。在这里，我们形成了一个简单的通用的任务调度功能，其实这也是现代操作系统的基本功能之一，对操作系统而言，这些任务就是一个个的“<strong>进程</strong>”，中间数据被称为“<strong>进程上下文</strong>”，而操作系统有一个专门的模块负责“<strong>进程调度</strong>”的工作，这里举例的固定时间片是一种最简单的调度方式。         </p><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p><center><img src="/imgs/mssd-try/try3/process.png" alt="process"></center><br>这是一张进程状态转换图，         </p><ul><li>新建一个进程时处于“<strong>新建态</strong>”，至于加载到“就绪态”还是“就绪/挂起态”就取决于操作系统翻不翻你牌子；      </li><li>“<strong>就绪态</strong>”指的是进程已经就绪的状态，比如进程执行所需要的资源（比如键盘、鼠标、显卡等外设）可用，并且已经被操作系统翻牌，在等待执行的队伍里排着队；   </li><li>“<strong>阻塞态</strong>”指的是进程未就绪，比如执行的所需资源还未到位，进程本身处于等待的状态；      </li><li>“就绪态”和“阻塞态”还分别有对应的“<strong>就绪/挂起态</strong>”和“<strong>阻塞/挂起态</strong>”，这是进程本身处于就绪或未就绪的状态，但操作系统还没有翻他们牌子；     </li><li>“<strong>运行态</strong>”和“<strong>退出态</strong>”不难理解，就是进程运行中的状态，以及进程完全运行结束而将退出的状态         </li></ul><p>进程上下文保存在一个特殊的称为<strong>进程控制块（PCB）</strong>的结构里，其中包含        </p><ol><li>进程标识信息（各种标识符）</li><li>进程状态信息（寄存器、栈指针等）</li><li>进程控制信息（调度和状态的相关信息，比如进程状态、优先级、事件等）     </li></ol><p>常见的用户进程创建有两种，       </p><ol><li>用户运行一个程序，这个程序会放到一个进程上<br>比如直接运行一个编译好的c程序，或是一个python程序；       </li><li>由现有进程派生<br>比如在c程序中调用fork函数来派生出一个新的进程——        <pre class=" language-c"><code class="language-c"> <span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span> <span class="token keyword">int</span> <span class="token function">main</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>     <span class="token keyword">int</span> pid<span class="token punctuation">,</span> ppid<span class="token punctuation">;</span>     pid <span class="token operator">=</span> <span class="token function">fork</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// fork函数将派生出一个相同的进程，返回新进程的id（对于原始进程返回0）</span>     <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d first output from both processes\n"</span><span class="token punctuation">,</span> pid<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token function">sleep</span> <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"This is the child's pid,  output by the parent process\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"is printed inside the child process if the fork succeeds \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         pid <span class="token operator">=</span> <span class="token function">getpid</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// getpid函数可以获取当前进程的id</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"is the child pid printed by the child, obtained by getpid()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token keyword">else</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"fork failed\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span></code></pre></li></ol><p>进程间的通信通常由信号量、中断和共享空间实现，简单地说，      </p><ul><li><strong>信号量</strong>，事实上就是一个整型变量。操作系统负责维护一个信号量池，进程可以在该池注册带有名称的信号量，多个进程间可以对同一个信号量进行加法或减法操作（通常称为PV操作，该操作由操作系统管理，不会读写冲突），但减法的结果不能小于0，否则进程就会阻塞挂起，等待有足够的信号量可以减去。信号量有点像资源的指示标志，进程占用资源的时候作减法，释放资源的时候做加法。          </li><li><strong>中断</strong>，是Linux提供的一套机制，事实上<code>kill</code>指令就是对目标进程发送一个特定的中断信号，进程接收到中断之后会跳转到指定的中断处理函数进行处理（当然进程也可以设置忽略某一些信号）。在Linux中可以通过指令<code>kill -l</code>查看可用信号，如下：       <pre class=" language-bash"><code class="language-bash">  <span class="token comment" spellcheck="true"># kill -l</span>   1<span class="token punctuation">)</span> SIGHUP       2<span class="token punctuation">)</span> SIGINT       3<span class="token punctuation">)</span> SIGQUIT      4<span class="token punctuation">)</span> SIGILL       5<span class="token punctuation">)</span> SIGTRAP   6<span class="token punctuation">)</span> SIGABRT      7<span class="token punctuation">)</span> SIGBUS       8<span class="token punctuation">)</span> SIGFPE       9<span class="token punctuation">)</span> SIGKILL     10<span class="token punctuation">)</span> SIGUSR1  11<span class="token punctuation">)</span> SIGSEGV     12<span class="token punctuation">)</span> SIGUSR2     13<span class="token punctuation">)</span> SIGPIPE     14<span class="token punctuation">)</span> SIGALRM     15<span class="token punctuation">)</span> SIGTERM  16<span class="token punctuation">)</span> SIGSTKFLT   17<span class="token punctuation">)</span> SIGCHLD     18<span class="token punctuation">)</span> SIGCONT     19<span class="token punctuation">)</span> SIGSTOP     20<span class="token punctuation">)</span> SIGTSTP  21<span class="token punctuation">)</span> SIGTTIN     22<span class="token punctuation">)</span> SIGTTOU     23<span class="token punctuation">)</span> SIGURG      24<span class="token punctuation">)</span> SIGXCPU     25<span class="token punctuation">)</span> SIGXFSZ  26<span class="token punctuation">)</span> SIGVTALRM   27<span class="token punctuation">)</span> SIGPROF     28<span class="token punctuation">)</span> SIGWINCH    29<span class="token punctuation">)</span> SIGIO       30<span class="token punctuation">)</span> SIGPWR  31<span class="token punctuation">)</span> SIGSYS      34<span class="token punctuation">)</span> SIGRTMIN    35<span class="token punctuation">)</span> SIGRTMIN+1  36<span class="token punctuation">)</span> SIGRTMIN+2  37<span class="token punctuation">)</span> SIGRTMIN+3  38<span class="token punctuation">)</span> SIGRTMIN+4  39<span class="token punctuation">)</span> SIGRTMIN+5  40<span class="token punctuation">)</span> SIGRTMIN+6  41<span class="token punctuation">)</span> SIGRTMIN+7  42<span class="token punctuation">)</span> SIGRTMIN+8  43<span class="token punctuation">)</span> SIGRTMIN+9  44<span class="token punctuation">)</span> SIGRTMIN+10 45<span class="token punctuation">)</span> SIGRTMIN+11 46<span class="token punctuation">)</span> SIGRTMIN+12 47<span class="token punctuation">)</span> SIGRTMIN+13  48<span class="token punctuation">)</span> SIGRTMIN+14 49<span class="token punctuation">)</span> SIGRTMIN+15 50<span class="token punctuation">)</span> SIGRTMAX-14 51<span class="token punctuation">)</span> SIGRTMAX-13 52<span class="token punctuation">)</span> SIGRTMAX-12  53<span class="token punctuation">)</span> SIGRTMAX-11 54<span class="token punctuation">)</span> SIGRTMAX-10 55<span class="token punctuation">)</span> SIGRTMAX-9  56<span class="token punctuation">)</span> SIGRTMAX-8  57<span class="token punctuation">)</span> SIGRTMAX-7  58<span class="token punctuation">)</span> SIGRTMAX-6  59<span class="token punctuation">)</span> SIGRTMAX-5  60<span class="token punctuation">)</span> SIGRTMAX-4  61<span class="token punctuation">)</span> SIGRTMAX-3  62<span class="token punctuation">)</span> SIGRTMAX-2  63<span class="token punctuation">)</span> SIGRTMAX-1  64<span class="token punctuation">)</span> SIGRTMAX</code></pre>  不同的信号有不同的含义，其中10号的<code>SIGUSR1</code>和12号的<code>SIGUSR2</code>是两个可以用户自定义的中断信号。       </li><li><strong>共享空间</strong>，类似进程内部的全局变量，由操作系统负责维护，跟信号量一样各个共享空间拥有自己的标识符，不同进程都可以访问相同的共享空间，但是要注意防止访问冲突（比如一个进程对空间写操作，同时又有另一个进程对空间进程读或写操作），这通常是通过信号量来实现的。并且在共享的过程要注意避免进程死锁（各个进程各自占有一部分但并不充足资源，导致进程同时陷入无休止的阻塞状态），有一些专门用于检测死锁和防止死锁的算法，此处不展开讨论。       </li></ul><p>具体的实现不展开讲，因为我们接下来要用到的是线程而不是进程。<br>如果你对进程间通信感兴趣，也可以参考我本科期间的一个课程作业，一个 <a href="/others/chat.zip">简单的本地聊天程序</a> （具体使用方法参见<code>chat.c</code>里的注释）。      </p><h4 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h4><p>在单一程序中，进程的粒度似乎还是太大了，如果把一个程序的大任务细分多个小任务，明明大家都是同一个目标，却要使用独立的上下文，上下文频繁地保存和加载，这样似乎不太方便。于是就产生了粒度更小的<strong>线程</strong>，一个进程可以拥有多个线程，这些线程共享一个上下文环境。<br>相比于进程，       </p><ol><li>创建一个线程的速度要快得多；       </li><li>终止一个线程的速度也要快得多；     </li><li>线程间的切换也比进程间切换快，因为不需要交换上下文；     </li><li>线程的通信效率更高，因为线程可以直接通过共享全局变量来实现通信。            </li></ol><p>线程的通信方式比进程简单，这跟进程的“信号量+共享空间”的组合有些类似，      </p><ul><li><strong>线程锁</strong>，类似进程通信中的信号量，但这个变量只有两种状态——“上锁”和“解锁”，用于保护共享的内存空间不会出现读写冲突；     </li><li><strong>全局变量</strong>，类似进程通信中的共享空间。         </li></ul><p>简单的实现方式——      </p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span></span>pthread_mutex_t mutex<span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">// 线程锁（用于保护shared_variable变量）</span><span class="token keyword">int</span> shared_variable <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 线程间共享的全局变量</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">thread_write</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 写入前上锁（如果mutex锁住，则阻塞等待）</span>    shared_variable<span class="token operator">++</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 写入后解锁</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">thread_read</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 读取前上锁（如果mutex锁住，则阻塞等待）</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> shared_variable<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 读取后解锁</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    pthread_t id1<span class="token punctuation">,</span> id2<span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 初始化线程锁</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> thread_write<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 创建写线程</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> thread_read<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 创建读线程</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 阻塞主线程，等待线程id1执行完毕</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 阻塞主线程，等待线程id2执行完毕</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 销毁线程锁</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p><code>pthread_create</code>、<code>pthread_join</code>、<code>pthread_mutex_init</code>还可以传递其他参数，其他复杂的用法可以自行查阅资料。     </p><h4 id="处理器调度"><a href="#处理器调度" class="headerlink" title="处理器调度"></a>处理器调度</h4><p>在背景中我们提到了一种规定时间的调度方法，接下来我们也简单介绍其他一些处理器的调度策略。<br>假设有A-E五个进程集合，他们的启动时间和CPU占用总时间如下表所示——       </p><table><thead><tr><th style="text-align:center">进程</th><th style="text-align:center">启动时间</th><th style="text-align:center">CPU占用总时间</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">t=0</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">t=2</td><td style="text-align:center">6</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">t=4</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">t=6</td><td style="text-align:center">5</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">t=8</td><td style="text-align:center">2</td></tr></tbody></table><p><strong>先来先服务策略（First Come First Served, FCFS）</strong><br>非抢占策略，进程依次进入等待队列，先进入队列的先使用CPU，直到进程结束再从队列取出下一个进程。<br><img src="/imgs/mssd-try/try3/process-FCFS.png" alt="process">       </p><p><strong>轮转策略（Round Robin, RR）</strong><br>一种基于时钟的抢占策略，又称为时间片策略，设定一个时间片q，每次从进程等待队列中取出一个进程执行一个时间片，如果没执行完就放回等待队列的队尾，然后从队首取出下一个进程出来执行一个时间片。假设q=1，则<br><img src="/imgs/mssd-try/try3/process-RR.png" alt="process"><br><em>注意：这里假设如果同时发生“中断”和“新服务入队”，则先将新服务入队，再交换进程。</em>             </p><p><strong>最短进程优先（Shortest Process Next, SPN）</strong><br>一种基于预计处理时间的非抢占策略，每次从等待队列中取出预计处理时间最短的进程出来执行，直到进程结束再从进程取出下一个预计处理时间最短的进程。<br><img src="/imgs/mssd-try/try3/process-SPN.png" alt="process">       </p><p><strong>最短剩余时间（Shortest Remaining Time, SRT）</strong><br>一种基于预计剩余处理时间的抢占策略，在SPN的基础上，当有新进程加入队列时总会估算各个进程的剩余时间，然后选择预计剩余处理时间最短的进程出来执行。<br><img src="/imgs/mssd-try/try3/process-SRT.png" alt="process">       </p><p><strong>最高响应比优先（Highest Response Ratio Next, HRRN）</strong><br>非抢占策略，定义一个响应比参数，$响应比=\frac{等待时间+预计处理时间}{预计处理时间}$，每次从等待队列中挑选响应比最高的进程出来执行，直到进程执行结束再从队列中取出下一个响应比最高的进程。<br><img src="/imgs/mssd-try/try3/process-HRRN.png" alt="process">       </p><p><strong>反馈调度</strong><br>可以注意到FCFS、RR策略相对简单，不太能很好的利用CPU，而SPN、SRT、HRRN策略虽然不错，但依赖于处理时间和剩余处理时间的估计，而现实应用中这种时间估计往往是难以实现的。因此产生了反馈调度策略，对进程进行分级（而不是简单的一个等待队列），进程运行时间长调度的优先级越低，每被抢占一次就下降一级。<br><img src="/imgs/mssd-try/try3/process-feedback.png" alt="process"><br>反馈调度往往会和前述的简单调度（比如FCFS或RR）结合使用，同时长进程周转时间会出现惊人的增加的现象（长进程多次被抢占，优先级不断下降，而长期得不到调度），所以会有一些相应的补偿措施（比如设置允许被抢占的次数，每当超过这个次数才会对进程进行降级操作）。           </p><p><strong>实时调度</strong><br>嵌入式开发中还常见一些实时调度策略，与前述策略不同的时间，这些任务有最后期限的限制，这通常分为两种——一种是“硬实时”，要求必须满足最后期限的限制，否则将给系统带来不可接受的破坏或致命的错误（任务超时完成是无意义的）；另一种是“软实时”，希望能够满足最后期限的限制，但并非强制（即使超时完成任务也有意义）。    </p><h3 id="拆解目标检测demo"><a href="#拆解目标检测demo" class="headerlink" title="拆解目标检测demo"></a>拆解目标检测demo</h3><p>前边介绍了进程、线程以及处理器调度的概念和简单的使用，接下来我们考虑如何将我们的目标检测demo拆解成两个线程以提高展示的流畅性。       </p><p>我们的目标是将demo拆解成 <strong>目标检测</strong> 和 <strong>视频流获取和展示</strong> 两个线程，其中需要共享的数据包括 <strong>图像数据</strong> 和 <strong>检测结果</strong> 两部分（为了响应退出按钮，后续的示例程序还会额外增加一个作为退出标志的共享数据），每部分数据需要配备一把线程锁进行读写保护。       </p><p>最终程序如下——<br><strong><em>注意：这里不仅划分了线程，还针对不同线程的任务分配了cpu，比如RK3399上CPU0-3是四个小核，我们用来做视频流的获取、检测结果的标注和视频流的展示；CPU4-5是大核，我们用来做核心的目标检测任务。两个不同的线程使用不同的CPU核，互不冲突，合理地分配CPU对应用程序也会有一定的提升。</em></strong>     </p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/imgproc/imgproc.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/highgui/highgui.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"tengine_c_api.h"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"common.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span>    </span><span class="token comment" spellcheck="true">// 包含线程控制相关的库</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_PROTO "models/MobileNetSSD_deploy.prototxt"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_MODEL "models/MobileNetSSD_deploy.caffemodel"</span><span class="token keyword">struct</span> Box<span class="token punctuation">{</span>    <span class="token keyword">float</span> x0<span class="token punctuation">;</span>    <span class="token keyword">float</span> y0<span class="token punctuation">;</span>    <span class="token keyword">float</span> x1<span class="token punctuation">;</span>    <span class="token keyword">float</span> y1<span class="token punctuation">;</span>    <span class="token keyword">int</span> class_idx<span class="token punctuation">;</span>    <span class="token keyword">float</span> score<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> input_data<span class="token punctuation">,</span> <span class="token keyword">int</span> img_h<span class="token punctuation">,</span>  <span class="token keyword">int</span> img_w<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        std<span class="token operator">::</span>cerr <span class="token operator">&lt;&lt;</span> <span class="token string">"Failed to read image from camera.\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span><span class="token function">resize</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    img<span class="token punctuation">.</span><span class="token function">convertTo</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> CV_32FC3<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>img_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>img<span class="token punctuation">.</span>data<span class="token punctuation">;</span>    <span class="token keyword">int</span> hw <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w<span class="token punctuation">;</span>    <span class="token keyword">float</span> mean<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> img_h<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> w <span class="token operator">&lt;</span> img_w<span class="token punctuation">;</span> w<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                input_data<span class="token punctuation">[</span>c <span class="token operator">*</span> hw <span class="token operator">+</span> h <span class="token operator">*</span> img_w <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.007843</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">*</span>img_data <span class="token operator">-</span> mean<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                img_data<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span> threshold<span class="token punctuation">,</span><span class="token keyword">float</span><span class="token operator">*</span> outdata<span class="token punctuation">,</span><span class="token keyword">int</span> num<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> class_names<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"background"</span><span class="token punctuation">,</span>                            <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span>                            <span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span>                            <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span>                            <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span>                            <span class="token string">"sheep"</span><span class="token punctuation">,</span> <span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_h <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>height<span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_w <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>width<span class="token punctuation">;</span>    std<span class="token operator">::</span>vector<span class="token operator">&lt;</span>Box<span class="token operator">></span> boxes<span class="token punctuation">;</span>    <span class="token keyword">int</span> line_width<span class="token operator">=</span>raw_w<span class="token operator">*</span><span class="token number">0.002</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// printf("detect ruesult num: %d \n",num);</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>=</span>threshold<span class="token punctuation">)</span><span class="token punctuation">{</span>            Box box<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>class_idx<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>score<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            boxes<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>box<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// printf("%s\t:%.0f%%\n", class_names[box.class_idx], box.score * 100);</span>            <span class="token comment" spellcheck="true">// printf("BOX:( %g , %g ),( %g , %g )\n",box.x0,box.y0,box.x1,box.y1);</span>        <span class="token punctuation">}</span>        outdata<span class="token operator">+</span><span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span>boxes<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        Box box<span class="token operator">=</span>boxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x1<span class="token operator">-</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>y1<span class="token operator">-</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>line_width<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>ostringstream score_str<span class="token punctuation">;</span>        score_str<span class="token operator">&lt;&lt;</span>box<span class="token punctuation">.</span>score<span class="token punctuation">;</span>        std<span class="token operator">::</span>string label <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">string</span><span class="token punctuation">(</span>class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> score_str<span class="token punctuation">.</span><span class="token function">str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> baseLine <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span>Size label_size <span class="token operator">=</span> cv<span class="token operator">::</span><span class="token function">getTextSize</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>baseLine<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token operator">-</span> label_size<span class="token punctuation">.</span>height<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>label_size<span class="token punctuation">.</span>width<span class="token punctuation">,</span> label_size<span class="token punctuation">.</span>height <span class="token operator">+</span> baseLine<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CV_FILLED<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">putText</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> label<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">,</span>                    cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">float</span> outdata<span class="token punctuation">[</span><span class="token number">15</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 线程间共享变量——检测结果</span>cv<span class="token operator">::</span>Mat frame<span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 线程间共享变量——图像数据</span><span class="token keyword">int</span> detect_num<span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 线程间共享变量——检测结果</span><span class="token keyword">bool</span> quit_flag <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 程序间共享变量——退出标志</span>graph_t graph<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 与共享变量对应的线程锁</span>pthread_mutex_t m_frame<span class="token punctuation">,</span> m_outdata<span class="token punctuation">,</span> m_quit<span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">th_vedio</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 将线程绑定到cpu0-3上</span>    cpu_set_t mask<span class="token punctuation">;</span>    <span class="token function">CPU_ZERO</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">sched_setaffinity</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cpu_set_t<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error: setaffinity()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span>VideoCapture <span class="token function">capture</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_WIDTH<span class="token punctuation">,</span> <span class="token number">960</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_HEIGHT<span class="token punctuation">,</span> <span class="token number">540</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span><span class="token function">namedWindow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> CV_WINDOW_NORMAL<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">cvResizeWindow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> <span class="token number">1280</span><span class="token punctuation">,</span> <span class="token number">720</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        capture <span class="token operator">>></span> frame<span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">float</span> show_threshold<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 上锁</span>        <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> show_threshold<span class="token punctuation">,</span> outdata<span class="token punctuation">,</span> detect_num<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 解锁</span>        cv<span class="token operator">::</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 解锁</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> cv<span class="token operator">::</span><span class="token function">waitKey</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'q'</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 上锁</span>            quit_flag <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>            <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 解锁</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token function">usleep</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 注意必须sleep（不然太过频繁地取帧会影响检测线程的调度）</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">th_detect</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 将该线程绑定到cpu4-5上</span>    cpu_set_t mask<span class="token punctuation">;</span>    <span class="token function">CPU_ZERO</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">sched_setaffinity</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cpu_set_t<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error: setaffinity()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// input</span>    <span class="token keyword">int</span> img_h <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_w <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_size <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>input_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> img_size<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> node_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> tensor_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    tensor_t input_tensor <span class="token operator">=</span> <span class="token function">get_graph_input_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> node_idx<span class="token punctuation">,</span> tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_tensor_valid</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Get input node failed : node_idx: %d, tensor_idx: %d\n"</span><span class="token punctuation">,</span>node_idx<span class="token punctuation">,</span>tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> dims<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">set_tensor_shape</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dims<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">prerun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> repeat_count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>repeat <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">"REPEAT_COUNT"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>repeat<span class="token punctuation">)</span>        repeat_count <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">strtoul</span><span class="token punctuation">(</span>repeat<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> out_dim<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    tensor_t out_tensor<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 上锁</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>quit_flag<span class="token punctuation">)</span>  <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 解锁</span>        <span class="token keyword">struct</span> timeval t0<span class="token punctuation">,</span> t1<span class="token punctuation">;</span>        <span class="token keyword">float</span> total_time <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>f<span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> repeat_count<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token comment" spellcheck="true">// 上锁</span>            <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span>  img_w<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 解锁</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t0<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">set_tensor_buffer</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">run_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">float</span> mytime <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t1<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span>t0<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t0<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span><span class="token punctuation">;</span>            total_time <span class="token operator">+</span><span class="token operator">=</span> mytime<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"--------------------------------------\n"</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"repeat "</span> <span class="token operator">&lt;&lt;</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" times, avg time per run is "</span> <span class="token operator">&lt;&lt;</span> total_time <span class="token operator">/</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" ms\n"</span><span class="token punctuation">;</span>        out_tensor <span class="token operator">=</span> <span class="token function">get_graph_output_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">get_tensor_shape</span><span class="token punctuation">(</span> out_tensor<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 上锁</span>        detect_num <span class="token operator">=</span> out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">15</span> <span class="token operator">?</span> out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">:</span> <span class="token number">15</span><span class="token punctuation">;</span>        <span class="token function">memcpy</span><span class="token punctuation">(</span>outdata<span class="token punctuation">,</span> <span class="token function">get_tensor_buffer</span><span class="token punctuation">(</span>out_tensor<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>detect_num<span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 解锁</span>    <span class="token punctuation">}</span>    <span class="token function">free</span><span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> std<span class="token operator">::</span>string root_path <span class="token operator">=</span> <span class="token function">get_root_path</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>string proto_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string model_file<span class="token punctuation">;</span>    <span class="token keyword">int</span> res<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token punctuation">(</span> res<span class="token operator">=</span><span class="token function">getopt</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">,</span><span class="token string">"p:m:h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">switch</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token string">'p'</span><span class="token operator">:</span>                proto_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'m'</span><span class="token operator">:</span>                model_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'h'</span><span class="token operator">:</span>                std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"[Usage]: "</span> <span class="token operator">&lt;&lt;</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" [-h]\n"</span>                          <span class="token operator">&lt;&lt;</span> <span class="token string">"   [-p proto_file] [-m model_file]\n"</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token keyword">default</span><span class="token operator">:</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>model_name <span class="token operator">=</span> <span class="token string">"mssd_300"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>proto_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        proto_file <span class="token operator">=</span> DEF_PROTO<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"proto file not specified,using "</span><span class="token operator">&lt;&lt;</span> proto_file <span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>model_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        model_file <span class="token operator">=</span> DEF_MODEL<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"model file not specified,using "</span><span class="token operator">&lt;&lt;</span> model_file <span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// init tengine</span>    <span class="token function">init_tengine_library</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">request_tengine_version</span><span class="token punctuation">(</span><span class="token string">"0.1"</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">load_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> <span class="token string">"caffe"</span><span class="token punctuation">,</span> proto_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"load model done!\n"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// create graph</span>    graph <span class="token operator">=</span> <span class="token function">create_runtime_graph</span><span class="token punctuation">(</span><span class="token string">"graph"</span><span class="token punctuation">,</span> model_name<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_graph_valid</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"create graph0 failed\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 初始化线程锁</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 创建线程</span>    pthread_t id1<span class="token punctuation">,</span> id2<span class="token punctuation">;</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> th_vedio<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> th_detect<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 等待线程</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 销毁线程锁</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">postrun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">destroy_runtime_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">remove_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><hr><p>在本文中，我们将尝试把视频的获取与展示和检测任务分离开来，分别放在两个不同的线程上工作，同时将不同的线程绑定到不同的cpu核上，使得两者的工作不会冲突。但在实际使用中你会发现，模型对于小目标的检测能力还是有所欠缺，下一篇文章我们将探究如何改善检测模型的小目标检测能力。</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>基于MobileNet-SSD的目标检测Demo（一）</title>
      <link href="/2018/08/24/mssd-try2/"/>
      <url>/2018/08/24/mssd-try2/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=536622304&auto=0&height=32"></iframe>    <p>上一篇文章《<a href="/2018/08/21/mssd-try1/">训练MobileNet-SSD | Hey~YaHei!</a>》介绍了如何训练自己的MobileNet-SSD模型并部署在Tengine平台上。<br>本文将继续尝试根据实际情况删减多余类别进行训练，并用Depthwise Convolution进一步替换Standard Convolution。         </p><hr><h3 id="削减类别"><a href="#削减类别" class="headerlink" title="削减类别"></a>削减类别</h3><p>VOC数据集包含二十个类别的物体，分别是——aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, foa, train, tvmonitor，有时候我们想用VOC数据集训练，但并不需要这么多类别，而caffe-ssd提供的数据处理工具<code>create_list.sh</code>和<code>create_data.sh</code>默认是处理所有的20个分类的。如果我们不想重写这些数据处理工具，可以从根源入手，也就是直接修改数据集里的标注信息，把多余分类的信息删去。          </p><h4 id="处理数据集"><a href="#处理数据集" class="headerlink" title="处理数据集"></a>处理数据集</h4><p>首先观察一下VOC数据集的结构——<br><img src="/imgs/mssd-try/try1/dataset_tree.png" width="350">       </p><ul><li>Annotations：存放图片的标注信息，每张图片对应一个xml文件      </li><li>ImageSets：存放图片的分类列表，包含三个子目录：       <ul><li>Layout：存放与人体部位有关的图片列表文件</li><li>Main：存放物体分类中每一个分类的图片列表文件</li><li>Segmentation：存放与图像分别有关的图片列表文件</li></ul></li><li>JPEGImages：存放所有的图片</li><li><del><em>NewAnnotations：忽略吧……是我自己生成的目录</em></del></li><li>SegmentationClass：存放类别分割任务的蒙版文件</li><li>SegmentationObject：存放实体分割任务的蒙版文件</li></ul><p>JPEGImages目录下每张图片都包含一到多个物体，这些物体的位置、类别信息都记录再Annotations目录下的同名xml文件中，文件内容类似：     </p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>folder</span><span class="token punctuation">></span></span>VOC2007<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>folder</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filename</span><span class="token punctuation">></span></span>008973.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filename</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>database</span><span class="token punctuation">></span></span>The VOC2007 Database<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>database</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>PASCAL VOC2007<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>image</span><span class="token punctuation">></span></span>flickr<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>image</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flickrid</span><span class="token punctuation">></span></span>335707085<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flickrid</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>owner</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flickrid</span><span class="token punctuation">></span></span>kjmurray<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flickrid</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>Katherine Murray<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>owner</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>size</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>width</span><span class="token punctuation">></span></span>500<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>width</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>height</span><span class="token punctuation">></span></span>333<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>height</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>depth</span><span class="token punctuation">></span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>depth</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>size</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>segmented</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>segmented</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>cow<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">></span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">></span></span>271<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">></span></span>43<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">></span></span>444<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">></span></span>279<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span></code></pre><p>而caffe-ssd的数据处理工具正是根据这些xml文件提供的标记进行处理的，所以说，<strong>我们可以通过遍历xml文件，判断object的类别，如果是我们不需要的，则把对应的object标签删去来达到削减类别的目的</strong>，除此之外还要处理对应的图片路径列表和图片大小列表，删除多余的项。     </p><p>根据这一思路，可以写一个简单的py脚本来实现：       </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true">#-*- coidng: utf-8 -*-</span><span class="token keyword">import</span> xml<span class="token punctuation">.</span>etree<span class="token punctuation">.</span>ElementTree <span class="token keyword">as</span> ET<span class="token keyword">import</span> osVOC_ROOT <span class="token operator">=</span> <span class="token string">"/home/zhengkai/data/VOCdevkit/"</span>CLASS2KEEP <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'background'</span><span class="token punctuation">,</span>            <span class="token string">'aeroplane'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span>            <span class="token string">'bottle'</span><span class="token punctuation">,</span> <span class="token string">'bus'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'chair'</span><span class="token punctuation">,</span>            <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'bicycle'</span><span class="token punctuation">,</span> <span class="token string">'motorbike'</span><span class="token punctuation">,</span>            <span class="token string">'boat'</span><span class="token punctuation">,</span> <span class="token string">'sofa'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">process_xml</span><span class="token punctuation">(</span>src_path<span class="token punctuation">,</span> dst_path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    解析并处理xml文件——    解析源文件，删除无关object标签，    如果存在有效object，则写入到目标文件，并返回True；    否则，直接返回False。    """</span>    tree <span class="token operator">=</span> ET<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>src_path<span class="token punctuation">)</span>    root <span class="token operator">=</span> tree<span class="token punctuation">.</span>getroot<span class="token punctuation">(</span><span class="token punctuation">)</span>    no_objs <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">for</span> obj <span class="token keyword">in</span> root<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">"object"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        cls <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        <span class="token keyword">if</span> cls <span class="token operator">not</span> <span class="token keyword">in</span> CLASS2KEEP<span class="token punctuation">:</span>            root<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>obj<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            no_objs <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token keyword">if</span> <span class="token operator">not</span> no_objs<span class="token punctuation">:</span>        tree<span class="token punctuation">.</span>write<span class="token punctuation">(</span>dst_path<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 记录有效的xml文件名</span>    valid_lst <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 处理xml文件，新的xml文件写入到NewAnnotations目录下</span>    <span class="token keyword">for</span> dataset <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"VOC2007/"</span><span class="token punctuation">,</span> <span class="token string">"VOC2012/"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        raw_anno_dir <span class="token operator">=</span> VOC_ROOT <span class="token operator">+</span> dataset <span class="token operator">+</span> <span class="token string">"Annotations/"</span>        dst_anno_dir <span class="token operator">=</span> VOC_ROOT <span class="token operator">+</span> dataset <span class="token operator">+</span> <span class="token string">"NewAnnotations/"</span>        <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>dst_anno_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Create a new dir: "</span> <span class="token operator">+</span> dst_anno_dir<span class="token punctuation">)</span>            os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>dst_anno_dir<span class="token punctuation">)</span>        <span class="token keyword">for</span> xml_filename <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>raw_anno_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> process_xml<span class="token punctuation">(</span>raw_anno_dir <span class="token operator">+</span> xml_filename<span class="token punctuation">,</span> dst_anno_dir <span class="token operator">+</span> xml_filename<span class="token punctuation">)</span><span class="token punctuation">:</span>                valid_lst<span class="token punctuation">.</span>append<span class="token punctuation">(</span>xml_filename<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 处理图片路径列表txt文件，根据valid_lst筛选有效的图片路径</span>    <span class="token keyword">for</span> filename <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"test.txt"</span><span class="token punctuation">,</span> <span class="token string">"trainval.txt"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"new_"</span> <span class="token operator">+</span> filename<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> nf<span class="token punctuation">:</span>            <span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> of<span class="token punctuation">:</span>                <span class="token keyword">for</span> line <span class="token keyword">in</span> of<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> valid_lst<span class="token punctuation">:</span>                        nf<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"Annotations"</span><span class="token punctuation">,</span> <span class="token string">"NewAnnotations"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 处理图片大小列表txt文件，根据valid_lst筛选有效的图片大小</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"new_test_name_size.txt"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> nf<span class="token punctuation">:</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"test_name_size.txt"</span><span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> of<span class="token punctuation">:</span>            <span class="token keyword">for</span> line <span class="token keyword">in</span> of<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> valid_lst<span class="token punctuation">:</span>                    nf<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"Annotations"</span><span class="token punctuation">,</span> <span class="token string">"NewAnnotations"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="生成lmdb文件"><a href="#生成lmdb文件" class="headerlink" title="生成lmdb文件"></a>生成lmdb文件</h4><p>修改caffe-ssd数据处理工具中的标签映射文件<code>labelmap_voc.prototxt</code>，该文件由若干个类似下边的<code>item</code>组成：      </p><pre><code>item {    name: &quot;none_of_the_above&quot;    label: 0    display_name: &quot;background&quot;}</code></pre><ul><li>name：物体类别在xml文件中出现的名称</li><li>label：标签对应的数值（为方便处理，建议序号从0递增）</li><li>display_name：该类别最后要展示出来的名称</li></ul><p>删除映射文件中多余类别对应的<code>item</code>，然后按顺序重新为各个类别编号（修改label项）；     </p><p>修改<code>create_list.sh</code>脚本，将<strong>第29行</strong>的<code>Annotations</code>改为<code>NewAnnotations</code>——       </p><pre class=" language-bash"><code class="language-bash"><span class="token punctuation">[</span>27<span class="token punctuation">]</span> label_file<span class="token operator">=</span><span class="token variable">$bash_dir</span>/<span class="token variable">$dataset</span><span class="token string">"_label.txt"</span><span class="token punctuation">[</span>28<span class="token punctuation">]</span> <span class="token function">cp</span> <span class="token variable">$dataset_file</span> <span class="token variable">$label_file</span><span class="token punctuation">[</span>29<span class="token punctuation">]</span> <span class="token function">sed</span> -i <span class="token string">"s/^/<span class="token variable">$name</span>\/NewAnnotations\//g"</span> <span class="token variable">$label_file</span><span class="token punctuation">[</span>30<span class="token punctuation">]</span> <span class="token function">sed</span> -i <span class="token string">"s/$/.xml/g"</span> <span class="token variable">$label_file</span></code></pre><p>然后跟上一篇文章一样，依次执行脚本<code>create_list.sh</code>和<code>create_data.sh</code>即可。     </p><h4 id="训练和部署"><a href="#训练和部署" class="headerlink" title="训练和部署"></a>训练和部署</h4><p>训练和部署过程与 《<a href="/2018/08/21/mssd-try1/#训练">训练MobileNet-SSD/开始训练MobileNet-SSD/训练(部署) | Hey~YaHei!</a>》 基本相同；<br>微小的区别在于，     </p><ol><li>生成模型文件时<br> <code>./gen_model.sh 21</code>中<code>21</code>要换成实际的类别数量（含背景background）；      </li><li>要使用新的标签映射文件<code>labelmap.prototxt</code>；</li><li>应用程序中标签要对应修改<br> 如《<a href="/2018/08/04/RK3399-Tengine/#目标检测网络MobileNet-SSD">RK3399上Tengine平台搭建/目标检测网络MobileNet-SSD | Hey~YaHei!</a>》最后列出的代码中，<code>post_process_ssd</code>函数里的<code>class_names</code>数组常量要对应修改（索引号与<code>labelmap.prototxt</code>文件里的<code>label</code>标签一一对应）。         </li></ol><h3 id="Depthwise-Convolution和Standard-Convolution-Group-的比较"><a href="#Depthwise-Convolution和Standard-Convolution-Group-的比较" class="headerlink" title="Depthwise Convolution和Standard Convolution(Group)的比较"></a>Depthwise Convolution和Standard Convolution(Group)的比较</h3><p>观察chuanqi305的 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/deploy.prototxt" target="_blank" rel="noopener">MobileNet-SSD模型文件deploy.prototxt</a> 可以发现，其中的Depthwise Convolution都是使用特殊的caffe原生卷积层（group参数与num_output参数相等）来实现的。     </p><p>查阅caffe官方文档，</p><blockquote><p>group (g) [default 1]: If g &gt; 1, we restrict the connectivity of each filter to a subset of the input. Specifically, the input and output channels are separated into g groups, and the ith output group channels will be only connected to the ith input group channels.</p></blockquote><p>可以知道，group参数强制输入通道和输出通道分为若干组，一组输入通道卷积运算得到组序相同的输出通道，所以使 <code>group == num_output</code> 确实可以得到与Depthwise Convolution相同的结果。但是，“分组”？从字面的意思上看，不免让人怀疑，这个group是不是通过简单的循环实现的，如果真是如此，不同的group还会并行的计算吗？我们不妨做个简单的实验——<br>我在github上找到第三方实现的 <a href="https://github.com/yonghenglh6/DepthwiseConvolution" target="_blank" rel="noopener">caffe - Depthwise Convolution层</a>，根据其README的说明将其放到caffe源码下重新编译caffe-ssd得到专门实现的<code>DepthwiseConvolution</code>。<br>在单卡GTX1080Ti、Intel E5-2683环境下测试结果如下（分别重复运行100次，单位：秒/百次）：     </p><table><thead><tr><th style="text-align:center">caffe-mode</th><th style="text-align:center">Standard Convolution(Group)</th><th style="text-align:center">Depthwise Convolution</th></tr></thead><tbody><tr><td style="text-align:center">cpu-only</td><td style="text-align:center">26.13568</td><td style="text-align:center">23.40233</td></tr><tr><td style="text-align:center">gpu</td><td style="text-align:center">6.93938</td><td style="text-align:center">0.53499</td></tr><tr><td style="text-align:center">gpu-cudnn</td><td style="text-align:center">6.86799</td><td style="text-align:center">0.53779</td></tr></tbody></table><p>很显然，在cpu-only模式下，两者没有太大区别；在gpu模式下（无论是caffe自身的加速库还是cudnn加速库），专门实现的DepthwiseConvolution都要<strong>快10倍左右</strong>！<br>除此之外，<a href="https://github.com/yonghenglh6/DepthwiseConvolution" target="_blank" rel="noopener">Depthwise Convolution Layer | github</a>的README也给出了一些测试数据。      </p><p>那Tengine有专门实现的DepthwiseConvolution层吗？<br>从<a href="https://github.com/OAID/Tengine/blob/master/doc/operator_ir.md" target="_blank" rel="noopener">官方的文档</a>上看，确实没有专门的DepthwiseConvolution层，如果试着在模型文件里使用<code>DepthwiseConvolution</code>也会看到报错。不过！在源码 <a href="https://github.com/OAID/Tengine/blob/master/executor/operator/arm64/conv/conv_2d_dw.cpp#L222" target="_blank" rel="noopener">executor/operator/arm64/conv/conv_2d_dw.cpp | github, Tengine</a>可以看到有一个 <code>isDepthwiseSupported</code> 函数——      </p><pre class=" language-cpp"><code class="language-cpp"><span class="token keyword">static</span> <span class="token keyword">bool</span> <span class="token function">isDepthwiseSupported</span><span class="token punctuation">(</span><span class="token keyword">const</span> ConvParam <span class="token operator">*</span> param<span class="token punctuation">,</span> <span class="token keyword">const</span> TShape<span class="token operator">&amp;</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> input_c<span class="token operator">=</span>input_shape<span class="token punctuation">.</span><span class="token function">GetC</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> group<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>group<span class="token punctuation">;</span>    <span class="token keyword">int</span> kernel_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>kernel_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> kernel_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>kernel_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> stride_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>stride_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> stride_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>stride_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> dilation_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>dilation_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> dilation_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>dilation_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_h0<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_w0<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_h1<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_w1<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>group <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">||</span> input_c <span class="token operator">!=</span> group <span class="token operator">||</span> kernel_h <span class="token operator">!=</span> <span class="token number">3</span> <span class="token operator">||</span> kernel_w <span class="token operator">!=</span> <span class="token number">3</span> <span class="token operator">||</span>       pad_h0 <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> pad_w0 <span class="token operator">!=</span><span class="token number">1</span> <span class="token operator">||</span> pad_h0 <span class="token operator">!=</span> pad_h1 <span class="token operator">||</span> pad_w0 <span class="token operator">!=</span> pad_w1 <span class="token operator">||</span>       dilation_h <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> dilation_w <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> stride_w <span class="token operator">!=</span> stride_h<span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>也就是说，<strong>Tengine会自行判断<code>Convolution</code>层是否属于<code>DepthwiseConvolution</code>并相应作出优化</strong>，对应的汇编实现为 <a href="https://github.com/OAID/Tengine/blob/master/executor/operator/arm64/conv/dw_k3s1p1.S" target="_blank" rel="noopener">executor/operator/arm64/conv/dw_k3s1p1.S | github, Tengine</a>。     </p><h3 id="进一步替换Depthwise-Convolution"><a href="#进一步替换Depthwise-Convolution" class="headerlink" title="进一步替换Depthwise Convolution"></a>进一步替换Depthwise Convolution</h3><p>从《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》一文中可以看到，chuanqi305在设计MobileNet-SSD时还是保守地在<code>Conv14_1</code>到<code>Conv17_2</code>使用Standard Convolution，我们不妨进一步把这一部分也替换为深度向分解的卷积，替换的方式也很简单，举个例子：<br>对于某个传统的Convolution层        </p><pre><code>layer {  name: &quot;conv14_2&quot;  type: &quot;Convolution&quot;  bottom: &quot;conv14_1&quot;  top: &quot;conv14_2&quot;  param {    lr_mult: 1.0    decay_mult: 1.0  }  param {    lr_mult: 2.0    decay_mult: 0.0  }  convolution_param {    num_output: 512    pad: 1    kernel_size: 3    stride: 2    weight_filler {      type: &quot;msra&quot;    }    bias_filler {      type: &quot;constant&quot;      value: 0.0    }  }}layer {  name: &quot;conv14_2/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2&quot;  top: &quot;conv14_2&quot;}</code></pre><p>修改为</p><pre><code>layer {  name: &quot;conv14_2_new/dw&quot;  type: &quot;DepthwiseConvolution&quot;  bottom: &quot;conv14_1_new&quot;  top: &quot;conv14_2_new/dw&quot;  param {    lr_mult: 0.1    decay_mult: 0.1  }  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 2    group: 256    engine: CAFFE    weight_filler {      type: &quot;msra&quot;    }  }}layer {  name: &quot;conv14_2_new/dw/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2_new/dw&quot;  top: &quot;conv14_2_new/dw&quot;}layer {  name: &quot;conv14_2_new&quot;  type: &quot;Convolution&quot;  bottom: &quot;conv14_2_new/dw&quot;  top: &quot;conv14_2_new&quot;  param {    lr_mult: 0.1    decay_mult: 0.1  }  convolution_param {    num_output: 512    bias_term: false    kernel_size: 1    weight_filler {      type: &quot;msra&quot;    }  }}layer {  name: &quot;conv14_2_new/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2_new&quot;  top: &quot;conv14_2_new&quot;}</code></pre><p>要注意几点，    </p><ol><li>修改后的层要给个新的名字，避免初始化权重的时候从预训练好的模型误导入权重；      </li><li>训练模型<code>train.prototxt</code>和测试模型<code>test.prototxt</code>可别忘了加上BN层；      </li><li>部署在Tengine上的时候要记得把type从<code>DepthwiseConvolution</code>替换为<code>Convolution</code>。            </li></ol><p>……其他层也做类似的修改即可。<br>替换前后比较——      </p><table><thead><tr><th style="text-align:center">VOC2007-test</th><th style="text-align:center">MobileNet-SSD</th><th style="text-align:center">MobileNet-SSDLite</th></tr></thead><tbody><tr><td style="text-align:center">mAP</td><td style="text-align:center">0.727</td><td style="text-align:center">0.718</td></tr><tr><td style="text-align:center">FPS(1080Ti)</td><td style="text-align:center">258</td><td style="text-align:center">278</td></tr><tr><td style="text-align:center">caffemodel</td><td style="text-align:center">23MB</td><td style="text-align:center">16MB</td></tr></tbody></table><hr><p>本文介绍了如何削减VOC数据集上多余类别进行训练，并且尝试用深度向分解的卷积层进一步替换传统的卷积层，同时比较了专门优化加速的DepthwiseConvolution和Convolution(Group)在效率上的差别。<br>下一篇文章《<a href="/2018/09/10/mssd-try3">基于MobileNet-SSD的目标检测Demo（二）</a>》将介绍如何把<strong>目标检测</strong>和<strong>视频解码与显示</strong>分别放到两个线程上，来提高目标检测demo的流畅性。        </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>训练MobileNet-SSD</title>
      <link href="/2018/08/21/mssd-try1/"/>
      <url>/2018/08/21/mssd-try1/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=26142483&auto=0&height=32"></iframe>     <p>《<a href="/2018/08/04/RK3399-Tengine">RK3399上Tengine平台搭建 | Hey~YaHei!</a>》一文介绍了RK3399和Tengine并且尝试跑通了MobilNet-SSD网络，而随后又分别用《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》、《<a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>》《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》三篇文章分别介绍了MobileNet v1、SSD和MobileNet-SSD。<br>接下来，本文将尝试训练自己的MobileNet-SSD并且部署在Tengine平台上。     </p><hr><h3 id="安装配置cuda、caffe"><a href="#安装配置cuda、caffe" class="headerlink" title="安装配置cuda、caffe"></a>安装配置cuda、caffe</h3><p>cuda的安装网上有非常多的教程，比如《<a href="https://www.cnblogs.com/iloveblog/p/7683349.html" target="_blank" rel="noopener">Ubuntu16.04+cuda9.0安装教程 | 贝多芬的悲桑, cnblogs</a>》和《<a href="https://blog.csdn.net/huang826336127/article/details/78754767" target="_blank" rel="noopener">安装cuda-8.0 | 代码小哥, csdn</a>》，过程也很简单，在官网下载你需要的版本对应的<code>.run</code>文件，直接运行按提示安装即可。          </p><p>caffe由于要使用SSD框架，所以要编译安装caffe的ssd分支——       </p><h4 id="下载caffe-ssd源码"><a href="#下载caffe-ssd源码" class="headerlink" title="下载caffe-ssd源码"></a>下载caffe-ssd源码</h4><ul><li>直接用git把仓库克隆到本地并切换到ssd分支       <pre class=" language-bash"><code class="language-bash">  <span class="token function">git</span> clone https://github.com/weiliu89/caffe.git  <span class="token function">cd</span> caffe  <span class="token function">git</span> checkout ssd</code></pre></li><li>github服务器在海外，网络不是很稳定，你可以试着挂vpn下载源码。<br>  如果你的linux没有配置vpn，但windows或mac有，那也可以直接在 <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">weiliu89/caffe at ssd | github</a> 上下载zip压缩包，再拷贝到linux用<code>unzip</code>指令解压；       </li><li>如果你没有vpn，也可以到我的网盘上下载：<a href="https://pan.baidu.com/s/1wR0iJcvTgT7c4vwJF1pVUQ#list/path=%2Fblog-share%2Fmobilenet_ssd" target="_blank" rel="noopener">blog-share/mobilenet_ssd/caffe-ssd.zip | 百度网盘</a></li></ul><h4 id="编译caffe-ssd"><a href="#编译caffe-ssd" class="headerlink" title="编译caffe-ssd"></a>编译caffe-ssd</h4><p>编译过程可以参照 <a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener">Caffe | Installation</a> 来进行；     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> CAFFE_ROOT<span class="token operator">=</span>/your/caffe/root/path<span class="token comment" spellcheck="true"># 进入caffe源码的根目录</span><span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span><span class="token comment" spellcheck="true"># 从模板拷贝一份编译的配置文件</span><span class="token function">cp</span> Makefile.config.example Makefile.config<span class="token comment" spellcheck="true"># 按需要修改编译配置</span><span class="token comment" spellcheck="true"># vim Makefile.config</span><span class="token comment" spellcheck="true"># 开始编译（参数j表示编译使用的线程数量，一般数值越大越快，取决于你cpu支持的线程数）</span><span class="token function">make</span> -j8<span class="token comment" spellcheck="true"># 修改环境变量</span><span class="token keyword">echo</span> <span class="token string">"export PYTHONPATH=<span class="token variable">$CAFFE_ROOT</span>/python:<span class="token variable">$PYTHONPATH</span>"</span> <span class="token operator">>></span> ~/.bashrc<span class="token function">source</span> ~/.bashrc<span class="token comment" spellcheck="true"># 编译python包</span><span class="token function">make</span> py<span class="token comment" spellcheck="true"># 编译测试程序</span><span class="token function">make</span> <span class="token function">test</span> -j8<span class="token comment" spellcheck="true"># 测试</span><span class="token function">make</span> runtest -j8</code></pre><p>关于配置文件，一般直接用默认配置就行，       </p><ul><li>如果你想用cudnn加速而不是caffe自己提供的加速库（caffe不建议用cudnn），给<strong>第5行</strong>解除注释。      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>4<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># cuDNN acceleration switch (uncomment to build with cuDNN).</span>  <span class="token punctuation">[</span>5<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># USE_CUDNN := 1</span></code></pre></li><li>如果你不想用gpu，给<strong>第8行</strong>解除注释，      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>7<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CPU-only switch (uncomment to build without GPU support).</span>  <span class="token punctuation">[</span>8<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CPU_ONLY := 1</span></code></pre></li><li>如果你的cuda没有安装在默认位置，你可能需要在<strong>第28行</strong>修改变量<code>CUDA_DIR</code>       <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>27<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CUDA directory contains bin/ and lib/ directories that we need.</span>  <span class="token punctuation">[</span>28<span class="token punctuation">]</span> CUDA_DIR :<span class="token operator">=</span> /usr/local/cuda  <span class="token punctuation">[</span>29<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># On Ubuntu 14.04, if cuda tools are installed via</span>  <span class="token punctuation">[</span>30<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span>  <span class="token punctuation">[</span>31<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CUDA_DIR := /usr</span></code></pre></li><li>如果你用的是anaconda，或者你要用python3接口，你可能需要修改      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>64<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># NOTE: this is required only if you will compile the python interface.</span>  <span class="token punctuation">[</span>65<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># We need to be able to find Python.h and numpy/arrayobject.h.</span>  <span class="token punctuation">[</span>66<span class="token punctuation">]</span> PYTHON_INCLUDE :<span class="token operator">=</span> /usr/include/python2.7 \  <span class="token punctuation">[</span>67<span class="token punctuation">]</span>         /usr/lib/python2.7/dist-packages/numpy/core/include  <span class="token punctuation">[</span>68<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Anaconda Python distribution is quite popular. Include path:</span>  <span class="token punctuation">[</span>69<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Verify anaconda location, sometimes it's in root.</span>  <span class="token punctuation">[</span>70<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># ANACONDA_HOME := $(HOME)/anaconda2</span>  <span class="token punctuation">[</span>71<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span>  <span class="token punctuation">[</span>72<span class="token punctuation">]</span>         <span class="token variable"><span class="token variable">$(</span>ANACONDA_HOME<span class="token variable">)</span></span>/include/python2.7 \  <span class="token punctuation">[</span>73<span class="token punctuation">]</span>         <span class="token variable"><span class="token variable">$(</span>ANACONDA_HOME<span class="token variable">)</span></span>/lib/python2.7/site-packages/numpy/core/include \  <span class="token punctuation">[</span>74<span class="token punctuation">]</span>   <span class="token punctuation">[</span>75<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Uncomment to use Python 3 (default is Python 2)</span>  <span class="token punctuation">[</span>76<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_LIBRARIES := boost_python3 python3.5m</span>  <span class="token punctuation">[</span>77<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_INCLUDE := /usr/include/python3.5m \</span>  <span class="token punctuation">[</span>78<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></code></pre></li></ul><p><strong><em>编译过程中如果出现 <code>google::protobuf</code> 或者 <code>google::protoc</code> 相关的报错，你可能需要到 <a href="https://github.com/google/protobuf/releases" target="_blank" rel="noopener">google/protobuf | github</a> 下载合适版本的protobuf到本地编译并且配置环境变量（可以用<code>protoc --version</code>指令查看当前使用的protobuf版本）</em></strong>    </p><h3 id="开始训练MobileNet-SSD"><a href="#开始训练MobileNet-SSD" class="headerlink" title="开始训练MobileNet-SSD"></a>开始训练MobileNet-SSD</h3><p>首先，先跑通默认的 <a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">MobileNet-SSD</a>——      </p><h4 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h4><p>MobileNet-SSD默认使用<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">Pascal VOC</a>的2007和2012数据集，<br>下载以下数据集，并解压到同一个目录下：     </p><ul><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar" target="_blank" rel="noopener">VOC2007 - training/validation data</a></li><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar" target="_blank" rel="noopener">VOC2007 - test data</a></li><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar" target="_blank" rel="noopener">VOC2012 - training/validation data</a></li><li><a href="http://host.robots.ox.ac.uk:8080/" target="_blank" rel="noopener">VOC2012 - test data</a>    </li></ul><p>解压后目录如下图所示：<br><img src="/imgs/mssd-try/try1/dataset_tree.png" width="350">       </p><p>然后用caffe-ssd提供的 <a href="https://github.com/weiliu89/caffe/tree/ssd/data/VOC0712" target="_blank" rel="noopener">VOC数据集处理工具</a> 对数据集进行处理——    </p><ol><li>按实际情况修改并执行脚本<code>$CAFFE_ROOT/data/VOC0712/create_list.sh</code><br> 将<strong>第3行</strong>的<code>root_dir</code>变量修改为你的VOC数据集目录，比如按照我的目录树，则设置为<code>$HOME/data/VOCdevkit</code>       <pre class=" language-bash"><code class="language-bash"> <span class="token punctuation">[</span>1<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#!/bin/bash</span> <span class="token punctuation">[</span>2<span class="token punctuation">]</span>  <span class="token punctuation">[</span>3<span class="token punctuation">]</span> root_dir<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/ <span class="token punctuation">[</span>4<span class="token punctuation">]</span> sub_dir<span class="token operator">=</span>ImageSets/Main <span class="token punctuation">[</span>5<span class="token punctuation">]</span> bash_dir<span class="token operator">=</span><span class="token string">"$(cd "</span><span class="token punctuation">$(</span>dirname <span class="token string">"<span class="token variable">${BASH_SOURCE[0]}</span>"</span><span class="token punctuation">)</span><span class="token string">" &amp;&amp; pwd)"</span></code></pre></li><li>按实际情况修改并执行脚本<code>$CAFFE_ROOT/data/VOC0712/create_data.sh</code><br> 将<strong>第7行</strong>的<code>data_root_dir</code>变量修改为你的VOC数据集目录，比如按照我的目录树，则设置为<code>$HOME/data/VOCdevkit</code>       <pre class=" language-bash"><code class="language-bash"> <span class="token punctuation">[</span>1<span class="token punctuation">]</span> cur_dir<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">cd</span> <span class="token punctuation">$(</span> <span class="token function">dirname</span> $<span class="token punctuation">{</span>BASH_SOURCE<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">}</span> <span class="token variable">)</span></span> <span class="token operator">&amp;&amp;</span> <span class="token function">pwd</span> <span class="token punctuation">)</span> <span class="token punctuation">[</span>2<span class="token punctuation">]</span> root_dir<span class="token operator">=</span><span class="token variable">$cur_dir</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span> <span class="token punctuation">[</span>3<span class="token punctuation">]</span>  <span class="token punctuation">[</span>4<span class="token punctuation">]</span> <span class="token function">cd</span> <span class="token variable">$root_dir</span> <span class="token punctuation">[</span>5<span class="token punctuation">]</span>  <span class="token punctuation">[</span>6<span class="token punctuation">]</span> redo<span class="token operator">=</span>1 <span class="token punctuation">[</span>7<span class="token punctuation">]</span> data_root_dir<span class="token operator">=</span><span class="token string">"<span class="token variable">$HOME</span>/data/VOCdevkit"</span> <span class="token punctuation">[</span>8<span class="token punctuation">]</span> dataset_name<span class="token operator">=</span><span class="token string">"VOC0712"</span> <span class="token punctuation">[</span>9<span class="token punctuation">]</span> mapfile<span class="token operator">=</span><span class="token string">"<span class="token variable">$root_dir</span>/data/<span class="token variable">$dataset_name</span>/labelmap_voc.prototxt"</span></code></pre></li></ol><p>执行完毕后将会自动在<code>$data_root_dir</code>目录下生成<code>VOC0712</code>子目录，里边包含了从数据集VOC2007和VOC2012提取的图片和标记信息，并构建caffe能够高效读取的lmdb文件。<br><code>VOC0712</code>子目录结构如下图所示：<br><img src="/imgs/mssd-try/try1/lmdb_tree.png" width="350">       </p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p><strong>下载MobileNet-SSD源码：</strong>          </p><ul><li>直接用git克隆仓库到examples目录下     <pre class=" language-bash"><code class="language-bash">  <span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span>/examples  <span class="token function">git</span> clone https://github.com/chuanqi305/MobileNet-SSD.git</code></pre></li><li>或者到我的网盘上下载并解压到caffe的examples目录下：<a href="https://pan.baidu.com/s/1wR0iJcvTgT7c4vwJF1pVUQ#list/path=%2Fblog-share%2Fmobilenet_ssd" target="_blank" rel="noopener">blog-share/mobilenet_ssd/MobileNet-SSD.zip | 百度网盘</a>         </li></ul><p><strong>创建数据集软链接：</strong>       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> TRAINVAL_LMDB<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb/<span class="token function">export</span> TEST_LMDB<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb/<span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span>/examples/MobileNet-SSD<span class="token function">ln</span> -s TRAINVAL_LMDB ./trainval_lmdb<span class="token function">ln</span> -s TEST_LMDB ./test_lmdb</code></pre><p><strong>把VOC的标签映射文件复制过来：</strong>        </p><pre class=" language-bash"><code class="language-bash"><span class="token function">cp</span> <span class="token variable">$CAFFE_ROOT</span>/data/VOC0712/labelmap_voc.protxt <span class="token variable">$CAFFE_ROOT</span>/examples/MobileNet-SSD/labelmap.protxt</code></pre><p><strong>生成模型文件：</strong>     </p><pre class=" language-bash"><code class="language-bash">./gen_model.sh 21</code></pre><p>这里21指的是VOC的21个类别（含负样本），生成的模型文件默认放置在<code>example</code>目录下；          </p><p><strong>如果需要修改训练参数和测试参数</strong>，可以分别修改目录下的<code>solver_train.protxt</code>和<code>sovler_test.protxt</code>文件，<br>默认使用<code>example</code>目录下的训练模型和测试模型；     </p><p><strong>如果需要指定GPU和初始化权重</strong>，可以修改目录下的<code>train.sh</code>或<code>test.sh</code>文件，以<code>train.sh</code>为例：       </p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span><span class="token keyword">if</span> <span class="token operator">!</span> <span class="token function">test</span> -f example/MobileNetSSD_train.prototxt <span class="token punctuation">;</span><span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"error: example/MobileNetSSD_train.prototxt does not exist."</span>    <span class="token keyword">echo</span> <span class="token string">"please use the gen_model.sh to generate your own model."</span>        <span class="token keyword">exit</span> 1<span class="token keyword">fi</span><span class="token function">mkdir</span> -p snapshot<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/build/tools/caffe train -solver<span class="token operator">=</span><span class="token string">"solver_train.prototxt"</span> \-weights<span class="token operator">=</span><span class="token string">"mobilenet_iter_73000.caffemodel"</span> \-gpu 0 </code></pre><p><code>weights</code>参数指定初始化的权重文件，这里用了chuanqi305预训练迭代了73000次的模型；<br><code>gpu</code>参数指定使用的gpu，多个gpu可以用逗号隔开；<br>除此之外，如果需要继续之前中断的训练，还可以指定<code>snapshot</code>参数，<br>比如我想从最近的快照继续训练，可以这样修改<code>train.sh</code>——      </p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>latest<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">ls</span> -t snapshot/*.caffemodel <span class="token operator">|</span> <span class="token function">head</span> -n 1<span class="token variable">)</span></span><span class="token keyword">if</span> <span class="token operator">!</span> <span class="token function">test</span> -f example/MobileNetSSD_train.prototxt <span class="token punctuation">;</span><span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"error: example/MobileNetSSD_train.prototxt does not exist."</span>    <span class="token keyword">echo</span> <span class="token string">"please use the gen_model.sh to generate your own model."</span>        <span class="token keyword">exit</span> 1<span class="token keyword">fi</span><span class="token function">mkdir</span> -p snapshot<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/build/tools/caffe train -solver<span class="token operator">=</span><span class="token string">"solver_train.prototxt"</span> \-snapshot<span class="token operator">=</span><span class="token variable">$latest</span> \-gpu 0 </code></pre><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p><strong>合并BN层：</strong><br>训练后会在<code>snapshot</code>目录下产生一个相应的<code>caffemodel</code>文件；<br>按实际情况修改<code>merge_bn.py</code>文件并执行：      </p><pre class=" language-python"><code class="language-python"><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np  <span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">import</span> sys<span class="token punctuation">,</span>os  <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">]</span> caffe_root <span class="token operator">=</span> <span class="token string">'/your/caffe/root/path/'</span><span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">]</span> sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> caffe_root <span class="token operator">+</span> <span class="token string">'python'</span><span class="token punctuation">)</span>  <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">]</span> <span class="token keyword">import</span> caffe  <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">]</span> train_proto <span class="token operator">=</span> <span class="token string">'example/MobileNetSSD_train.prototxt'</span>       <span class="token comment" spellcheck="true"># 训练时所用的模型文件</span><span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">]</span> train_model <span class="token operator">=</span> <span class="token string">'mobilenet_iter_73000.caffemodel'</span>           <span class="token comment" spellcheck="true"># 训练后产生的caffemodel文件</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> deploy_proto <span class="token operator">=</span> <span class="token string">'example/MobileNetSSD_deploy.prototxt'</span>     <span class="token comment" spellcheck="true"># 部署时所要用的模型文件（去掉BN层）</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span> save_model <span class="token operator">=</span> <span class="token string">'MobileNetSSD_deploy.caffemodel'</span>             <span class="token comment" spellcheck="true"># 最终生成的caffemodel文件（合并BN层参数）</span></code></pre><p>生成的合并BN层后的caffemodel就在MobileNet-SSD项目的根目录下；<br>编辑<code>example/MobileNetSSD_deploy.prototxt</code>修改输入层，即把</p><pre><code>input: &quot;data&quot;input_shape {    dim: 1    dim: 3    dim: 300    dim: 300}</code></pre><p>改为     </p><pre><code>layer {    name: &quot;input&quot;    type: &quot;Input&quot;    top: &quot;data&quot;    input_param {        shape {            dim: 1            dim: 3            dim: 300            dim: 300        }    }}</code></pre><p>把 <code>example/MobileNetSSD_deploy.prototxt</code> 和 <code>MobileNetSSD_deploy.caffemodel</code> 拷贝到<strong>Tengine</strong>平台的<code>models</code>目录下，此时运行<code>mobilenet_ssd/MSSD</code>用的就是新训练好的模型啦！      </p><hr><p>本文简单介绍了如何用chuanqi305的MobileNet-SSD训练出自己的网络。<br>下一篇文章《<a href="/2018/08/24/mssd-try2/">基于MobileNet-SSD的目标检测Demo（一） | Hey~YaHei!</a>》将继续尝试根据实际情况删减多余类别进行训练。还可以注意到，        </p><ul><li>chuanqi305的MobileNet-SSD模型除了基础网络部分之外依旧保守的使用了Standard Conv，可以尝试将这一部分也改造为Depthwise Conv；      </li><li>同时，MobileNet-SSD使用带group的caffe原生Conv来进行Depthwise Conv操作，这是非常低效率的，下篇文章还将进一步比较Depthwise Conv和带group的原生Conv的效率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNet-SSD网络解析</title>
      <link href="/2018/08/08/MobileNets-SSD/"/>
      <url>/2018/08/08/MobileNets-SSD/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28941713&auto=0&height=32"></iframe>     <p>上一篇文章《<a href="/2018/08/06/SSD/#网络结构">SSD框架解析 - 网络结构| Hey~YaHei!</a>》和上上篇文章《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》我们分别解析了SSD目标检测框架和MobileNet v1分类模型。<br>在本文中将会把两者综合起来，一起分析<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合得到MobileNet-SSD网络的。         </p><hr><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>参照 <a href="https://github.com/chuanqi305/MobileNet-SSD/tree/master/template" target="_blank" rel="noopener">MobileNet-SSD(chuanqi305)的caffe模型（prototxt文件） | github</a>，绘制出MobileNet-SSD的整体结构如下（忽略一些参数细节）：<br><img src="/imgs/MobileNet-SSD/mobilenet-ssd.jpg" alt="mobilenet-ssd">    </p><p>图片中从上到下分别是MobileNet v1模型（统一输入大小为300x300）、chuanqi305的Mobilenet-SSD网络、VGG16-SSD网络。且默认都是用3x3大小的卷积核，除了MobileNet-SSD的Conv14_1、Conv15_1、Conv16_1、Conv17_1和VGG16-SSD的Conv8_1、Conv9_1、Conv10_1、Conv11_1用的是1x1大小的卷积核。<br>图中每个立方体代表对应层的<strong>输出</strong>特征图；        </p><ul><li>首先观察基础网络部分<br>  MobileNet-SSD从Conv0到Conv13的配置与MobileNet v1模型是完全一致的，相当于只是去掉MobileNet v1最后的全局平均池化、全连接层和Softmax层；     </li><li>再看SSD部分       <ul><li>在VGG16-SSD的方案中，用Conv6和Conv7分别替代了原VGG16的FC6和FC7；       </li><li>MobileNet-SSD和VGG16-SSD都是从六个不同尺度的特征图上提取特征来做Detections，它们的大小为：      <pre><code>  MobileNet-SSD   |   VGG16-SSD  ----------------+-----------------  19 x 19 x  512  |   38 x 38 x  512  10 x 10 x 1024  |   19 x 19 x 1024   5 x  5 x  512  |   10 x 10 x  512   3 x  3 x  256  |   5  x  5 x  256   2 x  2 x  256  |   3  x  3 x  256   1 x  1 x  128  |   1  x  1 x  128</code></pre><ul><li>从通道数量上看，两者是完全一致的</li><li>从特征图分辨率上看，MobileNet-SSD都只有VGG16-SSD的一半     <ul><li><strong>这意味着什么？</strong><br>  打个比方，假设对于那个分辨率最大的特征图，都能用4x4的感受野检测出一只猫，如下图所示，黑色是头，红色是身体，棕色是腿，黄色是尾巴。<br>  <center><img src="/imgs/MobileNet-SSD/cat.png" width="200"></center><br>  那用MobileNet-SSD可以检测出占原图$\frac{4}{19} \approx 0.211$大小的猫，而VGG16-SSD却可以检测出占原图$\frac{4}{38} \approx 0.105$大小的猫；       </li><li><strong>那为什么MobileNet-SSD为什么不和VGG16-SSD一样，从38x38分辨率的特征图开始做Detections呢？</strong><br>  回到上一篇博文《<a href="/2018/08/06/SSD/#网络结构">SSD框架解析 - 网络结构| Hey~YaHei!</a>》，VGG16是从Conv4_3也就是第10层卷积层取出38x38分辨率的特征图；<br>  再观察一下MobileNet v1-300的模型，想要取出38x38分辨率的特征图，最深也只能从Conv5也就是第6层卷积层取出，这个位置比较浅，实在很难保证网络提取出了足够有用的特征可以使用；         </li><li><strong>那可以通过增加最初输入图片的分辨率来解决这个问题吗？</strong><br>  倒也可以，比如把输入图片大小扩大到512x512，那么Conv11的输出就变为32x32，按上上一点的描述，可以检测出占原图$\frac{4}{32} = 0.125$大小的猫；<br>  但要付出相应的代价，仅考虑基础网络部分（Conv0到Conv13），参数数量和乘加运算量均提高为原来的 $(\frac{512}{300})^2 \approx 2.913$ 倍（不考虑padding的影响，计算方式可以参考《<a href="/2018/08/05/MobileNets_v1/#效率比较">MobileNets v1模型解析 - 效率比较 | Hey~YaHei!</a>》），MobileNet本身小模型的低参数量、低运算量优势变得不再明显。           </li></ul></li></ul></li><li>还有一个小细节，观察特征图到Detections的路径<br>  VGG16-SSD中用的都是3x3大小的卷积核，缺省框数量依次是<font color="red">4</font>、6、6、6、<font color="red">4</font>、<font color="red">4</font>；<br>  MobileNet-SSD中用的都是1x1大小的卷积核，缺省框数量依次是<font color="red">3</font>、6、6、6、<font color="red">6</font>、<font color="red">6</font>；<br>  <em>这一部分的改动不是很能理解，3x3卷积改1x1卷积可能是实践中发现改动后效果差不多但可以减少运算量；缺省框数量改动的原因就不得而知了~</em>      </li></ul></li></ul><h3 id="BN层合并"><a href="#BN层合并" class="headerlink" title="BN层合并"></a>BN层合并</h3><p>对比chuanqi305的 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/template/MobileNetSSD_train_template.prototxt" target="_blank" rel="noopener">train模型</a> 和 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/template/MobileNetSSD_deploy_template.prototxt" target="_blank" rel="noopener">deploy模型</a> 还能发现一件有趣的事情——<br><strong>deploy模型中的BN层和scale层都不见啦！！！</strong><br>BN层是这样随随便便就能丢弃的么？没道理啊！         </p><p>几经辗转，查阅资料之后发现，原来BN层是可以合并进前一层的卷积层或全连接层的，而且这还有利于减少预测用时。<br>参考《<a href="http://machinethink.net/blog/object-detection-with-yolo/#converting-to-metal" target="_blank" rel="noopener">Real-time object detection with YOLO - Converting to Metal</a>》    </p><p>合并的原理：卷积层、全连接层和BN层都是纯粹的线性转换。       </p><p>数学推导也很简单：<br><em>假设图片为 $x$ ，卷积层权重为 $w$ 。</em><br>那么对于卷积运算有，<br>$$ conv[j] = x[i]w[0] + x[i+1]w[1] + x[i+2]w[2] + … + x[i+k]w[k] + b $$<br>BN层运算为，<br>$$ bn[j] = \frac{\gamma (conv[j] - mean)}{\sqrt{variance}} + \beta = \frac{\gamma \cdot conv[j]}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} + \beta $$<br>代入$conv[j]$变为，<br>$$ bn[j] = x[i] \frac{\gamma \cdot w[0]}{\sqrt{variance}} + x[i+1] \frac{\gamma \cdot w[1]}{\sqrt{variance}} + … + x[i+k] \frac{\gamma \cdot w[k]}{\sqrt{variance}} + \frac{\gamma \cdot b}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} + \beta $$<br>两式对比可以得到，<br>$$ w_{new} = \frac{\gamma \cdot w}{\sqrt{variance}} $$<br>$$ \beta_{new} = \beta + \frac{\gamma \cdot b}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} = \beta + \frac{\gamma (b-mean)}{\sqrt{variance}} $$<br>注意，其中 $\gamma$、$mean$、$variance$、$\beta$ 都是训练出来的量，在预测阶段相当于一个常量。       </p><p>原文摘录如下：    </p><center><img src="/imgs/MobileNet-SSD/Converting to Metal.png" alt="Converting to Metal"></center>    <hr><p>本文介绍了chuanqi305的MobileNet-SSD网络是如何组成的以及实用的MergeBN技术，在下一篇博文中我们将尝试用该网络进行训练并部署在RK3399的Tengine平台上，并且进一步对该网络进行改进以满足我们实际场景的需要。        </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>SSD框架解析</title>
      <link href="/2018/08/06/SSD/"/>
      <url>/2018/08/06/SSD/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=38689090&auto=0&height=32"></iframe>       <p>上一篇文章《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》介绍了MobileNets v1的核心思想和网络结构，本文将解析MobileNet SSD网络的另外一部分——SSD框架。SSD的主要贡献，一方面在于从多个不同尺度的特征图获取特征信息进而预测目标的位置和类别，使网络同时对输入图片上的大小物体都比较敏感；另一方面在于其训练技巧值得借鉴，这在论文中有一定的阐述，更加详细的训练技巧可以结合作者开源的代码学习。         </p><hr><p>论文：《<a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector(2016)</a>》      </p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>与分析MobileNets v1模型不同，分析框架我们先从整体入手。          </p><p><img src="/imgs/SSD/SSD_architecture.png" alt="SSD architeture">     </p><ul><li>用预训练好的分类网络作为特征提取器（论文里使用的是VGG16）<br>  VGG16原模型如下图所示：<br>  <img src="/imgs/SSD/vgg16_raw.png" alt="vgg16"><br>  由于SSD论文里输入是 $300 \times 300$，我们重写一下VGG16模型的各层输出大小：<br>  <img src="/imgs/SSD/vgg16_300.png" alt="vgg16">     </li><li>论文中，SSD丢掉了VGG16最后的全局池化和全连接层（FC6和FC7）<br>  并且分别用 $3 \times 3 \times 1024$ 的卷积层Conv6和 $1 \times 1 \times 1024$ 的卷积层Conv7替代FC6和FC7作基础分类网络的最终特征抽取。         </li><li>随后是一系列不同尺度的卷积层在不同尺度上做特征提取      </li><li>融合不同尺度特征信息<br>  分别用卷积操作从 $38 \times 38$ 的Conv4_3、$19 \times 19$ 的Conv7、$10 \times 10$ 的Conv8_2、$5 \times 5$ 的Conv9_2、$3 \times 3$ 的Conv10_2、$1 \times 1$ 的Conv11_2抽取特征（直接回归出后述的预测框的位置以及各分类的置信度），各自Flatten之后拼接成“长条”状特征向量。       </li><li>非极大值抑制（Non-Maximum Suppression，NMS）      <ul><li>从置信度最高的框开始，如果其他预测框和该框的jaccard重叠率超过阈值，则丢弃     </li><li>从剩下的框找到置信度最高的框，如果其他预测框和该框的jaccard重叠率超过阈值，则丢弃     </li><li>……重复直到遍历所有的框</li></ul></li></ul><h3 id="感受野和缺省框"><a href="#感受野和缺省框" class="headerlink" title="感受野和缺省框"></a>感受野和缺省框</h3><center><img src="/imgs/SSD/bounding box.png" alt="bounding box"></center>       <h4 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h4><p>卷积是对特定小区域的特征提取，比如一张 $300 \times 300$ 的原图经过一定卷积操作之后得到 $8 \times 8$ 的特征图，特征图上的每个“像素点”其实对应原图的一个“感受野”，在这里一个“感受野”的大小为 $\frac{300}{8} \times \frac{300}{8}$ 也即 $37.5 \times 37.5$ （实际上不可能有$.5$，是$37$还是$38$要看卷积过程是否padding）。      </p><p>换句话说，特征图左上角的一个感受野其实是原图左上角一个 $37.5 \times 37.5$ 大小区域的一个特征抽取（或者说是这个区域的一个抽象化，融合了这个区域的信息）。     </p><h4 id="缺省框"><a href="#缺省框" class="headerlink" title="缺省框"></a>缺省框</h4><p><em>论文里称为default box，而源码里称为prior box.</em>          </p><p>在SSD中，为每个向Detection贡献特征的特征图上的每个感受野分配若干不同大小和长宽比（aspect ratio）的绑定框（Bounding Box），比如在作者开源的源码中，这些绑定框是这样生成的——<br>参考<a href="https://github.com/weiliu89/caffe/blob/ssd/src/caffe/layers/prior_box_layer.cpp" target="_blank" rel="noopener">SSD - PriorBoxLayer源码 | github</a><br>（其中，$D_{min}、x_{offset}、y_{offset}、AR、D_{max}$ 都是用户指定的超参数，而且 $D_{max}$ 是可选的）</p><ol><li>首先有一个最小的绑定框，其尺寸为 $D_{min} \times D_{min}$，绑定框中心相对感受野中心偏移 $(x_{offset}, y_{offset})$；       </li><li>根据用户指定的长宽比列表 $AR$ 生成若干绑定框<br> 对于某一特定长宽比 $AR_i$，生成尺寸分别为 $\frac{D_{min}}{\sqrt{AR_i}} \times D_{min}\sqrt{AR_i}$ 和 $D_{min}\sqrt{AR_i} \times \frac{D_{min}}{\sqrt{AR_i}}$ 的绑定框，其框中心同1；      </li><li>最后还生成一个尺寸为 $D_{max} \times D_{max}$ 的绑定框，其框中心同1；     </li></ol><p>事实上网络输出的预测框位置参数就是这每一个缺省框的中心点偏移量和长、宽偏移量（每个框共计4个参数）！<br><em>对于一个方框其实有两种不同的表示方法，都是四元组，即（中心点x坐标，中心点y坐标，长度，宽度）或（左上角x坐标，左下角x坐标，右下角x坐标，右下角y坐标）</em>    </p><h4 id="D-min-和-D-max-的设置建议"><a href="#D-min-和-D-max-的设置建议" class="headerlink" title="$D_{min}$ 和 $D_{max}$ 的设置建议"></a>$D_{min}$ 和 $D_{max}$ 的设置建议</h4><p>论文中也给出了超参数 $D_{min}$ 和 $D_{max}$ 的设置建议——      </p><ul><li>首先确定框的最大最小归一化尺寸 $scale_{min}$ 、 $scale_{max}$<br>  $$ scale = \frac{BoxSize}{ImageSize} $$<br>  比如按论文的设计 $300 \times 300$ 的输入图片，设 $scale_{min} = 0.2、scale_{max} = 0.9$，那框的最大尺寸就是 $270 \times 270$，最小尺寸为 $60 \times 60$；     </li><li>然后最浅层的特征图对应的bbox设 $\text{min_size} = scale_{min} \times ImageSize$；        </li><li>随后按等差数量设置各特征对应的bbox的 min_size，而 max_size 可以取下一层的 min_size（论文中默认不设置max_size）；      </li></ul><p>写成数学公式为：<br>$$s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1} (k-1), k \in [1,m]$$<br>其中，<br>$s_k$，是第k个特征图的 $min_size$ 参数；<br>$s_{min}$，是设计好的最小归一化尺寸，即前述 $scale_{min}$；<br>$s_{max}$，是设计好的最大归一化尺寸，即前述 $scale_{max}$；<br>$m$，是特征图的总数；<br><em>注意：上述“特征图”指的是对Detection有贡献的特征图，序号从浅层到深层递增</em>        </p><p>回过头再看看网络结构，注意从不同尺度融合特征信息的那一部分，用的都是一个 $3 \times 3$ 的卷积核，输出通道要么是 num_class + 4 的四倍要么是六倍，这里 num_class 就是最终分类的数量，数值 $4$ 其实指的是预测框的位置参数的数量，而四倍或六倍指的是特征图上每个感受野对应的绑定框数量。<br>按照论文，浅层的分辨率比较高，所以使用了 $AR=\{1,2,3\}$ 的分辨率组合，对于 $AR_i = 2$ 和 $AR_i = 3$ 分别会生成两个绑定框，而 $AR_i = 1$ 除了本身之外还会额外产生一个归一化尺寸为 $s’_k = \sqrt{s_k s_{k+1}}$ 的绑定框；而浅层分辨率较低，2或3的长宽比其实没太大区别或必要，所以只取了 $AR=\{1,2\}$。       </p><p>源码prototxt参数示例：       </p><pre><code>layer {  name: &quot;conv11_mbox_priorbox&quot;  type: &quot;PriorBox&quot;  bottom: &quot;conv11&quot;  # bottom[0]：特征向量  bottom: &quot;data&quot;    # bottom[1]：原始输入（主要提供宽、高参数）  top: &quot;conv11_mbox_priorbox&quot;  prior_box_param {    min_size: 30.0      # 最小的基本Box大小（可根据论文计算）    aspect_ratio: 2.0   # 所要用的长宽比（除了1.0）    flip: true          # 是否以0.5的概率翻转    clip: false         # 是否截断Box（若截断表示不允许Box超出图片范围）    variance: 0.1       # xmin的偏差因子    variance: 0.1       # ymin的偏差因子    variance: 0.2       # xmax的偏差因子    variance: 0.2       # ymax的偏差因子    offset: 0.5         # Box中心相对于感受野中心的x、y偏移量  }}</code></pre><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="框配对（Matching）"><a href="#框配对（Matching）" class="headerlink" title="框配对（Matching）"></a>框配对（Matching）</h4><p>前边讲到了感受野和缺省框，我们现在看看这些缺省框是如何跟实际标注的真实框（Ground Truth, GT）配对起来的。        </p><p>介绍一个重叠率计算公式，jaccard重叠率：<br>$$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|} $$</p><p>对于每个GT，      </p><ul><li>首先，在所有缺省框中挑选出jaccard重叠率最高的一个作为配对正样本，确保每个GT都有唯一一个配对的缺省框；       </li><li>然后，用其他缺省框挨个与GT计算jaccard，超过阈值（论文中设定为0.5）的都作为配对正样本；       </li><li>其他未配对的都作为负样本，但显然数量上负样本远多于正样本，这会导致网络过于重视负样本而loss不稳定；<br>  所以论文采取了Hard Negative Mining的策略，训练时按照分类的置信度为各个负样本框排序；<br>  挑选置信度高的一批作为实际训练的负样本，同时控制负样本的数量大概为正样本的三倍左右      </li></ul><h4 id="损失函数（MultiBoxLoss）"><a href="#损失函数（MultiBoxLoss）" class="headerlink" title="损失函数（MultiBoxLoss）"></a>损失函数（MultiBoxLoss）</h4><p>损失函数由位置损失 $L_{loc}$ 和分类损失 $L_{conf}$ 加权求和获得；<br>完整公式如下：<br>$$L(x,c,l,g) = \frac{1}{N} (L_{conf}(x,c) + \alpha L_{loc}(x,l,g))$$<br>其中，<br>$N$，是匹配正样本的总量（如果N=0，则令L=0）；<br>$x$、$c$，分别是分类的指示量和置信度；<br>$l$、$g$，分别是预测框和GT框；<br>$\alpha$，是位置损失的权重。        </p><p>位置损失是预测框l和真实框g之间的Smooth L1损失，<br>$$ L_{loc} = \sum^N_{i \in Pos} \sum_{m \in {cx, cy, w, h} x^k_{ij}} smooth_{L1} (l^m_i - \hat{g}_j^m) $$<br>$$ \hat{g}^{cx}_j = \frac{g^{cx}_j - d^{cx}_i}{d_i^w}，\hat{g}^{cy}_j = \frac{g^{cy}_j - d^{cy}_i}{d_i^h}，\hat{g}^w_j = log(\frac{g_j^w}{d_i^w})，\hat{g}^h_j = log(\frac{g_j^h}{d_i^h}) $$<br>其中，<br>$x^k_{ij}$，是指示量，当第i个匹配框和分类p的第j个真实框配对时值为1，否则为0；<br>$cx$、$cy$、$w$、$h$，分别是框的中心点x坐标、中心点y坐标，宽度、高度；<br>$d$，是绑定框（网络本身预设的绑定框）；<br>$l$，是预测框（网络输出的加上预测偏移量的框）；<br>$g$，是GT框（数据集标注的真实框）。     </p><p>分类损失是分类置信度之间的softmax损失，<br>$$ L_{conf}(x,c) = -\sum^N_{i \in Pos} x^p_{ij} log(\hat{c}^p_i) - \sum_{i \in Neg}log(\hat{c}^0_i) $$<br>$$ \hat{c}^p_i = \frac{exp(c^p_i)}{\sum_p exp(c_i^p)} $$        </p><p>源码prototxt参数示例：     </p><pre><code>layer {  name: &quot;mbox_loss&quot;  type: &quot;MultiBoxLoss&quot;  bottom: &quot;mbox_loc&quot;        # 用于预测位置的特征向量  bottom: &quot;mbox_conf&quot;       # 用于预测分类的特征向量  bottom: &quot;mbox_priorbox&quot;   # 若干PriorBox的输出连接  bottom: &quot;label&quot;           # 训练用的标签  top: &quot;mbox_loss&quot;  include {    phase: TRAIN  }  propagate_down: true      # bottom[0] : mbox_loc，需要训练（反向传播）  propagate_down: true      # bottom[1] : mbox_conf，需要训练  propagate_down: false     # bottom[2] : mbox_priorbox，不需要训练  propagate_down: false     # bottom[3] : label，不需要训练  # 损失计算参数组  loss_param {    normalization: VALID    # 损失的归一化方式                            #    FULL：除以batch_size                            #    VALID：除以有效的数量（排除ignore_label参数指定的标签）                            #    NONE  }  # multibox损失参数组  multibox_loss_param {    loc_loss_type: SMOOTH_L1    # 位置预测的损失函数    conf_loss_type: SOFTMAX     # 分类预测的损失函数    loc_weight: 1.0             # loc_loss的权重，论文中的alpha    num_classes: 12             # 输出类别    share_location: true        # 是否让所有的预测框共享参数    match_type: PER_PREDICTION  #     overlap_threshold: 0.5      # 重叠阈值（训练时超过该阈值的Box作为正样本）    use_prior_for_matching: true    # 是否使用先验框（即前边是否有PriorBox）    background_label_id: 0      # 背景（background）标签的id值（与labelmap.prototxt文件匹配）    use_difficult_gt: true      # 是否使用difficult的Ground Truth（？）    neg_pos_ratio: 3.0          # 负样本:正样本 的比例    neg_overlap: 0.5            # 负样本阈值（低于该阈值的Box作为负样本）    code_type: CENTER_SIZE      # bounding box的编码方式    ignore_cross_boundary_bbox: false   #     mining_type: MAX_NEGATIVE   # 挖掘类型                                #   NONE：什么都不用，会发生正负样本步均衡                                #   MAX_NEGATIVE：为负样本排序，选择分类得分最高的一批作为训练的负样本                                #   HARD_EXAMPLE：选择基于“在线硬示例挖掘的基于训练区域的对象探测器”的硬实例（？）  }}</code></pre><h4 id="数据增强（Data-Augmentation）"><a href="#数据增强（Data-Augmentation）" class="headerlink" title="数据增强（Data Augmentation）"></a>数据增强（Data Augmentation）</h4><p>目标检测任务和图片分类任务不同，      </p><ul><li>图片分类任务可以随意对分类对象（也即整张图片）作伸缩变换       </li><li>目标检测任务只能对整张图片作伸缩变换，却不能对分类对象直接作伸缩变换      </li></ul><p>基本的数据增强，每张图片在输入会等概率地选取以下一种变换：      </p><ol><li>原始图片      </li><li>随机裁剪    </li><li>带jaccard重叠率约束的随机裁剪（论文给出的是0.1, 0.3, 0.5, 0.7, 0.9五种重叠率设置）        </li></ol><p>随机裁剪是有条件的，限制最低归一化尺寸（论文中为0.1），以及长宽比（论文中为$[1/2,2]$）；<br>实际操作中是先生成一系列满足条件的框，在从中挑选若干框来裁剪图像输入训练。         </p><p>仔细思考一下会发现，上述的数据增强方式只能对某个目标进行放大（zoom in）操作，这会导致小目标数据集缺失。<br>于是论文中在执行上述数据增强前又增加了一步缩小（zoom out）的增强措施——      </p><ol><li>首先生成一张画布，画布的长宽是原图片的1-4倍（随机数）；        </li><li>将原图放在画布的随机位置上；     </li><li>……接下来再送给前述的基本数据增强步骤       </li></ol><p><em>据论文介绍，该额外操作为mAP提升了两到三个百分点。</em>       </p><p>对于画布，从源码<a href="https://github.com/weiliu89/caffe/blob/ssd/src/caffe/data_transformer.cpp#L489" target="_blank" rel="noopener">data_transform.cpp/ExpandImage()函数（第489行） | github</a>上看，是用空格符初始化了一个字符串缓冲区来作为画布，这似乎意味着是用RGB(32,32,32)的固定颜色填充了画布（空格符的assic码为32）；<br>这种做法有些残暴，或许用原图的平均值、或者一个随机值来作固定颜色填充效果会更好一些；<br>甚至，用实际图片作为背景图可能会取得更好的效果，具体结论还有待实验。      </p><p>源码prototxt参数示例：     </p><pre><code>layer {  name: &quot;data&quot;  type: &quot;AnnotatedData&quot;  top: &quot;data&quot;  top: &quot;label&quot;  include {    phase: TRAIN  }  # 图像转换参数组  transform_param {    scale: 0.007843     # 归一化，1/127.5    mirror: true        # 0.5的概率镜像翻转    mean_value: 127.5   # 去均值（R通道）    mean_value: 127.5   # 去均值（G通道）    mean_value: 127.5   # 去均值（B通道）    # 缩放参数组    resize_param {      prob: 1.0                 # 缩放操作的概率      resize_mode: WARP         # 缩放模式                                #   WARP：放大或缩小以适应(width, height)                                #   FIT_SMALL_SIZE：                                #   FIT_LARGE_SIZE_AND_PAD：      height: 300               # 缩放后的高      width: 300                # 缩放后的宽      # 插值模式同opencv      interp_mode: LINEAR       # 线性      interp_mode: AREA         # 像素区域重采样      interp_mode: NEAREST      # 最近邻      interp_mode: CUBIC        # 三次样条      interp_mode: LANCZOS4     # Lanczos    }    emit_constraint {      emit_type: CENTER    }    # 色彩扭曲参数组    distort_param {      brightness_prob: 0.5      brightness_delta: 32.0      contrast_prob: 0.5      contrast_lower: 0.5      contrast_upper: 1.5      hue_prob: 0.5      hue_delta: 18.0      saturation_prob: 0.5      saturation_lower: 0.5      saturation_upper: 1.5      random_order_prob: 0.0    }    # 图像扩展参数组（zoom-out）    expand_param {      prob: 0.5                 # 概率      max_expand_ratio: 4.0     # 扩大倍数（4x4）    }  }  # 数据源参数组  data_param {    source: &quot;trainval_lmdb/&quot;    # 源文件    batch_size: 24           backend: LMDB               # 数据源类型  }  # AnnotatedData参数组  annotated_data_param {    # 采样器1：原图    batch_sampler {      max_sample: 1      max_trials: 1    }    # 采样器2：随机抠图（overlap=0.1）    batch_sampler {      sampler {        min_scale: 0.15         # 最小尺寸（原图的0.15倍）        max_scale: 1.0          # 最大尺寸        min_aspect_ratio: 0.5   # 最小长宽比        max_aspect_ratio: 2.0   # 最大长宽比      }      sample_constraint {        min_jaccard_overlap: 0.1    # 最小JACCARD_OVERLAP（重叠率）      }      max_sample: 1     # 最大采样数量      max_trials: 50    # 最大尝试数量（产生50个随机框，然后再筛选出1个输出）    }    # [略]采样器3：随机抠图（overlap=0.3）    # [略]采样器4：随机抠图（overlap=0.5）    # [略]采样器5：随机抠图（overlap=0.7）    # [略]采样器6：随机抠图（overlap=0.9）    # [略]采样器7：随机抠图（overlap=1.0）    label_map_file: &quot;labelmap.prototxt&quot;     # 标签映射文件  }}</code></pre><hr><p>本文介绍分析了SSD框架的核心思路和训练技巧，加上上一篇文章对MobileNet模型的解析，基本的前置理论准备完毕。在下一篇博文中，将分析<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合起来的——《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNets v1模型解析</title>
      <link href="/2018/08/05/MobileNets_v1/"/>
      <url>/2018/08/05/MobileNets_v1/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=488388731&auto=0&height=32"></iframe><br>上一篇文章《<a href="/2018/08/04/RK3399-Tengine">RK3399上Tengine平台搭建 | Hey~YaHei!</a>》中在RK3399上搭建了Tengine平台并试运行了MobileNet SSD网络，本文将为你解析MobileNets v1的实现思路。<br><strong><em>下边分解过程是按自己理解画的图，如果理解有误欢迎指正~</em></strong>       </p><hr><p>论文：《<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(2017)</a>》   </p><h3 id="深度向卷积分解（Depthwise-Separable-Convolution）"><a href="#深度向卷积分解（Depthwise-Separable-Convolution）" class="headerlink" title="深度向卷积分解（Depthwise Separable Convolution）"></a>深度向卷积分解（Depthwise Separable Convolution）</h3><h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center>   <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用同一个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>可以看到，传统的卷积中用 $N$ 个不同的卷积核不厌其烦地对原图进行特征提取来得到 $N$ 通道的输出，这其中必定从原图中提取到了大量的重复特征。有没有可能<strong>只用单个卷积核来做特征提取，最后依旧能输出多通道的特征图</strong>呢？这就是深度向卷积分解的核心思想。<br>观察①阶段中的第三个子阶段，该阶段将多张单通道特征图按通道堆叠起来之后“拍扁”，如果去掉这个“拍扁”的过程，其实就可以提取得到一张 $M$ 通道的特征图啦，再经过一个 $M$ 维空间到 $N$ 维空间的线性映射，就能够和普通的卷积操作一样得到一张 $N$ 通道的特征图 $O$。完整的卷积过程如下图所示——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>①、②阶段与传统卷积①阶段的前两个阶段完全相同，③阶段比传统卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上</li></ol><h4 id="效率比较"><a href="#效率比较" class="headerlink" title="效率比较"></a>效率比较</h4><p><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><p>对于普通的卷积操作，        </p><ul><li>输出 $N$ 通道特征图需要 $N$ 个卷积核，故参数数量为 $N \times D_K \times D_K$；     </li><li>一个 $D_K \times D_K$ 的卷积核在原图的某个位置的某个通道上需要进行 $D_K \times D_K$ 次乘加操作，输出特征图大小为 $D_O \times D_O$，原图通道数量为 $M$，共有 $N$ 个不同的卷积核，故乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M \times N$           </li></ul><p>对于深度向分解后的卷积操作，       </p><ul><li>特征提取只使用了一个 $D_K \times D_K$ 的卷积核，组合过程为了作线性映射用了 $N$ 个 $1 \times 1$ 的卷积核，故参数数量为 $N + D_K \times D_K$；       </li><li>特征提取过程只有一个卷积核，所以该过程乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M$。同样的，组合过程容易算得需要 $D_O \times D_O \times M \times N$ 次乘加操作。故乘加操作的总数量为 $D_O \times D_O \times M \times ( D_K \times D_K + N)$             </li></ul><p>总的来说，参数数量和乘加操作的运算量均下降为原来的 $\frac{1}{D_K^2} + \frac{1}{N}$，通常使用 $3 \times 3$ 的卷积核，也就是下降为原来的九分之一到八分之一左右。而从论文的实验部分来看，准确率也只有极小的下降。    </p><center><img src="/imgs/MobileNet/dw_vs_full.png" alt="dw_vs_full"></center>    <h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><center><img src="/imgs/MobileNet/v1_body_architecture.png" alt="v1_body_architecture"></center>    </p><ul><li>第一层使用普通的卷积层，最前端的特征提取非常重要，宁可存在重复的特征信息，也不该放掉；     </li><li>随后是一系列的深度向分解卷积层，用于逐层次提取特征，用步长不为1的卷积替代池化做降采样，同时整体也满足通道加深，特征图分辨率降低的CNN一般特点；        </li><li>最后也是常规的全局平均池化、全连接、Softmax；      </li><li>每次卷积操作之后都紧跟一个BN层（在预测阶段可以被合并）和一个RELU层；      <center><img src="/imgs/MobileNet/dw_vs_full_train.png" alt="dw_vs_full_train"></center>    </li><li>而且，深度向分解的卷积中绝大多数参数和运算都集中在 $1 \times 1$ 的pointwise卷积运算当中，这种运算恰恰是能够被 <code>GEneral Matrix Multiply(GEMM)</code> 函数高度优化的；    <center><img src="/imgs/MobileNet/resource.png" alt="resource"></center>    </li><li>论文中还提到两个压缩模型的因子，分别用于输入图片分辨率收缩和特征通道数量收缩来进一步精简模型；    </li></ul><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul><li>小模型，不容易过拟合，不需要太多正则化策略和数据增强策略     </li><li>参数数量少，可以为Depthwise Convolution加入一个很小的权重衰减甚至不需要权重衰减         </li></ul><hr><p>本文介绍了MobileNets v1的主要思想——Depthwise Separable Convolution以及网络的完整结构，其实这一思想也不是MobileNet首创，在MobileNets v1之前Xception就已经提出这种思路。此外，MobileNets v1还是只是一个传统的结构，而且没有像Xception一样去RELU来避免卷积后通道下降非线性单元对特征信息造成的损失，在今年Google新发的MobileNets v2就分析和缓解这一问题，并且引入了类似ResNet的shortcut设计。     </p><p>但，我们还是先一步步解剖好chuanqi305的MobileNets-SSD网络，所以暂时不就MobileNets v2的设计展开讨论，下篇文章将继续讨论<strong>目标检测框架SSD</strong>的设计——《<a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>》。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>RK3399上Tengine平台搭建</title>
      <link href="/2018/08/04/RK3399-Tengine/"/>
      <url>/2018/08/04/RK3399-Tengine/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=640866&auto=0&height=32"></iframe><h3 id="Tengine-amp-RK3399介绍"><a href="#Tengine-amp-RK3399介绍" class="headerlink" title="Tengine&amp;RK3399介绍"></a>Tengine&amp;RK3399介绍</h3><h4 id="Tengine"><a href="#Tengine" class="headerlink" title="Tengine"></a>Tengine</h4><p><a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">OADI/Tengine | github</a>       </p><blockquote><p>Tengine 是OPEN AI LAB 为嵌入式设备开发的一个轻量级、高性能并且模块化的引擎。<br>Tengine在嵌入式设备上支持CPU，GPU，DLA/NPU，DSP异构计算的计算框架，实现异构计算的调度器，基于ARM平台的高效的计算库实现，针对特定硬件平台的性能优化，动态规划计算图的内存使用，提供对于网络远端AI计算能力的访问支持，支持多级别并行，整个系统模块可拆卸，基于事件驱动的计算模型，吸取已有AI计算框架的优点，设计全新的计算图表示。    </p></blockquote><h4 id="RK3399"><a href="#RK3399" class="headerlink" title="RK3399"></a>RK3399</h4><p><a href="https://store.t-firefly.com/goods.php?id=44" target="_blank" rel="noopener">Firefly-RK3399 | Firefly</a><br><a href="http://www.t-firefly.com/doc/download/page/id/3.html" target="_blank" rel="noopener">Firefly-RK3399资料下载 | Firefly</a>     </p><blockquote><p>作为Firefly新一代的顶级开源平台，Firefly-RK3399采用了六核64位“服务器级”处理器Rockchip RK3399，拥有2GB/4GB DDR3和16G/32GB eMMC, 并新增DP 1.2、PCIe 2.1 M.2、Type-C、USB3.0 HOST等高性能数据传输和显示接口。Firefly-RK3399强大的性能配置将给VR、全景拍摄、视觉识别、服务器、3D等前沿技术带来里程碑的变革。</p></blockquote><h3 id="RK3399系统烧录"><a href="#RK3399系统烧录" class="headerlink" title="RK3399系统烧录"></a>RK3399系统烧录</h3><p>系统烧录是玩开发板重要的一步，学会如何为开发板烧录系统，就可以无所畏惧地瞎捣鼓——玩坏了大不了就重刷系统！<br>参考<a href="http://dev.t-firefly.com/thread-12613-1-1.html" target="_blank" rel="noopener">RK3399资料 | Firefly论坛</a>      </p><ol><li>下载烧录工具和系统镜像<br> <a href="https://pan.baidu.com/s/1i4AhyJV#list/path=%2F" target="_blank" rel="noopener">烧录工具下载地址 | 百度云</a><br> <a href="https://pan.baidu.com/s/1hsOCOJU#list/path=%2F" target="_blank" rel="noopener">系统镜像下载地址 | 百度云</a><br> 系统镜像选择<strong>Firefly-RK3399-ubuntu16.04-20180416112819</strong>，下载下来是一个tar压缩包，解压后得到一个img镜像文件；<br> 烧录工具的压缩包解压后包含一个<code>AndroidTool</code>的烧录工具以及一个<code>DriverAssitant</code>驱动程序；      </li><li>按照USB驱动<br> 解压<code>DriverAssitant_v4.5</code>的压缩包，运行其中的<code>Driverinstall.exe</code>程序，点击“驱动安装”，按照步骤安装即可； <center><img src="/imgs/RK3399-Tengine/系统烧录-驱动安装.png" alt="系统烧录-驱动安装"></center>      </li><li>使RK3399进入升级模式<br> 用USB线连接PC和RK3399，Type-A端接PC，Type-C端接RK3399；<br> RK3399断电，按住RECOVERY键并接上电源（或在通电情况下，按住RECOVERY然后轻按RESET重启），保持两三秒后松开RECOVERY键，此时启动PC的设备管理器（快捷键Win+X，可以找到设备管理器入口），如果看到多出一个<strong>Class for rockusb devices</strong>设备说明RK3399成功进入升级模式     </li><li>系统烧录<br> 运行<code>AndroidTool.exe</code>，切换到“升级固件”选项卡，点击“固件”并选择下载的镜像文件（扩展名为<code>.img</code>），然后点击“升级”开始烧录，右边的log会输出相关的信息，直到“下载固件成功”以及“重启设备成功”说明成功完成烧录。<br> <img src="/imgs/RK3399-Tengine/镜像烧录成功.png" alt="镜像烧录成功">     </li></ol><h3 id="RK3399远程访问"><a href="#RK3399远程访问" class="headerlink" title="RK3399远程访问"></a>RK3399远程访问</h3><p>有时候专门为RK3399外接显示器和键鼠不大方便，我们可以通过ssh或vnc来远程访问；<br>首先让RK3399连接上网络（有线或无线），然后快捷键<code>ctrl</code>+<code>alt</code>+<code>t</code>呼出终端，输入指令<code>ifconfig</code>查看当前的网络配置——       </p><p><center><img src="/imgs/RK3399-Tengine/ifconfig.png" alt="ifconfig"></center><br>其中<code>eth0</code>和<code>wlan0</code>分别是有线和无线网络的配置信息，我这里连接的是无线网，可以看到<code>wlan0</code>下有一项<code>inet addr</code>，这是设备在无线网络上的ip地址，把后边这串地址<code>192.168.50.176</code>记下来待会用得上。（如果你接的是有线网络，那么也可以在<code>eth0</code>下找到相应的<code>inet addr</code>地址）<br>推荐一个非常实用的免费远程连接工具：<a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener">MobaXterm</a>     </p><h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><p>烧录的系统镜像本身自带一个ssh服务器<code>openssh-server</code>，不需要我们额外安装。直接打开MobaXterm，点击左上角的Session<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh1.png" alt="Mobaxterm_ssh1">    </p><p>按照下图进行配置——<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh2.png" alt="Mobaxterm_ssh2">       </p><p>配置完就可以通过远程连接到RK3399的终端上——<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh3.png" alt="Mobaxterm_ssh3">      </p><p>既可以直接在PC上远程执行指令，也可以方便地在PC和RK3399之间传输文件。     </p><h4 id="vnc"><a href="#vnc" class="headerlink" title="vnc"></a>vnc</h4><p>ssh只能连接到RK3399上的纯文本模式的终端，如果你需要进一步控制RK3399的界面，可以额外安装vnc服务；<br>打开终端，刷新apt源：     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> update</code></pre><p>安装x11vnc：       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> x11vnc</code></pre><p>为vnc服务生成密码（按照提示输入密码，并写入文件）：         </p><pre class=" language-bash"><code class="language-bash">x11vnc -storepasswd</code></pre><p>添加服务：     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> vim /lib/systemd/system/x11vnc.service</code></pre><p>为x11vnc.service添加以下内容然后保存：</p><pre><code>[Unit]Description=Start x11vnc at startup.After=multi-user.target[Service]Type=simpleExecStart=/usr/bin/x11vnc -auth guess -once -loop -noxdamage -repeat -rfbauth /home/firefly/.vnc/passwd -rfbport 5900 -shared[Install]WantedBy=multi-user.target</code></pre><p>加载服务：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl daemon-reload</code></pre><p>启动服务：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">service</span> x11vnc start</code></pre><p>设置开机自启动：       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl <span class="token function">enable</span> x11vnc.service</code></pre><p>这样一来RK3399上的vnc服务就设置完毕，接下来直接用<code>MobaXterm</code>远程控制桌面；<br>和ssd一样点击左上角的Session选项，切换到vnc选项卡，如下图配置：<br><img src="/imgs/RK3399-Tengine/Mobaxterm_vnc.png" alt="Mobaxterm_vnc"><br>配置完毕后双击并输入刚刚在RK3399上设置的密码就可以远程控制桌面~~        </p><h3 id="安装Tengine"><a href="#安装Tengine" class="headerlink" title="安装Tengine"></a>安装Tengine</h3><p>RK3399的基本环境安顿好之后，接下来可以开始搭建Tengine的环境。       </p><ol><li>安装git      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span></code></pre></li><li>用git下载源码     <pre class=" language-bash"><code class="language-bash"> <span class="token function">git</span> clone https://github.com/OAID/tengine</code></pre></li><li>安装编译源码时需要依赖的包      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> apt <span class="token function">install</span> libprotobuf-dev protobuf-compiler libboost-all-dev libgoogle-glog-dev libopenblas-dev libopencv-dev</code></pre></li><li>进入Tengine目录，复制编译的配置文件      <pre class=" language-bash"><code class="language-bash"> <span class="token function">cd</span> ~/tengine <span class="token function">cp</span> makefile.config.example makefile.config</code></pre></li><li>编辑<code>makefile.config</code>文件（如果不需要修改配置，可以直接忽略这一步）      <pre class=" language-bash"><code class="language-bash"> vim makefile.config</code></pre> <em>后续需要用到MobileNet SSD网络，其中包含维度交换的<code>Permute</code>层，该层是ACL暂时不支持的，所以这里暂时不建议开启ACL支持</em>     </li><li>编译     <pre class=" language-bash"><code class="language-bash"> <span class="token function">make</span> <span class="token function">make</span> <span class="token function">install</span></code></pre></li><li>配置相关环境      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> <span class="token function">mkdir</span> -p /usr/local/AID/Tengine <span class="token function">sudo</span> <span class="token function">cp</span> -rpf ~/Tengine/install/* /usr/local/AID/Tengine <span class="token function">wget</span> ftp://ftp.openailab.net/tools/script/gen-pkg-config-pc.sh <span class="token function">chmod</span> +x ./gen-pkg-config-pc.sh <span class="token function">sudo</span> ./gen-pkg-config-pc.sh</code></pre></li></ol><h3 id="小试牛刀：运行Tengine自带的Demo"><a href="#小试牛刀：运行Tengine自带的Demo" class="headerlink" title="小试牛刀：运行Tengine自带的Demo"></a>小试牛刀：运行Tengine自带的Demo</h3><p>Tengine配置完毕，接下来我们试着运行Tengine自带的几个Demo。       </p><h4 id="分类网络SqueezeNet和MobileNet"><a href="#分类网络SqueezeNet和MobileNet" class="headerlink" title="分类网络SqueezeNet和MobileNet"></a>分类网络SqueezeNet和MobileNet</h4><ol><li>运行SqueezeNet<br> <code>./build/tests/bin/bench_sqz -r1</code>——（-r1 代表重复次数）       </li><li>运行MobileNet<br> <code>./build/tests/bin/bench_mobilenet -r1</code>——（-r1 代表重复次数）     </li></ol><p>运行后即可在终端看到输出结果。         </p><h4 id="目标检测网络MobileNet-SSD"><a href="#目标检测网络MobileNet-SSD" class="headerlink" title="目标检测网络MobileNet SSD"></a>目标检测网络MobileNet SSD</h4><p><a href="https://github.com/OAID/Tengine/tree/master/examples/mobilenet_ssd" target="_blank" rel="noopener">Mobilenet_SSD implementation with Tengine | github</a></p><p>在<code>example</code>目录下有一个<code>mobilenet_ssd</code>的子目录，一般情况下在目录执行      </p><pre class=" language-bash"><code class="language-bash">cmake <span class="token keyword">.</span><span class="token function">make</span></code></pre><p>就可以编译目录下的程序，然而……<br><img src="/imgs/RK3399-Tengine/tengine_cmake_not_found.png" alt="tengine_cmake_not_found"><br>好吧，烧录的系统上没有<code>cmake</code>，安装一下：    </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cmake</code></pre><p>不过make的时候又报了错——<br><img src="/imgs/RK3399-Tengine/tengine_headers_not_found.png" alt="tengine_headers_not_found"><br>看起来是找不到tengine的头文件，打开<code>CMakeLists.txt</code>文件瞧瞧，开头部分是这样的——     </p><pre class=" language-bash"><code class="language-bash">cmake_minimum_required <span class="token punctuation">(</span>VERSION 2.8<span class="token punctuation">)</span>project<span class="token punctuation">(</span>MSSD<span class="token punctuation">)</span>set<span class="token punctuation">(</span> INSTALL_DIR <span class="token variable">${TENGINE_DIR}</span>/install/<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_LIBS tengine<span class="token punctuation">)</span><span class="token punctuation">..</span>.</code></pre><p>好像这里引用了一个变量<code>TENGINE_DIR</code>但却没有提前指定，我们给它设置一下，变为——    </p><pre class=" language-bash"><code class="language-bash">cmake_minimum_required <span class="token punctuation">(</span>VERSION 2.8<span class="token punctuation">)</span>project<span class="token punctuation">(</span>MSSD<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_DIR /home/firefly/Tengine <span class="token punctuation">)</span>set<span class="token punctuation">(</span> INSTALL_DIR <span class="token variable">${TENGINE_DIR}</span>/install/<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_LIBS tengine<span class="token punctuation">)</span><span class="token punctuation">..</span>.</code></pre><p>再make一下，头文件是找到了，但printf好像有点问题——<br><img src="/imgs/RK3399-Tengine/tengine_printf.png" alt="tengine_printf"><br>打开源代码<code>mssd.cpp</code>，添加头文件<br><code>#include &lt;stdio.h&gt;</code><br>搜索一下<code>prinf</code>，如果<code>printf</code>前有<code>std::</code>就去掉（也就是把<code>std::printf</code>替换为<code>printf</code>），保存后再make一下……诶！通过了~~     </p><p>运行一下<br><code>./MSSD</code>     </p><p><img src="/imgs/RK3399-Tengine/tengine_model_not_found.png" alt="tengine_model_not_found"><br>ummmm没有模型文件，下载一个！<br>Tengine提供了一些训练好的模型——<a href="https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g#list/path=%2F" target="_blank" rel="noopener">Tengine_models | 百度云（提取码：57vb）</a><br>找到<code>mobilenet_ssd</code>文件夹把其中的<code>MobileNetSSD_deploy.prototxt</code>和<code>MobileNetSSD_deploy.caffemodel</code>下载下来放到<code>./models</code>目录下就行，再运行一下<code>./MSSD</code>——<br><img src="/imgs/RK3399-Tengine/tengine_mssd.png" alt="tengine_mssd"><br>没报错，有结果<del>，好了，收工</del>！     </p><p>等等，这些输出什么意思呢？      </p><ul><li>从prototxt文件里读出模型<br>  <code>proto file not specified,using /home/firefly/Tengine/models/MobileNetSSD_deploy.prototxt by default</code></li><li>从caffemodel文件里读出模型参数<br>  <code>model file not specified,using /home/firefly/Tengine/models/MobileNetSSD_deploy.caffemodel by default</code></li><li>读一张<code>ssd_dog.jpg</code>的文件作为输入<br>  <code>image file not specified,using /home/firefly/Tengine/tests/images/ssd_dog.jpg by default</code><br>  这张图片长这样：<br>  <img src="/imgs/RK3399-Tengine/tengine_mssd_input.jpg" alt="tengine_mssd_input">     </li><li>检测出了三个物体：     <pre><code>  repeat 1 times, avg time per run is 161.088 ms  detect ruesult num: 3  dog     :100%  BOX:( 138.529 , 209.238 ),( 324.026 , 541.275 )  car     :100%  BOX:( 466.138 , 72.3095 ),( 688.261 , 171.256 )  bicycle :99%  BOX:( 106.674 , 140.974 ),( 573.514 , 415.127 )</code></pre>  分别是狗、小车、自行车，用时161.088ms       </li><li>最后图片输出到了<code>save.jpg</code><br>  <code>[DETECTED IMAGE SAVED]: save.jpg</code><br>  这张图长这样：<br>  <img src="/imgs/RK3399-Tengine/tengine_mssd_output.jpg" alt="tengine_mssd_output">     </li></ul><p>啊就输入一张图片，输出检测好框好图片的结果。好没意思~改成动态检测的吧！<br>以下是修改后的源码，改动也不大，就是调用摄像头获取图片，处理完之后再输出显示（在RK3399上FPS大概为5-6）。      </p><pre class=" language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * License); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * *   http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, * software distributed under the License is distributed on an * AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY * KIND, either express or implied.  See the License for the * specific language governing permissions and limitations * under the License. */</span><span class="token comment" spellcheck="true">/* * Copyright (c) 2018, Open AI Lab * Author: chunyinglv@openailab.com */</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/imgproc/imgproc.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/highgui/highgui.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"tengine_c_api.h"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"common.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_PROTO "models/MobileNetSSD_deploy.prototxt"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_MODEL "models/MobileNetSSD_deploy.caffemodel"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_IMAGE "tests/images/ssd_dog.jpg"</span><span class="token keyword">struct</span> Box<span class="token punctuation">{</span>    <span class="token keyword">float</span> x0<span class="token punctuation">;</span>    <span class="token keyword">float</span> y0<span class="token punctuation">;</span>    <span class="token keyword">float</span> x1<span class="token punctuation">;</span>    <span class="token keyword">float</span> y1<span class="token punctuation">;</span>    <span class="token keyword">int</span> class_idx<span class="token punctuation">;</span>    <span class="token keyword">float</span> score<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// void get_input_data_ssd(std::string&amp; image_file, float* input_data, int img_h,  int img_w)</span><span class="token keyword">void</span> <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> input_data<span class="token punctuation">,</span> <span class="token keyword">int</span> img_h<span class="token punctuation">,</span>  <span class="token keyword">int</span> img_w<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// cv::Mat img = cv::imread(image_file);</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// std::cerr &lt;&lt; "Failed to read image file " &lt;&lt; image_file &lt;&lt; ".\n";</span>        std<span class="token operator">::</span>cerr <span class="token operator">&lt;&lt;</span> <span class="token string">"Failed to read image from camera.\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span><span class="token function">resize</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    img<span class="token punctuation">.</span><span class="token function">convertTo</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> CV_32FC3<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>img_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>img<span class="token punctuation">.</span>data<span class="token punctuation">;</span>    <span class="token keyword">int</span> hw <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w<span class="token punctuation">;</span>    <span class="token keyword">float</span> mean<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> img_h<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> w <span class="token operator">&lt;</span> img_w<span class="token punctuation">;</span> w<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span>            <span class="token punctuation">{</span>                input_data<span class="token punctuation">[</span>c <span class="token operator">*</span> hw <span class="token operator">+</span> h <span class="token operator">*</span> img_w <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.007843</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">*</span>img_data <span class="token operator">-</span> mean<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                img_data<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// void post_process_ssd(std::string&amp; image_file,float threshold,float* outdata,int num,std::string&amp; save_name)</span><span class="token keyword">void</span> <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span> threshold<span class="token punctuation">,</span><span class="token keyword">float</span><span class="token operator">*</span> outdata<span class="token punctuation">,</span><span class="token keyword">int</span> num<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> class_names<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"background"</span><span class="token punctuation">,</span>                            <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span>                            <span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span>                            <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span>                            <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span>                            <span class="token string">"sheep"</span><span class="token punctuation">,</span> <span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// cv::Mat img = cv::imread(image_file);</span>    <span class="token keyword">int</span> raw_h <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>height<span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_w <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>width<span class="token punctuation">;</span>    std<span class="token operator">::</span>vector<span class="token operator">&lt;</span>Box<span class="token operator">></span> boxes<span class="token punctuation">;</span>    <span class="token keyword">int</span> line_width<span class="token operator">=</span>raw_w<span class="token operator">*</span><span class="token number">0.002</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"detect ruesult num: %d \n"</span><span class="token punctuation">,</span>num<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>=</span>threshold<span class="token punctuation">)</span>        <span class="token punctuation">{</span>            Box box<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>class_idx<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>score<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            boxes<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>box<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%s\t:%.0f%%\n"</span><span class="token punctuation">,</span> class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> box<span class="token punctuation">.</span>score <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"BOX:( %g , %g ),( %g , %g )\n"</span><span class="token punctuation">,</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>x1<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y1<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        outdata<span class="token operator">+</span><span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span>boxes<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        Box box<span class="token operator">=</span>boxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x1<span class="token operator">-</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>y1<span class="token operator">-</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>line_width<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>ostringstream score_str<span class="token punctuation">;</span>        score_str<span class="token operator">&lt;&lt;</span>box<span class="token punctuation">.</span>score<span class="token punctuation">;</span>        std<span class="token operator">::</span>string label <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">string</span><span class="token punctuation">(</span>class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> score_str<span class="token punctuation">.</span><span class="token function">str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> baseLine <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span>Size label_size <span class="token operator">=</span> cv<span class="token operator">::</span><span class="token function">getTextSize</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>baseLine<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token operator">-</span> label_size<span class="token punctuation">.</span>height<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>label_size<span class="token punctuation">.</span>width<span class="token punctuation">,</span> label_size<span class="token punctuation">.</span>height <span class="token operator">+</span> baseLine<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CV_FILLED<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">putText</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> label<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">,</span>                    cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// cv::imwrite(save_name,img);</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"======================================\n";</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"[DETECTED IMAGE SAVED]:\t"&lt;&lt; save_name&lt;&lt;"\n";</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"======================================\n";</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> std<span class="token operator">::</span>string root_path <span class="token operator">=</span> <span class="token function">get_root_path</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>string proto_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string model_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string image_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string save_name<span class="token operator">=</span><span class="token string">"save.jpg"</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> res<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token punctuation">(</span> res<span class="token operator">=</span><span class="token function">getopt</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">,</span><span class="token string">"p:m:i:h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">switch</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token string">'p'</span><span class="token operator">:</span>                proto_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'m'</span><span class="token operator">:</span>                model_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'i'</span><span class="token operator">:</span>                image_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'h'</span><span class="token operator">:</span>                std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"[Usage]: "</span> <span class="token operator">&lt;&lt;</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" [-h]\n"</span>                          <span class="token operator">&lt;&lt;</span> <span class="token string">"   [-p proto_file] [-m model_file] [-i image_file]\n"</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token keyword">default</span><span class="token operator">:</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>model_name <span class="token operator">=</span> <span class="token string">"mssd_300"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>proto_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        proto_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_PROTO<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"proto file not specified,using "</span><span class="token operator">&lt;&lt;</span>proto_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>model_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        model_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_MODEL<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"model file not specified,using "</span><span class="token operator">&lt;&lt;</span>model_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>image_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        image_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_IMAGE<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"image file not specified,using "</span><span class="token operator">&lt;&lt;</span>image_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// init tengine</span>    <span class="token function">init_tengine_library</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">request_tengine_version</span><span class="token punctuation">(</span><span class="token string">"0.1"</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">load_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> <span class="token string">"caffe"</span><span class="token punctuation">,</span> proto_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"load model done!\n"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// create graph</span>    graph_t graph <span class="token operator">=</span> <span class="token function">create_runtime_graph</span><span class="token punctuation">(</span><span class="token string">"graph"</span><span class="token punctuation">,</span> model_name<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_graph_valid</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"create graph0 failed\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// input</span>    <span class="token keyword">int</span> img_h <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_w <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_size <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>input_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> img_size<span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span>VideoCapture <span class="token function">capture</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_WIDTH<span class="token punctuation">,</span> <span class="token number">1920</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_HEIGHT<span class="token punctuation">,</span> <span class="token number">1080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span>Mat frame<span class="token punctuation">;</span>    <span class="token keyword">int</span> node_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> tensor_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    tensor_t input_tensor <span class="token operator">=</span> <span class="token function">get_graph_input_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> node_idx<span class="token punctuation">,</span> tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_tensor_valid</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Get input node failed : node_idx: %d, tensor_idx: %d\n"</span><span class="token punctuation">,</span>node_idx<span class="token punctuation">,</span>tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> dims<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">set_tensor_shape</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dims<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">prerun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> repeat_count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>repeat <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">"REPEAT_COUNT"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>repeat<span class="token punctuation">)</span>        repeat_count <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">strtoul</span><span class="token punctuation">(</span>repeat<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>outdata<span class="token punctuation">;</span>    <span class="token keyword">int</span> out_dim<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">struct</span> timeval t0<span class="token punctuation">,</span> t1<span class="token punctuation">;</span>        <span class="token keyword">float</span> total_time <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>f<span class="token punctuation">;</span>        capture <span class="token operator">>></span> frame<span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> repeat_count<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span>  img_w<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t0<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">set_tensor_buffer</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">run_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">float</span> mytime <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t1<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span>t0<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t0<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span><span class="token punctuation">;</span>            total_time <span class="token operator">+</span><span class="token operator">=</span> mytime<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"--------------------------------------\n"</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"repeat "</span> <span class="token operator">&lt;&lt;</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" times, avg time per run is "</span> <span class="token operator">&lt;&lt;</span> total_time <span class="token operator">/</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" ms\n"</span><span class="token punctuation">;</span>        tensor_t out_tensor <span class="token operator">=</span> <span class="token function">get_graph_output_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//"detection_out");</span>        <span class="token function">get_tensor_shape</span><span class="token punctuation">(</span> out_tensor<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        outdata <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">get_tensor_buffer</span><span class="token punctuation">(</span>out_tensor<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> num<span class="token operator">=</span>out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">float</span> show_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">;</span>        <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> show_threshold<span class="token punctuation">,</span> outdata<span class="token punctuation">,</span> num<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> cv<span class="token operator">::</span><span class="token function">waitKey</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'q'</span> <span class="token punctuation">)</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">postrun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">free</span><span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">destroy_runtime_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">remove_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>报错，<br><img src="/imgs/RK3399-Tengine/opengl_error.png" alt="opengl_error"><br>烧录的系统没带opengl，没法调用opencv的imshow，树莓派也有一样的问题，安装 <code>libgl1-mesa-dri</code> 然后重启板子就能解决。       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libgl1-mesa-dri<span class="token function">sudo</span> <span class="token function">reboot</span></code></pre><hr><p>本篇文章中我们在RK3399上搭建了Tengine平台并试运行了MobileNet SSD网络，接下来我们将细致解析MobileNets分类网络和SSD目标检测框架，最后进一步解析源码作者<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合起来的。      </p><ul><li><a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>       </li><li><a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>      </li><li><a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>     </li></ul><p>随后还将结合实际的使用场景，尝试对MobileNet-SSD的网络结构以及训练参数细节进行分析优化~      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RK3399 </tag>
            
            <tag> Tengine </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>漫谈池化层</title>
      <link href="/2018/05/07/%E6%BC%AB%E8%B0%88%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
      <url>/2018/05/07/%E6%BC%AB%E8%B0%88%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=530995191&auto=0&height=32"></iframe><br>BGM：<strong>《革命机valvrave》ED1</strong><br>有点小high，建议先调低音量再播放；<br>还有个现场版，像是中二版的凤凰传奇hhhhh——<a href="https://www.bilibili.com/video/av20638023" target="_blank" rel="noopener">TMRevolution×水樹奈々 - Preserved Roses_革命デュアリズム_ | bilibili</a><br><em>话说水树奈奈是不是有点像隔壁控制学院的ly哈？</em><br>不过现场版的调音好像有问题，重低音效果有点差~     </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li><li>《<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">Deep Learning 深度学习(2017)</a>》        </li></ol><h3 id="基本的池化操作"><a href="#基本的池化操作" class="headerlink" title="基本的池化操作"></a>基本的池化操作</h3><p>简单的聚合操作，取均值、取最值等，分别称为<strong>平均池化</strong>（Average Pooling）和<strong>最大池化</strong>（Max Pooling）；<br>一般使池化核的大小与步长相等，不重叠、全覆盖地进行降采样；      </p><p>平均池化和最大池化的tensorflow实现参见 <a href="/2018/04/18/CNN/#池化层">卷积神经网络CNN / tensorflow实现 / 池化层 | Hey~YaHei!</a></p><h4 id="池化的意义"><a href="#池化的意义" class="headerlink" title="池化的意义"></a>池化的意义</h4><p>既对数据进行降采样（down-sampling）操作，又可以用p范数（p-norm）作非线性映射的“卷积”<br>$$p范数：||A||_p = (\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^p)^{1/p}, p&gt;=1$$<br><em>当 $p \to \infty$ 时即为最大池化</em>      </p><p>具体作用为：       </p><ol><li>特征不变性<br> 使模型更关注包含一定的自由度，能容忍特征微小的位移     </li><li>特征降维<br> 降采样使后续操作的计算量得到减少      </li><li>一定程度防止过拟合     </li></ol><h4 id="平均池化和最大池化的区别"><a href="#平均池化和最大池化的区别" class="headerlink" title="平均池化和最大池化的区别"></a>平均池化和最大池化的区别</h4><p>论文 <a href="https://cs.nyu.edu/~ylan/files/publi/boureau-cvpr-10.pdf" target="_blank" rel="noopener">Learning Mid-Level Features For Recognition(2010)</a> 第5节      </p><p>CNN特征提取的误差主要来自两个方面：    </p><ol><li>邻域大小受限造成的估计值<strong>方差增大</strong><br> 平均池化能有效减少该误差，更多的保留图像的背景信息；<br> 均匀采样的方差只有总体方差的 $\frac{1}{N}$；<br> 但如果模型中杂波方差较大（也即第二个误差明显），最后输出类别的概率分布将出现明显的混叠，导致分类准确率下降     </li><li>卷积层参数误差造成估计值<strong>均值偏移</strong><br> 最大池化能有效减少该误差，更多的保留图像纹理信息；<br> 最大值采样的方差为总体方差的 $\frac{1}{\sqrt{log(N)}}$ （推导过程参见论文），受第一种误差影响较大；    </li></ol><h3 id="改进的池化操作"><a href="#改进的池化操作" class="headerlink" title="改进的池化操作"></a>改进的池化操作</h3><h4 id="重叠池化（Overlapping-Pooling）"><a href="#重叠池化（Overlapping-Pooling）" class="headerlink" title="重叠池化（Overlapping Pooling）"></a>重叠池化（Overlapping Pooling）</h4><p>论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks(2012)</a> 第3.4节——      </p><blockquote><p>3.4 Overlapping Pooling<br>Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z × z centered at the location of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s &lt; z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s = 2; z = 2, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.       </p></blockquote><p><em>卧槽，这篇论文居然有1.3万的被引？！？！？！？</em><br>思路很简单，让池化的步长略小于池化核的大小，使相邻的池化图像块出现一定程度的重叠；<br>据论文介绍，相比于大小2x2、步长2的传统池化操作，3x3、步长2的重叠池化操作使他们的模型的分类错误率top-1和top-5分别降低0.4%和0.3%；<br>并且他们通过实验观察到（有点主观），重叠池化具有一定的正则化作用；         </p><h4 id="随机池化（Stochastic-Pooling）"><a href="#随机池化（Stochastic-Pooling）" class="headerlink" title="随机池化（Stochastic Pooling）"></a>随机池化（Stochastic Pooling）</h4><p>论文：<a href="https://arxiv.org/pdf/1301.3557.pdf" target="_blank" rel="noopener">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks(2013)</a>     </p><p>按一定概率随机选取其中一个元素，介于平均池化和最大池化之间，并且受dropout启发，具有更好的正则化效果；<br>可以看作是，在输入图片的许多副本（有一些小的局部变形）上进行标准的最大池化操作；      </p><h5 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h5><ul><li>训练（随机采样）      <ol><li>归一化卷积层的输出计算，并以此作为每个元素的概率<br> $$ p_i = \frac{a_i}{\sum_{k \in R_j} a_k} $$<br> 其中，<br> $p_i$ 是第i个输出的采样概率，<br> $a_i$ 是前层（卷积层）的第i个输出，<br> $R_j$ 是池化核扫过的第j个区域      </li><li>用基于概率 $p$ 的多项分布的随机数对池化核扫过的区域进行随机采样作为区域池化的输出<br> $$ s_j = a_l \text{ where } l~P(p_1, …, p_{|R_j|})  $$<br> 其中，<br> $s_j$ 是第j个区域的输出，<br> $a_l$ 是随机采样抽中的前层的一个输出        </li><li>反向传播时，将抽中的元素用于梯度计算       </li></ol></li><li>预测（加权平均）<br>  $$ s_j = \sum_{i \in R_j} p_i a_i $$<br>  依旧计算每个前层输出的概率，以概率为权重，对池化核扫过的区域内每个输出作加权平均作为区域的输出    </li></ul><h5 id="分析比较"><a href="#分析比较" class="headerlink" title="分析比较"></a>分析比较</h5><ul><li>与最大池化相比（主要体现在训练上）<br>  最大池化仅仅保留区域内的最大值；而随机池化以各元素值的大小为概率，使非最大值的元素也有一定概率参与训练    </li><li>与平均池化相比（主要体现在预测上）<br>  平均池化不考虑权重，直接对区域内元素作算术平均；而随机池化采用加权平均，经过实验可以得知相对于平均池化有比较大的提升     </li><li>与dropout相比<br>  同样在训练中产生了很多不同的模型；<br>  考虑包含n个大小为d的区域的随机池化，其产生的可能模型有 $n^d$ 种，其中d通常为 $10^4 - 10^6$，且n通常取 $4, 9, 16$（相比之下，dropout中 $n=2$，因为只有激活/失活两种状态）；<br>  再者，dropout似乎不适宜用于卷积层，而随机池化可以；     </li></ul><h5 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h5><ul><li>准确度<br>  根据论文，相比于最大池化、平均池化、dropout，随机池化在CIFAR-10、MNIST、CIFAR-100、SVHN数据集上都取得比较明显的提升     </li><li>数据集规模<br>  论文以MNIST、CIFAR-10数据集为例，随机抽取数据集的子集对不同池化操作进行比较，如下图所示（左MNIST，右CIFAR-10）：<br>  <img src="/Handson-ML/SP_Reduced_Traing_Set_Size.png" alt="SP_Reduced_Traing_Set_Size">      <ul><li>数据集很小时，随机池化与最大池化效果相当，平均池化效果明显要差一些；     </li><li>随着数据集增大，随机池化与最大池化的效果几乎一直比平均池化要好（CIFAR-10在数据集比较大时，平均池化和最大池化效果相当）；      </li><li>随着数据集增大，随机池化与平均池化在MNIST上表现相当；在CIFAR-10上表现更优      </li></ul></li><li>不同训练、预测操作的组合<br>  论文以CIFAR-10数据集为例，对不同的池化方式在训练、预测上的不同组合进行比较，如下图所示：<br>  <img src="/Handson-ML/SP_Method_Combination.png" alt="SP_Method_Combination"><br>  可以看到，<strong>随机池化训练 + 加权平均预测</strong> 是最佳组合方式     </li><li>网络输出可视化<br>  作者用反卷积技术对网络输出进行可视化，以此比较最大池化、平均池化、随机池化以及训练过程中使用多项分布抽样、均匀抽样的不同表现；<br>  如下图所示（FF、UN分别表示多项分布抽样、均匀分布抽样的随机池化）：<br>  <img src="/Handson-ML/SP_Visualization.png" alt="SP_Visualization"><br>  可以看到，最大池化可以比较好的保留图片中的纹理信息，而平均池化已经失去可辨识的纹理信息了；<br>  多项分布抽样的随机池化表现良好，甚至比最大池化保留更多的纹理信息；<br>  随着更多的FF被替换为UN，纹理信息逐步丧失，而且越靠近底层的随机池化影响越严重；     </li></ul><h4 id="空间金字塔池化（Spatial-Pyramid-Pooling-SPP）"><a href="#空间金字塔池化（Spatial-Pyramid-Pooling-SPP）" class="headerlink" title="空间金字塔池化（Spatial Pyramid Pooling, SPP）"></a>空间金字塔池化（Spatial Pyramid Pooling, SPP）</h4><p>论文：<a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(2015)</a>     </p><p><img src="/Handson-ML/Muti-scale_CNN.png" alt="Muti-scale_CNN">     </p><p>传统CNN中，输入的图片数据必须是特定尺度的，以确保进入FC或SVM前的尺寸固定（连接矩阵的大小固定）；<br>因而往往在输入数据前需要对图片进行尺寸处理，比如裁剪、缩放；<br>而SPP操作不仅可以支持多尺度的图片输入，而且多方面特征提取更具鲁棒性，在目标检测任务上能够明显提高精度；      </p><p><img src="/Handson-ML/Spatial_Pyramid_Pooling.png" alt="Spatial_Pyramid_Pooling">     </p><ul><li><code>convolutional layers</code> 包含卷积、激活函数、池化操作，假设最后一层输出 $N_f$ 个 $a \times b$的特征图<br>  论文中假设 $N_f = 256, a = b = 13$       </li><li><code>spatial pyramid pooling layers</code> 是一个典型三层SPP，包含三组动态参数（核大小、步长由 $a$ 和 $b$ 决定）的最大池化操作<br>  <em>这里的池化都是不padding、允许重叠的</em>       <ul><li>第一组（最右）为每个特征图生成一个1x1的池化图<br>  换言之，采用核大小为 $a \times b$ 、步长为 $[a, b]$ 的最大池化    </li><li>第二组（中间）为每个特征图生成一个2x2的池化图<br>  换言之，采用核大小为 $\lceil a/2 \rceil \times \lceil b/2 \rceil$、步长为 $\lfloor a/2 \rfloor, \lfloor b/2 \rfloor$ 的最大池化    </li><li>第三组（最左）为每个特征图生成一个4x4的池化图<br>  换言之，采用核大小为 $\lceil a/4 \rceil \times \lceil b/4 \rceil$、步长为 $\lfloor a/4 \rfloor, \lfloor b/4 \rfloor$ 的最大池化    </li><li>最后将三组池化图展开拼接起来，得到一个 $(1+4+16) \times N_f$ 的固定大小输出<br>  注意到这里的N是由最后一个卷积层的参数之一，也就是一个网络超参数，不受输入数据的大小影响    </li></ul></li><li>最后跟常规操作相同，将SSP的输出展平之后接FC<br>  比如论文中的fc6的大小为4096       </li></ul><p>tensorflow实现：<br>参考 <a href="http://baijiahao.baidu.com/s?id=1584420462327782297&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">深度学习中空间金字塔池化的TensorFlow实现 | 百度百家号</a>     </p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">spp_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_<span class="token punctuation">,</span> levels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'SPP_layer'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''Multiple Level SPP layer. Works for levels=[1, 2, 3, 6].'''</span>     shape <span class="token operator">=</span> input_<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>         pool_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>         <span class="token keyword">for</span> l <span class="token keyword">in</span> levels<span class="token punctuation">:</span>             <span class="token comment" spellcheck="true"># 计算池化核大小</span>            kernel_size <span class="token operator">=</span> <span class="token punctuation">[</span>                <span class="token number">1</span><span class="token punctuation">,</span>      <span class="token comment" spellcheck="true"># batch</span>                np<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># row</span>                np<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># col</span>                <span class="token number">1</span>       <span class="token comment" spellcheck="true"># channel</span>            <span class="token punctuation">]</span>            pool_strides <span class="token operator">=</span> <span class="token punctuation">[</span>                <span class="token number">1</span><span class="token punctuation">,</span>                 np<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>                 np<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>                 <span class="token number">1</span>            <span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># 最大池化</span>            pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>input_<span class="token punctuation">,</span> ksize<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> strides<span class="token operator">=</span>pool_strides<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>             <span class="token comment" spellcheck="true"># print("Pool Level {:}: shape {:}".format(l, pool.get_shape().as_list())) </span>            h <span class="token operator">=</span> int<span class="token punctuation">(</span>pool<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             w <span class="token operator">=</span> int<span class="token punctuation">(</span>pool<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             d <span class="token operator">=</span> int<span class="token punctuation">(</span>pool<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             <span class="token comment" spellcheck="true"># 池化结果的二维池化块拉成一维</span>            pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>pool<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>pool<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> h <span class="token operator">*</span> w<span class="token punctuation">,</span> d<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             pool_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pool<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 将几个一维池化块拼接得到最终的SPP表示</span>        spp_pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>pool_outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>         <span class="token keyword">return</span> spp_pool</code></pre><h3 id="池化不是必要操作"><a href="#池化不是必要操作" class="headerlink" title="池化不是必要操作"></a>池化不是必要操作</h3><p>比如<strong>Stride Convolutional Layer</strong>可以代替池化层降采样，得到一个只含卷积操作的网络<br>论文：<a href="https://arxiv.org/pdf/1412.6806.pdf" target="_blank" rel="noopener">Striving for Simplicity: The All Convolutional Net(2015)</a>    </p><p>甚至也有论文表示CNN对微小平移和变形的稳定性与池化层无关，<br>参考论文 <a href="https://arxiv.org/pdf/1804.04438.pdf" target="_blank" rel="noopener">Learned Deformation Stability in Convolutional Neural Networks(2018)</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>失眠者自助指南</title>
      <link href="/2018/05/04/%E5%A4%B1%E7%9C%A0%E8%80%85%E8%87%AA%E5%8A%A9%E6%8C%87%E5%8D%97/"/>
      <url>/2018/05/04/%E5%A4%B1%E7%9C%A0%E8%80%85%E8%87%AA%E5%8A%A9%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=519935307&auto=0&height=32"></iframe><br>BGM：<strong>《Just Because!》ED</strong>      </p><hr><p>啊最近又失眠，脑阔疼 (◦`~´◦)<br>从去年年中就开始隔三岔五失个眠，高三的时候也是如此，大概是个易失眠体质了。<br>有的人会安慰失眠的人说“别老胡思乱想就好”，<br>那这跟安慰抑郁症患者说“看开点就好”或者安慰感冒患者“别流鼻涕就好”没啥区别吧，<br>感冒患者也不想流鼻涕，抑郁症患者也想看开啊，我又不是自己想要胡思乱想。<br>说出来你可能不信，是它们先动的手！      </p><p>嘛~<strong><em>The only person that can save you, is you.</em></strong><br>那标题就叫“失眠者自助指南”吧哈哈哈哈哈哈      </p><p>今天睡醒昏昏沉沉的，丝毫不想学习；<br>周六闲来无趣唠唠高三、大四跟“失眠之魔”斗智斗勇的两三事儿；<br>（好像是我第一次在博客上发无关技术、无关笔记的东西？）     </p><p>最常见的办法就是<strong>喝牛奶</strong>，牛奶据说确实含有镇静成份，比如色氨酸。<br>但也有说法是，大脑与血液之间存在“血脑屏障”，大脑所能从牛奶吸收的色氨酸是极其有限的。<br>不过咧，皮肤温度上升确实可以有效地助眠，所以喝杯<strong>热牛奶</strong>多多少少对睡眠还是有帮助的。<br>由此看来，其实<strong>洗热水澡</strong>或者<strong>泡热水脚</strong>的助眠效果应该会更好。<br>可参考 <a href="https://www.zhihu.com/question/23648474/answer/25306001" target="_blank" rel="noopener">牛奶真的助眠么？ | 知乎</a><br>从我个人来看，喝热牛奶几乎对我的睡眠毫无帮助，甚至可能会出现反作用——尤其是在我肠胃不好的时候，难以消化牛奶，对肠胃造成负担。但空腹确实会影响睡眠，睡前<strong>喝热粥</strong>效果会更好，一方面防止空腹，另一方面吃点热食有助睡眠且粥对肠胃的负担要小得多；<br>睡前洗澡需要等头发干才能入睡（<a href="https://zhidao.baidu.com/question/5924661.html" target="_blank" rel="noopener">晚上头发不干就睡觉有什么坏处 | 百度知道</a>），等你头发干了皮肤温度也降得差不多啦，助眠效果甚微；<br>泡热水脚还不错，每次烫完脚上床睡觉挺舒服的，不过偶尔泡太长好像反而不利于睡眠？ummm经验之谈，不知道是不是个例。<br>也可以试试换个枕头，抬高颈部，比如买个护颈枕或者糖果枕——睡眠过程中<strong>头部后仰</strong>（类似做人工呼吸时候患者的状态），一方面可以预防颈椎病，另一方面有助于打开气道，使呼吸顺畅（据说还能防止打鼾）。可能起初会觉得这种枕头不舒服，但是习惯之后会发现睡眠质量有所提升（如果确实习惯不了，那还不如换回你原来的枕头算了）    </p><p>还有一点从《自控力》一书习得的小技巧，这是一种<strong>冥想</strong>技巧，原本是用来训练大脑，扩大“意志力储备”的，不过我发现这有助于把思维引导到“无意义”的方向上，避免胡思乱想。原文如下：    </p><blockquote><p>专心呼吸是一种简单有效的冥想技巧，它不但能训练大脑，还能增强意志力。它能减轻你的压力，指导大脑处理内在的干扰（比如冲动、担忧、欲望）和外在的诱惑（比如声音、画面、气味）。新研究表明，定期的思维训练能帮人戒烟、减肥、戒毒、保持清醒。无论你“要做”和“不要”的是什么，这种5分钟冥想都有助于你增强意志力。<br>让我们开始吧。<br>1．原地不动，安静坐好。<br>坐在椅子上，双脚平放在地上，或盘腿坐在垫子上。背挺直，双手放在膝盖上。冥想时一定不能烦躁，这是自控力的基本保证。如果你想挠痒的话，可以调整一下胳膊的位置，腿交叉或伸直，看自己是否有冲动但能克制。简单的静坐对于意志力的冥想训练至关重要。你将学会，不再屈服于大脑和身体产生的冲动。<br>2．注意你的呼吸。<br>闭上眼睛。要是怕睡着，你可以盯着某处看，比如盯着一面白墙，但不要看家庭购物频道。注意你的呼吸。吸气时在脑海中默念“吸”，呼气时在脑海中默念“呼”。当你发现自己有点走神的时候，重新将注意力集中到呼吸上。这种反复的注意力训练，能让前额皮质开启高速模式，让大脑中处理压力和冲动的区域更加稳定。<br>3．感受呼吸，弄清自己是怎么走神的。<br>几分钟后，你就可以不再默念“呼”、“吸”了。试着专注于呼吸本身。你会注意到空气从鼻子和嘴巴进入和呼出的感觉，感觉到吸气时胸腹部的扩张和呼气时胸腹部的收缩。不再默念“呼”、“吸”后，你可能更容易走神。像之前一样，当你发现自己在想别的事情时，重新将注意力集中到呼吸上。如果你觉得很难重新集中注意力，就在心里多默念几遍“呼”和“吸”。这部分的训练能锻炼你的自我意识和自控能力。<br>刚开始的时候，你每天锻炼5分钟就行。习惯成自然之后，请试着每天做10 15分钟。如果你觉得有负担，那就减少到5分钟。每天做比较短的训练，也比把比较长的训练拖到明天好。这样，你每天都会有一段固定的时间冥想，比如早晨洗澡之前。如果你做不到，可以对时间进行适当的调整。</p></blockquote><p>不需要非坐正凝视冥想，稍作变通，平躺在床上，闭上眼睛，<strong>专注呼吸</strong>即可。<br>这是我高三时候用过的最有效的助眠方法之一，一开始就像《自控力》所说，很快思维就会跑偏，但经过几次的“训练”，专注呼吸的持续时间会逐渐增加，到最后可以避免自己胡思乱想，从而达到助眠的目的。     </p><p>与专注呼吸的冥想法类似的，之前看过某本暗示心理学的书之后自己提炼出另外一个<strong>自我暗示</strong>或者说是自我催眠、自我欺诈的助眠方法。<br>首先你需要有一个点状光源，不能太刺眼，暖色调的柔光灯泡为佳。平躺在床上，放松一下身心（可以结合专注呼吸的方法），然后眼睛盯着准备好的点光源，不断暗示自己“越来越困”，并且眼睛缓缓闭上（很慢很慢的合上，这像是一种欺诈，为之前的暗示佐证自己确实“很困”）。<br>讲真，这种方法挺好用的，但有一个条件限制——需要你开着灯，开一晚上灯（如果你的灯能定时关闭那就更棒了）。平时都住在寝室的我哪有机会实现这样的条件哇，怕是要被室友打死。         </p><p>也是从心理学书上学到的，还有另外一个相对治本的办法——<strong>运动</strong>。       </p><ul><li>睡前运动算是一种<strong>疲劳助眠</strong>，“疲劳”有种和“困”类似的感觉，而且如前所述，运动提升皮肤温度也有助于睡眠，不过不宜做剧烈运动，这个……可能会做噩梦；<br>  之前试过睡前做俯卧撑，emmm小有效果吧    </li><li>日常的运动有助于<strong>减压</strong>，某本书说，散步是最好的减压运动，其次是慢跑<br>  很感谢yang高三的时候每天傍晚吃完饭都拉我到操场上散半小时步（哈，好怀念高中的日子）      </li></ul><p>也有一些简单粗暴的助眠方法，比如<strong>酒精助眠</strong>了解一下~<br><!--*然而，这些方法到大四的时候似乎都不大管用*       于是开始尝试点简单粗暴的助眠方法，**酒精助眠**了解一下~      曹孟德说“何以解忧，唯有杜康”，杜康是怎么解忧的我不知道，或许是文化影响产生的安慰效应，或许是通过麻痹大脑。    酒精助眠也不是说就把自己喝醉，只是喝到微醺，半昏半沉，头脑一定程度的麻痹，以这种状态入睡。       我喝的是啤酒，不同啤酒有不同的度数，像雪花青岛只有2.x度、百威3.x度、蓝带4.x接近5度、前几天喝的奥丁格甚至有8.9度的特度啤酒。酒精摄入少了，就达不到助眠效果；但酒精摄入过多，入睡是舒服了，第二天起床怕是要头疼。另外啤酒利尿，喝完啤酒要留点缓冲时间，排完尿再上床，防止半夜憋尿憋醒（酒醒了重新入睡未必有那么顺利）。每个人的酒量是不同的，需要自己一点点尝试，比如我自己的话，喝一听300ml的蓝带就恰到好处。-      还好大四失眠不像高三频繁，只是间歇性的，每次开始出现失眠，连着三五天睡前喝个小酒，然后美滋滋的入睡；    当身体习惯了这种“按时入睡”的感觉，失眠症状就会暂时解除。-->      </p><p>前阵子在知乎上看到一种很专业的入睡技巧，比较复杂，我还没试过——<br><a href="https://zhuanlan.zhihu.com/p/34952593" target="_blank" rel="noopener">如何在 2 分钟内入睡（二战时期美国飞行员训练法）| 知乎</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>目标函数</title>
      <link href="/2018/05/03/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/"/>
      <url>/2018/05/03/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=529814551&auto=0&height=32"></iframe><br>BGM：<strong>《刻刻》ED</strong><br>小哥哥唱歌听起来像是有点绕舌？      </p><hr><p>参考：《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》Chap9      </p><p>目标函数（target function）、损失函数（loss function）、代价函数（cost function）是一个东西~     </p><h3 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h3><p>记某分类任务共 $N$ 个训练样本，<br>网络最后的分类层第i个样本的输入特征为 $x_i$，<br>真实标记为 $y_i \in {1,2,…,C}$ （C为类别总数），<br>网络最终输出预测结果（指示每种分类的可能性） $h = (h_1, h_2, …, h_C)^T$       </p><h4 id="交叉熵（cross-entropy）"><a href="#交叉熵（cross-entropy）" class="headerlink" title="交叉熵（cross entropy）"></a>交叉熵（cross entropy）</h4><p>又称Sotfmax损失函数，目前分类任务最常用的损失函数；<br>用指数变换的形式，将网络输出转换成概率——<br>$$ L_{cross-entropy-loss} = L_{softmax-loss} = -\frac{1}{N} \sum_{i=1}^N log( \frac{e^{h_{y_i}}}{\sum_{j=1}^C e^{h_j}} ) $$     </p><h4 id="合页（hinge）"><a href="#合页（hinge）" class="headerlink" title="合页（hinge）"></a>合页（hinge）</h4><p>主要在SVM中广泛使用，有时也用于神经网络模型；<br>设计理念为“对错误越大的样本施加越严重的惩罚”；<br>一般在分类任务中，交叉熵效果要略优于合页<br>$$ L_{hinge-loss} = \frac{1}{N} \sum_{i=1}^N max(0, 1-h_{y_i}) $$     </p><h4 id="坡道（ramp）"><a href="#坡道（ramp）" class="headerlink" title="坡道（ramp）"></a>坡道（ramp）</h4><p>论文：<a href="http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=39D2D095B2A9E803DB744CE374E869EE?doi=10.1.1.127.2310&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Trading convexity for scalability(2006)</a><br>合页损失函数对噪声的抵抗能力较差，非凸损失函数的引入可以很好的解决这个问题；<br>而坡道损失函数、Tukey’s biweight损失函数分别是分类任务、回归任务中非凸损失函数的代表，又称“鲁棒损失函数”；<br>它们在误差较大的区域进行截断，使得较大的误差不会大程度地影响整个误差函数；<br>但其非凸性质在传统机器学习中难以优化，但得益于神经网络模型的训练机制，这点非凸性质不成问题：<br>$$ L_{ramp-loss} = L_{hinge-loss} - \frac{1}{N} \sum_{i=1}^N max(0, s-h_{y_i}) $$<br>展开为：<br>$$ L_{ramp-loss} = \frac{1}{N} \sum_{i=1}^N ( max(0, 1-h_{y_i}) - max(0, s-h_{y_i}) ) $$<br>其中，s指定了截断点的位置，如下图所示（s=-0.5）——<br><img src="/Handson-ML/ramp_loss.png" alt="ramp_loss">       </p><p>此外，论文 <a href="http://www.unc.edu/~yfliu/papers/rsvm.pdf" target="_blank" rel="noopener">Robust Truncated Hinge Loss Support Vector Machines(2007)</a> 经过理论推导指出，一般设置 $s = - \frac{1}{C-1}$     </p><h4 id="交叉熵的变体"><a href="#交叉熵的变体" class="headerlink" title="交叉熵的变体"></a>交叉熵的变体</h4><p>基于交叉熵设计的新型损失函数，考虑了增加类间距离、减少类内差异等不同要素，提升网络学习特征的判别能力；       </p><p><strong>大间隔交叉熵</strong>（large-margin softmax）：<br>论文：<a href="https://arxiv.org/pdf/1612.02295.pdf" target="_blank" rel="noopener">Large-Margin Softmax Loss for Convolutional Neural Networks(2017)</a>         </p><p>首先考虑传统的交叉熵损失函数，根据 $h=W^T x_i$ 将其展开为——<br>$$ L_{softmax-loss} = -\frac{1}{N} \sum_{i=1}^N log( \frac{ e^{ W^T_{y_i} x_i } }{ \sum_{j=1}^C e^{ W^T_j x_i } } ) $$<br>根据内积定义进一步展开为——<br>$$ L_{softmax-loss} = -\frac{1}{N} \sum_{i=1}^N log( \frac{ e^{ ||W_{y_i}|| ||x_i|| cos(\theta_{y_i}) } }{ \sum_{j=1}^C e^{ ||W_j|| ||x_i|| cos(\theta_j) } } ) $$<br>比如二分类，传统交叉熵使得学到的参数满足 $W_1^T x_i &gt; W_2^T x_i$，也即<br>$$ ||W_1|| ||x_i|| cos(\theta_1) &gt; ||W_2|| ||x_i|| cos(\theta_2) $$<br>大间隔交叉熵则在上式的基础上，引入超参数m拉开两个分类的差距，即——<br>$$ ||W_1|| ||x_i|| cos(m \theta_1) &gt; ||W_2|| ||x_i|| cos(\theta_2), 0&lt;= \theta_1 &lt;= \frac{\pi}{m} 且 m \in N^+ $$<br>m起到控制间隔大小的作用，m越大，类间间隔（即差距）越大，类间分类的置信度越大；当 $m=1$ 时将退化为传统交叉熵       </p><p>完整定义：<br>$$ L_{large-margin-softmax-loss} = -\frac{1}{N} log( \frac{ e^{ ||W_{y_i}|| ||x_i|| \phi(\theta_{y_i}) } }{ e^{ ||W_{y_i}|| ||x_i|| \phi(\theta_{y_i}) + \sum_{j \ne y_i} e^{ ||W_j|| ||x_i|| cos(\theta_j) } } } ) $$<br>其中，<br>$$<br>\begin{equation}<br>\phi(\theta) = \left\{<br>\begin{aligned}<br>&amp; cos(m \theta),  &amp; 0 &lt;= \theta &lt;= \frac{\pi}{m}\\<br>&amp; D(\theta),      &amp; \frac{\pi}{m} &lt; \theta &lt; \pi<br>\end{aligned}<br>\right.<br>\end{equation}<br>$$<br>式中 $D(\theta)$ 只需满足“<strong>单调递减</strong>”，且 $D(\frac{\pi}{m}) = cos(\frac{\pi}{m})$ 即可；<br>为了简化网络前向和反向计算，论文推荐了一种具体的形式如下——<br>$$ \phi(\theta) = (-1)^k cos(m \theta) - 2k, \theta \in [\frac{k \pi}{m}, \frac{(k+1)\pi}{m}] $$<br>其中，k为整数且满足 $k \in [0, m-1]$      </p><p>二分类情况下的比较——<br><img src="/Handson-ML/large-margin_softmax_loss.png" alt="large-margin-softmax-loss">     </p><p>此时训练目标要比传统交叉熵损失函数更加困难；<br>不过好处是可以起到防止模型过拟合的作用；<br>在分类性能方面，大间隔交叉熵要优于传统交叉熵和合页；         </p><p><strong>中心损失函数</strong>（center loss function）：<br>论文：<a href="https://www.dl.icdst.org/pdfs/files1/c8edf1770d6ac7f5415f8daef7fb9bce.pdf" target="_blank" rel="noopener">A Discriminative Feature Learning Approach for Deep Face Recognition(2016)</a>       </p><p>中心损失函数定义：<br>$$ L_{center-loss} = \frac{1}{2} \sum_{i=1}^N ||x_i c_{y_i} ||^2_2 $$<br>其中， $c_{y_i}$ 为第 $y_i$ 类所有深度特征的均值（中心）；<br>这将迫使样本与中心不要距离太远，否则加大惩罚；      </p><p>中心损失函数只考虑了类内差异，所以通常要与考虑类间举例的损失函数（如交叉熵）配合使用，变化为——<br>$$ L_{final} = L_{cross-entropy-loss} + \lambda L_{center-loss}(h, y_i) $$<br>展开为——<br>$$ L_{final} = -\frac{1}{N} \sum_{i=1}^N log ( \frac{e^{h_{y_i}}}{\sum_{j=1}^C e^{h_j}} ) + \frac{\lambda}{2} \sum_{i=1}^N ||x_i - c_{y_i} ||^2_2 $$<br>式中，$\lambda$ 为两个损失函数之间的权衡因子，$\lambda$越大类内差异占整个目标函数的比重越大；       </p><p>中心损失函数搭配传统交叉熵函数在分类性能上优于单独的传统交叉熵；<br>尤其在人脸识别任务上有较大的提升      </p><h3 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h3><p>回归任务通常用残差衡量预测值和真实值的靠近程度；<br>记回归问题第i个输入特征 $x_i$，<br>真实标记为 $y^i = (y_1, y_2, …, y_M)^T$，M为标记向量的总维度；<br>预测值为 $\hat{y}^i$，<br>$l^i_t = y^i_t - \hat{y}^i_t$ 表示样本i上预测值与真实值在第t维上的预测误差；        </p><p>l1、l2、Tukey’s biweight损失函数如下图所示——<br><img src="/Handson-ML/regression.png" alt="regression"></p><h4 id="l1损失函数"><a href="#l1损失函数" class="headerlink" title="l1损失函数"></a>l1损失函数</h4><p>$$ L_{l_1-loss} = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^M |l^i_T| $$         </p><h4 id="l2损失函数"><a href="#l2损失函数" class="headerlink" title="l2损失函数"></a>l2损失函数</h4><p>$$ L_{l_2-loss} = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^M (l^i_T)^2 $$<br>一般l1和l2在回归精度上所差无几，但一些情况下l2会略优于l1；<br>l2收敛略快于l1；         </p><h4 id="Tukey’s-biweight损失函数"><a href="#Tukey’s-biweight损失函数" class="headerlink" title="Tukey’s biweight损失函数"></a>Tukey’s biweight损失函数</h4><p>论文：<a href="https://arxiv.org/pdf/1505.06606.pdf" target="_blank" rel="noopener">Robust Optimization for Deep Regression(2015)</a>       </p><p>非凸损失函数；<br>定义：<br>$$<br>\begin{equation}<br>L_{Tukey’s-biweights-loss} = \left\{<br>\begin{aligned}<br>&amp; \frac{c^2}{6N} \sum_{i=1}^N \sum_{t=1}^M [1-(1-(\frac{l^i_t}{c})^2)^3] ,  &amp; |l^i_t| &lt;= c\\<br>&amp; \frac{c^2 M}{6},                                                          &amp; |l^i_t| &gt; c<br>\end{aligned}<br>\right.<br>\end{equation}<br>$$<br>其中，常数c指定了函数拐点，通常取 $c=4.6851$，此时该损失函数可以与l2在最小化符合标准正态分布的残差类似的回归效果      </p><h3 id="其他任务：KL散度"><a href="#其他任务：KL散度" class="headerlink" title="其他任务：KL散度"></a>其他任务：KL散度</h3><p>实际问题往往不能简单划为回归任务或者分类任务。<br>如年龄估计中，经常会表达“看起来30岁左右”；<br>此时通常采用一个“标记的分布”来描述，如均值为30的一个正态分布；<br>此外，在头部倾斜角度估计、多标记分类、图像语义分割等问题上也存在类似的问题；     </p><p>通常先将h转化为一个合法的分布（比如用softmax函数）；<br>用KL散度（Kullback-Leibler divergence）来衡量真实标记和预测分布的误差，此时也称为KL损失：<br>$$ L_{KL-loss} = \sum_k y_k log \frac{y_k}{\hat{y}_k} $$<br>因为$y_k$是已知的常量（真实标记），上式等价为：<br>$$ L_{KL-loss} = - \sum_k y_k log \hat{y}_k $$</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>经典的CNN分类架构</title>
      <link href="/2018/05/02/%E7%BB%8F%E5%85%B8%E7%9A%84CNN%E5%88%86%E7%B1%BB%E6%9E%B6%E6%9E%84/"/>
      <url>/2018/05/02/%E7%BB%8F%E5%85%B8%E7%9A%84CNN%E5%88%86%E7%B1%BB%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=600349&auto=0&height=32"></iframe><p>BGM：<strong>《GOSICK》ED2</strong>     </p><hr><p>从原文《<a href="/2018/04/18/CNN/">卷积神经网络CNN | Hey~YaHei!</a>》抽取出来；     </p><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》       </li></ol><p>CNN的典型组合方式是，以 <strong>卷积层+激活函数（一般是relu）+池化层</strong> 作为一组操作；<br>一张图片经过若干组这样的操作的stack之后，变得越来越小，同时越来越深（feature map越来越多）；<br>在stack的顶层，经过一个常规的前馈神经网络（由若干个带激活的全连接层组成）后产生预测结果（通常经过一个softmax层）——<br><img src="/Handson-ML/12Typical_CNN_Architectures.png" alt="Typical_CNN_Architectures">    </p><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>经典的CNN框架，1998年由Yann LeCun提出，被广泛地应用于手写体数字识别（MNIST）；      </p><p>详细配置：<br><img src="/Handson-ML/12LeNet-5.png" alt="LeNet-5">       </p><ul><li>MNIST数据为 $28 \times 28$ ，LeNet-5将其填零补充到 $32 \times 32$ 并在投喂给网络前进行归一化；<br>  而其他层不再进行padding，所以可以看到每经过一层，图片就发生一次萎缩       </li><li>平均池化层除了对输入进行平均之外，还经过一次仿射变换（缩放因子、偏置项都作为可学习的参数），并且在输出前经过一次激活       </li><li>C3层的神经元只与S2层的3到4个输出连接      </li><li>Out层不是简单的将输入和权重进行点乘，而是计算输入和权重的欧式距离的平方<br>  <em>而现在更好的方式是使用交叉熵</em>        </li></ul><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Imagenet Classification with Deep Convolutional Neural Networks(2012)</a>        </p><p>赢得2012年的ImageNet ILSVRC比赛；<br>与LeNet-5很像，只是更大更深（约60万个参数）；<br>他的突出成就是首次<strong>连续叠加卷积层</strong>而不是在每个卷积层之间插入池化层         </p><p>详细配置：<br><img src="/Handson-ML/12AlexNet.png" alt="AlexNet"></p><ul><li>加入了一些正则化技术      <ul><li>为F8、F9加入dropout（dropout rate为50%）       </li><li>数据增强（随机平移图片、水平翻转、改变光照条件）        </li></ul></li><li>在C1、C3层的激活函数之后紧跟一个局部响应归一化操作（Local Response Normalization, LRN）<br>  使得强烈激活的神经元在相邻特征图的相同位置上的神经元被抑制（仿生）；<br>  这促进不同的特征图得到差异化，能够关注到不同的特征。       <ul><li>LRN的具体操作如下：<br>  $$ b_i = a_i (k + \alpha \sum_{j = max(0, i - r/2)}^{min(i+r/2, f_n -1)} a_j^2)^\beta$$<br>  其中，<br>  $a_i, b_i$ 分别是LRN的第i个特征图输入、正则化输出；<br>  $k, \alpha, \beta, r$ 是超参量，$k$ 称为偏置，$r$ 称为深度半径；<br>  $f_n$ 是特征图的数量；<br>  比如，当 $r=2$ 时，强烈激活的神经元会抑制他的上一个和下一个特征图中相同位置的神经元；        </li><li>AlexNet采用的配置为 $r=2, \alpha = 0.00002, \beta = 0.75, k = 1$；<br>  tensorflow提供了相应的 <code>local_response_normalization()</code> 操作；        </li></ul></li></ul><p>变种：<strong>ZFNet</strong><br>论文：<a href="http://www.uvm.edu/~cdanfort/csc-reading-group/zeiler-eccv-2014.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks(2014)</a><br>2013年ILSVRC比赛冠军        </p><h3 id="VGG-Nets"><a href="#VGG-Nets" class="headerlink" title="VGG-Nets"></a>VGG-Nets</h3><p>论文：<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION(2015)</a><br>2014年ILSVRC亚军    </p><p>详细配置：<br><img src="/Handson-ML/12VGGNet1.png" alt="12VGGNet1"><br><img src="/Handson-ML/12VGGNet2.png" alt="12VGGNet2">      </p><p>特点（相比AlexNet）：</p><ol><li>普遍使用小卷积核、以及一些保持输入大小的技巧<br> 增加网络深度时确保各层输入大小随深度增加而不减小     </li><li>网络卷积的通道数逐层增加（$3 \to 64 \to 128 \to 256 \to 512$）    </li></ol><h3 id="NIN（Network-in-Netwrok）"><a href="#NIN（Network-in-Netwrok）" class="headerlink" title="NIN（Network in Netwrok）"></a>NIN（Network in Netwrok）</h3><p>论文：<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network In Network(2014)</a>     </p><p>特点：    </p><ol><li>用多层感知机（多层FC和非线性函数的组合）替代线性卷积层<br> 线性卷积层复杂度有限，层间映射只能将前层特征或输入简单地线性组合成后层特征；<br> 高复杂度的多层感知机增加了卷积层的非线性能力，使得前层特征能有更多的复杂性和可能性映射到后层特征；<br> 该思想随后也被Inception和ResNet借鉴     </li><li>使用全局平均池化（Global Average Pooling）操作替代全连接层作为“分类器”<br> 全局池化分别作用于每张特征图，最后映射到样本的真实标记；<br> 此时每张特征图上的响应将很自然的对应到不同的样本类别，使得NIN更具可解释性；<br> 该技术随后也被Inception借鉴     </li></ol><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>论文：<a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" rel="noopener">Going Deeper with Convolutions(2015)</a><br>2014年ILSVRC冠军；<br>利用子网络inception模块，网络更加高效（大约只有AlexNet的十分之一），使得网络可以更深；       </p><p>Inception模块：<br><img src="/Handson-ML/12Inception_Module.png" alt="Inception_Module"><br>三层结构。其中，$3 \times 3 + 1(S)$ 表示核大小为3x3，步长为1，使用SAME填充方式；        </p><ul><li>每个卷积层都使用relu激活函数；</li><li>第二层分别使用了1x1，3x3，5x5的卷积核，这是为了模块在多尺度上捕捉特征      </li><li>各个卷积层、池化层步长均为1，也就是说他们的输出大小与输入大小相同，这使得他们的输出可以被直接由Depth Concat层连接        </li><li>1x1卷积层的作用：       <ul><li>降采样，1x1卷积层也可以称为瓶颈层（bottleneck layer），在3x3、5x5卷积层之前降采样，可以有效地减小计算开销      </li><li>与3x3、5x5卷积层成对存在，使得图片被一个双层网络扫描而不是一个简单的线性分类器（<em>啊？这在说啥</em>）        <blockquote><p>Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a single,<br>powerful convolutional layer, capable of capturing more complex patterns. Indeed, instead of<br>sweeping a simple linear classifier across the image (as a single convolutional layer does), this<br>pair of convolutional layers sweeps a two-layer neural network across the image</p></blockquote></li></ul></li></ul><p>GoogLeNet框架：<br>包括9个Inception模块，每个卷积层都使用relu激活函数：<br><img src="/Handson-ML/12GoogLeNet.png" alt="GoogLeNet">       </p><ul><li>头两层（卷积层、池化层）步长均为2，相当于图片的长、宽都除以4，整体压缩为原来的十六分之一，以减少计算量        </li><li>LRN保证了前两层捕捉到多尺度的特征       </li><li>1x1和3x3的卷积层对，构成一个比简单的线性分类器表现更好的卷积层      </li><li>又一个LRN保证前层捕捉到多尺度的特征      </li><li>步长为2的池化层进一步把图片压缩为原来的四分之一，减少计算量       </li><li>接下来是连续的九个Inception模块，中间插入了两个最大池化层以进一步减少计算量（每一个池化层将图片压缩为四分之一）</li><li>平均池化层使用了不填充的步长为1的池化核，这种策略称为<strong>全局平均池化（Global Average Pooling）</strong><br>  这有效地令前层产生一个特征图，这个特征图实际上是每个预测分类的置信图（<em>因为其他特征会在平均过程中被摧毁？</em>）；<br>  这样一来，模型在输出前就不再需要多个全连接层，相当于减少了参数的数量并且减少过拟合       </li><li>最后dropout层起正则化的作用，全连接层和softmax层产生预测结果       </li></ul><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>论文：<a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition(2015)</a>        </p><p>其他：    </p><ol><li><a href="https://mp.weixin.qq.com/s/7fWh2dovmfbsF8afaX9UOg" target="_blank" rel="noopener">机器之心 - 一文简述ResNet及其多种变体(2018)</a> 【<a href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035" target="_blank" rel="noopener">原文</a>】      </li><li><a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Training Very Deep Networks(2015)</a><br> 受LSTM中门机制的启发，提出高速公路网络，对前馈神经网络进行修正，使信息能够在多个神经网络层之间高效流动；这一思想也影响了ResNet网络    </li></ol><p>2015年ILSVRC冠军；<br>网络更深（152层），如此深的网络能够被训练主要得益于<strong>Skip Connections（或Shortcut Connections）</strong>技术；      </p><p><strong>残差学习（Residual Learning）</strong>：<br>在常规网络的基础上，网络输出前增加一个与输入加和的操作；<br>这时网络不再直接训练目标函数 $h(x)$，而是训练 $f(x) = h(x) - x$；<br><img src="/Handson-ML/12Residual_Learning.png" alt="Residual_Learning">          </p><p>对于常规网络，权重被初始化为一个接近0的数，网络一开始只能输出一个非常接近于0的结果；<br>但是残差网络输出的却是一个接近输入的结果，而往往我们所要训练的目标函数跟这个输入还是比较接近的，这将大大加速我们的训练过程；       </p><p>而且，如果加入了很多skip connections，网络可以在很早的时候就开始取得进步（有效地学习），即使中间有部分层还没开始进步也不影响，因为信号的传递很容易穿过整个网络；<br>深层残差网络可以看作是很多残差单元（Residual Unit, RU）的堆叠，每个残差单元是一个包含一个skip connection的小网络——<br><img src="/Handson-ML/12Deep_Residual_Network.png" alt="Deep_Residual_Network">        </p><p>ResNet框架：<br>他的最底层和最顶层跟GoogLeNet差不多；<br><img src="/Handson-ML/12ResNet.png" alt="ResNet">        </p><ul><li>头两层与GoogLeNet完全相同         </li><li>但ResNet没有LRN也没有1x1和3x3的卷积层对       </li><li>接下来是一个深层的残差网络，由许多残差单元组成（每个残差单元包含两个卷积层以及BN层、relu激活函数、skip connection，步长为1、补零、保持大小）<br>  每经过一定层数，图片会经过一个步长为2的卷积层而被压缩，然后接下来的残差单元的大小都增加一倍<br>  此时这个残差单元的输入不能直接加到输出上（大小不一致），而应该在加和前让输入经过一个1x1步长2的卷积层——<br>  <img src="/Handson-ML/12Double_Units.png" alt="Double_Units">       </li><li>最后两层也与GoogLeNet完全相同，不过这里不需要dropout       </li></ul><p>各种不同深度的ResNet：<br>（只计算卷积层和全连接层，除去底层的卷积层和顶层的全连接层，不考虑每次倍增的时候多出来的1x1卷积层）       </p><ul><li>ResNet-34     <ul><li>3个64输出的RUs</li><li>4个128输出的RUs</li><li>6个256输出的RUs</li><li>3个512输出的RUs</li></ul></li><li>ResNet-152<br>  RU结构发生变化，使用三层卷积（1x1的64输出卷积层作为瓶颈层，然后一个3x3的64输出卷积层，再紧接一个1x1的 $4 \times 64=256$ 输出卷积层来恢复深度）     <ul><li>3个256输出的新RUs</li><li>8个512输出的新RUs</li><li>36个1024输出的新RUs</li><li>3个2048输出的新RUs   </li></ul></li></ul><h3 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3><p>网络越来越深，参数越来越少；<br>目前而言，ResNet兼具简单、表现出色两个特点；<br><em>2016年和2017年中国拿了第一，但是好像没有提出什么有趣的东西，是刷榜吗？</em>      </p><p>其他值得研究的框架——<br><a href="https://arxiv.org/pdf/1602.07261v2.pdf" target="_blank" rel="noopener">Inception-v4(2016)</a>：吸收兼并GoogLeNet和ResNet的思想，并提出网络inception-resnet-v1和inception-resnet-v2<br><a href="https://arxiv.org/pdf/1707.07012.pdf" target="_blank" rel="noopener">NASNet(2018)</a>：2017谷歌提的框架，准确率很诱人，但据说规模大的很，吐槽的人倒也不少        </p><p>最后附一个tensorflow内置的<a href="https://github.com/tensorflow/models/tree/master/research/slim" target="_blank" rel="noopener">常见框架模块及其预训练模型</a>       </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>自编码器</title>
      <link href="/2018/04/25/autoencoder/"/>
      <url>/2018/04/25/autoencoder/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=489970553&auto=0&height=32"></iframe><br>BGM：<strong>《末日时在做什么？有没有空？可以来拯救吗？》第九话插入曲</strong><br>这番名字真是狗血，但剧情和音乐出奇的不错       </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap15<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><p>自编码器工作方式非常简单，就是学习如何模仿输入来产生输出；<br>我们会采取各种约束（比如限制输出大小、加噪等）来避免自编码器纯粹地把输入作为输出，从而得到一个高效的数据表示方式；<br>简而言之，自编码器通过尝试学习某些约束下的特征函数来产生输入的编码，也即一种高效的数据表示；<br>自编码器可以无监督学习输入数据的编码方式、降低数据维度、作为特征检测器、作为生成模型等等……      </p><h3 id="数据的高效表示"><a href="#数据的高效表示" class="headerlink" title="数据的高效表示"></a>数据的高效表示</h3><p>论文 <a href="https://www.sciencedirect.com/science/article/pii/0010028573900042" target="_blank" rel="noopener">Perception in chess(1973)</a> 研究了记忆、概念、模式匹配之间的联系；     </p><p>自编码器可以分为Encoder和Decoder两部分，<br>Encoder也称识别网络，用于将输入转换为某种内部表示；<br>Decoder也称生成网络，用于将内部表示转换成输出；<br>架构跟MLP一样，不过他的<strong>输出神经元数与输入数相等</strong>，<strong>中间层的神经元数小于输入数</strong>；<br>也就是说，中间层的输出必定是输入的一个不完全表示，我们的目的就在于训练出一个输出与输入相近的网络——<br><strong>可以理解为Encoder是对输入的一个有损压缩，Decoder对其进行解压，我们要训练一个损耗率尽可能小的网络</strong><br><img src="/Handson-ML/15Simple_Autoencoder.png" alt="15Simple_Autoencoder">       </p><h3 id="简单的线性自编码器"><a href="#简单的线性自编码器" class="headerlink" title="简单的线性自编码器"></a>简单的线性自编码器</h3><p>无非线性激活，MSE损失函数，可以实现一个PCA；       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> fully_connected<span class="token comment" spellcheck="true"># 三维输入编码为二维表示</span>n_inputs <span class="token operator">=</span> <span class="token number">3</span>n_hidden <span class="token operator">=</span> <span class="token number">2</span>n_outputs <span class="token operator">=</span> n_inputslearning_rate <span class="token operator">=</span> <span class="token number">0.01</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>hidden <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 纯线性，无激活</span>outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># MSE</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>reconstruction_loss<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>X_train<span class="token punctuation">,</span> X_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>     <span class="token comment" spellcheck="true"># 载入数据集</span>n_iterations <span class="token operator">=</span> <span class="token number">1000</span>codings <span class="token operator">=</span> hidden     <span class="token comment" spellcheck="true"># 自编码器的目的是获取编码，也即中间层的输出</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        training_op<span class="token punctuation">.</span>run<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_train<span class="token punctuation">}</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 无监督</span>    codings_val <span class="token operator">=</span> codings<span class="token punctuation">.</span>eval<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_test<span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><h3 id="深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）"><a href="#深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）" class="headerlink" title="深层自编码器（Deep autoencoders, or Stacked autoencoders）"></a>深层自编码器（Deep autoencoders, or Stacked autoencoders）</h3><p>深层自编码器为中心对称的“三明治”结构，最中间的一层产生实际的编码，称编码层（Coding Layer），如——<br><img src="/Handson-ML/15Stacked_Autoencoder.png" alt="15Stacked_Autoencoder">       </p><p>深度学习的训练技术（如<a href="/2018/04/08/梯度消失与梯度爆炸">解决梯度爆炸与梯度消失的技术</a>、<a href="/2018/04/09/复用预训练层">复用预训练层</a>、<a href="/2018/04/10/优化器">优化器</a>、<a href="/2018/04/11/正则化技术">正则化技术</a>等）依旧适用；<br>与之前讲述的网络的区别在于，自编码器没有标注，是无监督学习     </p><h3 id="权重共享"><a href="#权重共享" class="headerlink" title="权重共享"></a>权重共享</h3><p>由于结构是中心对称的，Encoder和Decoder可以直接共享权重，但tensorflow中的 <code>fully_connected()</code> 没法共享权重，所以需要手动书写全连接层——    </p><pre class=" language-python"><code class="language-python">activation <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>eluregularizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>l2_regularizer<span class="token punctuation">(</span>l2_reg<span class="token punctuation">)</span>initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>weights1_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_inputs<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">]</span><span class="token punctuation">)</span>weights2_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">]</span><span class="token punctuation">)</span>weights1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights1_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights1"</span><span class="token punctuation">)</span>weights2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights2_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights2"</span><span class="token punctuation">)</span>weights3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights2<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights3"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>weights4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights1<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights4"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>biases1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden1<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases1"</span><span class="token punctuation">)</span>biases2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden2<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases2"</span><span class="token punctuation">)</span>biases3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden3<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases3"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>biases4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases4"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>hidden1 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> weights1<span class="token punctuation">)</span> <span class="token operator">+</span> biases1<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>hidden3 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> weights3<span class="token punctuation">)</span> <span class="token operator">+</span> biases3<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden3<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 只对weights1和weights2施加正则化</span>loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> reg_lossoptimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="逐层训练"><a href="#逐层训练" class="headerlink" title="逐层训练"></a>逐层训练</h3><p>逐层训练然后将各层堆叠起来，要比直接训练一个堆叠好的自编码器要快得多；<br>比如训练一个三层的自编码器：     </p><ol><li>只保留第一隐藏层进行训练    </li><li>叠加第二、第三隐藏层进行训练（<em>这里第一隐藏层和第三隐藏层的权重不共享？</em>）       </li><li>各层训练完毕，叠加起来得到一个完整的自编码器       </li></ol><p><img src="/Handson-ML/15Training_One_Autoencoder_at_A_Time.png" alt="15Training_One_Autoencoder_at_A_Time">     </p><p>可以按照这个描述，分别构造多个计算图来进行训练；<br>更巧妙的方式是添加一些操作在同一张计算图中区分训练阶段——<br><img src="/Handson-ML/15A_Single_Graph_To_Train_A_Stacked_Autoencoder.png" alt="15A_Single_Graph_To_Train_A_Stacked_Autoencoder">     </p><ol><li>中间部分是一个完整的自编码器   </li><li>左侧是第一训练阶段，旁路了第二和第三隐藏层，<code>Phase 1 Outputs</code> 跟 完整模型中的 <code>Outputs</code> 是共享参数的；<br> 这一阶段目标是使得最终输出与输入接近，训练 <code>Hidden 1</code> 和 <code>Outputs</code> 的权重      </li><li>右侧是第二训练阶段，旁路了输出层，第一隐藏层参数固定，只训练第二和第三隐藏层<br> 这一阶段目标是使得 <code>Hidden 3</code> 的输出与 <code>Hidden 1</code> 的输出接近，训练 <code>Hidden 2</code> 和 <code>Hidden 3</code> 的权重       </li></ol><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Build the whole stacked autoencoder normally.</span><span class="token comment" spellcheck="true"># In this example, the weights are not tied.</span><span class="token comment" spellcheck="true"># [...] </span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 阶段一的输出层，与完整模型的输出层共享参数</span>    phase1_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4    phase1_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>phase1_outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    phase1_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights4<span class="token punctuation">)</span>    phase1_loss <span class="token operator">=</span> phase1_reconstruction_loss <span class="token operator">+</span> phase1_reg_loss    phase1_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase1_loss<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    phase2_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3 <span class="token operator">-</span> hidden1<span class="token punctuation">)</span><span class="token punctuation">)</span>    phase2_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights3<span class="token punctuation">)</span>    phase2_loss <span class="token operator">=</span> phase2_reconstruction_loss <span class="token operator">+</span> phase2_reg_loss    <span class="token comment" spellcheck="true"># 忽略weights1和biases1</span>    train_vars <span class="token operator">=</span> <span class="token punctuation">[</span>weights2<span class="token punctuation">,</span> biases2<span class="token punctuation">,</span> weights3<span class="token punctuation">,</span> biases3<span class="token punctuation">]</span>    phase2_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase2_loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>train_vars<span class="token punctuation">)</span></code></pre><p>由于阶段一已经计算了 <code>hidden 1</code> 的输出，而且阶段二中 <code>hidden 1</code> 参数固定；<br>在内存足够的情况下可以先计算出整个batch上的输出并保留，以减少阶段二的训练时间；<br>这与<a href="/2018/04/09/复用预训练层/#加速训练：复用frozen层的输出结果（牺牲空间）">复用预训练层 - 复用frozen层输出加速训练</a>类似      </p><h4 id="重构效果可视化"><a href="#重构效果可视化" class="headerlink" title="重构效果可视化"></a>重构效果可视化</h4><p>直接显示压缩再解压后的结果，与输入进行直观的比较进行判断；<br>可以初步判断重构的效果       </p><h4 id="特征可视化"><a href="#特征可视化" class="headerlink" title="特征可视化"></a>特征可视化</h4><p>对于高层神经元，尤其是最后一个隐藏层的神经元，可以直接观察特定的输入时哪些神经元激活程度比较高；<br>但底层神经元关注的是更抽象、更小的特征，是我们无法直接理解的特征；   </p><ol><li>用权重分布图来观察每一个神经元的关注点<br> 比如观察第一隐藏层前五个神经元的关注点：     <pre class=" language-python"><code class="language-python"> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># train autoencoder</span>     weights1_val <span class="token operator">=</span> weights1<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>     plot_image<span class="token punctuation">(</span>weights1_val<span class="token punctuation">.</span>T<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre> <img src="/Handson-ML/15Visualzing_The_Feature.png" alt="15Visualzing_The_Feature"><br> <em>【越是关注的像素点，权重越大，在灰度图中就越亮】</em><br> 可以看到前四个神经元关注的都是某些局部的小区域，第五个神经元则似乎更关注竖直的笔画       </li><li>随机输入一个图像，然后用反向传播不断改变图像以最大化某个神经元的激活程度<br> 经过一定的迭代次数之后，图像将被扭曲为明显带有该神经元所关注特征的图像       </li><li>如果用自编码器用于某些任务（如分类任务）的前期无监督预训练<br> 那么可以直接通过观察这些任务的最终表现，来判断自编码器的效果      </li></ol><h3 id="用自编码器作无监督预训练"><a href="#用自编码器作无监督预训练" class="headerlink" title="用自编码器作无监督预训练"></a>用自编码器作无监督预训练</h3><p><a href="/2018/04/09/复用预训练层/#无监督预训练">《Handson-ML》笔记 - 无监督预训练</a><br>论文：<a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep" target="_blank" rel="noopener">Greedy Layer-Wise Training of Deep Networks(2006)</a>   </p><p><img src="/Handson-ML/15Unsupervised_Pretraining.png" alt="15Unsupervised_Pretraining">       </p><h3 id="不同约束下的自编码器"><a href="#不同约束下的自编码器" class="headerlink" title="不同约束下的自编码器"></a>不同约束下的自编码器</h3><p>添加约束，避免自编码器纯粹地把输入作为输出，从而得到一个更加高效的数据表示方式       </p><h4 id="降噪自编码器（Denoising-Autoencoders）"><a href="#降噪自编码器（Denoising-Autoencoders）" class="headerlink" title="降噪自编码器（Denoising Autoencoders）"></a>降噪自编码器（Denoising Autoencoders）</h4><p>论文：<br><a href="https://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf" target="_blank" rel="noopener">Extracting and Composing Robust Features with Denoising Autoencoders(2008)</a> 提出自编码器可以用于特征提取；<br><a href="http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf" target="_blank" rel="noopener">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(2010)</a> 提出降噪自编码器；      </p><p>降噪自编码器通过<strong>对输入加入高斯噪声</strong>或者<strong>在输入之后紧接一个dropout层</strong>，可以有效的避免输入噪声对模型的影响；<br><img src="/Handson-ML/15Denoising_Autoencoders.png" alt="15Denoising_Autoencoders">        </p><p>具体实现：<br>【高斯噪声方案】      </p><pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>X_noisy <span class="token operator">=</span> X <span class="token operator">+</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 其他层</span></code></pre><p>【dropout层方案】    </p><pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.7</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder_with_default<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>X_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 其他层</span></code></pre><h4 id="稀疏自编码器（Sparse-Autoencoders）"><a href="#稀疏自编码器（Sparse-Autoencoders）" class="headerlink" title="稀疏自编码器（Sparse Autoencoders）"></a>稀疏自编码器（Sparse Autoencoders）</h4><p>基本思想是为损失函数添加适当的稀疏度损失项，使得对于每个输入，编码层只有一小部分神经元被显著激活；<br>这使得编码器可以更好地提取出特征；<br><em>如果只让你用一句话描述自己的想法，那你可能会深思熟虑如何更好的表达</em>      </p><p>在每次迭代前，都必须先评估编码层的实际稀疏程度——计算编码层中活跃神经元的平均数量；<br>为了获取比较准确的平均数量，batch的大小一定不能太小；       </p><p>接下来我们为损失函数添加一个关于神经元活跃程度的惩罚项，比如用MSE；<br>但更好的方式是用具有更大梯度的<strong>KL散度（Kullback–Leibler divergence）</strong>；        </p><p><img src="/Handson-ML/15Sparsity loss.png" alt="15Sparsity loss">     </p><p>对于两个离散概率分布P和Q，KL散度表示为——<br>$$ D_{KL}(P||Q) = \sum_i P(i) log \frac{P(i)}{Q(i)} $$       </p><p>具体到稀疏自编码器上，对于评估的激活概率q和目标激活概率p，KL散度为（激活/不激活 是二项分布的）——<br>$$ D_{KL}(p||q) = p log \frac{p}{q} + (1-p) log \frac{1-p}{1-q} $$       </p><p>一旦计算出编码层上每个神经元的稀疏度损失，就可以把他们都加和到损失函数上进行训练；<br>为了权衡稀疏度损失和重构损失的重要性，可以为加和的稀疏度损失额外添加一个数值合适的权重超参数进行训练；       </p><p>具体实现：    </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算KL散度</span><span class="token keyword">def</span> <span class="token function">kl_divergence</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> p <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p <span class="token operator">/</span> q<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span>learning_rate <span class="token operator">=</span> <span class="token number">0.01</span>sparsity_target <span class="token operator">=</span> <span class="token number">0.1</span>sparsity_weight <span class="token operator">=</span> <span class="token number">0.2</span><span class="token comment" spellcheck="true"># [...] # Build a normal autoencoder (in this example the coding layer is hidden1)</span><span class="token comment" spellcheck="true"># 注意：编码层的激活程度必须是在(0,1)区间的</span><span class="token comment" spellcheck="true">#    比如可以用sigmoid函数强制激活程度归一化为(0,1)区间的数值</span><span class="token comment" spellcheck="true">#    hidden1 = tf.nn.sigmoid(tf.matmul(X, weights1) + biases1)</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>hidden1_mean <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 计算整个batch上的平均值</span>sparsity_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kl_divergence<span class="token punctuation">(</span>sparsity_target<span class="token punctuation">,</span> hidden1_mean<span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算稀疏度损失总和</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算重构损失（MSE）</span>loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> sparsity_weight <span class="token operator">*</span> sparsity_loss    <span class="token comment" spellcheck="true"># 计算全局损失</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span></code></pre><p>把重构损失 <code>reconstruction_loss</code> 的计算改为交叉熵可以加速收敛，但要注意交叉熵要求输入归一化，因此——     </p><pre class=" language-python"><code class="language-python">logits <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 训练时outputs不是必要的，只是为了看到重构结果才计算outputs</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span> <span class="token punctuation">)</span></code></pre><h4 id="变分自编码器（Variational-Autoencoders）"><a href="#变分自编码器（Variational-Autoencoders）" class="headerlink" title="变分自编码器（Variational Autoencoders）"></a>变分自编码器（Variational Autoencoders）</h4><p>论文：<a href="https://arxiv.org/pdf/1312.6114v10.pdf" target="_blank" rel="noopener">Auto-Encoding Variational Bayes(2014)</a>     </p><p>主要特点：     </p><ol><li>概率自编码器，输出是有一定偶然性的，即使是训练完之后      </li><li>生成自编码器，能够生成类似他们在训练集上采样的实例     </li></ol><p>这跟RBMs有些类似，但变分自编码器更加容易训练而且采样更快！     </p><p>其结构如下所示，编码层不再直接输出编码，而是在 $\mu$ 附近随机采样——<br><img src="/Handson-ML/15Variational_Autoencoder.png" alt="15Variational_Autoencoder">    </p><p>比如下图的输入数据，编码层将在以 $\mu$ 为中心，半径为 $\sigma$ 的范围内随机采样作为编码结果；<br><img src="/Handson-ML/15Variational_Autoencoder_Instance.png" alt="15Variational_Autoencoder_Instance">    </p><p>损失函数分为两个部分：    </p><ol><li>重构损失    </li><li><p>隐藏损失：即编码层上的在高斯分布下的损失（这一部分用高斯分布的目标分布和实际分布的KL散度来表示）<br> 高斯分布的噪声使传输给编码层的信息数量受限，迫使网络学习一些有意义的特征；<br> 这在数学计算上复杂了不少，不过可以用下列这个式子进行简化——<br> $$ L_l = \frac{1}{2} \sum \sigma^2 + \mu^2 - 1 - log(eps + \sigma^2) $$<br> 通常取 $eps=10^{-10}$，用于防止 $log(0)$ 的情况出现；<br> 有一种变种的损失函数——<br> $$ L_l = \frac{1}{2} \sum e^\gamma + \mu^2 - 1 - \gamma $$<br> 其中 $\gamma = log(\sigma^2)$，即 $\sigma = e^{\gamma / 2}$；<br> 该变种使得不同尺度下的 $\sigma$ 更容易被捕获，从而加速收敛；        </p><pre class=" language-python"><code class="language-python"> <span class="token comment" spellcheck="true"># [...] 超参数</span> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>framework<span class="token punctuation">.</span>arg_scope<span class="token punctuation">(</span>         <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>         activation_fn<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu<span class="token punctuation">,</span>         weights_initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># [...] 前层，hidden3为编码层</span>     hidden3_mean <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 从hidden2习得平均值</span>     hidden3_gamma <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 从hidden2习得gamma（与标准差相关）</span>     hidden3_sigma <span class="token operator">=</span> tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> hidden3_gamma<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算标准差sigma</span>     noise <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>hidden3_sigma<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 高斯分布噪声</span>     hidden3 <span class="token operator">=</span> hidden3_mean <span class="token operator">+</span> hidden3_sigma <span class="token operator">*</span> noise  <span class="token comment" spellcheck="true"># 在以平均值为中心，标准差为半径的范围内随机采样</span>     <span class="token comment" spellcheck="true"># [...] 后层</span>     logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden5<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 用于计算损失函数</span>     outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 实际输出</span> <span class="token comment" spellcheck="true"># 计算重构损失</span> reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>                         tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算隐藏损失</span> latent_loss <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>                         tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>hidden3_gamma<span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3_mean<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> hidden3_gamma<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算全局损失</span> cost <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> latent_loss <span class="token comment" spellcheck="true"># [...] 优化器等训练过程略</span></code></pre></li></ol><h4 id="其他自编码器"><a href="#其他自编码器" class="headerlink" title="其他自编码器"></a>其他自编码器</h4><ul><li>Contractive autoencoder(CAE)<br>  论文：<a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" target="_blank" rel="noopener">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction(2011)</a><br>  关于输入的编码上的导数比较小，使得两个相似的输入得到相似的编码         </li><li>Stacked convolutioncal autoencoders<br>  论文：<a href="http://people.idsia.ch/~ciresan/data/icann2011.pdf" target="_blank" rel="noopener">Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction(2011)</a><br>  通过用卷积层来重构图像的方式来提取图像特征     </li><li>Generative stochastic network(GSN)<br>  论文：<a href="https://arxiv.org/pdf/1503.05571v2.pdf" target="_blank" rel="noopener">GSNs: Generative Stochastic Networks(2015)</a><br>  降噪自编码器的一般化，能够生成数据       </li><li>Winner-take-all(WTA) autoencoder<br>  论文：<a href="https://arxiv.org/pdf/1409.2752v2.pdf" target="_blank" rel="noopener">Winner-Take-All Autoencoders(2015)</a><br>  训练过程中，计算时只保留激活程度前k%的神经元的激活程度，其余都置为0；<br>  这将稀疏化编码，类似的WTA方法也可以应用于生成稀疏化的卷积自编码器；      </li><li>Adversarial autoencoders<br>  论文：<a href="https://arxiv.org/pdf/1511.05644v2.pdf" target="_blank" rel="noopener">Adversarial Autoencoders(2016)</a><br>  分为两个网络，一个训练来重构输入的数据，与此同时另外一个训练来找到前者重构效果不好的输入数据；<br>  以此来迫使前者学习出鲁棒性比较好的编码方式      </li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>循环神经网络RNN</title>
      <link href="/2018/04/22/RNN/"/>
      <url>/2018/04/22/RNN/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=30431340&auto=0&height=32"></iframe><br>BGM：<strong>《Aldnoah Zero》ED1</strong><br>歌词混好几种语言       </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap14<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><blockquote><p>【占坑待填】<br>Handson ML对RNN的介绍比较简略，先占着坑          </p></blockquote><h3 id="Basic-RNNs的tensorflow实现"><a href="#Basic-RNNs的tensorflow实现" class="headerlink" title="Basic RNNs的tensorflow实现"></a>Basic RNNs的tensorflow实现</h3><ul><li><a href="/Handson-ML/14.1Static_Unrolling_Through_Time(Plain).html">Jupyter - Static Unrolling Through Time(Plain)</a><br>  底层操作实现的静态展开</li><li><a href="/Handson-ML/14.2Unrolling_Through_Time(High-level_API).html">Jupyter - Unrolling Through Time(High-level_API)</a><br>  高层操作实现的静态展开和动态展开          </li><li><a href="/Handson-ML/14.3Handling_Variable_Length_Sequences.html">Jupyter - Handling Variable Length Sequences</a><br>  处理变长的输入输出，另外还有一种办法，参见<a href="#机器翻译（Encoder-Decoder）">5.2 机器翻译（Encoder-Decoder）</a>       </li></ul><h3 id="训练RNN（BackproPagation-Through-Time-BPTT）"><a href="#训练RNN（BackproPagation-Through-Time-BPTT）" class="headerlink" title="训练RNN（BackproPagation Through Time, BPTT）"></a>训练RNN（BackproPagation Through Time, BPTT）</h3><p>一般将RNN按时间展开，然后使用常规的反向传播进行训练；<br>如下图所示：      </p><p><img src="/Handson-ML/14BPTT.png" alt="BPTT">        </p><p>注意这里损失函数 <code>C(·)</code> 是由最后几个输出计算得到的，而不是最后一个输出！      </p><h4 id="序列分类器"><a href="#序列分类器" class="headerlink" title="序列分类器"></a>序列分类器</h4><p>类似CNN做MNIST分类器，RNN也可以实现手写数字的识别；<br>按照《Handson-ML》，准确率也可以达到98%以上      </p><h4 id="预测时间序列"><a href="#预测时间序列" class="headerlink" title="预测时间序列"></a>预测时间序列</h4><p>比如股价预测——<br>随机截取一些连续的20个时间点的股价作为mini-batch，并且右移1个时间点作为标注；<br>即训练一个预测下一个时间点的股价的模型；<br><img src="/Handson-ML/14Stock_Price_Prediction.png" alt="14Stock_Price_Prediction">       </p><p>按前述直接用动态展开的BasicRNNCell组成的RNN网络——    </p><pre class=" language-python"><code class="language-python">n_steps <span class="token operator">=</span> <span class="token number">20</span>n_inputs <span class="token operator">=</span> <span class="token number">1</span>n_neurons <span class="token operator">=</span> <span class="token number">100</span>n_outputs <span class="token operator">=</span> <span class="token number">1</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>但是此时的网络输出outputs是一个包含100元素（<code>n_neurons=100</code>）的向量，<br>而我们需要的只是一个元素，即使下一时刻的预测值，最简单的思路是对cell进行包装：<br>其实就是在BasicRNNCell之后再加一个fully connected层（无激活函数）<br><img src="/Handson-ML/14OutputProjectionWrapper.png" alt="OutputProjectionWrapper">           </p><p>具体实现——         </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [...]</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>OutputProjectionWrapper<span class="token punctuation">(</span>basic_cell<span class="token punctuation">,</span> output_size<span class="token operator">=</span>n_outputs<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>此时的outputs就是一个单元素的输出啦~<br>尽管OutputProjectionWrapper可以解决这一问题，但每一步的神经元输出都需要过一层FC，效率不是很高；<br>更好的办法是（只需要过一次FC）：      </p><ol><li>将RNN的输出从 <code>[batch_size, n_steps, n_neurons]</code> 重整为 <code>[batch_size * n_steps, n_neurons]</code>；        </li><li>再通过一个fully connected整合成 <code>[batch_size * n_steps, n_outputs]</code> 大小的输出；        </li><li>最后展开成 <code>[batch_size, n_steps, n_outputs]</code> 大小的最终输出<br><img src="/Handson-ML/14More_Efficient_Method.png" alt="14More_Efficient_Method">       </li></ol><p>具体实现——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [...]</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>basic_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将RNN的输出从 [batch_size, n_steps, n_neurons] 重整为 [batch_size * n_steps, n_neurons]</span>stacked_rnn_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>rnn_outputs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_neurons<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 通过一个fully connected整合成 [batch_size * n_steps, n_outputs] 大小的输出</span>stacked_outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>stacked_rnn_outputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 最后展开成 [batch_size, n_steps, n_outputs] 大小的最终输出</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>stacked_outputs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h3><p>直接堆叠RNN就可以得到一个深层的RNN——<br><img src="/Handson-ML/14Deep_RNN.png" alt="14Deep_RNN">         </p><p>具体实现——<br>借助 <code>tf.contrib.rnn.MultiRNNCell()</code> 可以将多个RNN堆叠起来       </p><pre class=" language-python"><code class="language-python">n_neurons <span class="token operator">=</span> <span class="token number">100</span>n_layers <span class="token operator">=</span> <span class="token number">3</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span>multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>basic_cell<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>此时 <code>states</code> 是一个元组，每个元素都是每一层对应的大小为 <code>[batch_size, n_neurons]</code> 的Tensor；<br>如果为 <code>MultiRNNCell</code> 指定参数 <code>state_is_tuple=False</code> ，那么 <code>states</code> 就只是一个 <code>[batch_size, n_layers * n_neurons]</code> 大小的Tensor；      </p><h4 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h4><p><em>……暂略……</em>    </p><h4 id="使用Dropout"><a href="#使用Dropout" class="headerlink" title="使用Dropout"></a>使用Dropout</h4><p>如果只是在RNN层之前或者之后，可以直接添加Dropout层【可以参见<a href="/2018/04/11/正则化技术/#dropout">正则化技术 - Dropout</a>】；<br>但如果是在RNN层与RNN层之间添加Dropout，就必须使用 <code>DropoutWrapper</code>：        </p><pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.5</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用DropoutWrapper包装BasicRNNCell</span><span class="token comment" spellcheck="true"># ... input_keep_prob参数是输入前dropout层的keep_prob（不指定则步添加dropout）</span><span class="token comment" spellcheck="true"># ... 相应的还有output_keep_prob参数</span>cell_drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 直接堆叠若干个RNN层，层与层之间不能直接使用Dropout层</span>multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>cell_drop<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span>rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>然而头疼的是，<code>DropoutWrapper</code> 并不支持 <code>is_training</code> 参数，也就是说，它在预测是依旧存在dropout层；<br>解决方法有两种——      </p><ol><li>自己重写一个支持 <code>is_training</code> 参数的 <code>DropoutWrapper</code> 类（<em>什么鬼设定</em>）</li><li><p>针对训练和预测构建不同的计算图，具体如下：        </p><pre class=" language-python"><code class="language-python"> <span class="token keyword">import</span> sys is_training <span class="token operator">=</span> <span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"train"</span><span class="token punctuation">)</span> X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span> y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span> cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span> <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># 如果是训练，则用DropoutWrapper包装；如果是预测就拉倒</span>     cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span> multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>cell<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span> rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [...] build the rest of the graph</span> init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span> saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># 如果是训练，那就执行初始化器并进行训练</span>         init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true"># [...] # train the model</span>             save_path <span class="token operator">=</span> saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">"/tmp/my_model.ckpt"</span><span class="token punctuation">)</span>     <span class="token keyword">else</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 如果是预测，那就直接载入模型并使用</span>         saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">"/tmp/my_model.ckpt"</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># [...] # use the model</span></code></pre></li></ol><h4 id="长期依赖问题"><a href="#长期依赖问题" class="headerlink" title="长期依赖问题"></a>长期依赖问题</h4><p><a href="/2018/04/08/梯度消失与梯度爆炸/">梯度消失与梯度爆炸</a> 所述技术对RNN也是有效的；<br>但是使用这些技术之后，随着时间步增多，训练将变得十分缓慢；<br>最简单粗暴的解决方法是<strong>缩短时间步</strong>，但如此一来RNN忽略比较遥远的过去，也即存在长期依赖的问题；<br>为了解决这一问题，LSTM出现啦！       </p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>论文：<br>【起源】：<a href="https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw" target="_blank" rel="noopener">Long Short-Term Memory(2006)</a><br>【改进】：       </p><ol><li><a href="https://arxiv.org/pdf/1402.1128.pdf" target="_blank" rel="noopener">LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION(2014)</a>        </li><li><a href="https://arxiv.org/pdf/1409.2329v5.pdf" target="_blank" rel="noopener">RECURRENT NEURAL NETWORK REGULARIZATION(2015)</a>       </li><li><a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Recurrent Nets that Time and Count(2000)</a> 【提出peephole connection】      </li><li><a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)</a> 【提出GRU和Encoder-Decoder架构】<br>其他：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks(2015)</a>      </li></ol><p>tensorflow中的使用：<br>直接将先前的 <code>tf.contrib.rnn.BasicRNNCell</code> 替换为 <code>tf.contrib.rnn.BasicLSTMCell</code> 即可；<br>区别在于，<code>BasicLSTMCell</code> 的states包含两个向量，如果要合并在一起的话可以指定参数 <code>state_is_tuple=False</code>；<br>如果要使用变种的LSTM，则使用 <code>tf.contrib.rnn.LSTMCell</code>：<br>比如使用带peephole connection的LSTM，则指定参数 <code>use_peepholes=True</code>          </p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>LSTMCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> use_peepholes<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks(2015)</a> 一问介绍了RNN的一些应用；   </p><p>接下来只介绍RNN在自然语言处理（NLP）上的两个应用</p><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>参考：</p><ol><li><a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">Tensorflow Tutorial - Word2Vec</a> 或 中文的<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/word2vec.html" target="_blank" rel="noopener">字词的向量表示</a>   </li><li><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" target="_blank" rel="noopener">Deep Learning, NLP, and Representations(2014)</a>      </li><li><a href="http://ruder.io/word-embeddings-2017/" target="_blank" rel="noopener">Word Embeddings in 2017: Trends and future directions(2017)</a>  </li></ol><p>词表示的方式：       </p><ol><li>独热码：稀疏表示    </li><li>顺序编码：稠密表示          </li><li>词嵌入：前两者的折中，而且可以通过训练，使得两个词的距离表示它们的相似程度      </li></ol><p>tensorflow实现：    </p><pre class=" language-python"><code class="language-python">vocabulary_size <span class="token operator">=</span> <span class="token number">50000</span>embedding_size <span class="token operator">=</span> <span class="token number">150</span><span class="token comment" spellcheck="true"># 创建一个待训练的词向量变量</span>embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>random_uniform<span class="token punctuation">(</span><span class="token punctuation">[</span>vocabulary_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 占位符，输入单词的顺序编码</span>train_inputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用embedding_lookup获取对应单词的顺序编码对应的词向量表示</span>embed <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>embedding_lookup<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> train_inputs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 投喂语料库训练出合适的词向量</span><span class="token comment" spellcheck="true"># 投喂前需要预处理语料库，比如将一些未知的单词都置为&lt;UNK>、将链接都置为&lt;URL></span></code></pre><p>当然，也可以直接下载已经训练好的词向量载入给 <code>embedding</code> 然后直接使用；      </p><h4 id="机器翻译（Encoder-Decoder）"><a href="#机器翻译（Encoder-Decoder）" class="headerlink" title="机器翻译（Encoder-Decoder）"></a>机器翻译（Encoder-Decoder）</h4><p>参考：<a href="https://www.tensorflow.org/tutorials/seq2seq" target="_blank" rel="noopener">Tensorflow Tutorial - Seq2Seq</a> 【<a href="https://github.com/google/seq2seq" target="_blank" rel="noopener">代码</a>】    </p><p>训练时：<br><img src="/Handson-ML/14Translate.png" alt="14Translate">    </p><ul><li>源语句投喂给Encoder，注意应该使第一个单词最先进入Decoder      </li><li>目标语句投喂给Decoder     </li><li>Decoder对每个时间步产生一个评分并交由Softmax层得到单词的输出概率      </li></ul><p>预测时：<br><img src="/Handson-ML/14Translate_Inference.png" alt="14Translate_Inference">     </p><p>Google - Seq2Seq项目的特别之处：     </p><ol><li>独特的处理变长序列的方式<br> 前述提到用 <strong>sequence_length参数+静态展开</strong> 或 <strong>动态展开</strong> 的方式来处理变长序列；<br> 而Seq2Seq采用另一种方式——<br> 先将语句根据不同长度段分别放入不同的桶中（比如有接收长度为1~6的语句桶、接收长度为7~12的语句桶）；<br> 再用符号 <code>&lt;pad&gt;</code> 将桶内的语句填充到同一规格（比如<code>I drink milk &lt;eos&gt;</code>被填充为<code>I drink milk &lt;eos&gt; &lt;pad&gt; &lt;pad&gt;</code>）；<br> （注意，源语句在前面填充符号，目标语句在末尾填充符号）<br> 然后用一个 <code>target_weights</code> 向量表征每个单词的权重（比如<code>I drink milk &lt;eos&gt; &lt;pad&gt; &lt;pad&gt;</code>的权重为<code>[1,1,1,1,0,0]</code>）；<br> 这样，当损失函数与 <code>target_weights</code> 向量相乘时，<code>&lt;pad&gt;</code> 就被忽略了；      </li><li>使用<strong>Sampled Softmax</strong>技术<br> 论文：<a href="https://arxiv.org/pdf/1412.2007v2.pdf" target="_blank" rel="noopener">On Using Very Large Target Vocabulary for Neural Machine Translation</a><br> 当输出字典非常大（比如50000）时，Decoder将产生50000维的向量，使得计算softmax函数时变得非常复杂；<br> 为了避免这个问题，可以使Decoder输出小得多的向量（如1000维），然后用采样技术来评估损失；<br> 这在tensorflow中可以借助函数 <code>sampled_softmax_loss()</code> 实现    </li><li>使用注意力机制<br> 论文：<a href="https://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a>    </li><li>Seq2Seq使用了 <code>tf.nn.legacy_seq2seq</code> 模块，模块中包含了各种各样的Encoder-Decoder模型<br> 比如 <code>embedding_rnn_seq2seq()</code> 函数创建一个与前述机器翻译训练时的图中相同的模型       </li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络CNN</title>
      <link href="/2018/04/18/CNN/"/>
      <url>/2018/04/18/CNN/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=458725210&auto=0&height=32"></iframe><br>BGM：<strong>《四月是你的谎言》ED2</strong>        </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li><li>《<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">Deep Learning 深度学习(2017)</a>》        </li></ol><h3 id="CNN原理"><a href="#CNN原理" class="headerlink" title="CNN原理"></a>CNN原理</h3><p>卷积神经网络主要由<strong>卷积层+激活函数+池化层</strong>组成，并且在最后用全连接层输出——<br><img src="/Handson-ML/12CNN.png" alt="12CNN">     </p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p><a href="/ML-Andrew/ML-Andrew-notes3.html#反向传播算法?_blank">机器学习-吴恩达/3 非线性分类器——神经网络/反向传播算法 | Hey~YaHei!</a><br>论文：<a href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf" target="_blank" rel="noopener">Learning representation by back-propagating errors(1986)</a>   </p><p>前向传播：<br>$$ z_i = \omega_i^T x_{i}  $$<br>其中，<br>$z_i$ 为第i层的损失，即 $z_i = Cost(x_{i+1}, y)$；<br>$x_i$ 为第i层的输入；<br>$\omega_i$ 为第i层的参数；<br>$x_0$ 为原始输入    </p><p>根据链式法则，$z_i$ 对参数 $\omega_i$ 和输入 $x_i$ 求偏导——<br>$$ \frac{\partial z_i}{\partial \omega_i} = \frac{\partial z_i}{\partial x_{i+1}} \frac{\partial x_{i+1}}{\partial \omega_i} $$<br>$$ \frac{\partial z_i}{\partial x_i} = \frac{\partial z_i}{\partial x_{i+1}} \frac{\partial x_{i+1}}{\partial x_i} $$<br>其中，<br>由于 $x_{i+1}$ 由 $x_i$ 经过 $\omega_i$ 的作用得到，则 $\frac{\partial x_{i+1}}{\partial \omega_i}$ 和 $\frac{\partial x_{i+1}}{\partial x_i}$ 可以直接求得；<br>剩下的部分 $\frac{\partial z_i}{\partial x_{i+1}}$ 是由后一层计算得到；    </p><p>总的来说，误差由后往前传播，<br>$\frac{\partial z_i}{\partial \omega_i}$ 用于梯度下降，如 $\omega_i \gets \omega_i - \eta \frac{\partial z}{\partial \omega_i}$；<br>$\frac{\partial z_i}{\partial x_i}$ 用于前层 $\frac{\partial z_{i-1}}{\partial \omega_{i-1}}$ 的计算     </p><p>这里对不同参数的求导操作非常繁琐，Theano、Tensorflow都采用符号微分的方法进行自动求导（编译时就计算得到导数的数学表示）；<br>具体可以参见“花书”《Deep Learning 深度学习》（中文版）P126-139       </p><h4 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h4><p>卷积层并不是使用严格数学意义的卷积运算，而是使用保留卷积性质但抛弃可交换性的互相关函数；<br><em>卷积运算具有可交换性，这在数学证明上是很有用的，但在神经网络的应用中却是一个比较鸡肋的性质</em><br>卷积操作选用一定大小的卷积核（下图黄色区域）在原始数据上移动，与重合部分数据做乘和运算；<br>依次类推，最终输出一张特征图（Feature Map）<br><img src="/Handson-ML/12Convolutional_Kernel.gif" alt="12Convolutional_Kernel">       </p><p>卷积核的作用相当一个滤波器，其参数是经过学习得到的，可以用于提取图片中的特征；<br>由于核参数是随机初始化的，所以它们很可能会提取出不同的特征；<br>由低层的卷积层提取简单特征，然后逐层堆叠卷积层，将简单特征逐渐抽象为更高层次的语义概念；       </p><p>大核的卷积层可以用多层的小核的卷积层实现；<br>比如用三层3x3卷积核的卷积层可以提取到一层7x7卷积核的卷积层一样的特征——<br><img src="/Handson-ML/12Multi_Conv_Layers.png" alt="12Multi_Conv_Layers"><br>而且，使用多层小核卷积层由以下优势：    </p><ol><li>减少参数<br> 7x7卷积核有 $7 \times 7 = 49$ 个参数，而三层3x3卷积核只有 $3 \times 3 \times 3 = 27$ 个参数      </li><li>增加网络深度<br> 增加网络容量和复杂度    </li></ol><p>卷积操作的变体：     </p><ol><li>扩大原有卷积核在前层的感受野<br> 论文：<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable Convolutional Networks(2017)</a>     </li><li>感受野形状可变（而不再是简单的矩形区域）<br> 论文：<a href="https://arxiv.org/pdf/1511.07122.pdf" target="_blank" rel="noopener">Multi-Scale Context Aggregation by Dilated Convolutions(2016)</a>      </li></ol><h4 id="池化层（Pool）"><a href="#池化层（Pool）" class="headerlink" title="池化层（Pool）"></a>池化层（Pool）</h4><p>池化操作与卷积操作类似，但池化层是<strong>不需要参数</strong>的；<br>选用一定大小的池化核在原始数据上移动，与重合部分数据做一定的聚合运算（取均值、取最值、按一定概率随机取值等）；<br>依次类推，最终输出一张特征图；       </p><p>详见 《<a href="/2018/05/07/漫谈池化层?_blank">漫谈池化层 | Hey~YaHei!</a>》     </p><h4 id="全连接层（Fully-Connection）"><a href="#全连接层（Fully-Connection）" class="headerlink" title="全连接层（Fully Connection）"></a>全连接层（Fully Connection）</h4><p>参考：<a href="https://www.zhihu.com/question/41037974" target="_blank" rel="noopener">全连接层的作用是什么？——魏秀参的回答 | 知乎</a>     </p><p>全连接层在CNN中起“分类器”作用，将卷积层、池化层、激活函数学到的特征表示映射到样本的标记空间；<br>由于最后一层卷积层输出一个若干个二维数据（总体为三维），所以输入FC前通常需要将其展平（Flatten）为一维；       </p><p>实际上，全连接层可以用卷积操作代为实现：      </p><ol><li>如果FC前为FC，则该FC可以转换成1 x 1的卷积     </li><li>如果FC前为卷积层，则该FC可以转换为H x W的卷积（H、W为前层输出的高、宽）       </li></ol><p>由于全连接层参数冗余，一些网络如ResNet、GoogLeNet等采用全局平均池化（GAP）取代FC来融合学到的深度特征；    </p><p>近期研究也发现，FC可以在模型表示能力迁移过程中（尤其是原任务与目标任务差异较大时）充当“防火墙”，保证模型表示能力的迁移；     </p><h3 id="tensorflow实现"><a href="#tensorflow实现" class="headerlink" title="tensorflow实现"></a>tensorflow实现</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>借助 <code>tf.nn.conv2d()</code> ——            </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_sample_images<span class="token comment" spellcheck="true"># 读入一些图片</span>dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>load_sample_images<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>batch_size<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 创建两个三维filter</span><span class="token comment" spellcheck="true"># ... 大小7*7，通道数由图片决定</span><span class="token comment" spellcheck="true"># ... 一个水平filter，一个垂直filter（只有某一行或列为1）</span>filters_test <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>filters_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># vertical line</span>filters_test<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># horizontal line</span><span class="token comment" spellcheck="true"># 图片占位符</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加卷积层</span><span class="token comment" spellcheck="true"># ... X为输入</span><span class="token comment" spellcheck="true"># ... filters是所用的一系列filter，列表最后一维是filter的索引</span><span class="token comment" spellcheck="true"># ... stride是步长，针对输入而言的，比如这里batch、行、列、通道的步长分别为1,2,2,1</span><span class="token comment" spellcheck="true"># ... padding为填充方式，SAME表示填零，VALID表示不填零（可能会舍弃结尾的部分元素）</span>convolution <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>X<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"SAME"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    output <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>convolution<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> dataset<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示第一张图片的第二个feature map</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>padding参数，SAME和VALID的区别——<br><img src="/Handson-ML/12Padding.png" alt="padding">          </p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>借助<code>tf.nn.max_pool()</code>、<code>tf.nn.avg_pool()</code> 等——       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_sample_images<span class="token comment" spellcheck="true"># 读入一些图片</span>dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>load_sample_images<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>batch_size<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 图片占位符</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加最大池化层</span><span class="token comment" spellcheck="true"># ... X为输入的数据</span><span class="token comment" spellcheck="true"># ... ksize为池化核大小，这里表示每次一张图片，池化核大小为2x2，池化核每次只应用于一个通道</span><span class="token comment" spellcheck="true"># ...... 注意：tensorflow不支持多对象池化，所以第一维必须是1</span><span class="token comment" spellcheck="true"># .......... 而且，只支持平面池化或者深度池化</span><span class="token comment" spellcheck="true"># .......... 也就是说，要么深度（通道）参数置为1，要么长宽都置为1</span><span class="token comment" spellcheck="true"># ... stride为步长，同卷积层，这里batch、行、列、通道的步长分别为1,2,2,1</span><span class="token comment" spellcheck="true"># ... padding为填充方式，同卷积层，这里表示不填充</span>max_pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>X<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"VALID"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    output <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>max_pool<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> dataset<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示第一张图片池化后的效果</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="内存占用计算（以卷积层为例）"><a href="#内存占用计算（以卷积层为例）" class="headerlink" title="内存占用计算（以卷积层为例）"></a>内存占用计算（以卷积层为例）</h3><p>考虑 $N_f$ 个大小为 $m_f \times n_f \times c$ 的三维filter组成的卷积层，<br>假设输入数据（比如图片）大小为 $m_i \times n_i$，每个batch有 $N_i$ 个instances，<br>那么每个filter输出feature map的大小也为 $m_i \times n_i$；        </p><p>此时,<br>该卷积层包含参数数量为——<br>$$ N_p = (m_f \times n_f \times c \underbrace{+1}_{\text{偏置单元}}) \times N_f $$<br>该卷积层输出的feature maps的变量总数为——<br>$$ N_v = m_i \times n_i \times N_i $$<br>该卷积层每一趟要执行的运算次数为——<br>$$ N_o = \underbrace{m_i \times n_i \times N_i}_{\text{feature maps的变量总数}} \times \underbrace{m_f \times n_f \times c}_{\text{卷积核大小}} $$        </p><p>比如，200个大小为 $5 \times 5 \times 3$ 的filter组成的卷积层，输入大小为 $150 \times 100$ 的三通道图片；<br>那么该层参数数量为 $ (5 \times 5 \times 3 + 1) \times 200 = 15200 $ ；<br>如果每个batch大小为1，使用float32存储变量，那么需要占用内存 $200 \times 150 \times 100 \times 32 = 96,000,000bits$ （约11.4MB）；<br>需要进行 $200 \times 150 \times 100 \times 5 \times 5 \times 3 = 225,000,000$ 次float乘法；      </p><p>解决内存溢出的办法：     </p><ol><li>加大步长达到降维的目的（使feature map比输入小）       </li><li>减少一些层      </li><li>使用占用空间更少的变量类型        </li><li>分布式运算</li></ol><h3 id="经典的CNN分类架构"><a href="#经典的CNN分类架构" class="headerlink" title="经典的CNN分类架构"></a>经典的CNN分类架构</h3><p>目前常见的CNN分类架构有LeNet-5、AlexNet(2012)、NIN(2014)、VGG-Nets(2015)、GoogLeNet(2015)、ResNet(2015)等；<br>详见 《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》     </p><h3 id="目标检测架构"><a href="#目标检测架构" class="headerlink" title="目标检测架构"></a>目标检测架构</h3><p>参见 <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741534&amp;idx=1&amp;sn=02dc164ffcedbf22124b97841ba67fe5&amp;chksm=871adf60b06d567690fa2328b161c012a464687768e50f812a51b5533a7d68b99af1cf8f02b8&amp;scene=0#rd" target="_blank" rel="noopener">从RCNN到SSD，这应该是最全的一份目标检测算法盘点 | 机器之心(2018)</a>【<a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9" target="_blank" rel="noopener">原文</a>】     </p><h3 id="CNN可视化"><a href="#CNN可视化" class="headerlink" title="CNN可视化"></a>CNN可视化</h3><p>论文：    </p><ol><li><a href="https://cs.nyu.edu/~fergus/drafts/deconv_iccv_names.pdf" target="_blank" rel="noopener">Adaptive Deconvolutional Networks for Mid and High Level Feature Learning(2011)</a> 提出反卷积技术     </li><li><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks(2013)</a> 用反卷积技术实现CNN可视化（以AlexNet为例）    </li></ol><h3 id="网络压缩"><a href="#网络压缩" class="headerlink" title="网络压缩"></a>网络压缩</h3><p>深度神经网络面临严峻的<strong>过参数化（over-parameterization）</strong>问题，<br>如论文 <a href="http://export.arxiv.org/pdf/1306.0543" target="_blank" rel="noopener">Predicting Parameters in Deep Learning(2014)</a> 指出只给定很小一部分参数子集（约5%）就可以完整地重构剩余的参数；    </p><p>但事实上，参数的冗余在模型<strong>训练过程中</strong>是十分必要的，因为面临一个极其复杂的非凸优化问题，对现有基于梯度下降的优化算法而言，参数冗余保证了网络能够收敛到一个比较好的最优值。一定程度上，网络越深，参数越多，模型越复杂，最终效果也往往越好；     </p><p>压缩既指体积上的压缩，也指时间上的压缩。<br>绝大多数压缩算法旨在将一个庞大而复杂的<strong>预训练模型</strong>转化为一个精简的小模型；<br>按对网络结构的破坏程度分，可以分为前端压缩和后端压缩——     </p><ul><li><strong>前端压缩</strong><br>  不改变原网络结构，仅仅在原模型基础上减少网络层数或滤波器个数，可以完美适配现有的深度学习库；<br>  主要包括知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝等     </li><li><strong>后端压缩</strong><br>  尽可能减少模型大小，对原始网络造成极大程度的改造（往往不可逆），必须开发相应配套的运行库甚至专门的硬件设备；<br>  主要包括低秩近似、未加限制的剪枝、参数量化、二值网络等    </li><li>前端压缩和后端压缩是互补的关系<br>  通过相互结合，将前端压缩和后端压缩级联起来，可以在最大程度上减少模型复杂度       </li><li>此外，也有人试图设计更加紧凑的网络结构，对新的网络结构进行训练<br>  这也能减小模型复杂度，但不是严格意义上的网络压缩    </li></ul><!-- 妈耶！好难啊，先放一放#### 低秩近似       基本思想：      CNN的卷积操作由矩阵乘法完成，权重矩阵往往稠密巨大，带来计算和存储上的巨大开销；      可以将稠密的矩阵用若干个小规模矩阵近似重构出来，这类算法大多采用低秩近似的技术      对于权重矩阵 $W \in R^{m \times n}$ ，用若干低秩矩阵 $M_i$ 组合来进行表示——      $$ W = \sum^n_{i=1} \alpha_i M_i $$        其中，$M_i \in R^{m \times n}$ 且其秩为 $r_i << min(m,n)$；       并且可以对每个低秩矩阵进一步分解为小规模矩阵的乘积——       $$ M_i = G_i H_i^T $$        其中，$G_i \in R^{m \times r_i}$， $H_i \in R^{n \times r_i}$      当 $r_i$ 很小时，可以大幅度降低总体的存储和计算开销（以全连接层为例）——     * 原始权重矩阵 $W$          参数总数为 $nm$；         计算 $W^T X$          包含 $nm$ 次乘法 和 $n(m-1)$ 次加法；      * 低秩矩阵 $W_i$         参数总数为 $n(m r_i + n r_i + 1)$；        计算 $W^T X = \sum_{i=1}^n \alpha_i M_i^T X = \sum_{i=1}^n \alpha_i H_i G_i^T X$         包含 $r_i(m+n) + n$ 次乘法 和 $r_i((n-1)+(m-1)) + n$ 次加法；         *注意：先计算 $G_i^T X$，再计算 $H_i (G_i^T X)$*           这里看起来参数总数反而变多了（实际上后边有更简单的表示法），但在 $r_i << min(m, n)$ 时可以明显看到计算量减少了-->       <h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p>有效的数据扩充通过扩充训练样本数量，增加样本多样性，一方面可以避免过拟合，另一方面会给模型性能带来提升；       </p><h4 id="简单的数据扩充"><a href="#简单的数据扩充" class="headerlink" title="简单的数据扩充"></a>简单的数据扩充</h4><ul><li>图像水平翻转<br>  数据集扩充一倍    </li><li>随机抠取<br>  一般用较大的正方形（0.8~0.9倍的原图大小）在原图的随机位置抠取图像块；<br>  抠取次数决定数据集的扩充倍数；<br>  同时用设定好的比例抠取图像，避免了图像缩放带来的分辨率失真        </li><li>尺度变换<br>  将原图等比率缩放为原图的0.8、0.9、1.1、1.2、1.3等倍数；<br>  缩放次数决定数据集的扩充倍数；<br>  增加CNN在物体尺度上的鲁棒性       </li><li>旋转<br>  将原图旋转-30度、-15度、15度、30度等角度；<br>  旋转次数决定数据集的扩充倍数；<br>  增加CNN在方向上的鲁棒性        </li><li>色彩抖动<br>  在RGB颜色空间对色彩分布进行轻微扰动，在HSV颜色空间对图像饱和度、明度、色调进行轻微扰动；    </li></ul><p>实践中往往会在上述几种方式叠加使用；<br>相关实践代码可以参见：<a href="https://github.com/aleju/imgaug" target="_blank" rel="noopener">aleju/imgaug | github</a>，一个图像数据集的扩充python库     </p><h4 id="特殊的数据扩充"><a href="#特殊的数据扩充" class="headerlink" title="特殊的数据扩充"></a>特殊的数据扩充</h4><ul><li><p>Fancy PCA<br>  论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Imagenet Classification with Deep Convolutional Neural Networks(2012)</a><br>  和<a href="/2018/05/02/经典的CNN分类架构/#AlexNet">AlexNet</a>网络一同提出；<br>  论文指出，Fancy PCA可以近似捕获自然图像的一个重要特性——<strong>物体特质与光照强度和颜色变化无关</strong>        </p><ol><li>对整个数据集的R、G、B进行PCA操作，得到特征向量 $p_i$ 和特征值 $\lambda_i$，其中 $i=1,2,3$；     </li><li>计算一组随机值$[p_1, p_2, p_3][\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T$，将其作为扰动加到原像素值中；<br> 其中，$\alpha_i$ 为0均值0.1标准差的高斯分布随机值      </li><li>每一个epoch之后，重新选取一次$\alpha_i$进行扰动       </li></ol></li><li><p>监督式数据扩充<br>  海康威视在2016ImageNet竞赛的场景分类任务中提出；<br>  在以物体为中心的图像分类任务中，随机抠取图像块可以取得比较好的效果；<br>  但对于依靠图像整体蕴含的高层语义的场景分类任务中，随机抠取图像块很可能会抠取到关联性比较差的结果（比如“海滩”中抠取到“树”和“天空”）；<br>  可以借助图像标记信息解决这一问题：    </p><ol><li>根据原数据训练一个分类的初始模型     </li><li>利用该模型对每张图生成激活图（activation map）或热力图（heat map）<br> 可以直接将最后一层卷积层特征按深度方向加和得到；<br> 也可以参照论文 <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">Learning Deep Features for Discriminative Localization</a> 生成分类激活图（class activation map）<br> 该热力图可以指示图像区域与场景标记之间的相关概率    </li><li>根据上述概率映射回原图选择较强相关的图像区域作为抠取的图像块</li></ol></li></ul><h3 id="图像预处理：中心式归一化"><a href="#图像预处理：中心式归一化" class="headerlink" title="图像预处理：中心式归一化"></a>图像预处理：中心式归一化</h3><p>在训练集上计算各通道的均值，然后对训练集、验证集、测试集上每一个像素点的各通道都减去该均值；<br>其原理在于，自然图像一般是一类平稳的数据分布，通过减均值操作可以移除图像的共性部分而凸显个体的差异；<br>比如下图通过减均值操作之后，可以发现背景部分被有效“移除”了，而只保留车、建筑等显著区域<br><img src="/Handson-ML/12Image_Preprocess.png" alt="12Image_Preprocess">     </p><h3 id="超参数设定"><a href="#超参数设定" class="headerlink" title="超参数设定"></a>超参数设定</h3><h4 id="输入数据像素大小"><a href="#输入数据像素大小" class="headerlink" title="输入数据像素大小"></a>输入数据像素大小</h4><p>CNN需要对输入的图像统一大小，通常为了便于GPU设备并行，都会统一将图像压缩为 $2^n$ 大小；<br>在设备、时间条件允许的情况下，一般分辨率高的数据有助于网络性能的提升，尤其是对基于注意力模型的网络；<br>一般CNN最后采用FC作为分类器，如果改变了原模型的图像分辨率，通常也需要重新设定FC输入的滤波器大小以及其他相关参数     </p><h4 id="卷积层参数"><a href="#卷积层参数" class="headerlink" title="卷积层参数"></a>卷积层参数</h4><p>包括卷积核大小、卷积步长、卷积核个数（即输出的特征图数量）；      </p><p>实践中通常采用3x3和5x5的小核，小的卷积核有以下作用：      </p><ol><li>增加模型复杂度，防止欠拟合    </li><li>减少参数数量     </li></ol><p>卷积操作可以选择性的搭配填充操作（padding），有以下作用：     </p><ol><li>充分利用和处理输入数据的边缘信息    </li><li>搭配合适的参数可以保持输入、输出大小不变，避免随着网络深度增加输入大小急剧减小<br> 对于fxf的卷积核、步长为1的卷积操作，在边缘各添加 $p=(f-1)/2$ 个像素可以维持输入输出大小不变       </li></ol><p>为了便于GPU设备方便存储，卷积核个数也即输出的特征图数量通常为 $2^n$ ；    </p><p>可参考：<a href="https://www.quora.com/How-can-I-decide-the-kernel-size-output-maps-and-layers-of-CNN?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa" target="_blank" rel="noopener">How can I decide the kernel size, output maps and layers of CNN? | Quora</a>  </p><p>论文：    </p><ol><li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">Efficient BackProp(1998)</a>     </li><li><a href="https://arxiv.org/pdf/1606.02228v2.pdf" target="_blank" rel="noopener">Systematic evaluation of CNN advances on the ImageNet(2017)</a> 比较了ILSVRC上各种技术、模块在不同参数下的表现<br> 对应github评估项目：<a href="https://github.com/ducha-aiki/caffenet-benchmark" target="_blank" rel="noopener">ducha-aiki/caffenet-benchmark | github</a>     </li></ol><p>通常，     </p><ul><li>网络越深越好，但这是以更大的数据集、学习任务复杂度增加为代价的；     </li><li>batch size设为几百，具体视任务而定，批大小约大计算资源的需求就越高，批大小不宜太小（这会导致估计产生较大的偏差）；     </li><li>一开始使用较少的特征图数量，然后逐渐增加并且一边观察误差的变化趋势；    </li><li>小核可以捕捉图像的细节，大核容易丢失图像的细微特征；    </li><li>可以参考类似任务的网络配置；     </li></ul><h4 id="池化层参数"><a href="#池化层参数" class="headerlink" title="池化层参数"></a>池化层参数</h4><ul><li>池化核一般比较小，如2x2、3x3等<br>  为了不丢弃过多输入而损失网络性能，很少使用超过3x3的池化核     </li><li>最常用的是2x2大小、2步长的池化操作<br>  此时输出缩小为原来的四分之一，也即丢弃了75%的响应值     </li></ul><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><p><strong>随机打乱训练数据</strong>：<br>信息论指出——    </p><blockquote><p>从相似的事件中学习总是比从相似事件中学习更具信息量    </p></blockquote><p>在每轮（epoch）训练中随机打乱训练数据，使得模型每次在不同批次看到“不同”的数据，<br>不仅可以提高模型收敛速度，还对模型的泛化性能有略微的提升      </p><p><strong>学习率设定</strong>：     </p><ul><li>初始学习率不宜过大，以0.01和0.001为宜；<br>  如果刚训练几步模型的loss就急剧上升，说明初始学习率过大       </li><li>使用一定的学习计划策略<br>  可参见 <a href="/2018/04/10/优化器/#学习计划（learning-schedules）">优化器/学习计划 | Hey~YaHei!</a>     </li><li>训练过程中观察学习曲线（loss随步数的变化）对学习率进行诊断<br>  <img src="/Handson-ML/learning_rate.png" alt="learning_rate">     </li></ul><p><strong>批规范化操作（BN操作）</strong>：<br>参见 <a href="/2018/04/08/梯度消失与梯度爆炸/#批量归一化（Batch-Normalization-BN）">梯度消失与梯度爆炸/批量归一化 | Hey~YaHei!</a>     </p><p><strong>优化器</strong>：<br>参见 <a href="/2018/04/10/优化器/">优化器 | Hey~YaHei!</a>     </p><p><strong>微调预训练好的神经网络</strong>：<br>用目标任务数据在预训练模型上继续进行训练过程；      </p><ul><li>网络已经在原始数据上收敛，微调时采用更小的学习率（一般在$10^{-4}$及其以下）    </li><li>CNN浅层有更强的泛化特征，深层对应更抽象的语义特征；<br>  微调时往往对前层更新的少，对深层更新的多，故可以设置不同的学习率；      </li><li>微调策略   <ul><li>数据较少且任务非常相似时，可仅微调最后的几层    </li><li>数据较多且任务相似时，可以微调更多甚至全部的网络层    </li><li>当数据较少、差异较大时，可以尝试微调，但不一定能成功<br>  这种情况下还可以借助<strong>部分原始数据与目标数据协同训练</strong>；<br>  论文：<a href="https://arxiv.org/pdf/1702.08690.pdf" target="_blank" rel="noopener">Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning(2017)</a><br>  在浅层特征空间选择目标数据的近邻作为协同训练的原始数据子集；<br>  微调阶段改造为多目标学习任务：将目标任务基于原始数据子集、将目标任务基于全部目标数据；     </li></ul></li></ul><hr><p>2018-05-02<br>将经典的CNN分类架构抽离出来作为单独一篇博文：<br>《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》     </p><hr><p>2018-05-07<br>将池化层原理部分抽离出来作为单独一篇博文：<br>详见 《<a href="/2018/05/07/漫谈池化层?_blank">漫谈池化层 | Hey~YaHei!</a>》         </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>正则化技术</title>
      <link href="/2018/04/11/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
      <url>/2018/04/11/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28160278&auto=0&height=32"></iframe><br>BGM：<strong>《命运石之门：负荷领域的既视感》主题曲</strong>        </p><hr><p>参考：   </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap11<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li></ol><h3 id="提前终止（Early-Stopping）"><a href="#提前终止（Early-Stopping）" class="headerlink" title="提前终止（Early Stopping）"></a>提前终止（Early Stopping）</h3><p>每经过一定迭代次数之后将模型用于验证集上的评估，暂存、更新最近几次在验证集上有一定loss下降的模型；<br>当连续几次在验证集上没有出现明显的loss下降（甚至有可能回升）时终止训练；<br>提前终止通常表现的很好，如果和其他正则化技术共同使用可以获得更好的表现      </p><h3 id="L1、L2正则化"><a href="#L1、L2正则化" class="headerlink" title="L1、L2正则化"></a>L1、L2正则化</h3><p><strong>L2正则化</strong>：<br>又称权重衰减（weight decay）、岭回归（ridge regression）、Tikhonov正则化（Tikhonov regularization）；<br>$$ l_2 = \frac{1}{2} \lambda ||\omega||^2_2 $$<br>其中 $\lambda$ 控制正则项大小，取值越大对模型复杂度的约束程度越大；<br>一般将该l2惩罚项加入到目标函数中，通过目标函数的误差反向传播；        </p><p><strong>L1正则化</strong>：<br>又称Elastic网络正则化；<br>$$ l_1 = \lambda ||\omega||_1 = \sum_i |\omega_i| $$<br>L1正则化不仅能够约束参数量级，还可以使参数稀疏化，使优化后部分参数置为0，并且也有去除噪声的效果；<br>L1和L2惩罚可以联合使用，如 $ \lambda_1 ||\omega||_1 + \lambda_2 ||\omega||_2^2 $     </p><p>具体实现：<br>tensorflow中很多输出变量的函数（如 <code>get_variable</code>、<code>fully_connected</code>）都接受名为 <code>*_regularizer</code> 的参数进行正则化；      </p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> arg_scope<span class="token punctuation">(</span>        <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>        weights_regularizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>l1_regularizer<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 为fully_connected指定L1正则化</span>    hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">)</span>    hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">)</span>    logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">,</span>scope<span class="token operator">=</span><span class="token string">"out"</span><span class="token punctuation">)</span></code></pre><p>注意在计算总的loss时要手动把正则化loss加上——       </p><pre class=" language-python"><code class="language-python">reg_losses <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>REGULARIZATION_LOSSES<span class="token punctuation">)</span>loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>add_n<span class="token punctuation">(</span><span class="token punctuation">[</span>base_loss<span class="token punctuation">]</span> <span class="token operator">+</span> reg_losses<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"loss"</span><span class="token punctuation">)</span></code></pre><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>论文：<br>【提出】<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing co-adaptation of feature detectors(2012)</a><br>【细节讨论】<a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting(2014)</a>           </p><p>核心思想：<br>在每一步训练中，所有神经单元（包含输入神经单元，但不包含输出神经单元）有一定概率p被忽略（但在预测时使用整个网络）；<br>这里p称为<strong>dropout rate</strong>，通常取 $p=0.5$；<br>对应的还有一个<strong>keep probability</strong>，即 $q = 1 - p$；<br>每步训练时都有一部分单元缺失，使得每个单元都有机会在本次训练中占有一定地位，从而使得各个单元可以更好地从训练集中学习，使整个网络在工作时更有弹性；<br>部分“队友”的缺失、部分“输入数据”的缺失，在这种训练下，每个单元有更强的鲁棒性；<br>由于各个单元是一定概率q参与训练的，所以在需要对这些单元进行一定补偿——       </p><ol><li>可以在训练时，对每个单元的输出都除以q        </li><li>也可以在预测时，为每个单元的输出都乘以q<br>这两种方式虽然不完全等价，但实际效果时差不多的          </li></ol><p>具体实现：<br>tensorflow提供专门的dropout层，默认时用第二种补偿方式；         </p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> dropout<span class="token comment" spellcheck="true"># ...</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>bool<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span>keep_prob <span class="token operator">=</span> <span class="token number">0.5</span>X_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X_drop<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">)</span>hidden1_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1_drop<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">)</span>hidden2_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2_drop<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"outputs"</span><span class="token punctuation">)</span></code></pre><blockquote><p>tensorflow提供了两个dropout()，一个在tensorflow.conrtib.layers包里，一个在tensorflow.nn包里——<br>前者在训练时有效，预测时失效；<br>后者在训练和预测时都是有效的；<br>所以一般来说，我们需要的是前者的dropout       </p></blockquote><p>如果说，在训练中加了dropout还是发现过拟合了，可以考虑增加dropout_rate（减小keep_prob）；        </p><p>缺点：       </p><ul><li>dropout减缓了收敛的速度    </li><li>而且，<strong>dropout对卷积层似乎没有作用</strong>（一般只用于全连接层）<br>  参考 <a href="https://www.quora.com/Why-would-I-need-to-apply-a-dropout-layer-before-a-convolutional-layer" target="_blank" rel="noopener">Why would I need to apply a dropout layer before a convolutional layer? | Quora</a><br>  据说是因为卷积层参数数量远少于全连接层，一般不存在过拟合的问题；<br>  大多数经典框架都只在全连接层上使用dropout，参考《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》<br>  不过，<br>  <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting(2014)</a>、<a href="https://arxiv.org/pdf/1511.07289v3.pdf" target="_blank" rel="noopener">FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)(2016)</a>、<a href="http://torch.ch/blog/2015/07/30/cifar.html" target="_blank" rel="noopener">92.45% on CIFAR-10 in Torch | Torch</a>、<a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" target="_blank" rel="noopener">Using convolutional neural nets to detect facial keypoints tutorial | danielnouri</a> 等也对卷积层使用了dropout并有一定的提升；<br>  也有一些关于对卷积层使用dropout或者dropout的变体的论文：<br>  <a href="https://arxiv.org/ftp/arxiv/papers/1512/1512.00242.pdf" target="_blank" rel="noopener">Towards Dropout Training for Convolutional Neural Networks(2015)</a>、<br>  <a href="https://arxiv.org/pdf/1411.4280.pdf" target="_blank" rel="noopener">Efficient Object Localization Using Convolutional Networks(2015)</a>、<br>  <a href="http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf" target="_blank" rel="noopener">Analysis on the Dropout Effect in Convolutional Neural Networks(2016)</a>     </li></ul><h3 id="最大范数（Max-Norm）"><a href="#最大范数（Max-Norm）" class="headerlink" title="最大范数（Max-Norm）"></a>最大范数（Max-Norm）</h3><p>每步训练之后，对权重w进行一定约束——<br>$$ w \gets w \frac{r}{||w||_2} $$<br>其中，r为超参数max-norm，$||w||_2$ 表示w的L2范数；        </p><p>减小r将增加惩罚的力度，这将有助于抑制过拟合；<br>同时，如果没有使用BN层，那么max-norm也有抑制梯度消失与爆炸的作用；       </p><p>具体实现：<br>tensorflow没有直接提供max-norm的操作：         </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 借助tf.clip_by_norm构造max_norm操作</span>threshold <span class="token operator">=</span> <span class="token number">1.0</span>clipped_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>clip_by_norm<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clip_norm<span class="token operator">=</span>threshold<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>clip_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clipped_weights<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># [...]</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># [...]</span>        <span class="token keyword">for</span> X_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> zip<span class="token punctuation">(</span>X_batches<span class="token punctuation">,</span> y_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>training_op<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_batch<span class="token punctuation">,</span> y<span class="token punctuation">:</span> y_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 在每一步（训练一个batch）的最后手动调用一次max_norm的操作</span>            clip_weights<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>如果有很多权重需要增加max_norm操作，那代码将变得十分冗余；<br>更加简洁的做法是自己构造一个Regularization——       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">max_norm_regularizer</span><span class="token punctuation">(</span>threshold<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"max_norm"</span><span class="token punctuation">,</span> collection<span class="token operator">=</span><span class="token string">"max_norm"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">max_norm</span><span class="token punctuation">(</span>weights<span class="token punctuation">)</span><span class="token punctuation">:</span>        clipped <span class="token operator">=</span> tf<span class="token punctuation">.</span>clip_by_norm<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clip_norm<span class="token operator">=</span>threshold<span class="token punctuation">,</span> axes<span class="token operator">=</span>axes<span class="token punctuation">)</span>        clip_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clipped<span class="token punctuation">,</span> name<span class="token operator">=</span>name<span class="token punctuation">)</span>        tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span>collection<span class="token punctuation">,</span> clip_weights<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 注意，这个clip_weights需要在外部使用，所以把它放入collection当中</span>        <span class="token keyword">return</span> None <span class="token comment" spellcheck="true"># 不需要将loss加到整体的全局的loss上，所以只需要返回None</span>    <span class="token keyword">return</span> max_normmax_norm_reg <span class="token operator">=</span> max_norm_regularizer<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">,</span> weights_regularizer<span class="token operator">=</span>max_norm_reg<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 取出collection中的clip_weights操作</span>clip_all_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span><span class="token string">"max_norm"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># [...]</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># [...]</span>        <span class="token keyword">for</span> X_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> zip<span class="token punctuation">(</span>X_batches<span class="token punctuation">,</span> y_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>training_op<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_batch<span class="token punctuation">,</span> y<span class="token punctuation">:</span> y_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>clip_all_weights<span class="token punctuation">)</span></code></pre><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>人为修改训练集数据，达到扩充数据集的目的；<br>但这种修改必须是“可学习的”，比如添加白噪声是没意义的，因为白噪声不可学习；<br>比如做图片分类的时候，可以平移、旋转、缩放图片甚至改变亮度来做到数据增强，使得模型对图片的位置、角度、大小不那么敏感；         </p><p>tensorflow提供了一些图片操纵的函数，可以从api中查到；<br>图像数据的扩充还可以参考 <a href="/2018/04/18/CNN/#数据扩充">卷积神经网络CNN/数据扩充 | Hey~YaHei!</a>      </p><h3 id="验证集的使用"><a href="#验证集的使用" class="headerlink" title="验证集的使用"></a>验证集的使用</h3><p>验证集用于在训练阶段评测模型预测性能，一般在每轮（epoch）或每个批处理训练（step）后在训练集、验证集上分别做网络前向运算，绘制学习曲线，检验模型泛化能力；<br>比如图像分类准确率：<br>如图a，验证集准确率一直低于训练集准确率，无明显下降趋势，此时模型复杂度欠缺，是为欠拟合；<br>如图b，验证集准确率不仅低于训练集准确率，还有明显下降趋势，此时模型发生过拟合；<br><img src="/Handson-ML/over_fitting.png" alt="over_fitting">    </p><p>其他模型评估方法可参考 <a href="/ML-ZhouZhihua/《机器学习》Ch02.html">机器学习 - 周志华/Ch02 模型评估和选择 | Hey~YaHei!</a><br>以及 <a href="/ML-Andrew/ML-Andrew-notes2.html">机器学习 - 吴恩达/2 过拟合与正则化技术 | Hey~YaHei!</a>     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>优化器</title>
      <link href="/2018/04/10/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2018/04/10/%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=466795188&auto=0&height=32"></iframe><br>BGM：<strong>《文豪野犬》ED1</strong><br>唔，就是它，因为这个番才认识太宰治和芥川          </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap11<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><h3 id="常见的加速训练技术"><a href="#常见的加速训练技术" class="headerlink" title="常见的加速训练技术"></a>常见的加速训练技术</h3><ul><li><a href="/2018/04/08/梯度消失与梯度爆炸/#Xavier-Initialization-Glorot-Initialization">恰当的的权重初始化策略</a>    </li><li><a href="/2018/04/08/梯度消失与梯度爆炸/#relu——nonsaturating-activation-function">恰当的激活函数</a>      </li><li><a href="/2018/04/08/梯度消失与梯度爆炸/#批量归一化（Batch-Normalization-BN）">批量归一化</a></li><li><a href="/2018/04/09/复用预训练层">复用部分预训练网络</a></li><li><a href="/2018/04/10/优化器">使用更快的优化器</a></li></ul><p>常见的优化器有：Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam optimization<br>其中<a href="#Adam-optimization">Adam optimization</a>是目前表现最好的优化器；不过它除了学习率之外还有额外两个超参数需要手工调整      </p><h3 id="传统梯度下降"><a href="#传统梯度下降" class="headerlink" title="传统梯度下降"></a>传统梯度下降</h3><p>$$ \theta \gets \theta - \eta \bigtriangledown_\theta J(\theta) $$<br>固定的下降速度（梯度作为速度）        </p><h3 id="动量法（Momentum-optimization）"><a href="#动量法（Momentum-optimization）" class="headerlink" title="动量法（Momentum optimization）"></a>动量法（Momentum optimization）</h3><p>论文：<a href="https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods" target="_blank" rel="noopener">Some methods of speeding up the convergence of iteration methods(1964)</a>      </p><p>$$ m \gets \beta m + \eta \bigtriangledown_\theta J(\theta), 0 &lt;= \beta &lt;= 1 $$<br>$$ \theta \gets \theta - m $$<br>梯度作为加速度使用，这里m表示动量；<br>引入新的超参数 $\beta$ 与先前的 $m$ 相乘，以继承先前的动量，并防止动量过快增长；<br>当$\beta=0$时为完全摩擦（退化为传统梯度下降），当$\beta=1$时为完全光滑，通常来说取$\beta=0.9$<br>此时下降速度变为：$ \frac{1}{1-\beta} \eta \bigtriangledown_\theta J(\theta) $<br>取$\beta=0.9$时，下降速度理论上变为传统梯度下降的10倍！！     </p><p>具体实现：</p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>MomentumOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></code></pre><h3 id="涅斯捷罗夫加速梯度（Nesterov-Accelerated-Gradient-NAG）"><a href="#涅斯捷罗夫加速梯度（Nesterov-Accelerated-Gradient-NAG）" class="headerlink" title="涅斯捷罗夫加速梯度（Nesterov Accelerated Gradient, NAG）"></a>涅斯捷罗夫加速梯度（Nesterov Accelerated Gradient, NAG）</h3><p>又称<strong>Nesterov Momentum optimization</strong><br>论文：<a href="https://www.researchgate.net/publication/257291640_A_method_of_solving_a_convex_programming_problem_with_convergence_rate_O1k2" target="_blank" rel="noopener">A method of solving a convex programming problem with convergence rate O(1/k^2)(1983)</a>        </p><p>$$ m \gets \beta m + \eta \bigtriangledown_\theta J(\theta + \beta m), 0 &lt;= \beta &lt;= 1 $$<br>$$ \theta \gets \theta - m $$<br>动量法的变体，比动量法更快，而且振荡更小；<br>与动量法相比，区别在于计算的是损失函数在 $\theta + \beta m$ 上而不是 $\theta$ 上的梯度：<br><img src="/Handson-ML/11.3Nesterov.png" alt="Nesterov">            </p><p>具体实现：      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 与动量法的优化器一样，只不过打开了nesterov开关</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>MomentumOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> use_nesterov<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>论文：<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="noopener">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization(2011)</a>          </p><p>$$ s \gets s + \bigtriangledown_\theta J(\theta) \otimes \bigtriangledown_\theta J(\theta) $$<br>$$ \theta \gets \theta - \eta \bigtriangledown_\theta J(\theta) \oslash \sqrt{s+\epsilon} $$      </p><p>其中，<br>$\otimes$ 表示逐项相乘，$\oslash$ 表示逐项相除；<br>$s$ 继承上一次的 $s$，并且累加一个梯度的平方（如果沉浸在某个权重方向上，那么更新速度会越来越快）；<br>$\theta$ 的更新与传统梯度下降类似，不过更新时梯度除以一个 $\sqrt{s+\epsilon}$，$\epsilon$ 通常取 $10^{-10}$ 防止 $s=0$ 的情况；        </p><p>考虑两权重梯度差异显著的情况（如图，$\theta_1$ 平缓，$\theta_2$ 陡峭），<br>传统的梯度下降，会在陡峭的 $\theta_2$ 上很快到达一个比较的位置，而平缓的 $\theta_1$ 方向上梯度下降缓慢；<br>而AdaGrad能够检测到这种特殊情况，从而加速梯度下降<br><img src="/Handson-ML/11.3AdaGrad.png" alt="AdaGrad">            </p><p>简而言之，AdaGrad衰减了学习率，但是会在沉浸的权重方向上加速更新；<br>其学习过程中，学习率是自适应的，这有助于更佳直接地找到全局最优点，而且更容易调整超参数 $\eta$        </p><p>tensorflow中提供了相应的 <code>AdagradOptimizer</code> 优化器<br>AdaGrad在一些简单的任务如线性回归上可能会比较高效         </p><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>该优化器由eoffrey Hinton在其课堂上提出，而没有形成正式的论文，研究者通常用 <strong>slide 29 in lecture 6</strong> 来引用它；<br>幻灯片：<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a><br>视频：<a href="https://www.youtube.com/watch?v=defQQqkXEfE&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=29" target="_blank" rel="noopener">https://www.youtube.com/watch?v=defQQqkXEfE&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=29</a><br>AdaGrad的变体，解决了AdaGrad学习率衰减偏快的问题        </p><p>$$ s \gets \beta s + (1-\beta) \bigtriangledown_\theta J(\theta) \otimes \bigtriangledown_\theta J(\theta) $$<br>$$ \theta \gets \theta - \eta \bigtriangledown_\theta J(\theta) \oslash \sqrt{s+\epsilon} $$      </p><p>通常，$\beta$ 取0.9      </p><p>具体实现：    </p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>RMSPropOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> decay<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span></code></pre><h3 id="Adam-optimization"><a href="#Adam-optimization" class="headerlink" title="Adam optimization"></a>Adam optimization</h3><p>论文：<a href="https://arxiv.org/pdf/1412.6980v8.pdf" target="_blank" rel="noopener">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION(2015)</a><br>集成动量法和RMSProp的思想——      </p><p>$$ m \gets \beta_1 m + (1-\beta_1) \bigtriangledown_\theta J(\theta) $$<br>$$ s \gets \beta_2 s + (1-\beta_2) \bigtriangledown_\theta J(\theta) \otimes \bigtriangledown_\theta J(\theta) $$<br>$$ m \gets \frac{m}{1-\beta_1^T} $$<br>$$ s \gets \frac{s}{1-\beta_2^T} $$<br>$$ \theta \gets \theta - \eta m \oslash \sqrt{s + \epsilon} $$</p><p>其中，T表示迭代次数（从1记起）         </p><p>通常，取 $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}, \eta = 0.001$</p><h3 id="Jacobian优化-amp-Hessian优化"><a href="#Jacobian优化-amp-Hessian优化" class="headerlink" title="Jacobian优化 &amp; Hessian优化"></a>Jacobian优化 &amp; Hessian优化</h3><p>以上讨论都是基于一次偏导的Jacobian矩阵，实际上用基于二次偏导的Hessian矩阵可以取得更好的优化效果；<br>但这很难应用到DNN上，因为每层都会输出 $n^2$ 个Hessian矩阵（如果是Jacobian，只需要n个），其中n是参数数量，而DNN参数数量大的惊人，所以基于Hessian的优化反而会因为计算大量的Hessian矩阵而速度下降并且需要大量的空间来进行计算；        </p><h3 id="训练稀疏模型"><a href="#训练稀疏模型" class="headerlink" title="训练稀疏模型"></a>训练稀疏模型</h3><p>有时候需要一个占据空间少、预测时间短的稀疏模型，可以采用以下技术进行稀疏化：       </p><ol><li>用常规的方式训练模型，然后将数值很小的权重置为0       </li><li>用L1惩罚项进行正则化         </li><li>使用Dual Averaging技术（也叫Follow The Regularized Leader, FTRL）<br> 论文：<a href="https://scholar.google.fr/citations?view_op=view_citation&amp;citation_for_view=DJ8Ep8YAAAAJ:Tyk-4Ss8FVUC" target="_blank" rel="noopener">Primal-dual subgradient methods for convex problems(2009)</a><br> tensorflow也提供了相应的优化器 <code>FTRLOptimizer</code>，它是根据<a href="https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf" target="_blank" rel="noopener">Ad Click Prediction: a View from the Trenches(2013)</a>实现的，是FTRL的变体        </li></ol><h3 id="学习计划（learning-schedules）"><a href="#学习计划（learning-schedules）" class="headerlink" title="学习计划（learning schedules）"></a>学习计划（learning schedules）</h3><p>如图，当学习率太低时训练过慢，学习率太高时不容易得到最优解；<br>可以在开始训练时给一系列不同的学习率跑一小段时间，比较它们的loss曲线，来确定一个比较合适的学习率；<br><img src="/Handson-ML/11.3learning_rate.png" alt="">     </p><p>除此之外，还有一些有用的策略，称为学习计划learning schedules——       </p><ul><li>分段常数学习率（Predetermined piecewise constant learning rate）<br>  预设好每过多少epochs就改变学习率；需要比较多的人工调整      </li><li>性能调整（Performance scheduling）<br>  每N步用验证集计算一次loss，当loss不再减小的时候衰减学习率（通常乘以一个因子 $\gamma$）    </li><li>随时间指数衰减（Exponential scheduling）<br>  $$ \eta(t) = \eta_0 10^{-t/r}, t为迭代次数 $$<br>  需要人工调整参数 $\eta_0$ 和r，学习率每r步衰减一次；       </li><li>随时间乘方衰减（Power scheduling）<br>  $$ \eta(t) = \eta_0 (1+t/r)^{-c} $$<br>  通常取 $c=1$，有点类似Exponential scheduling，但衰减速度稍微慢一些</li></ul><p>论文<a href="http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/40808.pdf" target="_blank" rel="noopener">AN EMPIRICAL STUDY OF LEARNING RATES IN DEEP NEURAL NETWORKS FOR SPEECH RECOGNITION(2013)</a> 比较了一些主流的学习计划——<br>在语音识别任务中使用动量法优化的前提下，         </p><ol><li>performance scheduling和exponential scheduling都表现的很好        </li><li>但是exponential scheduling容易实现、容易调整、收敛更快       </li></ol><p>具体实现：</p><pre class=" language-python"><code class="language-python">initial_learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>decay_steps <span class="token operator">=</span> <span class="token number">10000</span>decay_rate <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span><span class="token number">10</span>global_step <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>learning_rate <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>exponential_decay<span class="token punctuation">(</span>initial_learning_rate<span class="token punctuation">,</span> global_step<span class="token punctuation">,</span> decay_steps<span class="token punctuation">,</span> decay_rate<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>MomentumOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> global_step<span class="token operator">=</span>global_step<span class="token punctuation">)</span></code></pre><p>由于AdaGrad, RMSProp和Adam optimization等优化器具有自适应调整学习率的能力，因而不需要额外的学习计划来衰减学习率；</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>复用预训练层</title>
      <link href="/2018/04/09/%E5%A4%8D%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B1%82/"/>
      <url>/2018/04/09/%E5%A4%8D%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=544000838&auto=0&height=32"></iframe><br>BGM：<strong>《Megalo Box》ED</strong><br>NakamuraEmi的歌有种说不出来的特别      </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap11<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><h3 id="迁移学习（transfer-learn）"><a href="#迁移学习（transfer-learn）" class="headerlink" title="迁移学习（transfer learn）"></a>迁移学习（transfer learn）</h3><p>如果已经训练好了一个网络（如可以识别猫、狗等动物），如果需要训练一个新的类似任务的网络（如只识别猫），可以直接使用已有网络的一部分底层，在这些层的基础上加几个层，训练时固定复用层的权重，只训练新加的几个层；     </p><ol><li>可以加速训练过程     </li><li>可以使用较小的训练集    </li><li>但是要求新网络的输入数据大小与复用网络的输入数据大小保持一致</li><li>仅适用于数据的低层次特征相类似的任务      </li><li>任务越接近，可以复用的底层越多；甚至非常接近的任务，可以只替换output层</li><li>如果需要进一步fine-tune，可以在充分训练新加的几个层之后，再整体训练一段时间（复用层的权重也参与训练）     </li></ol><h3 id="复用tensorflow模型"><a href="#复用tensorflow模型" class="headerlink" title="复用tensorflow模型"></a>复用tensorflow模型</h3><p>如果复用整个模型，直接用 <code>tf.Saver</code> 的 <code>restore</code> 方法即可；<br>如果只复用模型的一部分，可以借助 <code>tf.get_collection</code> 函数——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 创建初始化op</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建收集op</span><span class="token comment" spellcheck="true"># ... 第一个参数为key，指明获取哪些对象；key在tf.GraphKeys中有一系列定义（这里表示获取可训练的变量）</span><span class="token comment" spellcheck="true"># ... 第二个参数为scope，接受一个正则表达式，作为一个过滤器来筛选特定scope的对象</span><span class="token comment" spellcheck="true"># ...... 这里表示获取名为hidden1, hidden2, hidden3的三个scope中的所有对象</span><span class="token comment" spellcheck="true"># ... 返回一个对象名称组成的列表</span>reuse_vars <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>TRAINABLE_VARIABLES<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden[123]"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个用于读取原模型的Saver</span><span class="token comment" spellcheck="true"># ... tf.train.Saver的第一个参数为var_list，接受一个字典（一系列键值对）</span><span class="token comment" spellcheck="true"># ...... 在restore时，只从原模型中读出名字与键对应的对象，并且在当前模型中命名为值</span>reuse_vars_dict <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>var<span class="token punctuation">.</span>name<span class="token punctuation">,</span> var<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token keyword">for</span> var <span class="token keyword">in</span> reuse_vars<span class="token punctuation">]</span><span class="token punctuation">)</span>original_saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span>reuse_vars_dict<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建优化器和训练op</span><span class="token comment" spellcheck="true"># ... 只训练hidden4和输出层，而固定（freezing）hidden[123]的权重</span><span class="token comment" spellcheck="true"># ...... 此时hidden[123]称为frozen layers</span><span class="token comment" spellcheck="true"># ... tf.get_collection获取需要训练的变量列表，并将列表传递给优化器来指定训练的变量</span>train_vars <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>TRAINABLE_VARIABLES<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden4|outputs"</span><span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>train_vars<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个用于保存新模型的Saver</span>new_saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init<span class="token punctuation">)</span>    original_saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span><span class="token string">"./my_original_model.ckpt"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># .....训练略sess.run(training_op).....</span>    new_saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"./my_new_model.ckpt"</span><span class="token punctuation">)</span></code></pre><h3 id="复用其他框架的模型"><a href="#复用其他框架的模型" class="headerlink" title="复用其他框架的模型"></a>复用其他框架的模型</h3><ol><li>用其他框架的API获得其模型下的权重     </li><li>创建相应的placeholder占位符    </li><li>创建相应的网络结构      </li><li>创建op操作，将placeholder赋值给网络结构里的权重变量      </li><li>session运行时将已有权重投喂给placeholder      </li></ol><h3 id="加速训练：复用frozen层的输出结果（牺牲空间）"><a href="#加速训练：复用frozen层的输出结果（牺牲空间）" class="headerlink" title="加速训练：复用frozen层的输出结果（牺牲空间）"></a>加速训练：复用frozen层的输出结果（牺牲空间）</h3><p>由于frozen层权重固定，对一个训练集来说，每一组训练数据在frozen层的最终输出是固定的；<br>训练时我们往往需要在重复的数据上训练很多趟，所以如果记录下frozen层的最终输出，<br>可以避免数据在frozen层进行重复计算，从而达到加速训练的目的，<br><em>但相应地会占据大量的内存空间（取决于数据集大小和frozen最后一层输出的大小）</em>          </p><p>具体实现——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ... 假设hidden[123]固定，训练hidden4和outputs ...</span>n_epochs <span class="token operator">=</span> <span class="token number">100</span>n_batches <span class="token operator">=</span> <span class="token number">500</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 先投喂整个数据集来获得frozen层的最终输出</span>    hidden3_outputs <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>hidden3<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_train<span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 分趟训练</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 获取打乱的训练数据索引集（permutation相当于arange和shuffle的混合）</span>        shuffled_idx <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>permutation<span class="token punctuation">(</span>len<span class="token punctuation">(</span>hidden3_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 构造本趟训练的batch，这里不再直接使用训练集作为输入，而是从前边计算好的hidden3的输出直接输入给下一层</span>        <span class="token comment" spellcheck="true"># ... np.array_split将一个列表等分成若干份</span>        <span class="token comment" spellcheck="true"># ...... np.array可以给定一个索引列表，从中抽取出对应索引的元素组成一个新的列表</span>        <span class="token comment" spellcheck="true"># ...... 如：</span>        <span class="token comment" spellcheck="true"># ......... a = np.arange(10)</span>        <span class="token comment" spellcheck="true"># ......... a[[0,2,4,6]]</span>        <span class="token comment" spellcheck="true"># ......... >> array([0,2,4,6])</span>        hidden3_batches <span class="token operator">=</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>hidden3_outputs<span class="token punctuation">[</span>shuffled_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> n_batches<span class="token punctuation">)</span>        y_batches <span class="token operator">=</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>y_train<span class="token punctuation">[</span>shuffled_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> n_batches<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 对各个batch依次进行训练</span>        <span class="token keyword">for</span> hidden3_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> zip<span class="token punctuation">(</span>hidden3_batches<span class="token punctuation">,</span> y_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>training_op<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>hidden3<span class="token punctuation">:</span> hidden3_batch<span class="token punctuation">,</span> y<span class="token punctuation">:</span> y_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 直接向hidden3投喂数据</span></code></pre><blockquote><p>np.random.permutation 和 np.random.shuffle的区别：<a href="https://blog.csdn.net/u010099080/article/details/73395601" target="_blank" rel="noopener">https://blog.csdn.net/u010099080/article/details/73395601</a><br>… shuffle只接受list，然后打乱list内的元素并返回<br>… permutation既接受list也接受int<br>…… 如果是list，相当于shuffle( list )<br>…… 如果时int，相当于shuffle( arange(int) )      </p></blockquote><h3 id="Model-Zoos"><a href="#Model-Zoos" class="headerlink" title="Model Zoos"></a>Model Zoos</h3><p>Tensorflow提供了很多现成了主流网络模型，都包含在 <code>tensorflow.contrib.slim.python.slim.nets</code> 包中；    </p><p>tensorflow/models：<br><a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">https://github.com/tensorflow/models</a>          </p><p>caffe提供了更全面的主流网络模型：<br><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">https://github.com/BVLC/caffe/wiki/Model-Zoo</a>       </p><p>Saumitro Dasgupta写了一个转换器，可以方便地将caffe网络转换为tensorflow网络模型：<br><a href="https://github.com/ethereon/caffe-tensorflow" target="_blank" rel="noopener">https://github.com/ethereon/caffe-tensorflow</a>         </p><h3 id="无监督预训练"><a href="#无监督预训练" class="headerlink" title="无监督预训练"></a>无监督预训练</h3><p>如果有一个训练任务，没有类似任务的现成模型可以复用，只有少量标注好的数据和大量未标注的数据，可以——         </p><ol><li>继续标注数据       </li><li>如果标注繁琐或者成本过高，可以使用无监督预训练的方式【如<a href="/2018/04/25/autoencoder/#用自编码器作无监督预训练">用自编码器作无监督预训练</a>】<br> 论文：<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/35536.pdf" target="_blank" rel="noopener">Why Does Unsupervised Pre-training Help Deep Learning?(2009)</a><br> 使用无监督训练，从底层开始逐层训练各个隐藏层，每次只训练一个层而固定其他层的权重；<br> 无监督训练可以使用Restricted BoltzmannMachines (RBMs）、autoencoders等，目前autoencoders用的更多些；<br> 最后用监督训练的方式，fine-tune较高的layers：<br> <img src="/Handson-ML/11.2unsupervised_pretrain.png" alt="unsupervised pretrain"></li><li>寻找一个训练数据易收集且易标注的类似任务，先训练出一个模型来复用给这个任务     </li><li>在辅助任务上进行预训练，比如：       <ul><li>想训练一个人脸识别的模型，为每一个注册者获取上百张人脸照片是不切实际的。<br>  可以先获取大量随机人脸的照片，训练一个模型来检测两张人脸图片是否对应同一个人；<br>  该任务得到一个比较好的特征检测器，复用该任务的底层权重，进而用少量数据来训练出特定人脸的识别模型      </li><li>想为某个语言处理任务训练一个模型。<br>  可以先获取大量的语句，训练一个分辨语法是否正确的模型。<br>  （比如说把这些语句先都标记为good，然后打乱语序标记为bad进行训练）<br>  该任务得到一个有一定语言能力的模型，复用该任务的底层权重，进而用少量数据来训练目标任务的模型       </li><li><strong>Max Margin Learning</strong>，训练一个打分模型<br>  <em>SVM就是基于Max Margin Learning的一种分类器</em><br>  为预测结果进行打分，用损失函数来训练一个模型，使得预测的好结果比坏结果的分数高于某个阈值</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Handson-ML</title>
      <link href="/2018/04/08/Handson-ML/"/>
      <url>/2018/04/08/Handson-ML/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=34609001&auto=0&height=32"></iframe><br>BGM：<strong>《血界战线》ED</strong><br>中文名叫《方糖歌曲和苦味舞步》 好像也有叫 《甜蜜情歌和苦涩舞步》 的；<br>原版网易云没版权，这是双声道版，也还行~~这贝斯手很灵性emmm       </p><hr><p>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>》笔记<br>目前结合毕设，主要只看TensorFlow部分，也就是DL的部分——       </p><ul><li><a href="/Handson-ML/9up_and_running_with_tensorflow.html?_blank">09 Up and Running with Tensorflow</a><br>  tensorflow的基本使用     <ul><li><a href="/Handson-ML/9.1Linear_Regression(Normal_Equation).html?_blank">Jupyter - Linear_Regression(Normal_Equation)</a><br>  正规方程实现线性回归      </li><li><a href="/Handson-ML/9.2Linear_Regression(Gradient_Descent).html?_blank">Jupyter - Linear_Regression(Gradient_Descent)</a><br>  梯度下降实现线性回归</li></ul></li><li>10 Introduction to Artificial Neural Networks<br>  简单的神经网络      <ul><li><a href="/Handson-ML/10.1DNN_MNIST(High-level_API).html?_blank">Jupyter - DNN_MNIST(High-level_API)</a><br>  高层API操作实现DNN来完成手写体识别</li><li><a href="/Handson-ML/10.2DNN_MNIST(Plain).html?_blank">Jupyter - DNN_MNIST(Plain)</a><br>  底层操作实现DNN来完成手写体识别，宽度、深度、激活函数的选择      </li></ul></li><li>11 Training Deep Neural Nets<br>  深层神经网络训练中的问题与相关的解决技术<br>  常见的配置为：<br>  初始化（Initialization）：<strong>He Initialization</strong><br>  激活函数（Activation function）：<strong>ELU</strong><br>  归一化（Normalization）：<strong>Batch Normalization, BN</strong><br>  正则化（Regularization）：<strong>Dropout</strong><br>  优化器（Optimizer）：<strong>Adam</strong><br>  学习计划（Learning rate schedule）：<strong>无（Adam具备自适应学习率）</strong>          <ul><li><a href="/2018/04/08/梯度消失与梯度爆炸?_blank">Vanishing/Exploding Gradients Problem</a><br>  梯度爆炸与梯度消失：随机初始化、nonsaturating函数、批量归一化、梯度裁剪      </li><li><a href="/2018/04/09/复用预训练层?_blank">Reusing Pretrained Layers</a><br>  复用与训练层：加速训练过程、解决标注数据少等问题        </li><li><a href="/2018/04/10/优化器?_blank">Faster Optimizers</a><br>  优化器（Momentum, NAG, AdaGrad, RMSProp, Adam）等、训练稀疏模型、学习计划         </li><li><a href="/2018/04/11/正则化技术?_blank">Avoiding Overfitting Through Regularization</a><br>  正则化技术：提前终止、L1和L2范数惩罚、Dropout、最大范数、数据增强等         </li></ul></li><li>12 Distributing TensorFlow Across Devices and Servers<br>  暂略</li><li><a href="/2018/04/18/CNN?_blank">13 Convolutional Neural Networks</a><br>  CNN原理、CNN的tensorflow实现、CNN资源占用计算、几个常见的CNN典型框架      </li><li><a href="/2018/04/22/RNN?_blank">14 Recurrent Neural Networks</a><br>  <em>RNN原理【留坑待填】</em>、RNN的tensorflow实现、应用（词嵌入、机器翻译）         </li><li><a href="/2018/04/25/autoencoder?_blank">15 Autoencoders</a><br>  自编码器原理、自编码器的实现与训练、不同约束下的自编码器       </li><li>16 Reinforcement Learning<br>  暂略</li></ul><p>其他：<a href="/Handson-ML/CS20SI小记.pdf">CS20SI小记</a>         </p><hr><p>2018-04-25<br>呀嘿，真是高产的四月。<br>DL部分基本算是读完了吧，除了分布计算和强化学习的部分，先暂时搁着；<br>还有些地方尤其是原理的部分还需要填坑完善，一边读论文或者其他书籍一边慢慢填上这些坑吧~      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度消失与梯度爆炸</title>
      <link href="/2018/04/08/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"/>
      <url>/2018/04/08/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=668472&auto=0&height=32"></iframe><br>BGM：<strong>《钢之炼金术师FA》OP1</strong><br>这个吉他混音版真是太棒了~       </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap11<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li><a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>      </li></ol><h3 id="解决梯度爆炸和消失的常用技术"><a href="#解决梯度爆炸和消失的常用技术" class="headerlink" title="解决梯度爆炸和消失的常用技术"></a>解决梯度爆炸和消失的常用技术</h3><ol><li>随机初始化（Xavier Initialization、He Initialization等）</li><li>使用nonsaturating函数（如relu）</li><li>批量归一化（Batch Normalization, BN）</li><li>梯度裁剪（Gradient Clipping）   </li></ol><h3 id="Xavier-Initialization-Glorot-Initialization"><a href="#Xavier-Initialization-Glorot-Initialization" class="headerlink" title="Xavier Initialization(Glorot Initialization)"></a>Xavier Initialization(Glorot Initialization)</h3><p>论文：<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks(2010)</a><br>作者Xavier建议：<strong>使每一层的输入输出的方差相等，而且正反向传播的梯度也相等</strong>       </p><p>并针对sigmoid激活函数（logistic激活函数）提出一种初始化方式：     </p><ol><li>各权重用均值为0的正态分布随机数进行初始化，并且标准差根据输入、输出的维度确定——<br> $$ \sigma = \sqrt{ \frac{2}{n_{inputs} + n_{outputs} } } $$       </li><li>用[-r, r]的均匀分布随机数进行初始化——<br> $$ r = \sqrt{ \frac{6}{n_{inputs} + n_{outputs}} } $$        </li></ol><p>此外，另外一篇论文<a href="https://arxiv.org/pdf/1502.01852v1.pdf" target="_blank" rel="noopener">Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification(2015)</a><br>作者He提出类似的关于其他激活函数的初始化建议：           </p><ul><li>tanh<br>  $$ \sigma = 4 \sqrt{ \frac{2}{n_{inputs} + n_{outputs} } } $$<br>  $$ r = 4 \sqrt{ \frac{6}{n_{inputs} + n_{outputs}} } $$        </li><li>relu及其变体（He Initialization）<br>  $$ \sigma = \sqrt{2} \sqrt{ \frac{2}{n_{inputs} + n_{outputs} } } $$<br>  $$ r = \sqrt{2} \sqrt{ \frac{6}{n_{inputs} + n_{outputs}} } $$        </li></ul><p>数据敏感的参数初始化方式：<br>是一种根据自身任务数据集量身定制的参数初始化方式；<br>论文：<a href="https://arxiv.org/pdf/1511.06856.pdf" target="_blank" rel="noopener">Data-dependent Initializations of Convolutional Neural Networks(2016)</a><br>代码：<a href="https://github.com/philkr/magic_init" target="_blank" rel="noopener">philkr/magic_init | github</a>     </p><h3 id="relu——nonsaturating-activation-function"><a href="#relu——nonsaturating-activation-function" class="headerlink" title="relu——nonsaturating activation function"></a>relu——nonsaturating activation function</h3><ul><li>优势      <ul><li>sigmoid和tanh是saturate函数——<a href="https://blog.csdn.net/whu_paprika/article/details/54085670" target="_blank" rel="noopener">【机器学习】saturate的解释 | CSDN</a><br>  sigmoid函数介绍参见<a href="/note_for_MLA/Ch05 logRegres.html">机器学习实战 - 逻辑回归 | Hey~YaHei!</a><br>  sigmoid函数与tanh函数线性相关，具体参见<a href="https://www.zhihu.com/question/50396271?from=profile_question_card" target="_blank" rel="noopener">在神经网络中，激活函数sigmoid和tanh除了阈值取值外有什么不同吗？| 知乎</a><br>  sigmoid、tanh、relu的比较参见<a href="https://blog.csdn.net/zchang81/article/details/70224688" target="_blank" rel="noopener">深度学习——激活函数Sigmoid/Tanh/ReLU | CSDN</a><br>  sigmoid的输出限定在[0, 1]之间；<br>  tanh的输出限定在[-1, 1]之间；<br>  而relu的输出为[0, +∞)，为nonsaturae函数</li><li>计算非常简单、快速     </li></ul></li><li>存在问题（dying relus）<br>  当输入的加权和为负数时，relu将出现“死亡”而开始不停地输出0（因为当输入为负数时relu的梯度一直是0），最终导致网络崩溃；        </li><li><p>变体（解决die问题）<br>  论文 <a href="https://arxiv.org/pdf/1505.00853.pdf" target="_blank" rel="noopener">Empirical Evaluation of Rectified Activations in Convolution Network(2015)</a> 比较了各种不同变体的表现</p><ul><li><strong>leaky relu</strong><br>  $$ LeakyReLU_\alpha(z) = max(\alpha z, z) $$<br>  其中 $\alpha$ 使 $z&lt;0$ 时有一个小梯度，使得relu不会彻底“死亡”，在接下来的训练中有可能被“复活”；<br>  通常 $\alpha$ 取0.01      </li><li><strong>randomized leaky relu（RReLU）</strong><br>  leaky relu的变体，其中 $\alpha$ 在训练中是一个随机数，最终测试时固定为一个平均值        </li><li><strong>parametric leaky relu（PReLU）</strong><br>  leaky relu的变体，其中 $\alpha$ 作为模型的一个参数参与训练<br>  论文指出，PReLU在大型图片数据集上有很好的表现，但在小数据集上很快就过拟合   </li><li><p><strong>exponential linear unit（ELU）</strong><br>  论文 <a href="https://arxiv.org/pdf/1511.07289v5.pdf" target="_blank" rel="noopener">FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)(2016)</a> 指出，ELU可以减少训练时间，并且在测试集上有更好的表现<br>  $$\begin{equation}<br>  ELU_\alpha (z) = \left\{<br>  \begin{array}{rcl}<br>  \alpha (e^z - 1) &amp; &amp; ,z &lt; 0\\<br>  z &amp; &amp; ,z&gt;=0<br>  \end{array} \right.<br>  \end{equation}$$    </p><ul><li>ELU在 $z&lt;0$ 时有负数输出，使得单元的平均输出更接近于0，这有助于缓解梯度消失的问题     </li><li>$\alpha$ 通常取1，但也可以采用其他方式来确定它的值      </li><li>当 $z&lt;0$ 时有非0梯度，有助于避免dying relu问题     </li><li>函数光滑，有助于加速梯度下降（而且当 $\alpha=1$ 时，函数在 $z=0$ 上可导）     </li><li>ELU因为多了指数运算，其计算要比relu慢，但是因为能更快收敛，所以在训练有一定的补偿；不过在测试时，还是会比较慢     </li></ul></li></ul></li><li>选择     <ul><li>一般来说：ELU &gt; leaky ReLU（及其变体） &gt; ReLU &gt; tanh &gt; logistic         </li><li>优先使用<strong>ELU</strong>，但如果需要考虑预测的开销，那么可以使用<strong>leaky ReLU</strong>及其变体       </li><li>如果还要进一步节省时间和计算力，可以使用交叉验证来评估不同的激活函数      </li><li>如果网络出现过拟合而不想花时间去调参和测试，可以使用<strong>RReLU</strong>     </li><li>如果你拥有非常大的数据集，那也可以直接使用<strong>PReLU</strong>    </li></ul></li><li>使用<br>  tensorflow有预置的elu函数     <pre class=" language-python"><code class="language-python">  hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu<span class="token punctuation">)</span></code></pre>  虽然tensorflow没有预置的leaky relus，但是定义起来很容易     <pre class=" language-python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">leaky_relu</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0.01</span> <span class="token operator">*</span> z<span class="token punctuation">,</span> z<span class="token punctuation">,</span> name<span class="token operator">=</span>name<span class="token punctuation">)</span>  hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>leaky_relu<span class="token punctuation">)</span></code></pre></li></ul><h3 id="批量归一化（Batch-Normalization-BN）"><a href="#批量归一化（Batch-Normalization-BN）" class="headerlink" title="批量归一化（Batch Normalization, BN）"></a>批量归一化（Batch Normalization, BN）</h3><p>随机初始化可以在训练的开始显著减少梯度爆炸和消失的问题，但它不能保证训练过程中不再出现；<br>论文 <a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift(2015)</a> 提出了批量归一化技术来解决这个问题，以及Internal Covariate Shift问题（训练过程中，随着前一层参数的变化，本层的输入分布也随之发生变化）；   </p><p>BN有一种<strong>特征归一化</strong>（Feature Normalization, FN）的变体；<br>作用于网络最后一层的特征表示上（FN随后接的是目标函数层），用于提高习得特征的分辨能力；<br>可用于人脸识别、行人重检测、车辆重检测等任务上；<br>论文：<a href="https://arxiv.org/pdf/1703.08388.pdf" target="_blank" rel="noopener">DeepVisage: Making face recognition simple yet with powerful generalization skills(2017)</a>   </p><h4 id="区分：归一化、正则化、标准化"><a href="#区分：归一化、正则化、标准化" class="headerlink" title="区分：归一化、正则化、标准化"></a>区分：归一化、正则化、标准化</h4><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29974820" target="_blank" rel="noopener">机器学习里的黑色艺术：normalization, standardization, regularization</a><br><strong>归一化（Normalization）</strong>：数据预处理，将数据限定在特定范围内，消除量纲对建模的影响；<br><strong>标准化（Standardization）</strong>：数据预处理，使数据符合标准正态分布；<br><strong>正则化（Regularization）</strong>：在损失函数中添加惩罚项，增加建模模糊性，将建模关注点转移到整体趋势上；      </p><h4 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h4><p>归一化的方式有很多种：      </p><ol><li>最大最小值归一化 $x’ = \frac{x - min}{max - min}$</li><li>对数归一化 $ x’ = \frac{lg(x)}{lg(max)} $</li><li>反正切归一化 $ x’= \frac{2arctan(x)}{\pi} $</li><li>零平均归一化 $ x’ = \frac{x - mean}{std} $</li></ol><p>根据论文，该技术在每层的<strong>激活函数之前添加一个BN操作</strong>，使得输入的数据以零点为中心、归一化；并且在每一层中使用两个新的参数来调整输出的范围，使得模型的每一层能够学习到适宜的表示范围和平均值；<br>该算法需要评估每一个输入的mini-batch的平均值和标准差；       </p><p>对于某个mini-batch的输入，归一化的具体过程如下：<br>$$ \mathbb{ x^{(i)} } = \frac{ x^{(i)} - \mu_B }{ \sqrt{\sigma^2_B + \epsilon} } $$<br>$$ z^{(i)} = \gamma \mathbb{ x^{(i)} } + \beta $$<br>其中，<br>$\mu_B$ 和 $\sigma_B$ 分别是该mini-batch的平均数和标准差；<br>$\epsilon$ 是一个很小的数（通常取0.001），用于避免 $\sigma = 0$ 导致分母为0的情况；<br>$\gamma$ 和 $\beta$ 分别是每一层中的两个新的参数，调整后的 $\mathbb{x^{(i)}}$ 通过线性变化后得到归一化的 $z^{(i)}$ 输出      </p><p>在预测阶段因为不再有mini-batch，所以直接使用在整个训练集的平均数、标准差进行计算即可；<br>所以在训练时每层会增加四个参数：$\gamma, \beta, \mu, \sigma$ ，训练时采用移动平均的方式可以高效地获得在整个训练集各层的平均值和标准差；   </p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><ol><li>可以很好的解决梯度爆炸和消失的问题，甚至saturate函数如sigmoid和tanh都可以在深度网络中正常使用； </li><li>网络对权重初始化不再那么敏感    </li><li>可以使用更大的学习率，提高训练速度      </li><li>具有正则化的效果，可以减少dropout等其他正则化技术的使用</li><li>缺点：BN操作增加了模型的复杂度，预测时间将不可避免地增加<br> 但是，在训练完成后，BN层可以合并到前一层的卷积层或全连接层，具体参见《<a href="/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析 - BN层合并 | Hey~YaHei!</a>》        </li></ol><h4 id="在tensorflow中使用BN"><a href="#在tensorflow中使用BN" class="headerlink" title="在tensorflow中使用BN"></a>在tensorflow中使用BN</h4><p>tensorflow提供了现成的BN层操作： <code>tensorflow.contrib.layers.batch_norm</code><br>可以直接作为参数传递给 <code>fully_connected</code> 全连接网络： </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> batch_norm<span class="token comment" spellcheck="true"># ...</span><span class="token comment" spellcheck="true"># 训练标志</span><span class="token comment" spellcheck="true"># ... 如前所述，BN在训练和预测时模型略有不同</span><span class="token comment" spellcheck="true"># ... 训练时，模型需要每次都对mini-batch计算平均值和标准差，并用滑动平均的方式记录整个训练集上的平均值和标准差</span><span class="token comment" spellcheck="true"># ... 预测时则直接使用整个训练集上的平均值和标准差</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>bool<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># BN操作的参数，包括——</span><span class="token comment" spellcheck="true"># ... is_training：标志训练与否状态的占位符</span><span class="token comment" spellcheck="true"># ... decay：使学习率下降的一个因子，v &lt;- v*decay + v*(1-decay)</span><span class="token comment" spellcheck="true"># ... updates_collections：</span>bn_params <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'is_training'</span><span class="token punctuation">:</span> is_training<span class="token punctuation">,</span>    <span class="token string">'decay'</span><span class="token punctuation">:</span> <span class="token number">0.99</span><span class="token punctuation">,</span>    <span class="token string">'updates_collections'</span><span class="token punctuation">:</span> None<span class="token punctuation">}</span><span class="token comment" spellcheck="true"># 指定全连接网络使用BN操作</span>hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope <span class="token operator">=</span> <span class="token string">"hidden1"</span><span class="token punctuation">,</span>                  normalizer_fn <span class="token operator">=</span> batch_norm<span class="token punctuation">,</span> normalizer_params <span class="token operator">=</span> bn_params<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">,</span>                  normalizer_fn <span class="token operator">=</span> batch_norm<span class="token punctuation">,</span> normalizer_params <span class="token operator">=</span> bn_params<span class="token punctuation">)</span>logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"outputs"</span><span class="token punctuation">,</span>                  normalizer_fn <span class="token operator">=</span> batch_norm<span class="token punctuation">,</span> normalizer_params <span class="token operator">=</span> bn_params<span class="token punctuation">)</span></code></pre><p>如果觉得每次都需要指定归一化的函数和参数太麻烦，可以借助 <code>tf.contrib.framework.arg_scope()</code> 来简化代码：     </p><pre class=" language-python"><code class="language-python">bg_params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token comment" spellcheck="true"># ...}</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>framework<span class="token punctuation">.</span>arg_scope<span class="token punctuation">(</span>        <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token comment" spellcheck="true"># 第一个参数，为with块内统一参数的函数列表（这里只为fully_connected函数统一参数）</span>        normalizer_fn<span class="token operator">=</span>batch_norm<span class="token punctuation">,</span>     <span class="token comment" spellcheck="true"># 对于其他参数，关键字为函数对应的关键字参数名，值为对该参数赋予的值</span>        normalizer_params<span class="token operator">=</span>bn_params<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 在该with块中的fully_connected函数就无需再重复指明归一化的函数和参数</span>    hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">)</span>    hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">)</span>    logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"outputs"</span><span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p>这里<code>batch_norm</code> 层默认是不带scale的，如果有需要，可以在参数 <code>bn_params</code> 添加一项 <code>&quot;scale&quot;: True</code> 开启；     </p><h3 id="梯度裁剪（Gradient-Clipping）：简单粗暴"><a href="#梯度裁剪（Gradient-Clipping）：简单粗暴" class="headerlink" title="梯度裁剪（Gradient Clipping）：简单粗暴"></a>梯度裁剪（Gradient Clipping）：简单粗暴</h3><p>论文：<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks(2012)</a><br>直接限制梯度不能超过一个阈值，在RNN中用的比较多；<br>具体实现：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment" spellcheck="true"># 限制阈值</span>threshold <span class="token operator">=</span> <span class="token number">1.0</span><span class="token comment" spellcheck="true"># 创建优化器</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>GradientDescentOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用optimizer.compute_gradients计算梯度</span>grads_and_vars <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>compute_gradients<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用tf.clip_by_value裁剪梯度grad，如果小于-threshold则取-threshold，如果大于threshold则取threshold</span><span class="token comment" spellcheck="true"># 变量var不变</span>capped_gvs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>clip_by_value<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> <span class="token operator">-</span>threshold<span class="token punctuation">,</span> threshold<span class="token punctuation">)</span><span class="token punctuation">,</span> var<span class="token punctuation">)</span>          <span class="token keyword">for</span> grad<span class="token punctuation">,</span> var <span class="token keyword">in</span> grads_and_vars<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 用optimizer.apply_gradients更新梯度</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>capped_gvs<span class="token punctuation">)</span></code></pre><p>对于优化器，<code>minimize</code> 方法包含 <code>compute_gradients</code> 和 <code>apply_gradients</code>，计算梯度后直接更新；<br>由于需要裁剪梯度，所以需要单独地使用 <code>compute_gradients</code> 和 <code>apply_gradients</code></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>【搁置】机器学习-吴恩达</title>
      <link href="/2017/09/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
      <url>/2017/09/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
      
        <content type="html"><![CDATA[<p>做毕设，传统ML的学习暂时搁置~<br>唔~跟周志华的《机器学习》同步施工<br>教学视频源：<a href="https://www.bilibili.com/video/av9912938/?_blank" target="_blank" rel="noopener">【中英双语】机器学习（Machine Learning）- 吴恩达（Andrew Ng）</a>      </p><ul><li><a href="/ML-Andrew/ML-Andrew-notes1.html?_blank">1 绪论、线性回归与逻辑回归</a></li><li><a href="/ML-Andrew/ML-Andrew-notes2.html?_blank">2 过拟合与正则化技术</a></li><li><a href="/ML-Andrew/ML-Andrew-notes3.html?_blank">3 非线性分类器——神经网络</a></li><li><a href="/ML-Andrew/ML-Andrew-notes4.html?_blank">4 算法改进</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>【搁置】机器学习-周志华</title>
      <link href="/2017/08/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/"/>
      <url>/2017/08/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/</url>
      
        <content type="html"><![CDATA[<p>做毕设，传统ML的学习暂时搁置~<br>《机器学习》周志华 - 清华大学出版社     </p><ul><li><a href="/ML-ZhouZhihua/《机器学习》Ch01.html?_blank">Ch01 绪论</a></li><li><a href="/ML-ZhouZhihua/《机器学习》Ch02.html?_blank">Ch02 模型估计与选择</a>    </li><li><a href="/ML-ZhouZhihua/《机器学习》Ch03.html?_blank">Ch03 线性模型</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hello World~</title>
      <link href="/2017/08/24/hello%20world/"/>
      <url>/2017/08/24/hello%20world/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=592080&auto=0&height=32"></iframe>     <p>BGM：<strong>《Code Geass反叛的鲁路修》S01E14、S02E03插入曲</strong>，也算是夏丽的角色曲吧。<br><!--S01E14插入是在鲁路修行动时无意中把夏丽父母卷入战争致死，夏丽在喜欢鲁路修、却又得知鲁路修即是zero也是害死自己父母的罪魁祸首，感到十分矛盾与痛苦。为此鲁路修不得已用gease抹除了夏丽这方面的记忆；        S02E03插入是在夏丽死的时候，夏丽因为gease被清除，回想起之前的事情，在矛盾与痛苦中被鲁路修的“弟弟”洛洛（也是个悲惨的角色）杀死，几乎也是这个时候，鲁路修彻底告别自己平凡的一面；         歌曲是夏丽对平凡的鲁鲁修的爱情的内心描述，这个回音般的音效还是很有特色的，跟专辑的名字《Angel Feather Voice》一样，有点像天使的声音。       --></p><hr><p>前两年虚拟机崩了<br>博客上的数据全丢，而且没有备份，心疼<br>没有md文件的存根（只有少数几份在win上有备份），只剩下个编译后的网站——<a href="http://zkkzkk368.github.io?_blank" target="_blank" rel="noopener">Hey!YaHei~</a><br>最近没啥学习的动力，重操旧业把博客搭起来玩玩吧<br>这次依旧用<a href="https://hexo.io/?_blank" target="_blank" rel="noopener">hexo框架</a><br><a href="https://github.com/iissnan/hexo-theme-next?_blank" target="_blank" rel="noopener">NexT主题</a>也更换为<a href="https://github.com/viosey/hexo-theme-material?_blank" target="_blank" rel="noopener">Material主题</a>（这个主题看起来还不错）<br>前者其实更加简约，但它的首页感觉有些难受<br>额外搞了个<a href="https://github.com/ele828/hexo-prism-plugin?_blank" target="_blank" rel="noopener">prism</a>语法高亮插件，美滋滋……     </p><p>主要更学习笔记吧，其实自己的笔记别人未必看得懂<br>欢迎订阅RSS（以QQ邮箱为例）：<br><img src="/imgs/RSS_demo.png" alt="RSS_demo">     </p><p>兴许偶尔会写点别的~~<br>估计没人会来看，纯属自娱自乐    </p><h2 id="原博客目录"><a href="#原博客目录" class="headerlink" title="原博客目录"></a>原博客目录</h2><p>（带星号的表示新博客上也有一样的文章）    </p><ul><li>2014-10-29 <a href="http://zkkzkk368.github.io/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93?_blank" target="_blank" rel="noopener">*第一二期作业总结</a><br>  当初大一俱乐部的作业     </li><li>2015-02-05 <a href="http://zkkzkk368.github.io/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93?_blank" target="_blank" rel="noopener">*居然……还我数据？！？！</a><br>  折腾linux的时候躺过的坑    </li><li>2015-03-23 <a href="http://zkkzkk368.github.io/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF?_blank" target="_blank" rel="noopener">*俱乐部春纳网页后端小结</a><br>  大一俱乐部纳新网站制作小结    </li><li>2015-04-05 <a href="http://zkkzkk368.github.io/2015/04/06/fedora%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE?_blank" target="_blank" rel="noopener">fedora安装与配置</a><br>  闲着无聊把ubuntu换成了fedora     </li><li>2015-04-19 <a href="http://zkkzkk368.github.io/2015/04/20/Hexo%E7%9A%84%E4%BD%BF%E7%94%A8?_blank" target="_blank" rel="noopener">Hexo的使用</a><br>  嗯~第一次搭博客哈哈      </li><li>2015-04-22 <a href="http://zkkzkk368.github.io/2015/04/23/python%E7%88%AC%E8%99%AB%E5%88%9D%E6%8E%A2?_blank" target="_blank" rel="noopener">python爬虫初探</a><br>  第一次写爬虫    </li><li>2015-05-07 <a href="http://zkkzkk368.github.io/2015/05/08/C%E8%AF%AD%E8%A8%80%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%BC%96%E7%A8%8B%EF%BC%88WIN%EF%BC%89?_blank" target="_blank" rel="noopener">C语言图形化编程（WIN）</a><br>  C大程写大作业前的学习小结    </li><li>2015-05-10 <a href="http://zkkzkk368.github.io/2015/05/11/javascript%E7%AC%94%E8%AE%B0?_blank" target="_blank" rel="noopener">JS笔记</a><br>  闲来无事小补了一发js，后来写数据结构的bonus居然用上了   </li><li>2015-06-02 <a href="http://zkkzkk368.github.io/2015/06/03/%E7%AE%80%E5%8D%95%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%A7%A3%E6%9E%90%E5%99%A8?_blank" target="_blank" rel="noopener">简单的正则表达式解析器</a><br>  《代码之美》里一段有趣的代码    </li><li>2015-08-11 <a href="http://zkkzkk368.github.io/2015/08/12/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7?_blank" target="_blank" rel="noopener">《vim实用技巧》笔记</a><br>  已烂尾…    </li><li>2015-08-30 <a href="http://zkkzkk368.github.io/2015/08/31/%E3%80%8AC%E4%B8%93%E5%AE%B6%E7%BC%96%E7%A8%8B%E3%80%8B%E5%B0%8F%E8%AE%B0?_blank" target="_blank" rel="noopener">《C专家编程》小记</a><br>  二刷《C专家编程》，随手记了点东西   </li><li>2015-08-30 <a href="http://zkkzkk368.github.io/2015/08/31/%E6%8C%87%E9%92%88&amp;%E6%95%B0%E7%BB%84?_blank" target="_blank" rel="noopener">指针&amp;数组</a>        </li><li>2015-09-20 <a href="http://zkkzkk368.github.io/2015/09/20/Note_for_Python?_blank" target="_blank" rel="noopener">python笔记</a></li><li>2015-09-20 <a href="http://zkkzkk368.github.io/2015/09/20/Note_For_Linux?_blank" target="_blank" rel="noopener">《鸟哥的Linux私房菜》笔记</a></li><li>2015-10-17 <a href="http://zkkzkk368.github.io/2015/10/17/%E3%80%8AJava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E3%80%8B%E7%AC%94%E8%AE%B0?_blank" target="_blank" rel="noopener">《Java核心技术》笔记</a><br>  烂尾…      </li></ul><hr><p>2017-07-24    </p><!--> 『情』束缚的不是『头脑』，而是『内心』。> ——《东京暗鸦》11卷5章2节，字野耕平    -->      <p>唉~     </p><hr><p>2017-08-27<br>由于markdown与mathjax的语法冲突，mathjax的_时常被markdown错误的渲染<br>故修改<code>node_modules/marked/lib/marked.js</code>文件中的正则表达式来放弃markdown对_的渲染<br>参考：<a href="http://www.cnblogs.com/Ai-heng/p/7282110.html" target="_blank" rel="noopener">hexo博客MathJax公式渲染问题</a>     </p><hr><p>2018-04-25<br>修改Material的模板文件<code>themes/material/layout/layout.ejs</code>——<br>在末尾追加添加相关js代码；<br>将mardown语法 <code>[text](url?_blank)</code> 转换成带属性 <code>target=&quot;_blank</code> 的 <code>&lt;a&gt;</code> 标签，<br>弥补markdown不支持“从新标签打开链接”的功能——     </p><pre class=" language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span>    <span class="token keyword">var</span> aTagArr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">.</span>slice<span class="token punctuation">.</span><span class="token function">apply</span><span class="token punctuation">(</span>document<span class="token punctuation">.</span><span class="token function">getElementsByTagName</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    aTagArr<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>e<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> e<span class="token punctuation">.</span>href<span class="token punctuation">.</span><span class="token function">lastIndexOf</span><span class="token punctuation">(</span><span class="token string">"_blank"</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>            e<span class="token punctuation">.</span>target <span class="token operator">=</span> <span class="token string">"_blank"</span><span class="token punctuation">;</span>            e<span class="token punctuation">.</span>href <span class="token operator">=</span> e<span class="token punctuation">.</span>href<span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">"?_blank"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span></code></pre><hr><p>2018-08-24        </p><ul><li>更新Material至1.5.6版本<br>  这个版本有点小bug，修复方式参见 <a href="https://github.com/viosey/hexo-theme-material/issues/686" target="_blank" rel="noopener">Issue #686 | github</a> 中michaelcai的回复；       </li><li>引入评论系统<br>  LiveRe评论系统，海外服务，可能加载会稍微慢点，偶尔也不大稳定（加载不出来），坐等LiveRe提供大陆服务吧       </li><li>更改字体源至ustc<br>  原先从google获取，有时候会加载不出来       </li><li>从bootcdn获取jquery和mathjax<br>  提高页面加载速度         </li></ul><hr><p>2018-08-28<br>图库从一闪链过来的（居然不防外链也是心大）<img src="/imgs/emoji/acfun/1001.png" alt="emoji_acfun_1001"></p>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>【搁置】《机器学习实战》笔记</title>
      <link href="/2017/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
      <url>/2017/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p><strong>暂时搁着……</strong><br>师兄说先学理论再看实战QAQ，先暂时搁着吧<br>以下页面均由jupyter生成     </p><p>目录：   </p><ul><li>Ch01 机器学习基础   </li><li><a href="/note_for_MLA/Ch02 kNN.html?_blank">Ch02 k-邻近算法</a></li><li><a href="/note_for_MLA/Ch03 trees.html?_blank">Ch03 决策树</a></li><li><a href="/note_for_MLA/Ch04 bayes.html?_blank">Ch04 基于概率论的分类方法：朴素贝叶斯</a></li><li><a href="/note_for_MLA/Ch05 logRegres.html?_blank">Ch05 Logistic回归</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>QT学习之路</title>
      <link href="/2017/01/29/QT%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/"/>
      <url>/2017/01/29/QT%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br>做srtp的时候看的一波资料<br><a href="/others/QT学习之路.pdf">QT学习之路.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《你好放大器》笔记</title>
      <link href="/2016/08/01/%E4%BD%A0%E5%A5%BD%E6%94%BE%E5%A4%A7%E5%99%A8/"/>
      <url>/2016/08/01/%E4%BD%A0%E5%A5%BD%E6%94%BE%E5%A4%A7%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/你好放大器.pdf">你好放大器.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 2 硬件 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《C++编程思想》笔记</title>
      <link href="/2016/07/03/C++%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
      <url>/2016/07/03/C++%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/C++编程思想.pdf">C++编程思想.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>心理学导论</title>
      <link href="/2016/03/09/%E5%BF%83%E7%90%86%E5%AD%A6%E5%AF%BC%E8%AE%BA/"/>
      <url>/2016/03/09/%E5%BF%83%E7%90%86%E5%AD%A6%E5%AF%BC%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/心理学导论.pdf">心理学导论.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>python-sh</title>
      <link href="/2015/09/20/sh%E6%A8%A1%E5%9D%97/"/>
      <url>/2015/09/20/sh%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="sh模块"><a href="#sh模块" class="headerlink" title="sh模块"></a>sh模块</h2><p><a href="http://amoffat.github.io/sh/" target="_blank" rel="noopener">sh官方文档</a>  </p><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="直接使用命令对应的函数"><a href="#直接使用命令对应的函数" class="headerlink" title="直接使用命令对应的函数"></a>直接使用命令对应的函数</h4><p>如：<code>print(sh.ls(&quot;/&quot;))</code>  </p><ol><li>命令参数分别以函数参数的形式给出<br> 如：<code>tar(&quot;cvf&quot;, &quot;/tmp/test.tar&quot;, &quot;/my/home/directory/&quot;)</code><br> 即执行linux命令<code>tar -cvf /tmp/test.tar /my/home/directory/</code>   </li><li>命令名中如果出现横线<code>-</code>则其对应的函数名应改为下划线<code>_</code><br> 如：linux命令<code>google-chrome</code>对应函数<code>google_chrome</code>  </li></ol><h4 id="自定义命令函数"><a href="#自定义命令函数" class="headerlink" title="自定义命令函数"></a>自定义命令函数</h4><p>如：  </p><pre><code>    lscmd = sh.Command(&quot;/bin/ls -l&quot;)      lscmd(&quot;/&quot;)</code></pre><p>即将带参数linux命令<code>/bin/ls -l</code>“封装”成<code>lscmd()</code>  </p><h4 id="提供参数的两种形式"><a href="#提供参数的两种形式" class="headerlink" title="提供参数的两种形式"></a>提供参数的两种形式</h4><p>以linux命令<code>curl http://duckduckgo.com/ -o page.html --silent</code>为例  </p><ol><li>以关键词参数的形式给出<br> <code>sh.curl(&quot;http://duckduckgo.com/&quot;, o=&quot;page.html&quot;, silent=True)</code>  </li><li>以分割的字符串的形式给出<br> <code>sh.curl(&quot;http://duckduckgo.com/&quot;, &quot;-o&quot;, &quot;page.html&quot;, &quot;--silent&quot;)</code>  </li></ol><h4 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h4><p>命令默认前台运行（阻塞式，block），如果将命令转至后台（非阻塞式，doesn’t block），添加关键词参数<code>_bg=True</code>即可<br>如：<code>sh.sleep(3, _bg=True)</code>  </p><h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><p>使用linux中的管道命令，直接用函数的嵌套调用即可<br>如：<code>sh.wc( sh.ls(&quot;/etc&quot;, &quot;-l&quot;), &quot;-l&quot; )</code><br>即linux命令<code>ls /etc -l | wc -l</code>  </p><p>默认情况下，被传递的命令完成后才会传递给管道命令<br>如上例中<code>ls</code>完成后才会传递并执行<code>wc</code>命令   </p><p>给被传递的命令函数加入参数<code>_piped=True</code>后，可以使两个命令同时执行，被传递的命令不断得将新产生的返回信息传递给管道命令<br>如上例中不等<code>ls</code>完成即会传递并执行<code>wc</code>命令，<code>ls</code>不断产生新的信息并不断传递给<code>wc</code>命令   </p><h4 id="数据流重定向"><a href="#数据流重定向" class="headerlink" title="数据流重定向"></a>数据流重定向</h4><p>使用参数<code>_out={filename}</code>和<code>_err={filename}</code>分别将标准输出和错误输出重定向到指定文件中<br>如果文件已存在，默认将清空文件内容后重新写入<br>如：<code>sh.ls(_out=&quot;files.list&quot;)</code>  </p><h4 id="标准输入stdin"><a href="#标准输入stdin" class="headerlink" title="标准输入stdin"></a>标准输入stdin</h4><p>可以给一个命令函数提供标准输入<br>只需要提供参数<code>_in={stdin}</code>即可<br>如：<code>print( sh.cat(_in=&quot;hello&quot;) )</code><br>“stdin”不仅可以是字符串，还可以是文件、队列和任何可迭代对象（列表、集合、字典等）  </p><h4 id="子命令的两种形式"><a href="#子命令的两种形式" class="headerlink" title="子命令的两种形式"></a>子命令的两种形式</h4><p>以linux命令<code>sudo ls /root</code>为例  </p><ol><li>使用命令函数下的对应子函数<br> <code>sh.sudo.ls(&quot;/root&quot;)</code>  </li><li>将子命令作为参数给出<br> <code>sh.sudo( &quot;ls&quot;, &quot;/root&quot; )</code>  </li></ol><p><strong>注意：对于sudo命令，用户必须设置NOPASSWD选项使该用户在命令执行时无需再输入密码才能正常执行</strong>  </p><h4 id="命令回传值的接收与处理"><a href="#命令回传值的接收与处理" class="headerlink" title="命令回传值的接收与处理"></a>命令回传值的接收与处理</h4><p>命令函数的返回内容除了其应有的输出，还包含<code>exit_code</code>属性记录命令的回传值（一般正常执行的回传值为0）<br>如：<br><code>print(sh.ls(&quot;/root&quot;))</code>打印ls命令的输出<br><code>print(sh.ls(&quot;/root&quot;).exit_code)</code>打印ls命令的回传值   </p><ul><li>命令执行失败会引起python出现相应的异常，可以借助python的try/except机制捕获并处理异常<br>  回传值为<code>x</code>的命令错误会触发<code>ErrorReturnCode_x</code>的异常<br>  如：回传值为<code>2</code>的命令错误会触发<code>ErrorReturnCode_2</code>的异常  </li><li>有些命令即使正常执行也会报错，可以在调用相应的函数时给出<code>ok_code={ok_code_list}</code>参数来告知哪些回传值是正常的<br>  如：<code>sh.weird_program(_ok_code=[0,3,5])</code>  </li></ul><h4 id="通配符的使用"><a href="#通配符的使用" class="headerlink" title="通配符的使用"></a>通配符的使用</h4><p>将使用通配符的字符串用<code>sh.glob()</code>函数处理（<strong>注意不是<code>glob.glob()</code>函数</strong>）<br>如：<br><code>sh.ls( sh.glob(&quot;*.py&quot;) )</code><br>即linux命令<code>ls *.py</code>  </p><h3 id="进阶使用"><a href="#进阶使用" class="headerlink" title="进阶使用"></a>进阶使用</h3><p>暂略……</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《vim实用技巧》小记</title>
      <link href="/2015/08/11/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>/2015/08/11/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>=。=最近在看《vim实用技巧》<br>对一些内容作点简单的记录，方便以后查阅  </p><h2 id="Vim解决问题的方式"><a href="#Vim解决问题的方式" class="headerlink" title="Vim解决问题的方式"></a>Vim解决问题的方式</h2><ul><li>多用<code>.</code>命令（一个微型宏）来重复一些简单的操作  </li><li>减少无关的移动，形成合理的撤销块  </li><li>善用复合命令减少操作<br>  <code>A</code>    == <code>$a</code>，插入在行末<br>  <code>C</code> == <code>c$</code>，替换行末字符并持续插入<br>  <code>s</code> == <code>cl</code>，替换当前字符并持续插入<br>  <code>S</code> == <code>^c</code>，替换前一字符并持续插入<br>  <code>I</code> == <code>^i</code>，从行首开始编辑<br>  <code>o</code> == <code>A&lt;CR&gt;</code>，在下方插入新行<br>  <code>O</code> == <code>ko</code>，在上方插入新行<br>  <code>……</code>  </li><li>尽可能使修改、移动变得可重复  </li><li>常用的重复与回退操作  <ul><li>一般的修改<code>{edit}</code>：<br>  <code>.</code>重复，<code>u</code>回退  </li><li>行内的查找<code>[f|F|t|T]{char}</code>：<br>  <code>;</code>重复，<code>,</code>回退  下</li><li>文档内的查找<code>[/|?]{pattern}&lt;CR&gt;</code>：<br>  <code>n</code>重复，<code>N</code>回退  </li><li>执行替换<code>:s/target/replacement</code>：<br>  <code>&amp;</code>重复，<code>u</code>回退  </li><li>执行一系列修改<code>qx{changes}q</code>：<br>  <code>@x</code>重复，<code>u</code>回退  </li></ul></li><li><code>.</code>范式<br>  用一个键移动，用另一个键执行的可重复修改操作  </li></ul><h2 id="普通模式"><a href="#普通模式" class="headerlink" title="普通模式"></a>普通模式</h2><ul><li>停顿思考时切换到普通模式</li><li>合理地切分撤销单元（模式间的切换）  </li><li>如果在插入模式中移动的光标，将会产生一个新的撤销单元  </li><li>删除单词<code>daw</code>，可以解读为”delete a word”，该命令可重复  </li><li>简单的算术运算<code>{num}&lt;C-a&gt;</code>加法，<code>{num}&lt;C-x&gt;</code>减法<br>  不需要把光标移动到数字上<br>  运算为num加减当前光标或光标以后的数字<br>  注意：默认情况下0开头的数字会被当作八进制，0x开头则为十六进制<br>  <em>可以通过<code>set nrformats=</code>关闭进制识别（都作为十进制数）</em>  </li><li>重复&amp;次数<br>  删除多个单词有两种风格——  <ol><li><code>dw</code>+<code>.</code>的重复风格<br> 常用风格，使用灵活，方便回退，无需数单词个数  </li><li><code>d2w</code>或<code>2dw</code>的次数风格<br> 多用于删除一整块词组，如<code>a couple of</code>;可以提高撤销单元的连贯性  </li></ol></li><li>操作 = 操作符 + 动作命令{motion}  <ul><li>操作符  <ul><li><code>c</code>：修改  </li><li><code>d</code>：删除</li><li><code>y</code>：复制到寄存器  </li><li><code>g~</code>：字母大小写反转  </li><li><code>gu</code>：字母转换为小写  </li><li><code>gU</code>：字母转换为大写  </li><li><code>&gt;</code>：增加缩进  </li><li><code>&lt;</code>：减小缩进  </li><li><code>=</code>：自动缩进</li><li><code>!</code>：用外部程序过滤{motion}跨越的行</li></ul></li><li>动作命令  <ul><li><code>[h|j|k|l]</code>：左下上右  </li><li><code>[-|+]</code>：上一行（下一行）的非空白字符</li><li><code>0</code>（数字）：行首（含空白）</li><li><code>^</code>：行首（不含空白），即本行第一个非空白字符  </li><li><code>$</code>：行末  </li><li><code>gg</code>：文首  </li><li><code>G</code>：文末  </li><li><code>[f|F|t|T]{char}</code>：右侧（左侧）的第一个char字符</li><li><code>[;|,]</code>：下一个（上一个）”f|F|t|T”的char字符</li><li><code>:{num}</code>：至第num行  </li><li><code>[w|b]</code>：后一个（前一个）或当前单词的头部（尾部）——含符号  </li><li><code>[W|B]</code>：后一个（前一个）或当前单词的头部（尾部）——跳过符号（仅字母和数字包括负号）</li><li><code>............................</code>  </li><li>动作命令可以待有修饰符前缀<ul><li><code>a</code>：即an，表示一个非空白对象  </li><li><code>i</code>：即inner，表示一个内含对象</li></ul></li></ul></li><li>当一个操作符被连续调用两次，就表示作用于当前行<br>  <em>特例：<code>g~~</code>、<code>guu</code>、<code>gUU</code></em>  </li></ul></li><li>操作符待决模式<br>  键入操作符后vim会进入操作符待决模式，此时vim会等待一个动作命令后执行操作<br>  在该模式下可以按<code>&lt;ESC&gt;</code>来退回普通模式<br>  因为该模式的存在，使得自定义操作符及动作指令能够存在  </li></ul><h2 id="插入模式"><a href="#插入模式" class="headerlink" title="插入模式"></a>插入模式</h2><ul><li>单词输入错误，应删除整个单词再重新输入<br>  <em>可以减少以后输入出错</em>  </li><li>删除  <ul><li><code>&lt;C-h&gt;</code>：删除前一个字符，相当于退格键  </li><li><code>&lt;C-w&gt;</code>：删除前一个单词  </li><li><code>&lt;C-u&gt;</code>：删除至行首  </li><li><strong>注意：只能删除本次插入模式下插入的内容</strong>  </li><li><em>这些命令不是插入模式独有，也不是vim独有，在命令行模式、shell中都能使用它们</em>  </li></ul></li><li>切换到普通模式使用<code>&lt;C-[&gt;</code>更加方便  </li><li>插入-普通模式：<code>&lt;C-o&gt;</code><br>  执行一个普通模式命令后，马上就可以返回到插入命令<br>  在插入模式下，通过<code>&lt;C-o&gt;zz</code>迅速把当前行移动到屏幕正中  </li><li>插入模式下的粘贴几个单词<br>  <code>&lt;C-r&gt;{register}</code>：将register号寄存器的内容粘贴到当前位置<br>  <em>普通模式下复制文本默认保存在0号寄存器中</em><br>  <strong>注意：当文本比较多的时候，应切换到普通模式下操作比较合适</strong>  </li><li>运算<br>  <code>&lt;C-r&gt;={expr}&lt;CR&gt;</code>：执行expr运算并且把结果插入到当前位置  </li><li>用字符编码插入字符  <ul><li>十进制ASCII字符：<code>&lt;C-v&gt;{code}</code><br>  只能插入三位十进制数的ASCII码，注意高位补0  </li><li>十六进制unicode字符：<code>&lt;C-v&gt;u{code}</code><br>  插入四位十六进制数的unicode码，注意高位补0  </li><li>通过二合字母插入字符<br>  <code>&lt;C-k&gt;{char1}{char2}</code><br>  二合字母集可以通过<code>:digraphs</code>查看<br>  <code>:digraph-table</code>可以获得二合字母集更详细的信息  </li></ul></li><li>查看当前字符的编码<br>  <code>ga</code>：屏幕下方会显示当前字符的十进制、十六进制、八进制编码信息  </li><li>虚拟替换模式<br>  <code>R</code>：替换模式，<code>&lt;Tab&gt;</code>会作为一个字符被替换<br>  <code>gR</code>：虚拟替换模式，<code>&lt;Tab&gt;</code>会作为多个字符（一般为八个）被逐个替换（直到末尾才替换掉<code>&lt;Tab&gt;</code>）<br>  <code>r</code>和<code>gr</code>同理～<br>  <strong>应尽量使用虚拟替换模式</strong>  </li></ul><h2 id="可视模式"><a href="#可视模式" class="headerlink" title="可视模式"></a>可视模式</h2><ul><li>三种可视模式  <ol><li><code>v</code>：操作字符文本  </li><li><code>V</code>：操作行文本  </li><li><code>&lt;C-v&gt;</code>：操作块文本  </li></ol></li><li>该模式与其他模式不同，是先选择文本后触发命令  </li><li>选择模式<code>&lt;C-g&gt;</code><br>  类似与windows的选择模式，输入的可见字符会替换掉选中的文本，之后vim进入插入模式<br>  这个模式只是为了迎合windows用户，应尽量少用  </li><li><code>gv</code>：重选上次的高亮选区  </li><li>该模式下的<code>.</code>命令有时候会出现异常<br>  <strong>所以应尽可能使用操作命令而不是可视命令</strong>  </li><li>有时候修改文本的范围很难用动作命令表达出来，这时候才用到可视命令  </li><li>修改多行文本时，只有第一行发生变化，只有当返回普通模式后，其他行才会发生变化  </li><li><code>&lt;C-v&gt;</code>不仅可以选中矩形的区域，还可以选中长短不一的块<br>  当在该模式下选中行末部分，可以实现长短不一的块的选择  </li></ul><h2 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h2><ul><li>三种形式的命令行模式  <ol><li>按下<code>:</code>的Ex命令  </li><li>按下<code>/</code>的查找命令  </li><li>用<code>&lt;C-r&gt;=</code>访问表达式寄存器  </li></ol></li><li>指定命令作用范围<ul><li>指定行<br>  <code>:{number}....</code><br>  只包含数字的Ex命令表示跳转<br>  <strong>可以使用特殊符号<code>$</code>表示最后一行，<code>%</code>表示当前文件的所有行</strong>  </li><li>用地址来指定一个范围<br>  <code>:{start},{end}....</code>，执行命令后光标将跳转到end行  </li><li>用高亮选取指定范围<br>  先用可视模式<code>v</code>,<code>V</code>,<code>&lt;C-v&gt;</code>选取高亮区<br>  再按下<code>:</code>，此时命令行会自动填充成<code>:&#39;&lt;,&#39;&gt;</code>，表示作用在高亮区上  </li><li>用模式指定<br>  <code>:/{pattern1},/{pattern2}/....</code><br>  例如：<code>:/&lt;html&gt;/,/&lt;\/html&gt;/</code>指定了html内的所有内容（包含<code>&lt;html&gt;</code>和<code>&lt;/html&gt;</code>）<br>  <em>注意：斜杠<code>/</code>有特殊含义，需要用反斜杠<code>\</code>转义</em>  </li><li>地址偏移<br>  直接对地址进行<code>+</code>或<code>-</code>运算<br>  如：<code>:/&lt;html&gt;/+1,/&lt;\/html&gt;/-1</code>则排除了<code>&lt;html&gt;</code>和<code>&lt;/html&gt;</code>而只包含了其中间的内容<br>  <strong>如果不加数字，默认偏移量为1</strong>  </li><li>特殊符号总结  <ul><li><code>1</code>：第一行  </li><li><code>0</code>：虚拟行，位于第一行上方，常用于插入到首行等功能</li><li><code>$</code>：最后一行  </li><li><code>.</code>：当前行  </li><li><code>%</code>：整个文件，相当于<code>:1,$</code>  </li><li><code>&#39;m</code>：包含位置标记m的行  </li><li><code>&#39;&lt;</code>和<code>&#39;&gt;</code>：高亮选取的起始和结束行  </li></ul></li></ul></li><li>复制命令：<code>:t</code>或<code>:co</code>或<code>:copy</code><br>  <code>:{range}t {address}</code><br>  与普通模式下的<code>y</code>命令不同，该命令不把文本保存到寄存器  </li><li>移动命令：<code>:move</code>或<code>:m</code><br>  <code>:{range}m {address}</code>  </li><li>在指定范围上执行普通模式命令<br>  <code>:{range}normal {commands}</code>，常用于多行的处理<br>  常用的命令如下：  <ul><li><code>:{range}normal .</code>：对多行重复同一操作   </li><li><code>:{range}normal A;</code>：对多行末尾补上分号  </li><li><code>:{range}normal i//</code>：将多行内容注释掉  </li></ul></li><li>重复上一条命令：<code>@:</code>  </li><li>使用<code>&lt;C-o&gt;</code>进入插入-普通模式时，命令记录会跳转到上一条命令，可以借此进行命令记录的跳转</li><li>自动补全  <ul><li><code>&lt;Tab&gt;</code>自动补全命令<br>  多次按<code>&lt;Tab&gt;</code>会正向遍历补全列表的内容  </li><li><code>&lt;S-Tab&gt;</code>反向遍历补全列表，<em>S表示<code>&lt;shift&gt;</code>按键</em>  </li><li><code>&lt;C-d&gt;</code>显示可用的补全列表  </li><li>自定义补全行为  <ul><li>bash shell形式：<code>set wildmode=longest,list</code>  </li><li>zsh形式：<code>set wildmenu</code>和<code>set wildmode=full</code>  <ul><li><code>wildmenu</code>为补全导航列表  </li></ul></li></ul></li></ul></li><li>将当前单词插入到命令行中<br>  常用于替换命令和查看帮助文档<br>  在命令行模式下，<code>&lt;C-r&gt;&lt;C-w&gt;</code>会复制当前单词到命令行中  </li><li>回溯历史命令  <ul><li>回溯所有命令<br>  在<code>:</code>下保持提示符为空按<code>&lt;up&gt;</code>(<code>&lt;C-p&gt;</code>)和<code>&lt;down&gt;</code>(<code>&lt;C-n&gt;</code>)  <ul><li><code>&lt;C-p&gt;`</code><c-n><code>和</code><up><code></code><down><code>的区别    前者方便，但不能过滤命令    解决方法：    在</code>.vim<code>文件中创建映射项</code>cnoremap <c-p> <up><code></code>cnoremap <c-n> <down>`  </down></c-n></up></c-p></down></up></c-n></li></ul></li><li>过滤回溯的命令<br>  如<code>:help&lt;up&gt;</code>回溯以”help”开头的命令  </li><li>命令历史容量设置<br>  缺省只有20<br>  推荐<code>set history=200</code>  </li><li>除了Ex命令，查找命令也会记录下来并保存在另一个文件中   </li></ul></li><li>一次性执行多条命令<br>  用<code>|</code>隔开命令即可  </li><li>命令行窗口<br>  像一个常规的vim缓冲区，其中每一行是命令历史中的一个条目<br>  可以在该窗口下修改命令历史记录，对之前使用过的命令做调整后方便重复利用<br>  <em>当打开命令行窗口时，它始终拥有焦点，除非关闭它，否则无法切换到其他窗口</em>  <ul><li><code>q:</code>：打开Ex命令的命令行窗口   </li><li><code>q/</code>：打开查找命令的命令行窗口  </li><li><code>&lt;C-f&gt;</code>：从命令行模式切换到命令行窗口（起始命令行模式下输入的内容仍然保留下来）  </li></ul></li><li>运行shell命令<br>  <code>!{command}</code>  <ul><li><code>%</code>代表当前文件名  </li><li><code>:shell</code>可以打开一个交互式的shell会话（通过<code>exit</code>退出）<br>  更好的方法是用<code>&lt;C-z&gt;</code>挂起vim所属进程，用<code>fp</code>唤醒挂起的作业<br>  <em>shell下的<code>jobs</code>命令可以查看当前的作业列表</em>  </li></ul></li><li>大量读取或写入命令输入输出<br>  <code>:read !{cmd}</code>将命令输出读取到当前缓冲区中<br>  <code>:write !{cmd}</code>：将当前缓冲区内容作为命令的标准输入   </li><li>借助shell命令过滤指定范围<br>  <code>:{range}!{filter}</code><br>  如用sort命令，<code>:2,$!sort -t &#39;,&#39; -k 2</code>表示第2行到最后一行之间，以逗号为分隔符的第二字段排序  </li><li>常见的和操作缓冲区文本的Ex命令  <ul><li><code>:{range}d {x}</code>：删除指定内容（并保存到寄存器x中）</li><li><code>:{range}y {x}</code>：复制指定内容到寄存器x中  </li><li><code>:{line}put {x}</code>：在指定行后粘贴寄存器x中的内容   </li><li><code>:{range}t {address}</code>：复制指定内容到address处  </li><li><code>:{range}m {address}</code>：移动指定内容到address处  </li><li><code>:{range}join</code>：连接指定行内容  </li><li><code>:{range}normal {commands}</code>：对指定范围执行普通模式命令  </li><li><code>:{range}s/{pattern}/{string}/{flag}</code>：替换  </li><li><code>:{range}global/{pattern}/{cmd}</code>：对指定范围内匹配pattern的所有行执行cmd命令（Ex）  </li></ul></li></ul><h2 id="管理多个文件"><a href="#管理多个文件" class="headerlink" title="管理多个文件"></a>管理多个文件</h2><h3 id="缓冲区列表"><a href="#缓冲区列表" class="headerlink" title="缓冲区列表"></a>缓冲区列表</h3><p>文件读取后在内存缓冲区中  </p><ul><li><code>:ls</code>命令可以列出所有被载入到内存中的缓冲区列表   <ul><li>每个条目开头的数字为系统自动分配而<strong>不可改变</strong>的缓冲区编号</li><li><code>%</code>：当前窗口中可见的缓冲区   </li><li><code>#</code>：轮换文件<br>  <strong>按<code>&lt;C-^&gt;</code>可以在两个文件之间快速轮换</strong>  </li></ul></li><li>缓冲区切换<ul><li>遍历缓冲区列表：  <ul><li>正向移动：<code>:bn</code>或<code>:bnext</code>  </li><li>反向移动：<code>:bp</code>或<code>:bprevious</code>  </li></ul></li><li><code>:buffer {N}</code>：跳转到指定编号的缓冲区  </li><li><code>:buffer {bufname}</code>：跳转到可以被bufname唯一标识的缓冲区（若多个缓冲区具有同一标识，可以通过<code>&lt;Tab&gt;</code>选择）  </li></ul></li><li><code>:bufdo {commands}</code>：对所有缓冲区执行Ex命令  </li><li><p>创建快速遍历缓冲区列表的键盘映射  </p><pre><code>  nnoremap &lt;silent&gt; [b :bp&lt;CR&gt;    nnoremap &lt;silent&gt; ]b :bn&lt;CR&gt;    nnoremap &lt;silent&gt; [B :bfirst&lt;CR&gt;    nnoremap &lt;silent&gt; ]B :blast&lt;CR&gt;  </code></pre></li><li><p>删除<code>:bd</code>或<code>:bdelect</code><br>  <code>:bd {N1,N2,N3....}</code>：删除列出的缓冲区<br>  <code>:{N,M} bd</code>：删除连续的缓冲区  </p></li></ul><h3 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h3><p>对一批文件进行分组，其<strong>文件顺序可调整*</strong>   </p><ul><li>初始的参数列表为启动时vim的文件列表   </li><li><code>:args</code>：查看参数列表<br>  其中<code>[]</code>表明了当前的活动文件  </li><li><code>:args {arglist}</code>：填充参数列表<br>  arglist可以是文件名、通配符、shell命令的输出结果等   <ul><li>用文件名指定文件<br>  <code>:args {file1,file2,....}</code>  </li><li>用Glob模式指定文件<br>  <code>*</code>：表示0到无穷多个字符，但不包含子目录<br>  <code>**</code>：表示0到无穷多个字符，包含子目录  </li><li>用反引号结构指定文件<br>  将shell命令用反引号括起来，其输出将填充到相应的位置<br>  如：<code>:args \</code>cat .chapters``  </li></ul></li><li>隐藏缓冲区<br>  缓冲区列表中，<code>+</code>代表缓冲区被修改过，此时切换缓冲区会弹出错误信息，可以用感叹号强制切换，此时列表中被标记为<code>a</code>的为活动缓冲区（active），被标记为<code>h</code>的为隐藏活动区（hidden）  <ul><li>处理隐藏缓冲区  <ul><li><code>:w</code>或<code>:write</code>：写入磁盘  </li><li><code>:e</code>或<code>:edit</code>：从磁盘中读入到缓冲区（回滚修改操作）  </li><li><code>:qa</code>或<code>:qall</code>：关闭所有窗口，放弃所有修改  </li><li><code>:wa</code>或<code>:wall</code>：全部写入磁盘   </li></ul></li><li>用<code>:argdo</code>或<code>:bufdo</code>修改一组缓冲区<br>  前提：<strong>打开<code>hidden</code>选项</strong><br>  打开后，对已修改的缓冲区执行<code>:next</code>,<code>:bnext</code>,<code>cnext</code>等命令无需再加感叹号   </li></ul></li></ul><h3 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h3><ul><li>分割  <ul><li><code>&lt;C-w&gt;s</code>：水平切分（同高度），新窗口仍为当前缓冲区   </li><li><code>&lt;C-w&gt;v</code>：垂直切分（同宽度），新窗口仍未当前缓冲区  </li><li><code>:sp {file}</code>或<code>:split {file}</code>：水平切分，新窗口为file内容   </li><li><code>:vsp {file}</code>或<code>:vsplit {file}</code>：垂直切分，新窗口为file内容  </li></ul></li><li>切换  <ul><li><code>&lt;C-w&gt;w</code>：轮换   </li><li><code>&lt;C-w&gt;{h|j|k|l}</code>：切换到左（下上右）侧的窗口  </li><li>若为GVIM，可以直接点击对用窗口进行切换    </li></ul></li><li>关闭  <ul><li><code>:clo</code>或<code>:close</code>或<code>&lt;C-w&gt;c</code>：关闭当前窗口   </li><li><code>:on</code>或<code>:only</code>或<code>&lt;C-w&gt;o</code>：关闭其他重口   </li></ul></li><li>改变窗口大小和重排列  <ul><li><code>&lt;C-w&gt;=</code>：使所有窗口等宽、等高  </li><li><code>&lt;C-w&gt;_</code>：使当前窗口高度最大化  </li><li><code>&lt;C-w&gt;|</code>：使当前窗口宽度最大化   </li><li><code>{N}&lt;C-w&gt;_</code>：使当前窗口高度调整为N行  </li><li><code>{N}&lt;C-w&gt;|</code>：使当前窗口宽度调整为N列  </li><li>若为GVIM，可以直接用鼠标拖动窗口的分界线   </li></ul></li></ul><h3 id="标签页"><a href="#标签页" class="headerlink" title="标签页"></a>标签页</h3><p>对窗口进行分组   </p><ul><li><code>:lcd {path}</code>：设置工作路径<br>  限定标签页的工程范围  <ul><li>只能改变当前窗口，而不是当前标签页  </li><li>如果要改变标签页的所有窗口，应用<code>:windo lcd {path}</code>   </li></ul></li><li>打开和关闭      <ul><li><code>:tabe {file}</code>或<code>tabedit {file}</code>：在新标签页中打开file  </li><li><code>&lt;C-w&gt;T</code>：将当前窗口移动到一个新标签页   </li><li><code>:tabc</code>或<code>:tabclos</code>：关闭当前标签页及其所有窗口   </li><li><code>:tabo</code>或<code>:tabonly</code>：关闭其他标签页及其所有窗口   </li></ul></li><li>切换  <ul><li>标签页从1开始编号   </li><li><code>:tabn {N}</code>或<code>:tabnext {N}</code>或<code>{N}gt</code>：切换到N号标签页   </li><li><code>:tabn</code>或<code>:tabnext</code>或<code>gt</code>：切换到下一标签页  </li><li><code>:tabp</code>或<code>:tabprevious</code>或<code>gT</code>：切换到上一标签页  </li></ul></li><li>重排列  <ul><li><code>:tabmove {N}</code>：将当前标签页移动到N号标签页之后  <ul><li><code>:tabmove 0</code>：表示移动到开头   </li><li><code>:tabmove</code>：表示移动到末尾   </li></ul></li><li>若为GVIM，可以直接用鼠标拖曳</li></ul></li></ul><h2 id="打开及保存文件"><a href="#打开及保存文件" class="headerlink" title="打开及保存文件"></a>打开及保存文件</h2><p>……待施工……</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.3 Linux </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>俱乐部春纳网页后端小结</title>
      <link href="/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF/"/>
      <url>/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="ubuntu下开发环境的搭建"><a href="#ubuntu下开发环境的搭建" class="headerlink" title="ubuntu下开发环境的搭建"></a>ubuntu下开发环境的搭建</h2><h3 id="php-apache-mysql-安装"><a href="#php-apache-mysql-安装" class="headerlink" title="php + apache + mysql 安装"></a>php + apache + mysql 安装</h3><pre><code>sudo apt-get install apache2sudo apt-get install libapache2-mod-php5 php5    sudo apt-get install mysql-server mysql-client</code></pre><p>参考资料：<a href="http://www.cnblogs.com/lynch_world/archive/2012/01/06/2314717.html" target="_blank" rel="noopener">ubuntu下安装Apache+PHP+Mysql</a>   </p><h3 id="apache的使用"><a href="#apache的使用" class="headerlink" title="apache的使用"></a>apache的使用</h3><p>把文件放到<code>/var/www/html</code>目录下，通过<code>localhost</code>访问<br>一般情况下apache是自动打开的，手动开启关闭重启的命令如下：          </p><pre><code>apachectl -k start          apachectl -k stop       apachectl -k restart       </code></pre><h3 id="mysql的使用"><a href="#mysql的使用" class="headerlink" title="mysql的使用"></a>mysql的使用</h3><h4 id="登录前"><a href="#登录前" class="headerlink" title="登录前"></a>登录前</h4><p>登录：<code>mysql [-h 服务器] -u 用户名 -p</code>然后按照提示输入密码<br>修改密码：<code>mysqladmin -u 用户名 password &quot;新密码&quot;</code>然后按照提示输入原密码      </p><h4 id="登录后"><a href="#登录后" class="headerlink" title="登录后"></a>登录后</h4><h5 id="用户相关操作、创建数据库（root）"><a href="#用户相关操作、创建数据库（root）" class="headerlink" title="用户相关操作、创建数据库（root）"></a>用户相关操作、创建数据库（root）</h5><p>创建用户：<code>insert into mysql.user(Host, User, Password) values(&#39;主机&#39;, &#39;用户名&#39;, password(&#39;密码&#39;));</code>          </p><ul><li>本地访问的主机是localhost， 如果要远程通过phpmyadmin管理数据库，那么主机是%*<br>创建数据库：<code>creat database 数据库名;</code><br>用户授权：<code>grant [select, update, insert, delect, all] on 数据库.表 to &#39;用户名&#39;@‘主机名’ identified by &quot;密码&quot;;</code>       </li><li>如果是所有数据库或所有表，用*代替*         </li><li>授权后刷新系统权限表：<code>flush privileges;</code>*<br>删除用户：<code>delect from user where User=&#39;用户名&#39; and Host=&#39;主机名&#39;;</code>;<br>删除后刷新系统权限表：<code>flush privileges;</code><br>删除数据库：<code>drop database 数据库名;</code><br>修改用户密码：<code>update mysql.user set password=password(&#39;新密码&#39;) where User=&#39;用户名&#39; and Host=&#39;主机名&#39;;</code><br>修改后刷新系统权限表：<code>flush privileges;</code>       </li></ul><h5 id="切换数据库："><a href="#切换数据库：" class="headerlink" title="切换数据库："></a>切换数据库：</h5><pre><code>use 数据库名;</code></pre><h5 id="显示数据："><a href="#显示数据：" class="headerlink" title="显示数据："></a>显示数据：</h5><pre><code>select 列 from 表名;</code></pre><p>若显示全部内容，列使用*      </p><h5 id="查看数据库："><a href="#查看数据库：" class="headerlink" title="查看数据库："></a>查看数据库：</h5><pre><code>show databases;</code></pre><p><strong>数据库中的charset尽量使用utf8,而不是gbk，防止暴露服务器信息</strong><br>参考资料：<a href="http://www.2cto.com/Article/201310/247718.html" target="_blank" rel="noopener">GBK字符编码（字符集）缺陷导致web安全漏洞</a>【虽然看不太懂】           </p><h3 id="phpmyadmin的使用"><a href="#phpmyadmin的使用" class="headerlink" title="phpmyadmin的使用"></a>phpmyadmin的使用</h3><h4 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h4><p>从官网下载后解压到<code>/var/www/html</code>，通过<code>localhost</code>访问      </p><h4 id="配置："><a href="#配置：" class="headerlink" title="配置："></a>配置：</h4><p>如果本地访问，直接使用就好<br>如果要连接到远程服务器的mysql，那么打开目录下的<code>libraries/config.defaut.php</code>文件，找到<code>$cfg[&#39;Servers&#39;][$i][&#39;host&#39;]</code>将值改为相应的服务器</p><h2 id="CI框架"><a href="#CI框架" class="headerlink" title="CI框架"></a>CI框架</h2><h3 id="安装：-1"><a href="#安装：-1" class="headerlink" title="安装："></a>安装：</h3><p>从官网下载后解压到<code>/var/www/html</code>目录下即可<br>通过<code>localhost/...../index.php/..控制器../..方法../..参数..</code>访问<br>相关子目录：            </p><blockquote><p><code>application</code>：用户主要目录（主要是views, controllers, models, config）<br><code>system</code>：框架的系统目录，一般不用管<br><code>user_guide</code>：用户手册，英文版，不需要可以删除         </p></blockquote><h3 id="设计模式：MVC（Model-View-Controller）"><a href="#设计模式：MVC（Model-View-Controller）" class="headerlink" title="设计模式：MVC（Model - View - Controller）"></a>设计模式：MVC（Model - View - Controller）</h3><h4 id="模型（Model）：处理页面和数据库的交互"><a href="#模型（Model）：处理页面和数据库的交互" class="headerlink" title="模型（Model）：处理页面和数据库的交互"></a>模型（Model）：处理页面和数据库的交互</h4><h5 id="数据库配置：-application-config-database-php文件"><a href="#数据库配置：-application-config-database-php文件" class="headerlink" title="数据库配置：.../application/config/database.php文件"></a>数据库配置：<code>.../application/config/database.php</code>文件</h5><p><code>$db[&#39;default&#39;][&#39;hostname&#39;]</code>：主机名<br><code>$db[&#39;default&#39;][&#39;username&#39;]</code>：mysql用户名<br><code>$db[&#39;default&#39;][&#39;password&#39;]</code>：mysql密码<br><code>$db[&#39;default&#39;][&#39;database&#39;]</code>：所用的数据库     </p><h5 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h5><pre class=" language-php"><code class="language-php"><span class="token delimiter">&lt;?</span> php              <span class="token keyword">Class</span> 模型名（首字母大写，与文件名相同，单文件名全小写） <span class="token keyword">extends</span> <span class="token class-name">CI_model</span>        <span class="token punctuation">{</span>                <span class="token keyword">public</span> <span class="token keyword">function</span> 方法名<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token punctuation">{</span>                        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>                <span class="token punctuation">}</span>        <span class="token punctuation">}</span></code></pre><p>特殊方法：__construct()，构建函数，当模型被调用时，首先自动调用这个函数<br>通常要在构建函数中载入数据库<code>$this-&gt;load-&gt;database();</code>      </p><h5 id="获取数据库内容"><a href="#获取数据库内容" class="headerlink" title="获取数据库内容"></a>获取数据库内容</h5><h6 id="获取整张表"><a href="#获取整张表" class="headerlink" title="获取整张表"></a>获取整张表</h6><p><code>$query=$this-&gt;db-&gt;get(&#39;表名&#39;);</code>返回Active Record类并且赋给变量$query<br><code>$query-&gt;result_array();</code>调用类的result_array()方法，返回一个包含各行内容的字典，可以在视图上通过foreach分别取出每一行<br><em>对于每一行，是一个以字段名为索引的字典</em></p><h6 id="获取某一行的内容"><a href="#获取某一行的内容" class="headerlink" title="获取某一行的内容"></a>获取某一行的内容</h6><p><code>$query=$this-&gt;db-&gt;get_where(&#39;表名&#39;, 筛选条件);</code>筛选条件是一个{‘字段名’=&gt;值}的数组<br><code>$query-&gt;row_array();</code>调用类的row_array()方法，返回一个包含匹配行的内容的字典        </p><h5 id="向数据库写入内容"><a href="#向数据库写入内容" class="headerlink" title="向数据库写入内容"></a>向数据库写入内容</h5><p><code>$this-&gt;input-&amp;gt;post(&#39;提交内容的name&#39;);</code>返回表当内对应name的value<br><code>$this-&gt;db-&gt;insert(&#39;表名&#39;, 内容)</code>写入数据库，内容是一个字典       </p><h4 id="视图（View）：要展示的静态页面"><a href="#视图（View）：要展示的静态页面" class="headerlink" title="视图（View）：要展示的静态页面"></a>视图（View）：要展示的静态页面</h4><p>可以把一个页面写成多个文件，显示时再拼接起来<br><em>比如可以创建一个templates文件夹，里面放一些各个网页通用头和尾</em><br>必须是php文件      </p><h5 id="html-gt-php改写"><a href="#html-gt-php改写" class="headerlink" title="html -&gt; php改写"></a>html -&gt; php改写</h5><p>连接的js和css等文件，还有图片等资源不能放在application目录下<br>需要在根目录下创建一个文件夹，把这些资源放进去<br>主要是修改连接的js和csss等文件、图片等资源的文件的地址          </p><ul><li>可以借助url的辅助函数<br>  需要在方法中加入<code>$this-&gt;load-&gt;helper(&#39;url&#39;);</code>载入辅助函数       <pre><code>  把根目录或者资源的目录写入`..../application/config/config.php`的`$config[&#39;base_url&#39;]`         </code></pre>  利用<code>base_url()</code>函数          <pre><code>  *如config中的base_url设置为localhost/join/resources*            *那么，base_url(‘image/background.jpg’)就会返回字符串loacalhost/join/resources/image/background.jpg*          </code></pre></li></ul><p>js，css文件内不能写php语句，如果需要使用这个函数，可以把内容直接复制到页面上，而不通过连接<br><strong>注意：改写的时候要注意不要不小心把原文件上的括号、分号给删掉了</strong>       </p><h4 id="控制器（Controller）：包含各种方法"><a href="#控制器（Controller）：包含各种方法" class="headerlink" title="控制器（Controller）：包含各种方法"></a>控制器（Controller）：包含各种方法</h4><h5 id="形式-1"><a href="#形式-1" class="headerlink" title="形式"></a>形式</h5><pre class=" language-php"><code class="language-php"><span class="token delimiter">&lt;?</span> php            <span class="token keyword">class</span> 控制器名（首字母大写，与文件名相同，但文件名全小写） <span class="token keyword">extends</span> <span class="token class-name">CI_controller</span>        <span class="token punctuation">{</span>                <span class="token keyword">public</span> <span class="token keyword">function</span> 方法名<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token punctuation">{</span>                        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>                <span class="token punctuation">}</span>        <span class="token punctuation">}</span></code></pre><p>特殊方法：<strong>construct()，构建函数<br>通常需要加入`parents::</strong>construct();<code>，……并不知道有什么卵用，照搬=。=</code>$this-&gt;load-&gt;model(‘模型名’);<code>载入模型</code>$this-&gt;load-&gt;view(‘视图文件名’, $data);`：不是只显示这个视图的内容，而是把这个文件的 内容写到当前方法的页面上，并且把变量$data内的数据传递到这个视图文件上           </p><h5 id="传递的数据在视图文件上的访问"><a href="#传递的数据在视图文件上的访问" class="headerlink" title="传递的数据在视图文件上的访问"></a>传递的数据在视图文件上的访问</h5><p>如<code>$data[&#39;news&#39;][&#39;title&#39;]</code><br>在视图上通过<code>$news[&#39;title]</code>来访问这个变量           </p><h5 id="调用模型上的方法："><a href="#调用模型上的方法：" class="headerlink" title="调用模型上的方法："></a>调用模型上的方法：</h5><p><code>$this-&gt;模型名-&gt;方法名；</code>        </p><h5 id="表单数据的传递"><a href="#表单数据的传递" class="headerlink" title="表单数据的传递"></a>表单数据的传递</h5><p>controllers要有一个方法来接受表单的数据<br>form的action应指向这个方法<br>这个方法用来处理表单传递的数据，并且调用模型的方法与数据库进行交互           </p><h2 id="提交到服务器"><a href="#提交到服务器" class="headerlink" title="提交到服务器"></a>提交到服务器</h2><h3 id="登录："><a href="#登录：" class="headerlink" title="登录："></a>登录：</h3><p><code>ssh 用户名@服务器</code>，然后输入密码        </p><h3 id="服务器与本地文件的传输"><a href="#服务器与本地文件的传输" class="headerlink" title="服务器与本地文件的传输:"></a>服务器与本地文件的传输:</h3><p>scp命令，用法与cp命令相似<br>比如上传本地<code>~/home.php</code>到服务器的主目录下:<br><code>scp ~/home.php 用户名@服务器:home.php</code></p><h3 id="网站的目录："><a href="#网站的目录：" class="headerlink" title="网站的目录："></a>网站的目录：</h3><p>一般来说，<br>对于nginx，放在<code>/usr/share/nginx/html/</code>目录下<br>对于apache，放在<code>/var/www/html/</code>目录下         </p><p>nginx的配置挺复杂的，还有待以后进一步的学习</p><h3 id="关闭ci框架的debug模式"><a href="#关闭ci框架的debug模式" class="headerlink" title="关闭ci框架的debug模式"></a>关闭ci框架的debug模式</h3><p><strong>网站挂上去后最好关闭debug模式（防止程序信息暴露在前台）</strong><br><code>..../application/config/database.php</code>中的<code>$db[&#39;default&#39;][&#39;db_debug&#39;]</code>修改为FALSE即可</p><h2 id="github"><a href="#github" class="headerlink" title="github"></a>github</h2><p>其实一开始写后端的时候并没有git= =时间紧啊，没时间给我慢慢玩<br>不过网站挂上去，在写查重和改写雨停的优化版的时候就玩了玩git          </p><h3 id="github在Windows上有客户端，直接使用就好"><a href="#github在Windows上有客户端，直接使用就好" class="headerlink" title="github在Windows上有客户端，直接使用就好"></a>github在Windows上有客户端，直接使用就好</h3><h3 id="在linux上需要用git"><a href="#在linux上需要用git" class="headerlink" title="在linux上需要用git"></a>在linux上需要用git</h3><h4 id="安装git："><a href="#安装git：" class="headerlink" title="安装git："></a>安装git：</h4><p><code>sudo apt-get install git</code></p><h4 id="配置git："><a href="#配置git：" class="headerlink" title="配置git："></a>配置git：</h4><h5 id="本地创建ssh-key："><a href="#本地创建ssh-key：" class="headerlink" title="本地创建ssh key："></a>本地创建ssh key：</h5><p><code>ssh-keygen -t rsa -C &quot;邮箱&quot;</code>，会提示确认路径和密码，一路回车默认即可          </p><h5 id="在github中添加ssh-key："><a href="#在github中添加ssh-key：" class="headerlink" title="在github中添加ssh key："></a>在github中添加ssh key：</h5><p>进入Account Settings，选择SSH Key，Add SSH Key，title随便填，key复制粘贴文件<code>~/.ssh/id_rsa.pub</code>的全部内容</p><h5 id="设置username和email"><a href="#设置username和email" class="headerlink" title="设置username和email"></a>设置username和email</h5><p>每次commit都会记录这两个信息<br><code>git config --global user.name=&quot;用户名&quot;</code><br><code>git config --global user.email=&quot;邮箱&quot;</code>           </p><h4 id="设置要git的目录"><a href="#设置要git的目录" class="headerlink" title="设置要git的目录"></a>设置要git的目录</h4><p>进入要git的目录<br>初始化：<code>git init</code><br>添加远程地址：<code>git remote add origin git@github.com:github用户名/远程仓库名.git</code>            </p><h5 id="使用（简单的git命令）"><a href="#使用（简单的git命令）" class="headerlink" title="使用（简单的git命令）"></a>使用（简单的git命令）</h5><p><code>git clone &lt;ADDRESS&gt;</code>：克隆代码库，可以是本地代码库，也可以是远程代码库（USERNAME@HOST:PATH）<br><code>git add &lt;-i&gt; &lt;FILE&gt; ....</code>：【-i，交互式】添加文件到代码库中<br><code>git rm &lt;FILE&gt;</code>：将文件移出代码库<br><code>git commit -m &lt;MESSAGE&gt;</code>：提交更改，将修改的文件提交到缓冲区<br><code>git push &lt;ORIGIN&gt; &lt;BRANCH&gt;</code>：推送代码库到远程代码库<br><code>git pull</code>：从远程同步代码库到本地并合并<br><code>git branch</code>：查看当前分支<br><code>git branch &lt;BRANCH&gt;</code>：创建分支（主分支为Master）<br><code>git branch -d &lt;BRANCH&gt;</code>：删除分支<br><code>git checkout &lt;BRANCH&gt;</code>：切换到指定分支<br><code>git checkout -- &lt;FILE&gt;</code>：从缓冲区替换本地改动<br><code>git log</code>：查看提交的历史记录<br><code>git status</code>：显示当前的修改状态<br><code>git reset &lt;LOG&gt;</code>：恢复到历史版本<br>参考资料：<br><a href="http://www.bootcss.com/p/git-guide/" target="_blank" rel="noopener">git - 简易指南</a><br><a href="http://www.eoeandroid.com/thread-274556-1-1.html" target="_blank" rel="noopener">【Github教程】史上最全github使用方法：github入门到精通</a></p><h2 id="附：ubuntu下的sublime-text"><a href="#附：ubuntu下的sublime-text" class="headerlink" title="附：ubuntu下的sublime text"></a>附：ubuntu下的sublime text</h2><h3 id="安装：sudo-apt-get-install-sublime-text"><a href="#安装：sudo-apt-get-install-sublime-text" class="headerlink" title="安装：sudo apt-get install sublime-text"></a>安装：<code>sudo apt-get install sublime-text</code></h3><h3 id="中文支持"><a href="#中文支持" class="headerlink" title="中文支持"></a>中文支持</h3><p>参考资料：<a href="http://jingyan.baidu.com/article/6fb756eca7af6c241858fbf2.html" target="_blank" rel="noopener">解决Ubuntu下Sublime Text 2无法输入中文的方法</a></p><h3 id="Markdown支持"><a href="#Markdown支持" class="headerlink" title="Markdown支持"></a>Markdown支持</h3><p>参考资料：<a href="http://www.jianshu.com/p/378338f10263" target="_blank" rel="noopener">sublime text 2 下的Markdown写作</a><br>但是语法高亮的设置我一直找不到,最后是在<code>Package Control: install Pakage</code>中直接安装MarkdownEditing，但是语法高亮效果不太满意,不知道有没有更好的解决方法</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>雨停的前端做的好赞！！！诶，不会美工的小码农真是不能比啊╮(╯▽╰)╭</p><p>=  =学习CI，用服务器的时候遇到好多逗比的问题去请教蛋蛋和六神       </p><p>感谢蛋蛋和六神的指导嘿嘿～～～                   </p><p>CI官网的教程写的有点晦涩，不太好懂，这里只是个人在使用中自己的理解，可能会有一些错误，欢迎指出～        </p><p>不过网站还不会挂咯，找个时间得学学怎么挂网站～～</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>居然……还我数据？！？！</title>
      <link href="/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93/"/>
      <url>/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>在学校的时候把windows还原了一遍，导致linux进不去了，orz<br>回来之后重装linux，结果不小心操作不当，把硬盘格式化了！！<br>后来发现硬盘又出了点问题，哎烦，查了好多东西，试了好多方法，而且恰巧在看鸟哥的第三章（硬盘分区方面的内容），折腾了三天半终于把这娃整好，真是感动<br>查的东西好杂，让我写个总结冷静冷静  </p><p>##还原windows导致linux无法进入的原因<br>windows是相当霸道的，在安装时，<strong>win会主动覆盖掉MBR，也就直接把linux的引导加载程序给覆盖掉</strong>，linux就自然无法进入啦</p><p>##那么，我们把linux引导加载程序grub修复了就好啦？<br>咳咳……蛋蛋让我去百度查查怎么修复grub，不过太弱了，看了三种修复的教程都没能看懂，最后似乎还把linux整坏了！！！只好重装linux，可又忘记重新分区，选了个“<strong>清除所有程序和数据</strong>”…………呵呵，linux比windows还霸道，清除的不只是linux的系统分区，而是全硬盘啊！！把我全硬盘都格式化成ext4，真是吓哭我了……      </p><p>##找回丢失的数据</p><ol><li><strong>硬盘被格式化并不会直接把上面的数据抹去，不被新数据覆盖的区段的数据其实是可以复原的；</strong>  </li><li><strong>而linux安装的位置在硬盘的头部，我的主要数据是放在DEF盘，并不会被覆盖，修复数据还是有戏的</strong><br><em>重要的数据放在C盘是极不安全的，一旦系统崩溃，还原、重装之后就被新的系统文件覆盖，找都找不回来……所以平常要把桌面、我的文档、收藏夹等数据设置在DEF盘，至少应该把文档所在的文件夹放在DEF盘，再创建快捷方式到桌面上……非常非常重要的数据要做好备份，备份到网络上（并不可靠）或者备份到移动硬盘等其他设备上</em>  </li></ol><p>所以呢，网上找找PE，通过UltraISO等软件把iso镜像烧进U盘里，插进计算机，开机启动项设置为该U盘，在PE下利用diskgenius等分区工具进行“<strong>重建分区表</strong>”就可以啦。它会自动搜索电脑上丢失的分区，把还存在数据恢复出来   </p><p><em>但是，恢复的数据会有少数被损坏，也就是说不可能完好无损的找回来，所以平时要保护好自己的数据啊？！？！？！！！</em><br><em>而且，找回来的分区必须设置为主分区= =好麻烦！改回逻辑分区的话需要把数据移到其他地方，把这些分区删除后新建扩展分区，在扩展分区下新建逻辑分区，再把数据移回来！</em><br><strong>因为一个硬盘只能存在四个主分区或扩展分区（其中扩展分区至多只能有一个）</strong><br>嗯……我比较懒，所以把原本win上的四个磁盘缩减为三个磁盘，都作为主分区，留下一个作为扩展分区给linux<br>另外呢，<strong>win系统分区必须激活设置为活动分区！！</strong>   </p><p>##安装windows<br>找回数据时，把dell出厂的镜像分区也给找回来了，但是还原程序被格掉，重装后也识别不了出厂镜像= =它那鬼镜像（wim）还分成了两组，每组两个，着实看不懂要怎么用……只好放弃正版win8，上网找了原版的win8.1企业版，在PE下通过winNT安装，再用激活工具破解，从此又过上了盗版的日子<br><em>不过镜像我还留着，据说有方法能够调用，但是实在复杂，等我有空好好研究研究</em>  </p><p>安装windows其实有个小插曲，一开始NT一直识别不了我下载的win8.1的安装程序，折腾半天后无奈的检验了一下SHA1，简直哭，SHA1值并不匹配——也就是下载的时候文件被损坏了！<br><strong>所以啊，以后安装重要的、安装过程复杂的程序、软件，一定一定要注意先校验一下MD5值或者SHA1值</strong>（在官网下载的话一般会提供）<br><em>否则如果在安装过程中才发现损坏，会很麻烦的。比如安装系统，装一半的时候才告诉你文件损坏，可是你原来的win早被格式化了，整起来可就麻烦多了（所幸现在系统安装程序在安装前大都保守地检验了一下安装文件的完整性）</em></p><p>###internal hard disk drive not found？<br>这条错误信息着实折腾了我好久，到底是无法读取硬盘呢还是无法调用驱动程序？<br>大概是后者吧，毕竟我在BIOS中能找到硬盘的信息，在PE下也能正常访问硬盘的数据<br>硬盘驱动？这个一般操作系统都会提供——一开始我以为只要把系统装上，这个问题就能够解决，然而并没有这么简单！<br>在PE下安装系统后，需要重启进行最后的设置，但是每次都不能成功进入系统，都卡在这句错误信息上！<br>怎么办？我尝试了百度上的各种办法，最后终于成功了？！——<br><em>在BIOS的boot选项卡下把Secure Boot设置为Disabled，把Boot List Option改为Legacy</em><br>竟然这样就把问题解决了？！百思不得其解，只好求教度娘这两个选项的含义</p><p>####Secure Boot  <a href="http://bbs.taobao.com/catalog/thread/154503-260136140.htm" target="_blank" rel="noopener">##参考资料##</a><br>若干年前，各大主板厂商推出UEFI取代历史悠久的BIOS，UEFI全称为“统一的可扩展固定接口”。<br>而Secure Boot是UEFI的一部分，它采用密钥，防止恶意软件侵入操作系统和硬件驱动程序</p><p>但是UEFI并不能得到广泛的推广，原因是微软的态度并不积极，他们要求安装Windows8时要关闭Secure Boot，而对预装的Windows8需要打开Secure Boot，所以我在安装windows时就必须把Secure Boot关闭啦！</p><p>####Boot List Option<br>硬盘启动模式，包括Legacy和UEFI两种  </p><p>UEFI如上文介绍，是一种新的BIOS；<br>Legacy则是传统的BIOS模式  </p><p>在UEFI下安装的系统以后只能由UEFI模式进入系统；<br>在Legacy下安装的系统以后只能由Legacy模式进入系统  </p><p>甚是复杂！！</p><p>##Linux安装<br>引导方式有两种——   </p><ol><li>win引导linux：<br> 把引导分区设置为挂载/boot的分区<br> 通过软件easybcd等软件进行引导  </li><li>linux引导win：（默认）引导程序将被安装到MBR上  </li></ol><p>##最后<br>电脑修的累死我咯，但是能修好，重新见到windows桌面，真是非常的愉悦，感觉三天半没白忙）<br>说不定以后找不到工作还能给人修修电脑啊哈哈哈哈哈哈！！！  </p><p>唯一不满意的是没能把出厂的原版系统找回来，等有空接着研究研究吧！</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>【百度俱乐部】第一二期作业总结</title>
      <link href="/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/"/>
      <url>/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>呃，要用markdown写文章呢差点忘了……<br>这期作业写了我八九个小时啊，翻阅了好多资料，干脆写份总结算了。下面就按照我做作业的思路、途中遇到的问题及解决方法展开来写吧——  </p><h2 id="大致设想"><a href="#大致设想" class="headerlink" title="大致设想"></a>大致设想</h2><p>首先，模仿百度主页嘛，找张图片，做个输入框，搞个submit的按钮，简单安上导航栏，右上角意思意思搞个“登陆”、“注册”的鬼玩意。   </p><h3 id="input-text的尺寸调整"><a href="#input-text的尺寸调整" class="headerlink" title="input text的尺寸调整"></a>input text的尺寸调整</h3><ul><li>text是没有width、height属性的</li><li>宽度其实可以通过字段size调整</li><li>高度可以通过style属性中的字体高度font-size来调整</li><li>另外还有style中的padding，可以调整输入框的内边距，不然输入的时候字压着边框太丑<h3 id="text和submit之间总存在缝隙"><a href="#text和submit之间总存在缝隙" class="headerlink" title="text和submit之间总存在缝隙"></a>text和submit之间总存在缝隙</h3>取消缝隙，首先要设置两个元素的margin为0，另外submit默认是有边框的，所以还要设置submit的border为0。但如果将两元素的代码分成两行，则间隙仍不能取消<br><input type="text" style="margin:0px"><input type="submit" style="margin:0px;border:0;background-color:blue">     </li></ul><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>submit<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px<span class="token punctuation">;</span><span class="token property">border</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token property">background-color</span><span class="token punctuation">:</span>blue</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span></code></pre><ul><li>其实回车也会留下空格的，所以只要把回车去掉，将两个元素挤在一行即可<br><input type="text" style="margin:0px"><input type="submit" style="margin:0px;border:0;background-color:blue"></li></ul><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>submit<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px<span class="token punctuation">;</span><span class="token property">border</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token property">background-color</span><span class="token punctuation">:</span>blue</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span></code></pre><h3 id="text和submit高低错位"><a href="#text和submit高低错位" class="headerlink" title="text和submit高低错位"></a>text和submit高低错位</h3><p>首先考虑用top，bottom属性设置上下留空白解决，但是无论怎么改变值都没有变化<br>查过资料后知道，设置left,right,top,bottom前需要设置position    </p><blockquote><p>position有五种值：<br><code>static</code>（默认）：不定位<br><code>relative</code>：相对于块<br><code>absolute</code>：相对于页面<br><code>fixed</code>：相对于视窗<br><code>inherit</code>：继承父元素的值   </p></blockquote><h2 id="栅格系统"><a href="#栅格系统" class="headerlink" title="栅格系统"></a>栅格系统</h2><p>我直接在<code>&lt;body&gt;&lt;/body&gt;</code>中加入  </p><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>row<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>col-md-1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>...<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>col-md-offset-1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>...<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span></code></pre><p>结果什么都没发生！！！<br>后来才知道，是忘了<code>&lt;link href=&quot;http://cdn.bootcss.com/bootstrap/3.2.0/css/bootstrap.css&quot; rel=&quot;stylesheet&quot;&gt;</code>……呵……呵……简直逗比~</p><blockquote><p>说道栅格系统，顺便截一段资料过来吧：<br>【<a href="http://v3.bootcss.com/css/#grid" target="_blank" rel="noopener">栅格系统  bootstrap</a>】</p><ul><li>行列必须包含在.container里</li><li>列（.col-xx-xx）必须包含在行（.row）里</li></ul></blockquote><p><table border="1"><br>        <tr><br>            <td> </td><br>            <td>超小屏幕 手机（&lt;768px) </td><br>            <td>小屏幕 平板 （&gt;=768px) </td><br>            <td>中等屏幕 桌面显示器 (&gt;=992px) </td><br>            <td>大屏幕 大桌面显示器 (&gt;=1200px) </td><br>        </tr><br>        <tr><br>            <td>栅格系统行为</td><br>            <td>总是水平</td><br>            <td colspan="3">开始是堆叠在一起的，当大于这些阈值时将变为水平排列C</td><br>        </tr><br>        <tr><br>            <td><font color="red">.container</font>最大宽度</td><br>            <td>None（自动）</td><br>            <td>750px</td><br>            <td>970px</td><br>            <td>1170px</td><br>        </tr><br>        <tr><br>            <td>类前缀</td><br>            <td><font color="red">.col-xs-</font></td><br>            <td><font color="red">.col-sm-</font></td><br>            <td><font color="red">.col-md-</font></td><br>            <td><font color="red">.col-lg-</font></td><br>        </tr><br>        <tr><br>            <td>列（column）数</td><br>            <td colspan="4">12</td><br>        </tr><br>        <tr><br>            <td>最大列宽</td><br>            <td>自动</td><br>            <td>~62px</td><br>            <td>~81px</td><br>            <td>~97px</td><br>        </tr><br>        <tr><br>            <td>槽（gutter）宽</td><br>            <td colspan="4">30px（每列左右均有15px）</td><br>        </tr><br>        <tr><br>            <td>可嵌套</td><br>            <td colspan="4">是</td><br>        </tr><br>        <tr><br>            <td>偏移（offsets）</td><br>            <td colspan="4">是</td><br>        </tr><br>        <tr><br>            <td>列排序</td><br>            <td colspan="4">是</td><br>        </tr><br></table><br>做作业的时候忘记练习表格了……所以手码下来……<br>哇靠！手码表格真是累死了……<br><br><br>列合并：colspan属性</p><h2 id="逗比功能：鼠标悬浮事件"><a href="#逗比功能：鼠标悬浮事件" class="headerlink" title="逗比功能：鼠标悬浮事件"></a>逗比功能：鼠标悬浮事件</h2><p>效果：鼠标移到submit上，submit会自动跳到输入框的另外一侧，就是不给你点，还要在输入框上出现（don’t touch me！），但是不要影响搜索！要搜索的话只能点击图片，为了别太坑还得做一个小小的提示框，但不能太大——简直逗比的我……  </p><h3 id="submit跳动"><a href="#submit跳动" class="headerlink" title="submit跳动"></a>submit跳动</h3><ul><li>鼠标悬浮事件Mouseover</li><li>在两个分别设置两个submit，让他们不能同时出现，display属性，toggle方法<br>  起始状态下，左边的dispaly为none<br>  触发事件后，对两个submit都采用toggle方法  <h3 id="在输入框中出现don’t-touch-me"><a href="#在输入框中出现don’t-touch-me" class="headerlink" title="在输入框中出现don’t touch me"></a>在输入框中出现don’t touch me</h3></li><li>text输入框value属性的获取和修改  <ul><li>获取：$(“…”).val()方法即可返回表单的字段   </li><li>修改：$(“…”).attr(“…”,”…”)方法即可将第一个参数匹配的属性修改为第二个参数的内容</li></ul></li><li>但是这个用起来似乎有点麻烦……在百度上看到另外一种解决办法<ul><li>document.#form.#text.value = document.#text.#text.value + “(Don’t touch me!)”;</li><li><strong>要注意的是：这里的#form和#text是form标签和input标签的<font color="red">name</font>属性而不是<del><font color="red"> id </font></del>属性！而且这里不需要打井号，如：document.form.text.value</strong><h3 id="点击图片自动跳转到百度搜索"><a href="#点击图片自动跳转到百度搜索" class="headerlink" title="点击图片自动跳转到百度搜索"></a>点击图片自动跳转到百度搜索</h3></li></ul></li><li>跳转<ul><li>在百度搜索试着搜索aaa，搜索页得到的网址是<code>http://www.baidu.com/baidu?wd=aaa</code></li><li>那么猜测，直接修改网址末尾的aaa为搜索的内容是否可行？结果是可以的……</li><li>那么，我们只需要把搜索内容连接到<code>http://www.baidu.com/baidu?wd=</code>末尾作为图片的超链接即可实现</li></ul></li><li>搜索内容的获取<ul><li>键盘keyup事件<br>  <em>与其相对的还有一个keydown事件，down是指按下键盘（包括输入法还未把输出到对话框）触发的时间，up是指松开键盘（包括输入法把内容输出到对话框上）触发的时间，</em><br>  <strong>显然这里要用的是应该是keyup事件</strong><br>  <em>另外，使用keyup事件不会在不通过键盘输入来修改内容的情况下触发，也就是说，前边用JQ修改输入框内容并不会触发事件，不影响内容的获取</em></li><li>将搜索内容与<code>http://www.baidu.com/baidu?wd=</code>组成的连接作为图片连接的href属性，使用<code>attr(&quot;href&quot;,&quot;...&quot;)</code>方法即可  <h3 id="警告框"><a href="#警告框" class="headerlink" title="警告框"></a>警告框</h3></li></ul></li><li>使用bootstrap，自己再对字体、尺寸稍作修改即可</li><li>但是千万别忘了<br>  <code>&lt;link href=&quot;http://cdn.bootcss.com/bootstrap/3.2.0/css/bootstrap.css&quot; rel=&quot;stylesheet&quot;&gt;</code></li><li>display:none</li><li><p>show()方法</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>使用JQ的格式——<br>注意：fuction紧跟后面只有一个右括号<br>ready的右括号在最后边，中间的代码即花括号的内容是fuction的内容   </p><pre class=" language-html"><code class="language-html">$(document).ready**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>font</span> <span class="token attr-name">color</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>red<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>(<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>font</span><span class="token punctuation">></span></span>**function**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>font</span> <span class="token attr-name">color</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>green<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>()<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>font</span><span class="token punctuation">></span></span>**{       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>br</span><span class="token punctuation">></span></span>}**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>font</span> <span class="token attr-name">color</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>red<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>)<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>font</span><span class="token punctuation">></span></span>**;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>br</span><span class="token punctuation">></span></span>    $(document).ready(function(){  $("...").click(function(){      $("...").attr("","");  });});</code></pre></li><li><p>选择器在圆括号内，除了document和this之外都要加双引号，类要加’.’，id要加’#’</p></li><li>JQ的内容一定要嵌套在<code>$(document).ready(function(){.....});</code>内</li><li>语句后边别忘了分号</li></ul><h2 id="登陆、注册弹框"><a href="#登陆、注册弹框" class="headerlink" title="登陆、注册弹框"></a>登陆、注册弹框</h2><h3 id="在百度搜到一种遮罩式弹框的模板"><a href="#在百度搜到一种遮罩式弹框的模板" class="headerlink" title="在百度搜到一种遮罩式弹框的模板"></a>在百度搜到一种遮罩式弹框的模板</h3><p><a href="http://www.jb51.net/article/34951.htm" target="_blank" rel="noopener">jQuery+css+html实现页面遮罩弹出框</a><br>具体代码如下：</p><h4 id="CSS部分"><a href="#CSS部分" class="headerlink" title="CSS部分:"></a>CSS部分:</h4><pre class=" language-css"><code class="language-css"><span class="token selector">body </span><span class="token punctuation">{</span>     <span class="token property">font-family</span><span class="token punctuation">:</span>Arial, Helvetica, sans-serif<span class="token punctuation">;</span>     <span class="token property">font-size</span><span class="token punctuation">:</span><span class="token number">12</span>px<span class="token punctuation">;</span>     <span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#main</span> </span><span class="token punctuation">{</span>     <span class="token property">height</span><span class="token punctuation">:</span><span class="token number">1800</span>px<span class="token punctuation">;</span>     <span class="token property">padding-top</span><span class="token punctuation">:</span><span class="token number">90</span>px<span class="token punctuation">;</span>     //打开弹窗的按钮    <span class="token property">text-align</span><span class="token punctuation">:</span>center<span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#fullbg</span> </span><span class="token punctuation">{</span>     <span class="token property">background-color</span><span class="token punctuation">:</span>gray<span class="token punctuation">;</span>     <span class="token property">left</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span>     <span class="token property">opacity</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">;</span>        //背景设置为灰色    <span class="token property">position</span><span class="token punctuation">:</span>absolute<span class="token punctuation">;</span>     <span class="token property">top</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span>     <span class="token property">z-index</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">;</span>     <span class="token property">filter</span><span class="token punctuation">:</span><span class="token function">alpha</span><span class="token punctuation">(</span>opacity=<span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token property">-moz-opacity</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">;</span>     <span class="token property">-khtml-opacity</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#dialog</span> </span><span class="token punctuation">{</span>     <span class="token property">background-color</span><span class="token punctuation">:</span><span class="token hexcode">#fff</span><span class="token punctuation">;</span>     <span class="token property">border</span><span class="token punctuation">:</span><span class="token number">5</span>px solid <span class="token function">rgba</span><span class="token punctuation">(</span><span class="token number">0</span>,<span class="token number">0</span>,<span class="token number">0</span>, <span class="token number">0.4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token property">height</span><span class="token punctuation">:</span><span class="token number">400</span>px<span class="token punctuation">;</span>     <span class="token property">left</span><span class="token punctuation">:</span><span class="token number">50%</span><span class="token punctuation">;</span>     <span class="token property">margin</span><span class="token punctuation">:</span>-<span class="token number">200</span>px <span class="token number">0</span> <span class="token number">0</span> -<span class="token number">200</span>px<span class="token punctuation">;</span>     <span class="token property">padding</span><span class="token punctuation">:</span><span class="token number">1</span>px<span class="token punctuation">;</span>     <span class="token property">position</span><span class="token punctuation">:</span>fixed <span class="token important">!important</span><span class="token punctuation">;</span> //弹出的浮动对话框     <span class="token property">position</span><span class="token punctuation">:</span>absolute<span class="token punctuation">;</span>     <span class="token property">top</span><span class="token punctuation">:</span><span class="token number">50%</span><span class="token punctuation">;</span>     <span class="token property">width</span><span class="token punctuation">:</span><span class="token number">400</span>px<span class="token punctuation">;</span>     <span class="token property">z-index</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">;</span>     <span class="token property">border-radius</span><span class="token punctuation">:</span><span class="token number">5</span>px<span class="token punctuation">;</span>     <span class="token property">display</span><span class="token punctuation">:</span>none<span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector">dialog p </span><span class="token punctuation">{</span>     <span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span> <span class="token number">0</span> <span class="token number">12</span>px<span class="token punctuation">;</span>     <span class="token property">height</span><span class="token punctuation">:</span><span class="token number">24</span>px<span class="token punctuation">;</span>     <span class="token property">line-height</span><span class="token punctuation">:</span><span class="token number">24</span>px<span class="token punctuation">;</span>     <span class="token property">background</span><span class="token punctuation">:</span><span class="token hexcode">#CCCCCC</span><span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#dialog</span> p<span class="token class">.close</span> </span><span class="token punctuation">{</span>     <span class="token property">text-align</span><span class="token punctuation">:</span>right<span class="token punctuation">;</span>     <span class="token property">padding-right</span><span class="token punctuation">:</span><span class="token number">10</span>px<span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#dialog</span> p<span class="token class">.close</span> a </span><span class="token punctuation">{</span>     <span class="token property">color</span><span class="token punctuation">:</span><span class="token hexcode">#fff</span><span class="token punctuation">;</span>     <span class="token property">text-decoration</span><span class="token punctuation">:</span>none<span class="token punctuation">;</span> <span class="token punctuation">}</span> </code></pre><h4 id="HTML部分"><a href="#HTML部分" class="headerlink" title="HTML部分:"></a>HTML部分:</h4><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>main<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>javascript:showBg();<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>点击这里查看效果<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>fullbg<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>dialog<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>close<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span> <span class="token attr-name">onclick</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>closeBg();<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>关闭<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span><span class="token punctuation">></span></span>正在加载，请稍后....<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span></code></pre><p>####JQ部分:</p><pre class=" language-js"><code class="language-js"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span>    <span class="token comment" spellcheck="true">//显示灰色 jQuery 遮罩层</span>    <span class="token keyword">function</span> <span class="token function">showBg</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">var</span> bh <span class="token operator">=</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"body"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">height</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">var</span> bw <span class="token operator">=</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"body"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">width</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"#fullbg"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">css</span><span class="token punctuation">(</span><span class="token punctuation">{</span>            height<span class="token punctuation">:</span>bh<span class="token punctuation">,</span>            width<span class="token punctuation">:</span>bw<span class="token punctuation">,</span>            display<span class="token punctuation">:</span><span class="token string">"block"</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"#dialog"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//关闭灰色 jQuery 遮罩</span>    <span class="token keyword">function</span> <span class="token function">closeBg</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"#fullbg,#dialog"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">hide</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span></code></pre><p>只要稍微修改一些数值就可以直接拿来用啦!!</p><h3 id="第二个弹框"><a href="#第二个弹框" class="headerlink" title="第二个弹框"></a>第二个弹框</h3><p>但是……直接套用模板，把html再复制一份企图创建第二个弹框就出现问题了，两个按钮弹出的是同一个框，咋办咧？<br>观察一下代码，发现JQ部分的<code>$(&quot;#dialog&quot;).show();</code>语句是对id为dialog的操作，如果在html里直接复制了第一个弹框，id都为dialog，show只为作用于第一个匹配项………<br>如果想要第二个弹框，就得让两个弹框的id不同，改为dialog1和dialog2单击后调用两个不一样的函数，也就是简单的复制一下showBg()函数，并且改为showBg1()和showBg2，并在css部分做出相应的修改就可以啦~     </p><h3 id="模态框"><a href="#模态框" class="headerlink" title="模态框"></a>模态框</h3><p>上面这个弹框太low了，其实bootstrap已经给我们提供了一个模态框的组件，方便又好看<br>但是偶尔会出现一些奇怪的bug…………</p><h2 id="给鼠标悬浮在submit上的时候加个音频"><a href="#给鼠标悬浮在submit上的时候加个音频" class="headerlink" title="给鼠标悬浮在submit上的时候加个音频"></a>给鼠标悬浮在submit上的时候加个音频</h2><p>嘿嘿，逗比功能就是拿来搞怪的，既然如此干嘛不个它加个笑声的音频呢？  </p><h3 id="找到一个简短的小黄人笑声的音频"><a href="#找到一个简短的小黄人笑声的音频" class="headerlink" title="找到一个简短的小黄人笑声的音频"></a>找到一个简短的小黄人笑声的音频</h3><h3 id="添加一个-lt-audio-gt-标签"><a href="#添加一个-lt-audio-gt-标签" class="headerlink" title="添加一个&lt;audio&gt;标签"></a>添加一个<code>&lt;audio&gt;</code>标签</h3><p>controls属性：如果不设置的话，默认不显示播放器；如果设置为controls=”controls”则显示播放器<br>play()方法：让指定的播放器播放音频<br>具体代码如下：  </p><h4 id="html部分："><a href="#html部分：" class="headerlink" title="html部分："></a>html部分：</h4><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>audio</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>laugh<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>...<span class="token punctuation">"</span></span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>audio/mpeg<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>audio</span><span class="token punctuation">></span></span></code></pre><h4 id="JQ部分：（插入在submit的mouseover事件下）"><a href="#JQ部分：（插入在submit的mouseover事件下）" class="headerlink" title="JQ部分：（插入在submit的mouseover事件下）"></a>JQ部分：（插入在submit的mouseover事件下）</h4><pre class=" language-js"><code class="language-js">laugh<span class="token punctuation">.</span><span class="token function">play</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong> 奇怪的是！这里要直接对这个对象使用方法，id不加井号；</strong><br><strong> 而<code>$(&quot;#laugh&quot;).play();</code>却行不通？！</strong>    </p><h2 id="咳咳，总结——"><a href="#咳咳，总结——" class="headerlink" title="咳咳，总结——"></a>咳咳，总结——</h2><p>感觉像是玩上瘾了，居然两天花了八九个小时做这个网页，而且这两天还是很多课不是没课的……<br>又在第三天花了四五个小时写下这个玩意儿……我也是够拼哒= =<br>虽然很好玩，做出来也很有成就感……但是………我的眼睛啊！！！！！不知道近视又是加深的几度？！</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
