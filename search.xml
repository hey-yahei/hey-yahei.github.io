<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>MixConv：混合感受野的深度可分离卷积</title>
      <link href="/2019/11/01/MixConv/"/>
      <url>/2019/11/01/MixConv/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28258452&auto=0&height=32"></iframe>        <p>继EfficientNet和CondConv之后，我们再来聊聊Google Brain的另一篇图像特征提取相关的论文《<a href="https://arxiv.org/pdf/1907.09595" target="_blank" rel="noopener">MixConv: Mixed Depthwise Convolutional Kernels(2019BMVC)</a>》，作者<strong>采用类似inception的大小核混合卷积结果，再结合深度可分离卷积的特性，做到少量的额外代价即可提升卷积的特征提取能力。</strong>      </p><h3 id="混合感受野的卷积"><a href="#混合感受野的卷积" class="headerlink" title="混合感受野的卷积"></a>混合感受野的卷积</h3><p>MixConv的作者在MobileNet上做了几个简单实验，把原版的3x3卷积统一替换为更大的卷积，然后观察模型特征提取能力的变化——    </p><center><img src="/imgs/MixConv/PerformanceWithDifferentKernelSize.png" alt=""></center>      <p>发现，随着卷积核的增大，其特征提取能力会随之增大，但超过某个值之后就反而会减小。并且指出，      </p><ol><li>大核卷积擅长提取大感受野特征，而小核卷积擅长提取小感受野特征       </li><li>高感受野特征和小感受野特征是互补关系，并不是说大感受野特征一定比小感受野特征要好（可以考虑卷积核与特征图尺寸相当的极端情况）</li><li>如果能够混合不同感受野的特征，那么对于提高特征提取能力将会有所帮助     </li></ol><p>其实混合感受野的卷积早就在著名的<a href="/2018/05/02/经典的CNN分类架构/#GoogLeNet">inception</a>上应用过，inception将输入分别投喂给不同的分支，每个分支采用不同大小的卷积核来提取出不同感受野的特征，最后将各个分支的结果连接起来作为输出。后来也出现了融合ResNet和Inception的Inception-ResNet网络。              </p><center><img src="/Handson-ML/12Inception_Module.png" alt=""></center>      <p>虽然混合感受野的特征确实有助于提高特征提取能力，但不得不承认，Inception模块需要付出不小的计算和存储代价！<br>然而有意思的是，因为深度可分离卷积的特性，在深度可分离卷积上采取混合感受野的代价会小的多——<br><img src="/imgs/MobileNet/depthwise%20conv.jpg" alt="">      </p><p>这是一个典型的深度可分离卷积，它将标准卷积拆解成一个<code>num_groups==num_channels</code>的DW卷积和一个1x1卷积级联的形式，前者对输入进行特征提取，后者对提取的特征进行组合从而映射到目标空间上去。（具体过程可以参考《<a href="/2018/08/05/MobileNets_v1/#深度向卷积分解（Depthwise-Separable-Convolution）">MobileNets v1模型解析 - 深度向卷积分解 | Hey~YaHei!</a>》，此处就不再赘述）<br>深度可分离卷积的参数和计算量主要集中在负责组合特征的1x1卷积上（比如MobileNetv1就有<strong>94.86%</strong>的计算量和<strong>74.59%</strong>的参数量集中在1x1卷积上，而3x3的DW卷积仅仅贡献了<strong>3.06%</strong>的计算量和<strong>1.06%</strong>的参数量），<strong>混合感受野只需要也只能作用在DW卷积上，所以总的来说，即使将深度可分离卷积改造成混合感受野的形式，也不会带来明显的额外计算、存储代价。</strong>       </p><h3 id="MixConv"><a href="#MixConv" class="headerlink" title="MixConv"></a>MixConv</h3><p>MixConv并不像inception对整个输入特征图分别采用不同大小的核进行卷积，而是延续DW卷积本身的特性，将特征图按通道分割为多个组，每组采取不同大小的核进行卷积，最后再连接起来。<br>比如一个<code>3x3</code>的DW卷积，转换成一个<code>3x3, 5x5, 7x7</code>组合的混合感受野DW卷积，<br><em>（请原谅我这老土的配色）</em><br><img src="/imgs/MixConv/MixConv.png" alt="">    </p><ul><li>如何对输入特征图进行分组？<br>  作者尝试了均匀划分和指数递增两种形式，<strong>均匀划分</strong>对不同大小的卷积核分配相同数量的通道，<strong>指数递增</strong>则考虑核大小对计算量的影响，将卷积按核大小排序（一般是采用3x3、5x5、7x7、9x9……），最小的核分得$n$通道，稍大的核分得$n \times 2^{-i}$通道。作者通过实验表明，指数递增的方案有更高效的特征提取效果          </li><li>空洞卷积尽管能用更小的代价获得大的感受野，但因为分辨率的减小，其效果不如直接的大核卷积     </li><li>因为深度可分离卷积将滤波和组合两个阶段拆开，滤波阶段输出的特征图虽然会按照感受野的大小有规律的分隔开，但组合阶段又会将他们的结果混合起来，不用担心不同感受野产生的特征图信息不能流通     </li></ul><center><img src="/imgs/MixConv/GroupAndDilate.png" alt=""></center>       <h3 id="MixNet"><a href="#MixNet" class="headerlink" title="MixNet"></a>MixNet</h3><p>最后作者也跟EfficientNet一样，将这一新设计与NAS结合起来，搜索得到一个比较高效率的特征提取器MixNet，该模型甚至用上了9x9乃至11x11的大核卷积——       </p><center><img src="/imgs/MixConv/MixNet.png" alt=""></center><br><center><img src="/imgs/MixConv/MixNet_result.png" alt=""></center>    ]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>EfficientNet：模型设计新范式</title>
      <link href="/2019/10/30/EfficientNet/"/>
      <url>/2019/10/30/EfficientNet/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1354451040&auto=0&height=32"></iframe><p>唔~2019年的图像特征提取方面似乎有了可喜的发展，大家不再盲目地堆大模型，而是寻找更加高效的特征提取结构，在显著减小参数量和计算量的同时模型的表现还能得到提升。同时我们可以看到深度可分离卷积及其变种几乎已经成为主流——<br><img src="/imgs/EfficientNet/imagenet_sota.png" alt="">       </p><p>接下来我将分三次分别介绍今年Google Brain的几篇思路清奇的论文，首先是《<a href="https://arxiv.org/pdf/1905.11946.pdf" target="_blank" rel="noopener">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks(2019ICML)</a>》。<br>该论文的主要贡献在于<strong>简化模型缩放流程，统合了宽度、深度、分辨率三个维度的缩放，建立起一个简单有效的联系</strong>。</p><h3 id="模型缩放"><a href="#模型缩放" class="headerlink" title="模型缩放"></a>模型缩放</h3><p>在设计好模型结构的情况下，我们通常可以通过对模型的缩放，以增大模型的大小来换取模型表现的提升，或以牺牲模型表现为代价换取模型大小的减小。最常用的缩放维度有三个——深度、宽度和输入分辨率   </p><ul><li><strong>深度</strong>      <ul><li>更深的网络往往能提取到更佳丰富和复杂的特征，并且有更强的泛化能力</li><li>但深层网络存在训练难的问题，尽管shortcut、BN等技术缓解了这一问题，但随着网络的加深边际收益依旧会明显变小</li><li>比如<a href="/2018/05/02/经典的CNN分类架构/#ResNet">ResNet18、ResNet34、ResNet50、ResNet101、ResNet152</a>，其数字代表的是模型中卷积层和全连接层数量（不考虑shortcut上用于downsample的卷积）；<a href="/2018/05/02/经典的CNN分类架构/#VGG-Nets">VGG11、VGG13、VGG16、VGG19</a>，其数字代表的是模型中卷积层和全连接层数量——这是一种典型的深度缩放    </li></ul></li><li><strong>宽度</strong>       <ul><li>更宽的网络往往能提取更细粒度的特征，并且也更容易训练（同时足够宽的网络能够容许Dropout和BN技术兼容使用，比如WideResNet）</li><li>比如<a href="/2018/08/05/MobileNets_v1/">MobileNet1.0、MobileNet0.75、MobileNet0.5、MobileNet0.25</a>，其数字代表的是模型中卷积通道数量的缩放因子，除此之外还有常见的WideResNet，这是一种典型的宽度缩放    </li></ul></li><li><strong>输入分辨率</strong>缩放更为常见，通过增加输入图片的尺寸，可以保留更多信息，从而提升模型的表现      </li><li>除此之外，<strong>下采样策略</strong>也是一种缩放维度，提前下采样可以缩小模型，反之则会增大模型      <ul><li>下采样策略可以看作更细粒度的输入分辨率缩放，它控制的是中间特征图的分辨率而非简单的模型输入分辨率</li><li>比如<a href="/2019/07/24/MobileNet-Family/#FD-MobileNet">FD-MobileNet</a>采用快速下采样策略并提升模型宽度，在保持模型大小几乎不变的情况下提高模型的特征提取能力</li></ul></li></ul><p>论文作者做了简单的实验，表明单一放大某一因素，其边际收益是不断减小的——<br>（其中，w, d, r分别表示宽度(Width)、深度(Depth)、分辨率(Resolution)的缩放因子）<br><img src="/imgs/EfficientNet/scales.png" alt="">    </p><h3 id="混合缩放"><a href="#混合缩放" class="headerlink" title="混合缩放"></a>混合缩放</h3><p>此前已经有其他论文（如《<a href="https://arxiv.org/pdf/1710.05941" target="_blank" rel="noopener">Searching for Activation Functions(2017)</a>》和《<a href="https://wvvw.aaai.org/ojs/index.php/AAAI/article/view/4405/4283" target="_blank" rel="noopener">Regularized Evolution for Image Classifier Architecture Search(2019AAAI)</a>》）讨论过多个缩放维度混合的模型缩放策略，而本文则是为深度、宽度、输入分辨率的缩放建立一个简单但有效的联系，从而简化缩放因子的选取过程。<br>其实思路非常简单，首先定义这三个缩放因子为$d$, $w$, $r$，并且让他们同步增长<br>$$<br>d = \alpha^\phi \\<br>w = \beta^\phi \\<br>r = \gamma^\phi<br>$$<br>那么此时模型的计算量会增长为大约原来的$(\alpha \cdot \beta^2 \cdot \gamma^2) ^ \phi$倍；<br>为了简单起见，作者约束了$\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$，计算量随$\phi$增长为原来的$2^\phi$倍，这样一来只需要找到合适的$\alpha, \beta, \gamma$，然后调整$\phi$的值就可以实现简单的混合缩放。     </p><h3 id="EfficientNet模型设计"><a href="#EfficientNet模型设计" class="headerlink" title="EfficientNet模型设计"></a>EfficientNet模型设计</h3><p>在上述基础上，作者提出了一种新的模型设计范式，并得到了EfficientNet系列模型——      </p><ol><li>用NAS搜索出符合预期大小的一个小模型，这里作者设定了计算量目标为400MFLOPS，于是搜出了一个跟MNasNet类似的小模型B0<br> <center><img src="/imgs/EfficientNet/EfficientNetB0.png" alt=""></center><br> 这里的Operator沿用了MNasNet的命名方式，<code>MBConv1</code>指的是MobileNetv1的原版深度可分离卷积，<code>MBConv6</code>指的是MobileNetv2的反残差结构，不过作者在论文中指出这里使用的是带SE模块的反残差结构</li><li>接下来通过网格搜索，找出满足$\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$的最优超参$\alpha, \beta, \gamma$，进而得到B1<br> 如作者搜索出来的结果为$\alpha=1.2, \beta=1.1, \gamma=1.15$，此时B1计算量约为800MFLOPS     </li><li>按照混合缩放的公式，逐步增加$\phi$值，依次得到各种规模下的B2-B7      </li></ol><p><strong><a href="https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_builder.py#L36" target="_blank" rel="noopener">Tensorflow-EfficientNet</a>的预设值跟论文描述似乎有些细微差别：</strong>    </p><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">w</th><th style="text-align:center">d</th><th style="text-align:center">r</th><th style="text-align:center">dropout rate</th></tr></thead><tbody><tr><td style="text-align:center">B0</td><td style="text-align:center">1.0</td><td style="text-align:center">1.0</td><td style="text-align:center">224</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">B1</td><td style="text-align:center">1.0</td><td style="text-align:center">1.1</td><td style="text-align:center">240</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">B2</td><td style="text-align:center">1.1</td><td style="text-align:center">1.2</td><td style="text-align:center">260</td><td style="text-align:center">0.3</td></tr><tr><td style="text-align:center">B3</td><td style="text-align:center">1.2</td><td style="text-align:center">1.4</td><td style="text-align:center">300</td><td style="text-align:center">0.3</td></tr><tr><td style="text-align:center">B4</td><td style="text-align:center">1.4</td><td style="text-align:center">1.8</td><td style="text-align:center">380</td><td style="text-align:center">0.4</td></tr><tr><td style="text-align:center">B5</td><td style="text-align:center">1.6</td><td style="text-align:center">2.2</td><td style="text-align:center">456</td><td style="text-align:center">0.4</td></tr><tr><td style="text-align:center">B6</td><td style="text-align:center">1.8</td><td style="text-align:center">2.6</td><td style="text-align:center">528</td><td style="text-align:center">0.5</td></tr><tr><td style="text-align:center">B7</td><td style="text-align:center">2.0</td><td style="text-align:center">3.1</td><td style="text-align:center">600</td><td style="text-align:center">0.5</td></tr></tbody></table><p>由此设计出来的模型有着相当惊人的特征提取效率，而且小模型的搜索空间比较小，更容易搜索出最优模型，搭配合理的模型缩放，放大后的模型甚至能比直接搜索的大模型更加有效——      </p><p><center><img src="/imgs/EfficientNet/compare_graph.png" alt=""></center><br><img src="/imgs/EfficientNet/compare_table.png" alt="">    </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>CondConv：按需定制的卷积权重</title>
      <link href="/2019/10/30/CondConv/"/>
      <url>/2019/10/30/CondConv/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28178158&auto=0&height=32"></iframe>    <p>最近正巧在看条件计算的东西，发现今年Google Brain发了一篇思路清奇的论文《<a href="https://arxiv.org/pdf/1904.04971v2" target="_blank" rel="noopener">CondConv: Conditionally Parameterized Convolutions for Efficient Inference(2019NeurIPS)</a>》，这思路简直让人拍案叫绝，只可惜这种模型需要重新定制卷积算子才能有效发挥它的作用，如果没有工业界的推动想必短期内很难产生实用价值吧。<br>先来说说论文的主要贡献：<strong>条件计算、集成技术、注意力机制三者间极为巧妙的结合！</strong>         </p><h3 id="条件计算"><a href="#条件计算" class="headerlink" title="条件计算"></a>条件计算</h3><p>如果是模型压缩是深度学习的一个边缘领域，那么条件计算一定是模型压缩里的边缘方向。如果你听过模型压缩，那你一定知道裁剪和量化，可能你还会知道一些紧凑网络的设计和知识蒸馏，但我打赌你十有八九没听过条件计算。      </p><blockquote><p>有一天我躺在床上睡觉，突然灵光一现——直观上讲，既然不同的layer乃至不同filter能提取出不同的特征，而对于不同的输入，我们所重视的特征必定也是不同的，是不是能设计出某个评估/预测模块从而智能地挑选合适layer、filter来计算而放弃无关layer、filter的计算呢？或者说当浅层特征足以完成推断，我们能不能提前从浅层特征图跳出而放弃后续深层特征的提取步骤呢？再或者，对于连续的视频流，有没有可能在浅层位置先预估出本帧图像的质量，从而判断是否放弃本帧图像的推断呢（比如有些视频流情况下我们不需要每一帧都做出准确推断的时候，能否提前中断来获取新的可能质量更好的一帧图像）？      </p><p>这种思路似乎有些诡异，但又有些合理。正当我为自己“天才般”的想法沾沾自喜的时候，发现其实很早以前就有人研究过这类问题（好吧QAQ）——<br>第一种思路通常被称为条件计算（Conditional Computation），属于模型压缩里比较冷门的一个小方向；<br>第二种思路被称为提前终止（Early Stop），跟条件计算稍微有些类似；<br>第三种思路好像没看到相关文献，不过想想似乎有些麻烦，比如我该怎么评估一份特征图质量的“好坏”？？      </p></blockquote><p>简单来说，<strong>条件计算是构建一种动态的网络结构，每次推断的时候先由决策网络（或模块）根据模型输入（甚至每一层的输入）推断出所要使用网络部件，然后利用原始网络的一个子集完成实际的推断过程。</strong>       </p><p>举几个典型的例子：      </p><ol><li>《<a href="https://arxiv.org/pdf/1603.01250" target="_blank" rel="noopener">Decision Forests, Convolutional Networks and the Models in-Between(2016)</a>》使用决策树进行分支判断，从而使用有限的网络结构完成一次次推断任务      <center><img src="/imgs/CondConv/DecisionTree.png" alt=""></center></li><li>《<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.pdf" target="_blank" rel="noopener">BlockDrop: Dynamic Inference Paths in Residual Networks(2018CVPR)</a>》设计一个独立的决策网络，根据模型输入的图片来挑选有限的block完成ResNet的实际推断         <center><img src="/imgs/CondConv/BlockDrop.png" alt=""></center>      </li><li>《<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xin_Wang_SkipNet_Learning_Dynamic_ECCV_2018_paper.pdf" target="_blank" rel="noopener">SkipNet: Learning Dynamic Routing in Convolutional Networks(2018ECCV)</a>》为每个block添加一个额外的评估模块，根据block的输入来判断是否跳过残差支路的计算过程      <center><img src="/imgs/CondConv/SkipNet.png" alt=""></center></li><li>《<a href="https://arxiv.org/pdf/1701.05221" target="_blank" rel="noopener">Parsimonious Inference on Convolutional Neural Networks: Learning and applying on-line kernel activation rules(2017)</a>》为每个卷积层设计一个LKAM模块，根据卷积层的输入特征图判断需要启用哪几个Filter      <center><img src="/imgs/CondConv/LKAM.png" alt=""></center>    </li></ol><p>看着思路好像挺简单，但是不得不承认，训练这样一个模型实在不容易——我做过几个简单的实验，决策网络或决策模块往往很容易倾向于直接“杀死”某个block、layer、filter让它“永世不得翻身”于是决策网络相当于沦为传统裁剪的一个辅助任务。。。总之，经常训出来的决策网络就不大聪明的亚子。</p><h3 id="集成技术"><a href="#集成技术" class="headerlink" title="集成技术"></a>集成技术</h3><ol><li>模型集成<br> 最简单的模型集成是针对相同的任务、相同的输入设计几个不同的网络（甚至有的时候直接取训练过程中的若干checkpoint，这种集成方法称为快照集成，使用简单，通常也会结合模拟退火来确保不同的“专家”能够具备差异化的知识），最后再推断阶段分别将同一个输入投喂给所有“专家”，然后组合他们的输出来得出结论——比如最简单的组合方式可以采取举手表决。      <center><img src="/imgs/CondConv/ModelEnsemble.png" alt=""></center>     </li><li>分支集成<br> 模型集成的规模往往有些庞大，退而求其次我们可以共享一部分浅层特征，然后产生若干分支，最后融合各分支提取的特征达到集成的目的     <center><img src="/imgs/CondConv/BranchEnsemble.png" alt=""></center></li><li>带条件计算的分支集成<br> 有的时候可能有的分支的“小专家”对一些样本不太擅长，我们可以选择不听取他们的意见，论文《<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.pdf" target="_blank" rel="noopener">HydraNets: Specialized Dynamic Architectures for Efficient Inference(2018CVPR)</a>》就把条件计算跟分支集成结合了起来     <center><img src="/imgs/CondConv/HydraNets.png" alt=""></center>      </li></ol><p>无论哪种集成技术，都免不了模型规模（包括参数量和计算量）会产生成倍的增长，往往需要付出很大的代价才能取得些微的提升。在我看来，集成技术并没有太大的实用价值，只不过比赛通常不重视模型规模的影响，所以大家一味追求高准确率而堆叠模型罢了。不过，集成也非一无是处，既然有一个甚至很多个聪明的专家，尽管他很庞大，但却是一位不错的老师，利用知识蒸馏技术将集成模型的知识蒸馏到常规的模型上或许也是一个不错的思路。     </p><h3 id="CondConv"><a href="#CondConv" class="headerlink" title="CondConv"></a>CondConv</h3><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>CondConv的核心思想是带条件计算的分支集成的一种巧妙变换，首先它采用更细粒度的集成方式，每一个卷积层都拥有多套权重，卷积层的输入分别经过不同的权重卷积之后组合输出：    </p><center><img src="/imgs/CondConv/MixedExpertsConv.png" alt=""></center>      <p>但这计算量依旧很大，既然输入相同，卷积又是一种线性计算，如果COMBINE也是一个线性计算（比如加权求和）……那有趣的事情就发生了——     </p><center><img src="/imgs/CondConv/CondConv.png" alt=""></center>     <p>作者将多套权重加权组合之后，只做一次卷积就能完成相当的效果！<br>简单来说，<strong>CondConv在卷积层设置多套卷积核，在推断时对卷积核施加SE模块，根据卷积层的输入决定各套卷积核的权重，最终加权求和得到一个为该输入量身定制的一套卷积核，最后执行一次卷积即可。</strong><br><em>事实上作者只使用了一层全连接，而不是标准的SE模块~</em>       </p><ul><li>从注意力机制的角度上看，这里将注意力机制应用到了卷积权重上    </li><li>从条件计算的角度上看，这里利用注意力机制为多套卷积核产生了对应权重，最终加权求和，是一种soft gate的方案    </li><li>从集成技术的角度上看，卷积层集成了若干套卷积核的知识，而且只需要付出少量的额外计算代价，当然内存依旧会爆炸式增长，但通常相对于计算力来说，内存是相当易得的     </li></ul><p>这不得不说是一种令人拍案叫绝的巧妙思路！！！！      </p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>作者用MobileNetv1进行实验，对0.25x、0.5x、0.75x、1.0x分别使用1、2、4、8、16、32个experts——     </p><center><img src="/imgs/CondConv/TestOnMobileNet.png" alt=""></center>        <h4 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h4><p>虽然好用，但这模型训起来稍微有些麻烦（麻烦指的是训练过程会比较慢，超参应该还是比较好调的），有两种训练方式——    </p><ol><li>使用<code>batch_size=1</code>的训练，每次前传都先组合权重再进行卷积    </li><li>使用<code>batch_size&gt;1</code>的训练，每次前传都直接分别卷积，最后在做组合    </li></ol><p>两种计算过程基本是等效的，但考虑到深度学习框架大多对大batch的训练过程有比较充分的优化，所以作者建议当experts数量不多余4时，采用方法2，超过4之后采用方法1.不过值得一提的是，BN层在batch_size比较小时是不利的，此时最好改用其他Normalization层。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Winograd数学原理【卡住了&gt;_&lt;】</title>
      <link href="/2019/09/18/winograd_theorem/"/>
      <url>/2019/09/18/winograd_theorem/</url>
      
        <content type="html"><![CDATA[<p>早前《<a href="/2019/08/21/winograd_convolution/">Winograd卷积原理 | Hey~YaHei!</a>)》一文已经介绍过Winograd的卷积原理，但有些细节似乎尚不明确——    </p><ul><li>Winograd为何能够奏效？</li><li>它背后的数学原理是什么？</li><li>变换矩阵如何得到？   </li></ul><p>本文将借鉴《<a href="https://zhuanlan.zhihu.com/p/82482351" target="_blank" rel="noopener">源于《孙子算经》的Cudnn | 知乎, Silicon Valley</a>》和《<a href="https://github.com/andravin/wincnn/blob/master/2464-supp.pdf" target="_blank" rel="noopener">wincnn/2464-supp.pdf | github, andravin</a>》，深入剖析Winograd背后的数学原理。    </p><h3 id="中国同余定理"><a href="#中国同余定理" class="headerlink" title="中国同余定理"></a>中国同余定理</h3><p>Winograd的基础来源于《孙子算经》中的中国同余定理（Chinese Remainder Theorem），同余定理是数论的重要内容（没学过数论的我，要读懂这个数学原理真的有些头疼QAQ）。</p><h4 id="《孙子算经》"><a href="#《孙子算经》" class="headerlink" title="《孙子算经》"></a>《孙子算经》</h4><p>《孙子算经》有一章：    </p><blockquote><p>今有物不知其数，三三数之剩二，五五数之剩三，七七数之剩二，问物几何？<br>答曰：二十三。<br>术曰：三三数之剩二，置一百四十；五五数之剩三，置六十三，七七数之剩二，置三十，并之。得二百三十三，以二百一十减之，即得。凡三三数之剩一，则置七十；五五数之剩一，则置二十一；七七数之剩一，则置十五；一百六以上以一百五减之即得。</p></blockquote><p>改用数学语言描述一下：    </p><blockquote><p>问题：已知同余方程组<br>$$\begin{cases} x \equiv 2(mod\ 3) \\ x \equiv 3(mod\ 5) \\ x \equiv 2(mod\ 7) \end{cases}$$<br>求x？</p></blockquote><p>其中，$x \equiv a(mod\ m)$称为<strong>x与a模m同余</strong>，当$a &lt; m$时，可以认为x模m余a；       </p><blockquote><p>解答：<br>$m_1 = 2 \times 70 = 140$<br>$m_2 = 3 \times 21 = 63$<br>$m_3 = 2 \times 15 = 30$<br>$x = (m_1 + m_2 + m_3)\ mod\ 105 = 23$</p></blockquote><p><em>看到这，没读过数论的我感到十分震惊——为啥拿着余数乘几个数，最后加起来模一模怎么就可以推出被模的数呢？？</em>      </p><h4 id="定理内容"><a href="#定理内容" class="headerlink" title="定理内容"></a>定理内容</h4><p>这里不加证明地直接引出中国同余定理的内容：    </p><blockquote><p>假设整数$m_1, m_2, …, m_n$两两互素，则对于任意的整数$a_1, a_2, …, a_n$，同余方程组<br>$$\begin{cases} x \equiv a_1(mod\ m_1) \\ x \equiv a_2(mod\ m_2) \\ … \\ x \equiv a_n(mod\ m_n) \end{cases}$$<br>都存在整数解，且若$X, Y$都满足方程组，则必有$x \equiv Y (mod\ N)$，其中$N=\prod^n_{i=1}m_i$<br>$$x \equiv \sum^n_{i=1} a_i \times \frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} (mod\ N)$$<br>其中$[(\frac{N}{m_i})^{-1}]_{m_i}$是一个整数，使得$\frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} \equiv 1(mod\ m_i)$   </p></blockquote><p>对《孙子算经》中的例子来说，就是<br>$$<br>m_1 = 3, m_2 = 5, m_3 = 7 \\<br>a_1 = 2, a_2 = 3, a_3 = 2 \\<br>N = m_1 \times m_2 \times m_3 = 105 \\<br>\frac{N}{m_1} = 35, \frac{N}{m_2} = 21, \frac{N}{m_3} = 15 \\<br>[(\frac{N}{m_1})^{-1}]_{m_1} = 2, [(\frac{N}{m_2})^{-1}]_{m_2} = 1, [(\frac{N}{m_3})^{-1}]_{m_3} = 1<br>$$    </p><p>原始的定理只针对整数，事实上，该定理同样适用于多项式。<br>接下来我们关注一下定理里的两个细节：    </p><h5 id="如何证明互素？"><a href="#如何证明互素？" class="headerlink" title="如何证明互素？"></a>如何证明互素？</h5><blockquote><p>互素，又称互质。若两个整数的公约数只有1，那么称它们为互素（质）整数。<br>当然这个定义可以引申到多项式上，比如$x$、$x+1$、$x-1$、$x^2+1$就是典型的互素多项式。       </p></blockquote><p>证明互素的核心是证明它们的公约数只有1，问题可以转化成最大公约数为1，说起最大公约数的计算，那就不得不说欧几里得算法啦！   </p><blockquote><p>欧几里得算法俗称辗转相除法，用大数除以小数得到余数（这里商是无意义的），然后迭代计算，用上一轮的除数、余数分别作为新一轮的被除数和除数，直到最后能够整除，最后一轮的除数即为原始的两个数的最大公约数。<br>以1997和615为例：<br>$1997\ mod\ 615 = 152$<br>$615\ mod\ 152 = 7$<br>$152\ mod\ 7 = 5$<br>$7\ mod\ 5 = 2$<br>$5\ mod\ 2 = 1$<br>$2\ mod\ 1 = 0$<br>故1997与615的最大公约数为1，记为<strong>gcd(1997, 615) = 1</strong></p></blockquote><p>至于，多项式之间如何做除法，可以直接用小学的长除法，除到最后余数的最高次幂小于除数的最高次幂即可。     </p><blockquote><p>以$x^4 + x^3 - x^2 + 3x + 2$除以$x^3 + 3x^2 + 3x + 2$为例：<br><em>（latex没法写长除公式，这里我只能用分数的形式来表述）</em><br>$$\begin{align} \frac{x^4 + x^3 - x^2 + 3x + 2}{x^3 + 3x^2 + 3x + 2} &amp;= \frac{-2x^3 - 4x^2 + x + 2}{x^3 + 3x^2 + 3x + 2} + (x) \\ &amp;= \frac{2x^2 + 7x + 6}{x^3 + 3x^2 + 3x + 2} + (x-2) \end{align}$$<br>那么这里$x^4 + x^3 - x^2 + 3x + 2$除以$x^3 + 3x^2 + 3x + 2$，商为$(x-2)$，余数为$(2x^2 + 7x + 6)$      </p></blockquote><h5 id="如何求解系数-frac-N-m-i-1-m-i-？"><a href="#如何求解系数-frac-N-m-i-1-m-i-？" class="headerlink" title="如何求解系数$[(\frac{N}{m_i})^{-1}]_{m_i}$？"></a>如何求解系数$[(\frac{N}{m_i})^{-1}]_{m_i}$？</h5><p>根据定理，系数$[(\frac{N}{m_i})^{-1}]_{m_i}$满足$\frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} \equiv 1(mod\ m_i)$，我们假设<br>$$\left( \frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} \right) \div m_i = y \cdots 1$$<br>那么有<br>$$\frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} - m_i y = 1$$<br>同时注意到，$\frac{N}{m_i}$与$m_i$必定互素，则有<br>$$\frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} - m_i y = 1 = gcd(\frac{N}{m_i}, m_i)$$<br>构成了一个形如$ax + by = gcd(a, b)$的贝祖公式，按照贝祖定理，可用扩展欧几里得算法求解出$x$和$y$，也即$[(\frac{N}{m_i})^{-1}]_{m_i}$和$y$（当然y没啥意义，我们要的是那个系数！）。    </p><blockquote><p>扩展欧几里得算法除了能和欧几里得算法一样求得最大公约数之外，还能得到模逆元；<br>其实非常简单，与递归实现的欧几里得算法相比，无非是每次递归都逐步恢复出逆元x跟y<br>以python实现为例：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># def euclid(a, b):</span><span class="token comment" spellcheck="true">#     while b != 0:</span><span class="token comment" spellcheck="true">#         a, b = b, a % b</span><span class="token comment" spellcheck="true">#     return a</span><span class="token keyword">def</span> <span class="token function">euclid</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> b <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> a    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> euclid<span class="token punctuation">(</span>b<span class="token punctuation">,</span> a <span class="token operator">%</span> b<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">ext_euclid</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> b <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> a    <span class="token keyword">else</span><span class="token punctuation">:</span>        x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> q <span class="token operator">=</span> ext_euclid<span class="token punctuation">(</span>b<span class="token punctuation">,</span> a <span class="token operator">%</span> b<span class="token punctuation">)</span>        x<span class="token punctuation">,</span> y <span class="token operator">=</span> y<span class="token punctuation">,</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> <span class="token punctuation">(</span>a <span class="token operator">//</span> b<span class="token punctuation">)</span> <span class="token operator">*</span> y<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> q</code></pre></blockquote><h3 id="Winograd算法"><a href="#Winograd算法" class="headerlink" title="Winograd算法"></a>Winograd算法</h3><p>前边中国同余定理已经解释明白了，接下来我们试着理解一下Winograd的数学原理（以$F(2,3)$为例）。<br>假设两个离散序列，<br>$$<br>\begin{align}<br>d[n] &amp;= [d_0, d_1] \\<br>g[n] &amp;= [g_0, g_1, g_2]<br>\end{align}<br>$$<br>经过Z变换变成多项式形式（为了方便起见，这里写作$x$而非$Z$），<br>$$<br>\begin{align}<br>d(x) &amp;= d_1x + d_0 \\<br>g(x) &amp;= g_2x^2 + g_1x + g_0<br>\end{align}<br>$$<br>此时，$d[n] * g[n] = Z^{-1}(d(x) \cdot g(x))$（时域卷积 =&gt; Z域的乘法），记<br>$$<br>\begin{align}<br>y &amp;= d(x) \cdot g(x) \\<br>  &amp;= (d_1x + d_0)(g_2x^2 + g_1x + g_0)<br>\end{align}<br>$$<br>于是两个序列的卷积变成了多项式的乘法（当然，乘积也是个多项式），我们继续往多项式版本的中国同余定理上套。<br>选取K个互素的一次多项式$m_i(x)$，使得$m(x) = \prod^{K-1}_{i=0}{m_i(x)}$的阶数等于$d(x) \cdot g(x)$，那就可以构造出同余方程<br>$$y(x) - R_{m(x)}[y(x)] \equiv g(x)d(x)\ (mod\ m(x))$$<br>其中，$R_{m(x)}[y(x)]$是$y(x)$除以$m(x)$的余数。<br>在这个例子中，至少需要3个互素一次多项式，不妨取<br>$$<br>\begin{align}<br>m_0(x) &amp;= x \\<br>m_1(x) &amp;= x + 1 \\<br>m_2(x) &amp;= x - 1<br>\end{align}<br>$$<br>那么有$m(x) = \prod{m_i(x)} = x(x+1)(x-1) = x^3 - x$，<br>除此之外，我们再记一个特殊的$m_3(x) = x - \infty$，没有具体的意义，该多项式是为了凑足m的阶数，使得$m’(x) = m_3(x) m(x) = x(x+1)(x-1)(x-\infty)$阶数高于$d(x) \cdot g(x)$，从而可以构造出同余方程<br>$$y(x) \equiv g(x)d(x)\ (mod\ m’(x))$$<br>记$M_i(x) = \frac{m(x)}{m_i(x)}$，则有<br>$$<br>\begin{align}<br>M_0(x) &amp;= -1 + x^2 \\<br>M_1(x) &amp;= -x + x^2 \\<br>M_2(x) &amp;=  x + x^2 \\<br>M_3(x) &amp;= -x + x^3<br>\end{align}<br>$$<br>依次抽取$x^0, x^1, x^2, x^3$的系数组成行向量并堆叠构造出变换矩阵<br>$$<br>B =<br>\begin{bmatrix}<br>-1 &amp;  0 &amp;  0 &amp;  0 \\<br> 0 &amp; -1 &amp;  1 &amp; -1 \\<br> 1 &amp;  1 &amp;  1 &amp;  0 \\<br> 0 &amp;  0 &amp;  0 &amp;  1<br>\end{bmatrix}<br>$$<br>注意<br>$$y(x)\ mod\ M_3(x) = y(x)\ mod\ m(x)$$    </p><p>接下来考虑一个重要的同余方程性质：   </p><blockquote><p>若$a \equiv b(mod\ m_i), i \in \{1, 2, …, n\}$<br>则$a \equiv b(mod\ [m_1, m_2, …, m_n])$，<br>其中$[m_1, m_2, …, m_n]$表示$m_1, m_2, …, m_n$的最小公倍数    </p></blockquote><p>利用该性质（因为$m_1, m_2, …, m_n$两两互素，所以$m’(x)$就是它们的最小公倍数），又根据同余方程的乘法规则    </p><blockquote><p>$$a \equiv b(mod\ m), c \equiv d(mod\ m) =&gt; ac \equiv bd(mod\ m)$$  </p></blockquote><p>原同余方程$y(x) \equiv g(x)d(x)\ (mod\ m’(x))$可以转化为同余方程组<br>$$<br>\begin{cases}<br>y(x) \equiv g(x)d(x)\ (mod\ m_0(x)) \\<br>y(x) \equiv g(x)d(x)\ (mod\ m_1(x)) \\<br>y(x) \equiv g(x)d(x)\ (mod\ m_2(x)) \\<br>y(x) \equiv g(x)d(x)\ (mod\ m_3(x))<br>\end{cases}<br>$$<br>的求解。原方程组又可以展开成<br>$$<br>\begin{cases}<br>y(x) \equiv [g(x)\ mod\ m_0(x)] \cdot [d(x)\ mod\ m_0(x)]\ (mod\ m_0(x)) \\<br>y(x) \equiv [g(x)\ mod\ m_1(x)] \cdot [d(x)\ mod\ m_1(x)]\ (mod\ m_1(x)) \\<br>y(x) \equiv [g(x)\ mod\ m_2(x)] \cdot [d(x)\ mod\ m_2(x)]\ (mod\ m_2(x)) \\<br>y(x) \equiv [g(x)\ mod\ m_3(x)] \cdot [d(x)\ mod\ m_3(x)]\ (mod\ m_3(x)) \\<br>\end{cases}<br>$$<br>噢？好像变成了g(x)和d(x)，橘势似乎明朗了起来=。=<br>$$<br>\begin{align}<br>&amp;\begin{cases}<br>g^{(0)}(x) = g(x)\ mod\ m_0(x) = g_0 \\<br>g^{(1)}(x) = g(x)\ mod\ m_1(x) = g_0 - g_1 + g_2 \\<br>g^{(2)}(x) = g(x)\ mod\ m_2(x) = g_0 + g_1 + g_2 \\<br>g^{(3)}(x) = g(x)\ mod\ m_3(x) = g_2<br>\end{cases} \\<br>&amp;\begin{cases}<br>d^{(0)}(x) = d(x)\ mod\ m_0(x) = d_0 \\<br>d^{(1)}(x) = d(x)\ mod\ m_1(x) = d_0 - d_1 \\<br>d^{(2)}(x) = d(x)\ mod\ m_2(x) = d_0 + d_1 \\<br>d^{(3)}(x) = d(x)\ mod\ m_3(x) = d_1 \\<br>\end{cases}<br>\end{align}<br>$$<br>依次抽取$d_0, d_1$的系数组成列向量并堆叠构造出变换矩阵<br>$$<br>A =<br>\begin{bmatrix}<br> 1 &amp;  0 \\<br> 1 &amp; -1 \\<br> 1 &amp;  1 \\<br> 0 &amp;  1<br>\end{bmatrix}<br>$$<br>那么回过头看看中国同余定理，<br>$$x \equiv \sum^n_{i=1} a_i \times \frac{N}{m_i} \times [(\frac{N}{m_i})^{-1}]_{m_i} (mod\ N)$$<br>在这个例子里表示为<br>$$y(x) \equiv \sum^3_{i=0} g^{(i)}(x)d^{(i)}(x) \times M_i(x) \times N_i(x)\ (mod\ m’(x))$$<br>显然我们还差一个$N_i(x)$，根据贝祖公式$m_i(x)n_i(x) + M_i(x)N_i(x) = 1$用扩展欧几里得算法依次求得：<br><strong><em>注意：此处《<a href="https://github.com/andravin/wincnn/blob/master/2464-supp.pdf" target="_blank" rel="noopener">wincnn/2464-supp.pdf | github, andravin</a>》给出的贝祖公式有误！！</em></strong><br>$$<br>\begin{cases}<br>N_0(x) = -1 \\<br>N_1(x) = \frac{1}{2} \\<br>N_2(x) = \frac{1}{2} \\<br>N_3(x) = 1<br>\end{cases}<br>$$<br>而<br>$$<br>\begin{cases}<br>N_0(x)g^{(0)}(x) = -g_0 \\<br>N_1(x)g^{(1)}(x) = \frac{1}{2}g_0 - \frac{1}{2}g_1 + \frac{1}{2}g_2 \\<br>N_2(x)g^{(2)}(x) = \frac{1}{2}g_0 + \frac{1}{2}g_1 + \frac{1}{2}g_2 \\<br>N_3(x)g^{(3)}(x) = g_2<br>\end{cases}<br>$$<br>抽取$g_0, g_1, g_2$的系数组成列向量并堆叠构造出矩阵<br>$$<br>G =<br>\begin{bmatrix}<br>-1 &amp; 0 &amp; 0 \\<br>\frac{1}{2} &amp; -\frac{1}{2} &amp; \frac{1}{2} \\<br>\frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{2} \\<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$</p><blockquote><p>我们代回去验证一下，<br>$$<br>\begin{cases}<br>g^{(0)}(x)d^{(0)}(x) \times M_0(x) \times N_0(x) = -g_0d_0x^2 + g_0d_0 \\<br>g^{(1)}(x)d^{(1)}(x) \times M_1(x) \times N_1(x) = \frac{1}{2}(g_0-g_1+g_2)(d_0-d_1)x^2 - \frac{1}{2}(g_0-g_1+g_2)(d_0-d_1)x \\<br>g^{(2)}(x)d^{(2)}(x) \times M_2(x) \times N_2(x) = \frac{1}{2}(g_0+g_1+g_2)(d_0+d_1)x^2 + \frac{1}{2}(g_0+g_1+g_2)(d_0+d_1)x \\<br>g^{(3)}(x)d^{(3)}(x) \times M_3(x) \times N_3(x) = g_2d_1x^3 - g_2d_1x<br>\end{cases}<br>$$<br>代入求得<br>$$y(x) \equiv g_2d_1x^3 + (g_2d_0 + g_1d_1)x^2 + (g_0d_1 + g_1d_0)x + g_0d_0\ (mod\ m’(x))$$<br>$m’(x)$阶数为4，大于3，则有<br>$$y(x) = g_2d_1x^3 + (g_2d_0 + g_1d_1)x^2 + (g_0d_1 + g_1d_0)x + g_0d_0 = g(x)d(x)$$    </p></blockquote><p>同样的由于$m’(x)$阶数较高，原同余方程可以推出<br>$$y(x) = \sum^3_{i=0} g^{(i)}(x)d^{(i)}(x) \times M_i(x) \times N_i(x)$$<br>改写成矩阵形式并做Z反变换变成<br>$$<br>\begin{bmatrix} y_0 \\ y_1 \\ y_2 \\ y_3 \end{bmatrix} =<br>\left(<br>\begin{bmatrix}<br>-1 &amp; 0 &amp; 0 \\<br>\frac{1}{2} &amp; -\frac{1}{2} &amp; \frac{1}{2} \\<br>\frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{2} \\<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>\begin{bmatrix} g_0 \\ g_1 \\ g_2 \end{bmatrix}<br>\right)<br>\odot<br>\left(<br>\begin{bmatrix}<br>-1 &amp;  0 &amp;  0 &amp;  0 \\<br> 0 &amp; -1 &amp;  1 &amp; -1 \\<br> 1 &amp;  1 &amp;  1 &amp;  0 \\<br> 0 &amp;  0 &amp;  0 &amp;  1<br>\end{bmatrix}<br>\begin{bmatrix}<br> 1 &amp;  0 \\<br> 1 &amp; -1 \\<br> 1 &amp;  1 \\<br> 0 &amp;  1<br>\end{bmatrix}<br>\begin{bmatrix} d_0 \\ d_1 \end{bmatrix}<br>\right)<br>$$<br>也即<br>$$y= (Gg) \odot (BAd)$$<br>……      </p><hr><p><strong><em>啊！卡住了，我已经推导不下去……</em></strong>    </p><ol><li>上述Winograd的推导针对的是离散序列的卷积，我该如何推广到深度学习的卷积（即互相关函数）上去？</li><li>为何m输<font color="red">入</font>r参数的离散序列卷积推导出来的变换矩阵G、A、B可以应用在m输<font color="red">出</font>r参数的互相关函数上，最后变成$y= A^T[(Gg) \odot (B^Td)]$的？</li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>从嵌套的角度理解张量</title>
      <link href="/2019/08/30/nested-dimension/"/>
      <url>/2019/08/30/nested-dimension/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=26447696&auto=0&height=32"></iframe>    <p>我们通常称呼一维张量为“向量”，用一条线来抽象它；称呼二维张量为“矩阵”，用一个平面来抽象它；而三维张量也可以用一个立方体来抽象它——我们所生活的世界是一个三维的世界，理解一维、二维、三维的对象自然是得心应手，但有时候也不得不面对更加高维的数据结构，这时候该怎么理解它们呢？本文将试图从嵌套的角度入手，用低维张量嵌套的形式来辅助理解高维张量。      </p><h3 id="C语言的数组"><a href="#C语言的数组" class="headerlink" title="C语言的数组"></a>C语言的数组</h3><p>众所周知，内存的排列是一维的，C语言本身也不存在真实的高维数组，我们通常用“数组的数组”这种形式来描述一个高维的数组。比如    </p><ul><li><code>int a[2]</code>定义了两个元素，它们组成一个数组a，也可以看成一个向量；</li><li><code>int b[3][2]</code>定义了三个数组组成了一个“数组的数组”b，这里的b可以看成一个矩阵；</li><li>同理，<code>int c[4][3][2]</code>就可以看成一个三维的张量       </li></ul><h3 id="用低维张量嵌套定义高维张量"><a href="#用低维张量嵌套定义高维张量" class="headerlink" title="用低维张量嵌套定义高维张量"></a>用低维张量嵌套定义高维张量</h3><p>事实上，C语言的高维数组定义方式是一种嵌套定义——<br>记将元素排列组成数组的操作为$\alpha$，那么对于元素$a_1$, $a_2$，$A = \alpha (a_1, a_2)$就组成了一个数组；<br>如果把A看作元素，那么$\alpha(A_1, A_2) = \alpha(\alpha(a_{11}, a{12}), \alpha(a_{21}, a_{22}))$就以嵌套的形式构成了一个二维数组也即矩阵。</p><p>我们再用图片来描述这种嵌套过程：      </p><ul><li>一维张量【$3$】<br><img src="/imgs/nested-dimension/1.jpg" alt="1">     </li><li>一维张量嵌套得到二维张量【$3 \times 3$】<br><img src="/imgs/nested-dimension/12.jpg" alt="12">     </li><li>二维张量（行先序）【$3 \times 3$】<br><img src="/imgs/nested-dimension/2.jpg" alt="2">   </li><li>用一维嵌套二维张量得到三维张量【$2 \times 3 \times 3$】<br><img src="/imgs/nested-dimension/23_1.jpg" alt="23_1">   </li><li>用二维嵌套一维张量得到三维张量【$3 \times 3 \times 2$】<br><img src="/imgs/nested-dimension/23_2.jpg" alt="23_2">   </li><li>用二维嵌套二维张量得到四维张量【$2 \times 2 \times 3 \times 3$】<br><img src="/imgs/nested-dimension/24.jpg" alt="24">   </li></ul><p>依次类推，我们甚至可以用更复杂的嵌套（比如对二维张量做两次二维嵌套得到六维张量），更高的基本维度（比如对三维张量做一维嵌套得到四维张量——卷积核的常用描述形式）。     </p><p>《<a href="/2019/08/21/winograd_convolution/">Winograd卷积原理 | Hey~YaHei!</a>》一文中，二维Winograd的实现也正是这种嵌套思想推导出来的。    </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>浅探Winograd量化</title>
      <link href="/2019/08/23/winograd_quantize/"/>
      <url>/2019/08/23/winograd_quantize/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=26209392&auto=0&height=32"></iframe>     <p>上一篇文章《<a href="/2019/08/21/winograd_convolution/">Winograd卷积原理 | Hey~YaHei!</a>》已经介绍过Winograd卷积的基本原理，但终究是理论上的推导，在实际应用的时候其实有些耐人寻味的地方：数学推导假设的是无限的精度和数值范围，但实际计算机的运算精度与数值范围都是有限的，不过按照论文《<a href="http://arxiv.org/pdf/1509.09308" target="_blank" rel="noopener">Fast algorithms for convolutional neural(CVPR2016)</a>》的报告，Winograd在浮点运算上的表现都不错：       </p><center><img src="/imgs/winograd/VGG_errors.jpg" alt="VGG_errors"></center>     <ul><li>$F(2\times2,3\times3)$的fp32精度损失甚至比Direct Convolution还小，这主要得益于乘法次数的减少，以及简单的变换矩阵（没有非常大或者非常小的数值）     </li><li>$F(4\times4,3\times3)$的fp32精度损失就比较大了，但似乎也还能接受      </li><li>fp16精度损失这三者表现的差不多       </li></ul><p>浮点运算的Winograd确实不错，但它却似乎也没那么轻易能套上量化——整型可没有浮点这么大的动态范围，如何保证运算过程整型不会溢出将是个令人头疼的问题。      </p><h3 id="溢出"><a href="#溢出" class="headerlink" title="溢出"></a>溢出</h3><p>假设将网络量化成int8，int8的权重和int8的输入，那么得益于<code>int8 * int8</code>，无论是Direct Convolution还是im2col+GEMM，这都能带来可观的加速。但在Winograd里可就不是这么回事了！<br><em>注意：im2col也没有对数值的大小进行变换</em>       </p><h4 id="F-2-3"><a href="#F-2-3" class="headerlink" title="$F(2,3)$"></a>$F(2,3)$</h4><p>回顾一下$F(2,3)$的变换矩阵：<br>$$<br>B^{T}=\left[\begin{array}{rrrr}{1} &amp; {0} &amp; {-1} &amp; {0} \\ {0} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {-1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {0} &amp; {-1}\end{array}\right],<br>G=\left[\begin{array}{rrr}{1} &amp; {0} &amp; {0} \\ {\frac{1}{2}} &amp; {\frac{1}{2}} &amp; {\frac{1}{2}} \\ {\frac{1}{2}} &amp; {-\frac{1}{2}} &amp; {\frac{1}{2}} \\ {0} &amp; {0} &amp; {1}\end{array}\right],<br>A^{T}=\left[\begin{array}{rrrr}{1} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {-1} &amp; {-1}\end{array}\right]<br>$$<br>$$<br>g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>d=\left[\begin{array}{llll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]^{T}<br>$$<br>接下来计算一下$U$矩阵和$V$矩阵：     </p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> sympy <span class="token keyword">import</span> Matrix<span class="token punctuation">,</span> SymbolBT <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>G <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>AT <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>g <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'g0'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'g1'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'g2'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>d <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'d0'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'d1'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'d2'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'d3'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>m <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'m0'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'m1'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'m2'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span><span class="token string">'m3'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"G * g:"</span><span class="token punctuation">,</span> G <span class="token operator">*</span> g<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"BT * d:"</span><span class="token punctuation">,</span> BT <span class="token operator">*</span> d<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"AT * m:"</span><span class="token punctuation">,</span> AT <span class="token operator">*</span> m<span class="token punctuation">)</span></code></pre><p>$$<br>Gg = \left[\begin{array}{l}{2g_0} &amp; {g_0+g_1+g_2} &amp; {g0-g1+g2} &amp; {2g_2}\end{array}\right]^T<br>\\<br>B^Td = \left[\begin{array}{l}{d_0-d_2} &amp; {d_1+d_2} &amp; {-d_1+d_2} &amp; {d_1-d_3}\end{array}\right]^T<br>\\<br>\frac{1}{2} A^Tm = \frac{1}{2} \left[\begin{array}{l}{m_0+m_1+m_2} &amp; {m_1-m_2-m_3}\end{array}\right]^T<br>$$</p><p>输入、输出变换过程包含8次加法和2次移位；<br>同时可以看到，为了保证计算不溢出，$Gg$需要额外的2个bits，而$B^Td$需要额外的1个bit，换言之，为了保证安全的<code>int8 * int8</code>，权重和输入分别得量化到int6和int7。</p><h4 id="F-2-times2-3-times3"><a href="#F-2-times2-3-times3" class="headerlink" title="$F(2\times2,3\times3)$"></a>$F(2\times2,3\times3)$</h4><pre class=" language-python"><code class="language-python">g <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'g{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>d <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'d{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>m <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'m{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"G * g * GT:"</span><span class="token punctuation">,</span> G <span class="token operator">*</span> g <span class="token operator">*</span> G<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span>              <span class="token number">4</span><span class="token operator">*</span>g0<span class="token punctuation">,</span>                         <span class="token number">2</span><span class="token operator">*</span>g0 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g1 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g2<span class="token punctuation">,</span>                         <span class="token number">2</span><span class="token operator">*</span>g0 <span class="token operator">-</span> <span class="token number">2</span><span class="token operator">*</span>g1 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g2<span class="token punctuation">,</span>               <span class="token number">4</span><span class="token operator">*</span>g2<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">*</span>g0 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g3 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g6<span class="token punctuation">,</span> g0 <span class="token operator">+</span> g1 <span class="token operator">+</span> g2 <span class="token operator">+</span> g3 <span class="token operator">+</span> g4 <span class="token operator">+</span> g5 <span class="token operator">+</span> g6 <span class="token operator">+</span> g7 <span class="token operator">+</span> g8<span class="token punctuation">,</span> g0 <span class="token operator">-</span> g1 <span class="token operator">+</span> g2 <span class="token operator">+</span> g3 <span class="token operator">-</span> g4 <span class="token operator">+</span> g5 <span class="token operator">+</span> g6 <span class="token operator">-</span> g7 <span class="token operator">+</span> g8<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>g2 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g5 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g8<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">*</span>g0 <span class="token operator">-</span> <span class="token number">2</span><span class="token operator">*</span>g3 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g6<span class="token punctuation">,</span> g0 <span class="token operator">+</span> g1 <span class="token operator">+</span> g2 <span class="token operator">-</span> g3 <span class="token operator">-</span> g4 <span class="token operator">-</span> g5 <span class="token operator">+</span> g6 <span class="token operator">+</span> g7 <span class="token operator">+</span> g8<span class="token punctuation">,</span> g0 <span class="token operator">-</span> g1 <span class="token operator">+</span> g2 <span class="token operator">-</span> g3 <span class="token operator">+</span> g4 <span class="token operator">-</span> g5 <span class="token operator">+</span> g6 <span class="token operator">-</span> g7 <span class="token operator">+</span> g8<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>g2 <span class="token operator">-</span> <span class="token number">2</span><span class="token operator">*</span>g5 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g8<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span>              <span class="token number">4</span><span class="token operator">*</span>g6<span class="token punctuation">,</span>                         <span class="token number">2</span><span class="token operator">*</span>g6 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g7 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g8<span class="token punctuation">,</span>                         <span class="token number">2</span><span class="token operator">*</span>g6 <span class="token operator">-</span> <span class="token number">2</span><span class="token operator">*</span>g7 <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>g8<span class="token punctuation">,</span>               <span class="token number">4</span><span class="token operator">*</span>g8<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"BT * d * B:"</span><span class="token punctuation">,</span> BT <span class="token operator">*</span> d <span class="token operator">*</span> BT<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span>  d0 <span class="token operator">+</span> d10 <span class="token operator">-</span> d2 <span class="token operator">-</span> d8<span class="token punctuation">,</span>   d1 <span class="token operator">-</span> d10 <span class="token operator">+</span> d2 <span class="token operator">-</span> d9<span class="token punctuation">,</span> <span class="token operator">-</span>d1 <span class="token operator">-</span> d10 <span class="token operator">+</span> d2 <span class="token operator">+</span> d9<span class="token punctuation">,</span>   d1 <span class="token operator">+</span> d11 <span class="token operator">-</span> d3 <span class="token operator">-</span> d9<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span> <span class="token operator">-</span>d10 <span class="token operator">+</span> d4 <span class="token operator">-</span> d6 <span class="token operator">+</span> d8<span class="token punctuation">,</span>   d10 <span class="token operator">+</span> d5 <span class="token operator">+</span> d6 <span class="token operator">+</span> d9<span class="token punctuation">,</span>  d10 <span class="token operator">-</span> d5 <span class="token operator">+</span> d6 <span class="token operator">-</span> d9<span class="token punctuation">,</span>  <span class="token operator">-</span>d11 <span class="token operator">+</span> d5 <span class="token operator">-</span> d7 <span class="token operator">+</span> d9<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span> <span class="token operator">-</span>d10 <span class="token operator">-</span> d4 <span class="token operator">+</span> d6 <span class="token operator">+</span> d8<span class="token punctuation">,</span>   d10 <span class="token operator">-</span> d5 <span class="token operator">-</span> d6 <span class="token operator">+</span> d9<span class="token punctuation">,</span>  d10 <span class="token operator">+</span> d5 <span class="token operator">-</span> d6 <span class="token operator">-</span> d9<span class="token punctuation">,</span>  <span class="token operator">-</span>d11 <span class="token operator">-</span> d5 <span class="token operator">+</span> d7 <span class="token operator">+</span> d9<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span><span class="token operator">-</span>d12 <span class="token operator">+</span> d14 <span class="token operator">+</span> d4 <span class="token operator">-</span> d6<span class="token punctuation">,</span> <span class="token operator">-</span>d13 <span class="token operator">-</span> d14 <span class="token operator">+</span> d5 <span class="token operator">+</span> d6<span class="token punctuation">,</span> d13 <span class="token operator">-</span> d14 <span class="token operator">-</span> d5 <span class="token operator">+</span> d6<span class="token punctuation">,</span> <span class="token operator">-</span>d13 <span class="token operator">+</span> d15 <span class="token operator">+</span> d5 <span class="token operator">-</span> d7<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"AT * m * A:"</span><span class="token punctuation">,</span> AT <span class="token operator">*</span> m <span class="token operator">*</span> AT<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span>    m0 <span class="token operator">+</span> m1 <span class="token operator">+</span> m10 <span class="token operator">+</span> m2 <span class="token operator">+</span> m4 <span class="token operator">+</span> m5 <span class="token operator">+</span> m6 <span class="token operator">+</span> m8 <span class="token operator">+</span> m9<span class="token punctuation">,</span>    m1 <span class="token operator">-</span> m10 <span class="token operator">-</span> m11 <span class="token operator">-</span> m2 <span class="token operator">-</span> m3 <span class="token operator">+</span> m5 <span class="token operator">-</span> m6 <span class="token operator">-</span> m7 <span class="token operator">+</span> m9<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">[</span><span class="token operator">-</span>m10 <span class="token operator">-</span> m12 <span class="token operator">-</span> m13 <span class="token operator">-</span> m14 <span class="token operator">+</span> m4 <span class="token operator">+</span> m5 <span class="token operator">+</span> m6 <span class="token operator">-</span> m8 <span class="token operator">-</span> m9<span class="token punctuation">,</span> m10 <span class="token operator">+</span> m11 <span class="token operator">-</span> m13 <span class="token operator">+</span> m14 <span class="token operator">+</span> m15 <span class="token operator">+</span> m5 <span class="token operator">-</span> m6 <span class="token operator">-</span> m7 <span class="token operator">-</span> m9<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>输出矩阵的各元素系数绝对值之和$\mu(\cdot)$：<br>$$<br>\mu(GgG^T) = \left[\begin{array}{llll}<br>{4} &amp; {6} &amp; {6} &amp; {4} \\<br>{6} &amp; {9} &amp; {9} &amp; {6} \\<br>{6} &amp; {9} &amp; {9} &amp; {6} \\<br>{4} &amp; {6} &amp; {6} &amp; {4}<br>\end{array}\right],<br>\mu(B^TdB) = \left[\begin{array}{llll}<br>{4} &amp; {4} &amp; {4} &amp; {4} \\<br>{4} &amp; {4} &amp; {4} &amp; {4} \\<br>{4} &amp; {4} &amp; {4} &amp; {4} \\<br>{4} &amp; {4} &amp; {4} &amp; {4}<br>\end{array}\right]<br>$$</p><p>输入、输出变换过程包含80次加法和4次移位（$\frac{1}{4}A^TmA$）；<br>同时可以看到，为了保证计算不溢出，$GgG^T$需要额外的4个bits，而$B^TdB$需要额外的2个bit，换言之，为了保证安全的<code>int8 * int8</code>，权重和输入分别得量化到int4和int6。     </p><h4 id="F-4-times4-3-times3"><a href="#F-4-times4-3-times3" class="headerlink" title="$F(4\times4,3\times3)$"></a>$F(4\times4,3\times3)$</h4><!--```pythondef max_l1_coeff(sym_matrix):    max_ = 0    for elem in sym_matrix:        d = elem.as_coefficients_dict()        l1_coeff = sum([abs(v) for v in d.values()])        if l1_coeff > max_:            max_ = l1_coeff    return max_BT = Matrix([    [4,  0, -5,  0, 1, 0],    [0, -4, -4,  1, 1, 0],    [0,  4, -4, -1, 1, 0],    [0, -2, -1,  2, 1, 0],    [0,  2, -1, -2, 1, 0],    [0,  4,  0, -5, 0, 1]])G = Matrix([    [ 6,  0,  0],    [-4, -4, -4],    [-4,  4, -4],    [ 1,  2,  4],    [ 1, -2,  4],    [ 0,  0, 24]])AT = Matrix([    [1, 1,  1, 1,  1, 0],    [0, 1, -1, 2, -2, 0],    [0, 1,  1, 4,  4, 0],    [0, 1, -1, 8, -8, 1]])g = Matrix(3, 3, [Symbol(f'g{i}') for i in range(3*3)])d = Matrix(6, 6, [Symbol(f'd{i}') for i in range(6*6)])m = Matrix(6, 6, [Symbol(f'm{i}') for i in range(6*6)])print(max_l1_coeff(G * g * G.T))     # Output: 576print(max_l1_coeff(BT * d * BT.T))   # Output: 100print(max_l1_coeff(AT * m * AT.T))   # Output: 361```--><p>$$<br>B^{T}=\left[\begin{array}{rrrrrr}<br>{4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1} &amp; {0} \\<br>{0} &amp; {-4} &amp; {-4} &amp; {1} &amp; {1} &amp; {0} \\<br>{0} &amp; {4} &amp; {-4} &amp; {-1} &amp; {1} &amp; {0} \\<br>{0} &amp; {-2} &amp; {-1} &amp; {2} &amp; {1} &amp; {0} \\<br>{0} &amp; {2} &amp; {-1} &amp; {-2} &amp; {1} &amp; {0} \\<br>{0} &amp; {4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1}<br>\end{array}\right],<br>G=\left[\begin{array}{rrr}<br>{6} &amp; {0} &amp; {0} \\<br>{-4} &amp; {-4} &amp; {-4} \\<br>{-4} &amp; {4} &amp; {-4} \\<br>{1} &amp; {2} &amp; {4} \\<br>{1} &amp; {-2} &amp; {4} \\<br>{0} &amp; {0} &amp; {24}<br>\end{array}\right]<br>$$</p><p>为了保证计算不溢出，$GgG^T$需要额外的10个bits（$G$最后一行绝对值之和最大，$24^2=576$），而$B^TdB$需要额外的7个bits（$B^T$最后一行的绝对值之和最大，$10^2=100$），换言之……压根没法保证<code>int8 * int8</code>的安全计算。这时候可能只能将int8扩展到int16，执行<code>int16 * int16</code>的乘法运算，即便如此，权重也只能量化到int6.     </p><h4 id="大家是怎么做的？"><a href="#大家是怎么做的？" class="headerlink" title="大家是怎么做的？"></a>大家是怎么做的？</h4><p><a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener">ncnn</a>和<a href="https://github.com/alibaba/MNN" target="_blank" rel="noopener">mnn</a>的量化计算本质上是<code>int16 * int16</code>而非<code>int8 * int8</code>，所以$F(2\times2,3\times3)$完全没有溢出的问题，而$F(4\times4,3\times3)$需要将权重量化到int6；<a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">Tengine</a>的量化计算是<code>int8 * int8</code>，1.6版本已经支持Winograd卷积，但目前还没有支持Winograd量化。</p><h4 id="思考：先变换后量化？"><a href="#思考：先变换后量化？" class="headerlink" title="思考：先变换后量化？"></a>思考：先变换后量化？</h4><p>对量化的输入和权重做变换时溢出问题着实让人头疼，既然如此，我们能不能先做变换再进行量化呢？<br><strong>不过，要是先变换再量化，乘出来之后就反量化不回去了QAQ</strong>       </p><!--#### 思考：变换矩阵的元素一定要是整数吗？     例如$F(2\times2,3\times3)$的$G$矩阵：     $$2G=\left[\begin{array}{rrr}{2} & {0} & {0} \\ {1} & {1} & {1} \\ {1} & {-1} & {1} \\ {0} & {0} & {2}\end{array}\right]$$    中间两行绝对值之和最大，$3^2=9$，所以$GgG^T$需要预留4bits来确保不会溢出，权重必须都量化为int4；     但如果不变化为整型，    $$G=\left[\begin{array}{rrr}{1} & {0} & {0} \\ {\frac{1}{2}} & {\frac{1}{2}} & {\frac{1}{2}} \\ {\frac{1}{2}} & {-\frac{1}{2}} & {\frac{1}{2}} \\ {0} & {0} & {1}\end{array}\right]$$$(\frac{3}{2})^2=\frac{9}{4}$，此时$GgG^T$只需要预留2bits来确保不会溢出，权重可以量化到int6，虽然乘以$\frac{1}{2}$或$\frac{1}{4}$（即整右移1位或2位）实际上也会丢失权重本身的精度，但并不是每个元素每次都会丢失2bits的精度。    此外，从另外一个角度看，考虑int8量化误差为$\frac{max-min}{2^8}$，乘以$\frac{1}{4}$之后量化误差仅为$\frac{max-min}{2^{10}}$，而int6的量化误差为$\frac{max-min}{2^6}$反而更大——似乎与其预留多少bits来防止溢出，还不如保留更多的量化位数，实际计算的时候再做相应的除法；    考虑到权重变换只需要做一次，甚至可以有更为极端的做法：    $$\frac{2}{3}G=\left[\begin{array}{rrr}{\frac{2}{3}} & {0} & {0} \\ {\frac{1}{3}} & {\frac{1}{3}} & {\frac{1}{3}} \\ {\frac{1}{3}} & {-\frac{1}{3}} & {\frac{1}{3}} \\ {0} & {0} & {\frac{2}{3}}\end{array}\right]$$    变换前将int8的权重值提升为float32进行变换，最后再对变换后矩阵$GgG^T$进行取整，由于$\frac{2}{3}G$绝对值之和最大的行为1，不需要预留任何bits就能够确保不会发生溢出。     做一个简单的实验：[hey-yahei/winograd_filter_transform.py | github](https://gist.github.com/hey-yahei/afec123324404eea100f239c6196118f)     > 取mobilenet的第一层卷积进行实验——> A：用不量化的权重和$G$变换卷积核（参照组）> B：用int4量化后的权重和$2G$变换卷积核（预留4bits确保不会溢出）> C：用int6量化后的权重和$G$变换卷积核（预留2bits确保不会溢出）> D：用int8量化后的权重（提升为fp32）和$\frac{2}{3}G$变换卷积核，再取整为int8（不会溢出）> 将B、C、D反量化，并分别乘以$\frac{1}{4}$、$1$、$\frac{9}{4}$后与A进行比较（计算绝对误差）| Method | sum | mean | max ||:---:|:---:|:---:|:---:||B(int4)|41.2|0.02683|0.1188||C(int6)|20.2|0.01316|0.0792||D(int8)|4.55|0.00296|0.0117|【***如果我代码没写错的话……***】     C方法和D方法确实有比较小的量化误差！！    权重变换只需要做一次，所以可以采取提升到float32再做变换的方式来保证中间过程的运算精度；     而输入、输出变换每张输入、输出特征图都需要做一次，所以像D方法把int8提升到float32做变换就降低了速度得不偿失，折中的做法是保证变换矩阵中尽可能多的$2^n$元素的前提下尽可能减少需要预留的bits数。--><h3 id="更快的变换"><a href="#更快的变换" class="headerlink" title="更快的变换"></a>更快的变换</h3><p>上一篇文章我们已经提到过——     </p><blockquote><p>尽管$V = B^{T} d B$和$Y = A^T M A$的计算过程中也有大量的乘法，但观察可以发现$F(4,3)$和$F(6,3)$的$A^T$矩阵和$B^T$中有相当多的元素恰好是$2^n$，也就是说，用Winograd计算量化的卷积应该会有神奇的加成</p></blockquote><p>现在我们来看看具体有哪些神奇的变化：     </p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> math <span class="token keyword">import</span> log2<span class="token keyword">def</span> <span class="token function">_op_count</span><span class="token punctuation">(</span>sym_matrix<span class="token punctuation">)</span><span class="token punctuation">:</span>    adds<span class="token punctuation">,</span> shifts<span class="token punctuation">,</span> muls <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>    abs_v_pool <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> elem <span class="token keyword">in</span> sym_matrix<span class="token punctuation">:</span>        d <span class="token operator">=</span> elem<span class="token punctuation">.</span>as_coefficients_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>        adds <span class="token operator">+=</span> len<span class="token punctuation">(</span>d<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>        <span class="token keyword">for</span> v <span class="token keyword">in</span> d<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            abs_v <span class="token operator">=</span> abs<span class="token punctuation">(</span>v<span class="token punctuation">)</span>            <span class="token keyword">if</span> abs_v <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">and</span> abs_v <span class="token operator">not</span> <span class="token keyword">in</span> abs_v_pool<span class="token punctuation">:</span>                abs_v_pool<span class="token punctuation">.</span>append<span class="token punctuation">(</span>abs_v<span class="token punctuation">)</span>                log <span class="token operator">=</span> log2<span class="token punctuation">(</span>abs_v<span class="token punctuation">)</span>                <span class="token keyword">if</span> log <span class="token operator">==</span> int<span class="token punctuation">(</span>log<span class="token punctuation">)</span><span class="token punctuation">:</span>                    shifts <span class="token operator">+=</span> <span class="token number">1</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    muls <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">"adds"</span><span class="token punctuation">:</span> adds<span class="token punctuation">,</span> <span class="token string">"shifts"</span><span class="token punctuation">:</span> shifts<span class="token punctuation">,</span> <span class="token string">"muls"</span><span class="token punctuation">:</span> muls<span class="token punctuation">}</span><span class="token keyword">def</span> <span class="token function">op_count_1D</span><span class="token punctuation">(</span>M1<span class="token punctuation">,</span> M2<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> _op_count<span class="token punctuation">(</span>M1 <span class="token operator">*</span> M2<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">op_count_2D</span><span class="token punctuation">(</span>M1<span class="token punctuation">,</span> M2<span class="token punctuation">)</span><span class="token punctuation">:</span>    t <span class="token operator">=</span> M1 <span class="token operator">*</span> M2    counter1 <span class="token operator">=</span> _op_count<span class="token punctuation">(</span>t<span class="token punctuation">)</span>    M3 <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token operator">*</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'t{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    counter2 <span class="token operator">=</span> _op_count<span class="token punctuation">(</span>M3 <span class="token operator">*</span> M1<span class="token punctuation">.</span>T<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">{</span>        <span class="token string">"adds"</span><span class="token punctuation">:</span> counter1<span class="token punctuation">[</span><span class="token string">'adds'</span><span class="token punctuation">]</span> <span class="token operator">+</span> counter2<span class="token punctuation">[</span><span class="token string">'adds'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token string">"shifts"</span><span class="token punctuation">:</span> counter1<span class="token punctuation">[</span><span class="token string">'shifts'</span><span class="token punctuation">]</span> <span class="token operator">+</span> counter2<span class="token punctuation">[</span><span class="token string">'shifts'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token string">"muls"</span><span class="token punctuation">:</span> counter1<span class="token punctuation">[</span><span class="token string">'muls'</span><span class="token punctuation">]</span> <span class="token operator">+</span> counter2<span class="token punctuation">[</span><span class="token string">'muls'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">}</span><span class="token keyword">print</span><span class="token punctuation">(</span>op_count_1D<span class="token punctuation">(</span>G<span class="token punctuation">,</span> g<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>op_count_1D<span class="token punctuation">(</span>BT<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>op_count_1D<span class="token punctuation">(</span>AT<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>op_count_2D<span class="token punctuation">(</span>G<span class="token punctuation">,</span> g<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>op_count_2D<span class="token punctuation">(</span>BT<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>op_count_2D<span class="token punctuation">(</span>AT<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>将输入、输出变换过程中的大量乘法替换成移位运算之后，理论上Winograd能变得更快！    </p><table><thead><tr><th style="text-align:center">Winograd</th><th style="text-align:center">原始乘法</th><th style="text-align:center">Win乘法</th><th style="text-align:center">Win量化乘法</th><th style="text-align:center">理论加速比</th><th style="text-align:center">含变换加速比</th><th style="text-align:center">含变换加速比（量化）</th></tr></thead><tbody><tr><td style="text-align:center">$F(2,3)$</td><td style="text-align:center">6</td><td style="text-align:center">4(4)</td><td style="text-align:center">4(4)</td><td style="text-align:center">1.50</td><td style="text-align:center">1.50</td><td style="text-align:center">1.50</td></tr><tr><td style="text-align:center">$F(4,3)$</td><td style="text-align:center">12</td><td style="text-align:center">12(6)</td><td style="text-align:center">7(6)</td><td style="text-align:center">2.00</td><td style="text-align:center"><font color="red">1.00</font></td><td style="text-align:center">1.71</td></tr><tr><td style="text-align:center">$F(6,3)$</td><td style="text-align:center">18</td><td style="text-align:center">28(8)</td><td style="text-align:center">13(8)</td><td style="text-align:center">2.25</td><td style="text-align:center"><font color="red">0.64</font></td><td style="text-align:center">1.38</td></tr><tr><td style="text-align:center">堆$F(2\times2,3\times3)$</td><td style="text-align:center">36</td><td style="text-align:center">24(24)</td><td style="text-align:center">24(24)</td><td style="text-align:center">1.50</td><td style="text-align:center">1.50</td><td style="text-align:center">1.50</td></tr><tr><td style="text-align:center">堆$F(4\times4,3\times3)$</td><td style="text-align:center">144</td><td style="text-align:center">126(72)</td><td style="text-align:center">78(76)</td><td style="text-align:center">2.00</td><td style="text-align:center">1.14</td><td style="text-align:center">1.85</td></tr><tr><td style="text-align:center">堆$F(6\times6,3\times3)$</td><td style="text-align:center">324</td><td style="text-align:center">396(144)</td><td style="text-align:center">184(144)</td><td style="text-align:center">2.25</td><td style="text-align:center"><font color="red">0.82</font></td><td style="text-align:center">1.76</td></tr><tr><td style="text-align:center">嵌$F(2\times2,3\times3)$</td><td style="text-align:center">36</td><td style="text-align:center">16(16)</td><td style="text-align:center">16(16)</td><td style="text-align:center">2.25</td><td style="text-align:center">2.25</td><td style="text-align:center">2.25</td></tr><tr><td style="text-align:center">嵌$F(4\times4,3\times3)$</td><td style="text-align:center">144</td><td style="text-align:center">48(36)</td><td style="text-align:center">38(36)</td><td style="text-align:center">4.00</td><td style="text-align:center">3.00</td><td style="text-align:center">3.79</td></tr><tr><td style="text-align:center">嵌$F(6\times6,3\times3)$</td><td style="text-align:center">324</td><td style="text-align:center">102(64)</td><td style="text-align:center">74(64)</td><td style="text-align:center">5.06</td><td style="text-align:center">3.12</td><td style="text-align:center">4.38</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Winograd卷积原理</title>
      <link href="/2019/08/21/winograd_convolution/"/>
      <url>/2019/08/21/winograd_convolution/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=4956877&auto=0&height=32"></iframe>     <p>Winograd算法最早于1980年由Shmuel Winograd在《<a href="https://share.weiyun.com/5yu1Eku" target="_blank" rel="noopener">Arithmetic complexity of computations(1980)</a>》中提出，主要用来减少FIR滤波器的计算量。<br>该算法类似FFT，将数据映射到另一个空间上，用加减运算代替部分乘法运算，在“加减运算速度远高于乘法运算”的前提下达到明显的加速效果（与FFT不同的是，Winograd将数据映射到一个实数空间而非复数空间）。<br>比如，<br>直接实现一个 $m$ 输出、$r$ 参数的FIR滤波器 $F(m,r)$，一共需要 $m \times r$ 次乘法运算；<br>但使用Winograd算法，忽略变换过程的话，仅仅需要 $m + r - 1$ 次乘法运算。      </p><h3 id="F-2-3"><a href="#F-2-3" class="headerlink" title="$F(2,3)$"></a>$F(2,3)$</h3><p><strong>如果直接计算 $F(2,3)$</strong>：<br>$$<br>F(2,3)=\left[\begin{array}{lll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} \\ {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]\left[\begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[\begin{array}{l}{d_0g_0+d_1g_1+d_2g_2} \\ {d_1g_0+d_2g_1+d_3g_2}\end{array}\right]<br>$$<br>其中，<br>$d_0, d_1, d_2$和$d_1, d_2, d_3$为连续的两个输入序列；<br>$g_0, g_1, g_2$为FIR的三个参数；<br>这个过程一共需要6次乘法，和4次加法       </p><p><strong>而Winograd算法指出，$F(2,3)$ 可以这样计算</strong>：<br>$$<br>F(2,3)=\left[\begin{array}{lll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} \\ {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]\left[\begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[\begin{array}{l}{m_{1}+m_{2}+m_{3}} \\ {m_{2}-m_{3}-m_{4}}\end{array}\right]<br>$$<br>其中，<br>$$<br>\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right) g_{0}} &amp; {m_{2}=\left(d_{1}+d_{2}\right) \frac{g_{0}+g_{1}+g_{2}}{2}} \\ {m_{4}=\left(d_{1}-d_{3}\right) g_{2}} &amp; {m_{3}=\left(d_{2}-d_{1}\right) \frac{g_{0}-g_{1}+g_{2}}{2}}\end{array}<br>$$      </p><p>该用矩阵运算可以表示成：<br>$$<br>Y=A^{T}\left[(G g) \odot\left(B^{T} d\right)\right]<br>$$<br>其中，$\odot$表示点乘，而<br>$$<br>B^{T}=\left[\begin{array}{rrrr}{1} &amp; {0} &amp; {-1} &amp; {0} \\ {0} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {-1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {0} &amp; {-1}\end{array}\right],<br>G=\left[\begin{array}{rrr}{1} &amp; {0} &amp; {0} \\ {\frac{1}{2}} &amp; {\frac{1}{2}} &amp; {\frac{1}{2}} \\ {\frac{1}{2}} &amp; {-\frac{1}{2}} &amp; {\frac{1}{2}} \\ {0} &amp; {0} &amp; {1}\end{array}\right],<br>A^{T}=\left[\begin{array}{rrrr}{1} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {-1} &amp; {-1}\end{array}\right]<br>$$<br>$$<br>g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>d=\left[\begin{array}{llll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]^{T}<br>$$      </p><p>这……似乎反而把问题变得十分复杂，但实际上它的计算量却真真切切地减少了——      </p><ol><li>由于$g$是固定的FIR滤波器参数，那么$Gg$可以提前计算并得到一个 $4\times1$ 的列向量    </li><li>由于$d$是变化的输入序列，所以每次计算FIR的时候都需要对输入$d$做一个变换$B^Td$，得到一个 $4 \times 1$ 的列向量，这个过程需要4次加法（注意$B^T$矩阵的元素值）      </li><li>然后$Gg$和$B^Td$进行点乘，共计4次乘法      </li><li>最后$A^T$与$[(Gg)\odot(B^Td)]$做乘法，共计4次加法</li></ol><p>过程1可以提前完成，变换过程2和计算过程3、4共计<strong>4次乘法和8次加法</strong>，相比于直接FIR的<strong>6次乘法、4次加法</strong>，乘法次数下降为原来的$\frac{2}{3}$（推广到一般情况，直接FIR跟Winograd的乘法次数分别是$m \times r$和$m+r-1$）。<br>但天下没有免费的午餐，既然速度得到提升，那么肯定需要付出代价——算法的加速往往是需要以额外的空间为代价的：原先FIR只需要存储3个参数$g$，但现在需要存储4个参数$Gg$（推广到一般情况，分别是$r$和$m+r-1$）；      </p><h3 id="F-2-times2-3-times3"><a href="#F-2-times2-3-times3" class="headerlink" title="$F(2\times2, 3\times3)$"></a>$F(2\times2, 3\times3)$</h3><p>参考arm的一份幻灯片：<a href="https://www.slideshare.net/embeddedvision/even-faster-cnns-exploring-the-new-class-of-winograd-algorithms-a-presentation-from-arm?from_action=save" target="_blank" rel="noopener">Even Faster CNNs: Exploring the New Class of Winograd Algorithms</a>      </p><p>接下来我们将一维的$F(2,3)$扩展到二维的$F(2\times2, 3\times3)$，有两种扩展方式，一是通过堆叠$F(2,3)$来实现$F(2\times2, 3\times3)$，二是通过嵌套$F(2,3)$来实现$F(2\times2, 3\times3)$。后者计算量减小幅度更大，但前者占用内存更少。以k3s1的卷积为例——<br><em>为了跟幻灯片的符号统一，在这一部分中用$k$来表示输入，$w$表示权重，$r$表示输出</em><br><img src="/imgs/winograd/conv_k3s1.jpg" alt="conv_k3s1">    </p><p>$$<br>W = \left[\begin{array}{lll}{w_{0}} &amp; {w_{1}} &amp; {w_{2}} \\ {w_{3}} &amp; {w_{4}} &amp; {w_{5}} \\ {w_{6}} &amp; {w_{7}} &amp; {w_{8}}\end{array}\right]<br>$$<br>对直接卷积来说，该过程一共需要36次乘法和32次加法。         </p><p>参考<a href="https://www.slideshare.net/embeddedvision/even-faster-cnns-exploring-the-new-class-of-winograd-algorithms-a-presentation-from-arm?from_action=save" target="_blank" rel="noopener">Even Faster CNNs: Exploring the New Class of Winograd Algorithms</a>，将输入按滑窗分块后展开成向量并堆叠成矩阵，将权重展开成向量——<br><img src="/imgs/winograd/F_2x2_3x3_1.jpg" alt="F_2x2_3x3_1">     </p><p>对矩阵和向量进行分块——<br><img src="/imgs/winograd/F_2x2_3x3_2.jpg" alt="F_2x2_3x3_2">    </p><h4 id="堆叠实现"><a href="#堆叠实现" class="headerlink" title="堆叠实现"></a>堆叠实现</h4><p>$$<br>\begin{aligned}<br>\left[\begin{array}{lll}{K_0} &amp; {K_1} &amp; {K_2} \\ {K_1} &amp; {K_2} &amp; {K_3} \end{array}\right]<br>\left[\begin{array}{l}{W_0} \\ {W_1} \\ {W_2} \end{array}\right] &amp;= \left[\begin{array}{l}{R_0} \\ {R_1} \end{array}\right] = \left[\begin{array}{l}{K_0W_0+K_1W_1+K_2W_2} \\ {K_1W_0+K_2W_1+K_3W_2} \end{array}\right] \\<br>\\<br>&amp;= \left[\begin{array}{l}{F_{(2,3)}(D_0,W_0)+F_{(2,3)}(D_1,W_1)+F_{(2,3)}(D_2,W_2)} \\ {F_{(2,3)}(D_1,W_0)+F_{(2,3)}(D_2,W_1)+F_{(2,3)}(D_3,W_2)} \end{array}\right]<br>\end{aligned}<br>$$<br>其中，$D_i$是$K_i$对应的输入序列，也即卷积输入的第$i$行<br>$$<br>D = \left[\begin{array}{llll}<br>{k_0} &amp; {k_4} &amp; {k_8} &amp; {k_{12}} \\<br>{k_1} &amp; {k_5} &amp; {k_9} &amp; {k_{13}} \\<br>{k_2} &amp; {k_6} &amp; {k_{10}} &amp; {k_{14}} \\<br>{k_3} &amp; {k_7} &amp; {k_{11}} &amp; {k_{15}}<br>\end{array}\right] = \left[\begin{array}{l} D_0 &amp; D_1 &amp; D_2 &amp; D_3 \end{array}\right]<br>$$</p><p>也就是说，$F(2\times2, 3\times3)$在这里分成了6次$F(2,3)$以及4次额外的加法，总计<strong>24次乘法和44次加法</strong>（注意：虽然这里做了6次$F(2,3)$但是输入序列的变换只需要做4次，所以加法次数是44次而非52次），相比于直接卷积的<strong>36次乘法和32次加法</strong>，乘法次数跟一维的$F(2,3)$一样，也下降为原来的$\frac{2}{3}$，同理也需要付出3倍于$F(2,3)$额外的空间代价（三个$W$）：   </p><h4 id="嵌套实现"><a href="#嵌套实现" class="headerlink" title="嵌套实现"></a>嵌套实现</h4><p>参考《<a href="https://www.cnblogs.com/shine-lee/p/10906535.html#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84winograd" target="_blank" rel="noopener">卷积神经网络中的Winograd快速卷积算法 | shinelee, 博客园</a>》<br>$$<br>\begin{aligned}<br>\left[\begin{array}{lll}{K_0} &amp; {K_1} &amp; {K_2} \\ {K_1} &amp; {K_2} &amp; {K_3} \end{array}\right] &amp;=<br>\left[ \begin{array}{c}{R_0} \\ {R_1}\end{array}\right] =<br>\left[ \begin{array}{c}{K_0 W_0 + K_1 W_1 + K_2 W_2} \\ {K_1 W_0 + K_2 W_1 + K_3 W_2} \end{array} \right] \\<br>\\<br>&amp;= \left[\begin{array}{l}{F_{(2,3)}(D_0,W_0)+F_{(2,3)}(D_1,W_1)+F_{(2,3)}(D_2,W_2)} \\ {F_{(2,3)}(D_1,W_0)+F_{(2,3)}(D_2,W_1)+F_{(2,3)}(D_3,W_2)} \end{array}\right]<br>\\<br>&amp;= \left[ \begin{array}{c} {A^{T}\left[(G W_0) \odot\left(B^{T} D_0 \right)\right] + A^{T}\left[(G W_1) \odot\left(B^{T} D_1 \right)\right] + A^{T}\left[(G W_2) \odot\left(B^{T} D_2 \right)\right]} \\ {A^{T}\left[(G W_0) \odot\left(B^{T} D_1 \right)\right] + A^{T}\left[(G W_1) \odot\left(B^{T} D_2 \right)\right] + A^{T}\left[(G W_2) \odot\left(B^{T} D_3 \right)\right]} \end{array} \right] \\<br>\\<br>&amp;=A^{T}\left[\left[G [W_0 \ W_1 \ W_2 ] G^{T}\right] \odot\left[B^{T} [D_0 \ D_1 \ D_2 \ D_3] B\right]\right]A \\<br>\\<br>&amp;=A^{T}\left[\left[G w G^{T}\right] \odot\left[B^{T} d B\right]\right] A \\<br>\\<br>&amp;\textit{(…w =&gt; g…)} \\<br>\\<br>&amp;=A^{T}\left[\left[G g G^{T}\right] \odot\left[B^{T} d B\right]\right] A<br>\end{aligned}<br>$$<br>也即，<br>$$F(2\times2, 3\times3) = A^{T} \left[ U \odot V \right] A$$<br>其中，<br>$$U = G g G^{T}$$<br>$$V = B^{T} d B$$      </p><p>与$F(2,3)$同理，可以推导出，$F(2\times2, 3\times3)$需要<strong>16次乘法和56次加法</strong>（$V=B^{T} d B$过程32次加法、$M=U \odot V$过程16次乘法、$Y=A^TMA$过程24次加法），相比于直接卷积的<strong>36次乘法和32次加法</strong>，乘法次数下降为原来的$\frac{16}{36}$。计算量的减少比堆叠实现要明显，但也需要更多的额外空间代价：直接计算只需要存储9个参数的$g$，$F(2\times2,3\times3)$则需要存储16个参数的$GgG^T$（推广到一般情况，分别为$r^2$和$(r+m-1)^2$）      </p><p><strong><em>后续讨论中，如非特别说明，二维Winograd均指的是嵌套实现。</em></strong>      </p><h3 id="G-、-B-T-、-A-T"><a href="#G-、-B-T-、-A-T" class="headerlink" title="$G$、$B^T$、$A^T$"></a>$G$、$B^T$、$A^T$</h3><p>Winograd算法需要推导出相应的变换矩阵$G$、$B^T$和$A^T$，但具体的推导过程似乎有些复杂，我现在还没弄懂。所幸 <a href="https://github.com/andravin/wincnn" target="_blank" rel="noopener">wincnn | github</a>提供了一个解算$G$、$B^T$、$A^T$的工具，除了前述的$F(2,3)$，常用的还有$F(4,3)$和$F(6,3)$，它们对应的变换矩阵如下：    </p><ul><li>$F(4,3)$<br>  $$<br>  B^{T}=\left[\begin{array}{rrrrrr}<br>  {4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-4} &amp; {-4} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {4} &amp; {-4} &amp; {-1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-2} &amp; {-1} &amp; {2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {2} &amp; {-1} &amp; {-2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  G=\left[\begin{array}{rrr}<br>  {1/4} &amp; {0} &amp; {0} \\<br>  {-1/6} &amp; {-1/6} &amp; {-1/6} \\<br>  {-1/6} &amp; {1/6} &amp; {-1/6} \\<br>  {1/24} &amp; {1/12} &amp; {1/6} \\<br>  {1/24} &amp; {-1/12} &amp; {1/6} \\<br>  {0} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  \\<br>  A^{T}=\left[\begin{array}{rrrrrr}<br>  {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {2} &amp; {-2} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {4} &amp; {4} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {8} &amp; {-8} &amp; {1}<br>  \end{array}\right]<br>  $$<br>  $$<br>  g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>  d=\left[\begin{array}{llllll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}} &amp; {d_4} &amp; {d_5}\end{array}\right]^{T}<br>  $$  </li><li>$F(6,3)$<br>  $$<br>  B^{T}=\left[\begin{array}{rrrrrrrr}<br>  {1} &amp; {0} &amp; {-21/4} &amp; {0} &amp; {21/4} &amp; {0} &amp; {-1} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {-17/4} &amp; {-17/4} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-1} &amp; {1} &amp; {17/4} &amp; {-17/4} &amp; {-1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {1/2} &amp; {1/4} &amp; {-5/2} &amp; {-5/4} &amp; {2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-1/2} &amp; {1/4} &amp; {5/2} &amp; {-5/4} &amp; {-2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {2} &amp; {4} &amp; {-5/2} &amp; {-5} &amp; {1/2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-2} &amp; {4} &amp; {5/2} &amp; {-5} &amp; {-1/2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-1} &amp; {0} &amp; {21/4} &amp; {0} &amp; {-21/4} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  \\<br>  G=\left[\begin{array}{rrr}<br>  {1} &amp; {0} &amp; {0} \\<br>  {-2/9} &amp; {-2/9} &amp; {-2/9} \\<br>  {-2/9} &amp; {2/9} &amp; {-2/9} \\<br>  {1/90} &amp; {1/45} &amp; {2/45} \\<br>  {1/90} &amp; {-1/45} &amp; {2/45} \\<br>  {32/45} &amp; {16/45} &amp; {8/45} \\<br>  {32/45} &amp; {-16/45} &amp; {8/45} \\<br>  {0} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  \\<br>  A^{T}=\left[\begin{array}{rrrrrrrr}<br>  {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {2} &amp; {-2} &amp; {1/2} &amp; {-1/2} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {4} &amp; {4} &amp; {1/4} &amp; {1/4} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {8} &amp; {-8} &amp; {1/8} &amp; {-1/8} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {16} &amp; {16} &amp; {1/16} &amp; {1/16} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {32} &amp; {-32} &amp; {1/32} &amp; {-1/32} &amp; {1} \\<br>  \end{array}\right]<br>  $$<br>  $$<br>  g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>  d=\left[\begin{array}{llllll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}} &amp; {d_4} &amp; {d_5} &amp; {d_6} &amp; {d_7}\end{array}\right]^{T}<br>  $$  </li></ul><p>注意：与$F(2,3)$不同，由于$F(4,3)$和$F(6,3)$的$A^T$、$B^T$出现了非$[0,1,-1]$的元素，所以除了点乘过程，还会引入额外的乘法运算。      </p><h3 id="变换过程的计算优化"><a href="#变换过程的计算优化" class="headerlink" title="变换过程的计算优化"></a>变换过程的计算优化</h3><p>$F(4,3)$和$F(6,3)$的变换矩阵相当有规律，无论是输入、输出还是权重的变换都有多的中间结果能够被复用；<br>以$F(4,3)$为例，我们将一些相关联的元素标上颜色——<br><img src="/imgs/winograd/F43_trans_matrix.jpg" alt="F43_trans_matrix">    </p><p>$$<br>\begin{align}<br>d’_0 &amp;= 4d_0 - 5d_2 + d_4 \\<br>d’_1 &amp;= -(4d_1 - d_3) - (4d_2 - d_4) \\<br>d’_2 &amp;= (4d_1 - d_3) - (4d_2 - d_4) \\<br>d’_3 &amp;= -2(d_1 - d_3) - (d_2 - d_4) \\<br>d’_4 &amp;= 2(d_1 - d_3) - (d_2 - d_4) \\<br>d’_5 &amp;= 4d_1 - 5d_3 + d_5<br>\end{align}<br>$$</p><p>$$<br>\begin{align}<br>g’_0 &amp;= \frac{1}{4}g_0 \\<br>g’_1 &amp;= -\frac{1}{6}(g_0 + g_2) - \frac{1}{6}g_1 \\<br>g’_2 &amp;= -\frac{1}{6}(g_0 + g_2) + \frac{1}{6}g_1 \\<br>g’_3 &amp;= (\frac{1}{24}g_0 + \frac{1}{6}g_2) + \frac{1}{12}g_1 \\<br>g’_4 &amp;= (\frac{1}{24}g_0 + \frac{1}{6}g_2) - \frac{1}{12}g_1 \\<br>g’_5 &amp;= g_2<br>\end{align}<br>$$</p><p>$$<br>\begin{align}<br>y_0 &amp;= (m_1 + m_2) + (m_3 + m_4) + m_0  \\<br>y_1 &amp;= (m_1 - m_2) + 2(m_3 - m_4) \\<br>y_2 &amp;= (m_1 + m_2) + 4(m_3 + m_4) \\<br>y_3 &amp;= (m_1 - m_2) + 8(m_3 - m_4) + m_5<br>\end{align}<br>$$</p><blockquote><p>以下“运算”指乘加运算$y = a * b + c$——包括加法（$y = 1 * b + c$）、乘法（$y = a * b + 0$）<br>而$a * b + c * d$实际包含两次乘法和一次加法即3次乘加运算     </p></blockquote><p>$B^Td$总计12次运算，$Gg$总计8次运算，$A^TM$总计10次运算。<br>扩展到二维，$B^TdB$总计$12 \times (6+6) = 144$次运算，$GgG^T$总计$8 \times (3+6) = 72$次运算，$A^TMA$总计$10 \times (6 + 4) = 100$次运算。     </p><blockquote><p>论文《<a href="http://arxiv.org/pdf/1509.09308" target="_blank" rel="noopener">Fast algorithms for convolutional neural(CVPR2016)</a>》$F(4,3)$的$B^Td$为13次，可能是由于$d’_3$、$d’_4$的计算过程不同导致的，<br>$$<br>\begin{align}<br>d’_3 &amp;= -(2d_1 - 2d_3) - (d_2 - d_4) \\<br>d’_4 &amp;= (2d_1 - 2d_3) - (d_2 - d_4)<br>\end{align}<br>$$<br>如果按上述过程计算，会多出一次乘加运算。     </p></blockquote><p><strong>同理，对于$F(6,3)$，</strong><br><img src="/imgs/winograd/F63_trans_matrix.jpg" alt="F63_trans_matrix"><br>$B^Td$总计26次运算，$Gg$总计13次运算，$A^TM$总计20次运算。<br>扩展到二维，$B^TdB$总计$26 \times (8+8) = 416$次运算，$GgG^T$总计$13 \times (3+8) = 143$次运算，$A^TMA$总计$20 \times (8 + 6) = 280$次运算。     </p><center><img src="/imgs/winograd/complexity.jpg" alt="complexity"></center>     <ul><li>$\alpha$、$\beta$、$\gamma$、$\delta$分别为点乘的乘法复杂度、输入变换的运算复杂度、权重变换的运算复杂度、输出变换的运算复杂度    </li><li>以上数值都经过归一化<br>  $$<br>  \begin{align}<br>  \alpha’ &amp;= \frac{\alpha}{m^2} = \frac{(m+r-1)^2}{m^2} \\<br>  \beta’  &amp;= \frac{\beta}{(m+r-1)^2} = \frac{\beta}{\alpha} \\<br>  \gamma’ &amp;= \frac{\gamma}{(m+r-1)^2} = \frac{\gamma}{\alpha} \\<br>  \delta’ &amp;= \frac{\delta}{(m+r-1)^2} = \frac{\delta}{\alpha}<br>  \end{align}<br>  $$</li><li>$tile=3$为3x3的直接卷积，$tile=4$对应$F(2\times2,3\times3)$，$tile=6$对应$F(4\times4,3\times3)$，$tile=8$对应$F(6\times6,3\times3)$       </li></ul><h3 id="计算复杂度分析"><a href="#计算复杂度分析" class="headerlink" title="计算复杂度分析"></a>计算复杂度分析</h3><p>假设输入特征图与输出特征图大小一致，均为$H \times W$，卷积输入通道数$C$、输出通道数$K$，批大小为$N$：<br>点乘的乘法复杂度为：<br>$$<br>\begin{align}<br>M &amp;= \frac{\alpha}{m^2} NHWCK \\<br>  &amp;= \alpha’ NHWCK<br>\end{align}<br>$$</p><p>变换过程的运算复杂度为：<br>$$<br>\begin{align}<br>T(D) &amp;= \frac{\beta}{m^2} NHWC \\<br>T(F) &amp;= \gamma CK \\<br>T(I) &amp;= \frac{\delta}{m^2} NHWK<br>\end{align}<br>$$</p><p>归一化（保持和M一致的量纲）：<br>$$<br>\begin{align}<br>\frac{T(D)}{M} &amp;= \frac{\beta}{\alpha K} = \frac{\beta’}{K} \\<br>\frac{T(F)}{M} &amp;= \frac{\gamma m^2}{NHW \alpha^2} \\<br>               &amp;= \frac{\gamma}{P \alpha^2} = \frac{\gamma’}{P} \\<br>\frac{T(I)}{M} &amp;= \frac{\delta}{C \alpha^2} = \frac{\delta’}{C}<br>\end{align}<br>$$</p><p>那么，整个卷积过程的计算复杂度为：<br>$$<br>\begin{align}<br>L &amp;= (1 + \frac{T(D)}{M} + \frac{T(F)}{M} + \frac{T(I)}{M}) M \\<br>  &amp;= (1 + \frac{\beta’}{K} + \frac{\gamma’}{P} + \frac{\delta’}{C}) \alpha’ NHWCK<br>\end{align}<br>$$</p><p>事实上，    </p><ul><li>$T(F)$只需要最初的时候做一次，通常也不考虑进卷积计算的复杂度</li><li>$T(D)$只对卷积输入做一次，其计算复杂度会被输出通道数量$K$平摊      </li><li>$T(I)$只对卷积输出做一次，其计算复杂度会被输入通道数量$C$平摊</li><li>显然，随着$C$、$K$增大，$T(D)$、$T(I)$对整体运算复杂度的影响就会减小，Winograd带来的加速效果越明显；<br>同理，当$C$、$K$太小时，$T(D)$、$T(I)$对整体运算复杂度的影响会大于Winograd带来的加速效果，其计算效率反而不如直接卷积    </li></ul><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p>接下来我们对$F(2,3)$、$F(4,3)$、$F(6,3)$、$F(2\times2,3\times3)$、$F(4\times4,3\times3)$、$F(6\times6,3\times3)$的效果做一简单比较。      </p><table><thead><tr><th style="text-align:center">Winograd</th><th style="text-align:center">原始乘法次数</th><th style="text-align:center">Win乘法次数</th><th style="text-align:center">理论加速比</th><th style="text-align:center">原始内存</th><th style="text-align:center">Win内存</th><th style="text-align:center">内存增长</th></tr></thead><tbody><tr><td style="text-align:center">$F(2,3)$</td><td style="text-align:center">6</td><td style="text-align:center">4</td><td style="text-align:center">1.50</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">1.33</td></tr><tr><td style="text-align:center">$F(4,3)$</td><td style="text-align:center">12</td><td style="text-align:center">6</td><td style="text-align:center">2.00</td><td style="text-align:center">3</td><td style="text-align:center">6</td><td style="text-align:center">2.00</td></tr><tr><td style="text-align:center">$F(6,3)$</td><td style="text-align:center">18</td><td style="text-align:center">8</td><td style="text-align:center">2.25</td><td style="text-align:center">3</td><td style="text-align:center">8</td><td style="text-align:center">2.67</td></tr><tr><td style="text-align:center">堆$F(2\times2,3\times3)$</td><td style="text-align:center">36</td><td style="text-align:center">24</td><td style="text-align:center">1.50</td><td style="text-align:center">9</td><td style="text-align:center">12</td><td style="text-align:center">1.33</td></tr><tr><td style="text-align:center">堆$F(4\times4,3\times3)$</td><td style="text-align:center">144</td><td style="text-align:center">72</td><td style="text-align:center">2.00</td><td style="text-align:center">9</td><td style="text-align:center">18</td><td style="text-align:center">2.00</td></tr><tr><td style="text-align:center">堆$F(6\times6,3\times3)$</td><td style="text-align:center">324</td><td style="text-align:center">144</td><td style="text-align:center">2.25</td><td style="text-align:center">9</td><td style="text-align:center">24</td><td style="text-align:center">2.67</td></tr><tr><td style="text-align:center">嵌$F(2\times2,3\times3)$</td><td style="text-align:center">36</td><td style="text-align:center">16</td><td style="text-align:center">2.25</td><td style="text-align:center">9</td><td style="text-align:center">16</td><td style="text-align:center">1.78</td></tr><tr><td style="text-align:center">嵌$F(4\times4,3\times3)$</td><td style="text-align:center">144</td><td style="text-align:center">36</td><td style="text-align:center">4.00</td><td style="text-align:center">9</td><td style="text-align:center">36</td><td style="text-align:center">4.00</td></tr><tr><td style="text-align:center">嵌$F(6\times6,3\times3)$</td><td style="text-align:center">324</td><td style="text-align:center">64</td><td style="text-align:center">5.06</td><td style="text-align:center">9</td><td style="text-align:center">64</td><td style="text-align:center">7.11</td></tr></tbody></table><ul><li>从理论的乘法加速比来看（只考虑点乘的乘法），Winograd都有相当理想的加速效果，嵌套实现的$F(6\times6,3\times3)$甚至有5.06倍的加速</li><li>考虑到$V = B^{T} d B$和$Y = A^T M A$的计算过程中也有大量的乘法，实际乘法加速效果并没有那么高，但变换过程中带来的额外代价会被输入通道、输出通道所平摊     </li><li>$F(2,3)$和$F(2\times2, 3\times3)$非常有意思，其$A^T$矩阵和$B^T$矩阵的元素均为$[0,1,-1]$——也就是说在变换过程中不会产生额外的乘法开销</li><li>从额外的内存开销上来看（仅考虑参数占用的内存），二维大于一维、嵌套实现大于堆叠实现</li><li>从加速效果上看，二维大于一维、嵌套实现大于堆叠实现</li><li>尽管$V = B^{T} d B$和$Y = A^T M A$的计算过程中也有大量的乘法，但观察可以发现$F(4,3)$和$F(6,3)$的$A^T$矩阵和$B^T$中有相当多的元素恰好是$2^n$，也就是说，用Winograd计算量化的卷积应该会有神奇的加成（<em>对浮点运算来说，能否直接通过对指数部分做加法实现？？</em>）    </li></ul><h3 id="Winograd卷积"><a href="#Winograd卷积" class="headerlink" title="Winograd卷积"></a>Winograd卷积</h3><p>以上介绍了一维和二维的Winograd算法，但实际在神经网络的特征图却通常都是三维的，没法直接往上套。不过前文在介绍二维Winograd的时候，我们除了嵌套之外还用了堆叠一维Winograd来达到二维Winograd的结果，同样的也可以用堆叠的二维Winograd来将其应用到三维的卷积当中。<br>以k3s1卷积和$F(2 \times 2, 3 \times 3)$为例——    </p><p>标准的卷积过程：<br><img src="/imgs/winograd/standard_conv.gif" alt="standard">      </p><p>Winograd的卷积过程：<br><img src="/imgs/winograd/winograd_conv.gif" alt="winograd">      </p><ol><li>由于winograd要求每次都固定的输入大小（这里$tile=4$），所以需要pad到合适的大小    </li><li>每次输入上取一个tile（这里每个tile的大小为$4 \times 4$）并变换成V，与U做哈达马乘积运算后再反变换得到一组输出（这里每组输出大小为$2 \times 2$），每个tile以$m$为间隔（这里为2）      </li><li>最后得到一个$4 \times 4$的输出，由于最后一行和最后一列的由pad元素多算出来的，所以剪掉他们得到$3 \times 3$的最终输出    </li><li>这里演示的是如何用多次$F(2 \times 2, 3 \times 3)$完成单通道特征图的Winograd卷积，对于多通道的情况以此类推即可    </li></ol><p>前述只讨论了一些比较简单的情况，事实上在CNN中，由于输入的特征图只需要变换一次，而却会被多个滤波器复用，所以输入变换过程的额外开销会被平摊——卷积的滤波器（也即输出通道）越多，那么输入变换产生的额外开销的影响就越小。    </p><p>至于特征图的额外空间代价，嵌套实现的$F(m \times m,r \times r)$会产生$(\frac{m+r-1}{m})^2$倍的内存占用（而<a href="/2019/03/28/Conv-Family/#im2col">im2col</a>为$r^2$倍）。        </p><hr><p>本文gif动图由以下工具绘制获得——    </p><ul><li>Excel</li><li><a href="https://www.snipaste.com/" target="_blank" rel="noopener">Snipaste</a>（定点截图）</li><li><a href="https://www.screentogif.com/?l=zh_cn" target="_blank" rel="noopener">ScreenToGif</a>（动图制作）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNet全家桶</title>
      <link href="/2019/07/24/MobileNet-Family/"/>
      <url>/2019/07/24/MobileNet-Family/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1329129037&auto=0&height=32"></iframe><p>MobileNet自2017年发布v1以来就被广泛应用在移动端，随后又分别在2018年和2019年发布了v2和v3。去年《<a href="/2018/08/05/MobileNets_v1/">MobileNets v1模型解析 | Hey~YaHei！</a>》一文中已经讨论过v1的主要贡献，趁着前阵子<em>（已经是两个月前了其实）</em>刚刚发布v3，不妨把整个MobileNet家族放在一起稍作讨论。     </p><h3 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h3><p>论文：《<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(2017)</a>》     </p><h4 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>用深度可分离卷积（DW卷积提取特征+点卷积组合特征）取代传统的卷积，大幅提升特征提取的效率</li><li>进而利用深度可分离卷积设计出高效的直筒式网络MobileNet       </li></ol><h4 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h4><h5 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h5><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center>   <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用M个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>可以看到，传统的卷积中用 $N$ 个不同的卷积核不厌其烦地对原图进行特征提取来得到 $N$ 通道的输出，这其中必定从原图中提取到了大量的重复特征。有没有可能<strong>只用单个卷积核来做特征提取，最后依旧能输出多通道的特征图</strong>呢？——这就是深度向卷积分解的核心思路（或者换个角度，DW卷积就是分组卷积的一种极端情况）。<br>观察①阶段中的第三个子阶段，该阶段将多张单通道特征图按通道堆叠起来之后“拍扁”，如果去掉这个“拍扁”的过程，其实就可以提取得到一张 $M$ 通道的特征图啦，再经过一个 $M$ 维空间到 $N$ 维空间的线性映射，就能够和普通的卷积操作一样得到一张 $N$ 通道的特征图 $O$。完整的卷积过程如下图所示——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>①、②阶段与传统卷积①阶段的前两个阶段完全相同，③阶段比传统卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上</li></ol><h5 id="效率比较"><a href="#效率比较" class="headerlink" title="效率比较"></a>效率比较</h5><p><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><p>对于普通的卷积操作，        </p><ul><li>输出 $N$ 通道特征图需要 $N$ 个卷积核，故参数数量为 $M \times N \times D_K \times D_K$；     </li><li>一个 $D_K \times D_K$ 的卷积核在原图的某个位置的某个通道上需要进行 $D_K \times D_K$ 次乘加操作，输出特征图大小为 $D_O \times D_O$，原图通道数量为 $M$，共有 $N$ 个不同的卷积核，故乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M \times N$           </li></ul><p>对于深度向分解后的卷积操作，       </p><ul><li>特征提取只使用了一个 $D_K \times D_K$ 的卷积核，组合过程为了作线性映射用了 $N$ 个 $1 \times 1$ 的卷积核，故参数数量为 $M \times N + M \times D_K \times D_K$；       </li><li>特征提取过程只有一个卷积核，所以该过程乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M$。同样的，组合过程容易算得需要 $D_O \times D_O \times M \times N$ 次乘加操作。故乘加操作的总数量为 $D_O \times D_O \times M \times ( D_K \times D_K + N)$             </li></ul><p>总的来说，参数数量和乘加操作的运算量均下降为原来的 $\frac{1}{D_K^2} + \frac{1}{N}$，通常使用 $3 \times 3$ 的卷积核，也就是下降为原来的九分之一到八分之一左右。而从论文的实验部分来看，准确率也只有极小的下降。    </p><center><img src="/imgs/MobileNet/dw_vs_full.png" alt="dw_vs_full"></center>    <h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><center><img src="/imgs/MobileNet/v1_body_architecture.png" alt="v1_body_architecture"></center>    </p><ul><li>第一层使用普通的卷积层，最前端的特征提取非常重要，宁可存在重复的特征信息，也不该放掉；     </li><li>随后是一系列的深度向分解卷积层，用于逐层次提取特征，用步长不为1的卷积替代池化做降采样，同时整体也满足通道加深，特征图分辨率降低的CNN一般特点；        </li><li>最后也是常规的全局平均池化、全连接、Softmax；      </li><li>每次卷积操作之后都紧跟一个BN层（在预测阶段可以被合并）和一个RELU层；      <center><img src="/imgs/MobileNet/dw_vs_full_train.png" alt="dw_vs_full_train"></center>    </li><li>而且，深度向分解的卷积中绝大多数参数和运算都集中在 $1 \times 1$ 的pointwise卷积运算当中，这种运算恰恰是能够被 <code>GEneral Matrix Multiply(GEMM)</code> 函数直接实现而不需要经过im2col或im2row的；    <center><img src="/imgs/MobileNet/resource.png" alt="resource"></center>    </li><li>论文中还提到两个压缩模型的因子，分别用于输入图片分辨率收缩和网络宽度收缩来进一步精简模型；    </li><li>由于MobileNet本身为小模型，不容易过拟合，故不需要过多的正则化策略和数据增强策略；且Depthwise Convolution参数数量比较少，只需要加入一个很小的权重衰减甚至可以不需要权重衰减     </li></ul><h3 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h3><p>论文：《<a href="http://arxiv.org/pdf/1801.04381v3.pdf" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks(2018)</a>》         </p><h4 id="主要贡献-1"><a href="#主要贡献-1" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>取消通道收缩时的激活层：通道收缩时使用非线性激活会带来信息丢失      </li><li>将relu改为relu6以限制激活的输出范围       </li><li>引入反残差结构：MobileNet本身通道数量较少，引入“通道扩增-特征提取-通道收缩”的反残差结构有助于提高特征提取的能力    </li></ol><h4 id="通道收缩时使用非线性激活带来信息丢失"><a href="#通道收缩时使用非线性激活带来信息丢失" class="headerlink" title="通道收缩时使用非线性激活带来信息丢失"></a>通道收缩时使用非线性激活带来信息丢失</h4><p>用一个随机矩阵$T$，将一个初始的二维螺旋映射到一个$n$维空间后，经过一个非激活单元ReLU，再由$T^{-1}$反映射回二维空间。可以观察到，当n比较小（如n=2,3）时，反映射后的图形已经完全不像一个螺旋，出现明显的信息丢失：而当n比较大时，由于足够的冗余信息的存在，可以有效抵抗非线性单元带来的信息丢失：<br><img src="/imgs/MobileNet/relu_information_loss.jpg" alt="relu_information_loss">      </p><h4 id="反残差结构"><a href="#反残差结构" class="headerlink" title="反残差结构"></a>反残差结构</h4><p>v2参考了resnet，引入了shortcut设计，但不同的是：    </p><ul><li>resnet采用“通道收缩-特征提取-通道扩增”的残差结构，对输入特征进行压缩，也减少了特征提取的参数量和计算量；     </li><li>而MobileNet本身模型就不大，采用了相反的“通道扩增-特征提取-通道收缩”的反残差结构，以提高特征提取的能力      </li></ul><p><img src="/imgs/MobileNet/inverted_residual.jpg" alt="inverted_residual"><br><img src="/imgs/MobileNet/inverted_residual2.jpg" alt="inverted_residual2">     </p><p>同时可以看到，与残差结构不同，反残差结构在elt+的输入都是通道扩增前的小通道特征图，可以说对带宽是比较友好的。<br>对于通道扩增的倍数，论文里建议是取5-10，如MobileNet v2里取$t=6$。       </p><h4 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="/imgs/MobileNet/MobieleNet_v2.jpg" alt="MobieleNet_v2">        </p><h3 id="MobileNet-v3"><a href="#MobileNet-v3" class="headerlink" title="MobileNet v3"></a>MobileNet v3</h3><p>论文：《<a href="https://arxiv.org/pdf/1905.02244" target="_blank" rel="noopener">Searching for MobileNetV3(2019)</a>》      </p><h4 id="主要贡献-2"><a href="#主要贡献-2" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>用NAS搜索整体的网络架构    </li><li>用NetAdapt搜索合适的网络宽度    </li><li>引入注意力机制的SE模块    </li><li>引入h-swish激活函数</li></ol><h4 id="网络搜索"><a href="#网络搜索" class="headerlink" title="网络搜索"></a>网络搜索</h4><p>正如论文标题所示，MobileNet v3的结构主要是通过Network Architecture Search(NAS)和NetAdapt搜索出来的。不过也有手工调整的部分，比如最后一个stage，作者将全局平均池化提前，并取消其中繁冗的特征提取、通道收缩过程，从而进一步提高的模型的效率。<br><img src="/imgs/MobileNet/v3_last_stage.jpg" alt="v3_last_stage">         </p><h4 id="注意力机制：SE模块"><a href="#注意力机制：SE模块" class="headerlink" title="注意力机制：SE模块"></a>注意力机制：SE模块</h4><p>v3引入了《<a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">Squeeze-and-Excitation Networks(2017)</a>》的SE模块，通过全连接层对全局平均池化后的特征图进行通道重要性的评估，并将权重应用到特征图的通道上。<br><img src="/imgs/MobileNet/v3_block.jpg" alt="v3_block">        </p><h4 id="h-swish激活函数"><a href="#h-swish激活函数" class="headerlink" title="h-swish激活函数"></a>h-swish激活函数</h4><p>《<a href="http://arxiv.org/pdf/1702.03118" target="_blank" rel="noopener">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning(2017)</a>》提出了新型激活函数swish并取得不错的效果——<br>$$swish(x) = x \cdot \sigma(x)$$<br>其中$\sigma(\cdot)$为sigmoid函数，<br>$$\sigma(x)=\frac{1}{1+e^{-x}}$$     </p><p>不过sigmoid函数的指数运算终究太麻烦了，于是v3提出了hard-swish，通过分段函数ReLU6和相应的线性变换来近似sigmoid函数：<br>$$\text{h-swish}(x) = x \cdot \frac{ReLU6(x+3)}{6}$$      </p><p><strong>sigmoid和h-sigmoid、swish和h-swish的比较如下</strong>：<br><img src="/imgs/MobileNet/h-swish.jpg" alt="h-swish">      </p><p>作者发现，    </p><ol><li>近似版本的h-swish能起到跟swish十分接近的作用       </li><li>h-swish无论在软件还是硬件层面都非常容易实现，而且解决了量化网络时sigmoid的近似实现带来的数值精度损失问题    </li></ol><h4 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h4><p>左图为large版本，右图为small版本——<br><img src="/imgs/MobileNet/MobileNet_v3.jpg" alt="MobileNet_v3">      </p><ol><li>是否使用SE是platform-aware NAS权衡速度和精度之后做出的选择     </li><li>由于h-swish的速度比relu慢，而浅层的特征图比较大，所以网络的浅层更倾向于更快的relu，只有在深层使用h-swish；不过第一层卷积的通道数比较少，再加上这里h-swish替代relu有比较好的效果，所以v3还是倾向于第一层卷积依旧使用h-swish</li><li>尽管3x3卷积已经成为手工设计网络主流，但似乎5x5卷积也有用武之地</li></ol><h3 id="FD-MobileNet"><a href="#FD-MobileNet" class="headerlink" title="FD-MobileNet"></a>FD-MobileNet</h3><p>论文：《<a href="http://arxiv.org/pdf/1802.03750" target="_blank" rel="noopener">FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy(2018)</a>》     </p><p>Fast-Downsample MobileNet是MobileNet-v1的一个变种，采用快速下采样策略（提前下采样）来减小特征图尺寸，并增大网络宽度提高模型的特征提取能力，<strong>FD-MobileNet1.0和MobileNet0.5的比较</strong>——<br><img src="/imgs/MobileNet/FD-MobileNet.jpg" alt="FD-MobileNet">     </p><h3 id="ImageNet上的实验结果"><a href="#ImageNet上的实验结果" class="headerlink" title="ImageNet上的实验结果"></a>ImageNet上的实验结果</h3><h4 id="MobileNet-v1-1"><a href="#MobileNet-v1-1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h4><p><img src="/imgs/MobileNet/v1_experiment.jpg" alt="v1_experiment"></p><h4 id="MobileNet-v2-1"><a href="#MobileNet-v2-1" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h4><p><img src="/imgs/MobileNet/v2_experiment.jpg" alt="v2_experiment"></p><h4 id="MobileNet-v3-1"><a href="#MobileNet-v3-1" class="headerlink" title="MobileNet v3"></a>MobileNet v3</h4><p><img src="/imgs/MobileNet/v3_experiment.jpg" alt="v3_experiment"></p><h4 id="FD-MobileNet-1"><a href="#FD-MobileNet-1" class="headerlink" title="FD-MobileNet"></a>FD-MobileNet</h4><p><img src="/imgs/MobileNet/FD_experiment.jpg" alt="FD_experiment"></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>线性量化</title>
      <link href="/2019/07/23/Quantization/"/>
      <url>/2019/07/23/Quantization/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=638385&auto=0&height=32"></iframe>     <p>简单来说，量化就是将浮点存储（运算）转换为整型存储（运算）的一种模型压缩技术。如果按《<a href="https://www.jiqizhixin.com/articles/2018-05-22-9" target="_blank" rel="noopener">当前深度神经网络模型压缩和加速都有哪些方法？-机器之心</a>》一文的分类方式，量化属于参数共享的一种——将原始数据聚为若干类（比如int8量化为$2^8=256$类），量化后的整型值就相当于类的索引号。    </p><p>按照聚类中心是否均匀分布，可以把量化分为线性量化和非线性量化。       </p><ul><li>如《<a href="https://arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding(2016)</a>》就是典型的<strong>非线性量化</strong>，作者采用不规则的聚类中心，通过KMEANS对浮点权重进行量化，并在训练过程中逐步修正聚类中心——不规则的聚类中心往往能在相同的比特数下取得更小的量化误差，但由于必须先查表取得原始数据再进行计算，所以对计算的加速没有任何帮助，这种方法更多是用来压缩模型文件大小。   </li><li>与非线性量化不同，<strong>线性量化</strong>采用均匀分布的聚类中心，原始数据跟量化后的数据存在一个简单的线性变换关系。而卷积、全连接本身也只是简单的线性计算，因此在线性量化中可以直接用量化后的数据进行直接计算，不仅可以压缩模型文件的大小，还能带来明显的速度提升。        </li></ul><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ol><li><strong>更少的存储开销和带宽需求</strong><br> 使用更少的比特数存储数据，有效减少应用对存储资源的依赖（但现代系统往往拥有相对丰富的存储资源，这一点已经不算是采用量化的主要动机）     </li><li><strong>更快的计算速度</strong><br> 对多数处理器而言，整型运算的速度一般（但不总是）比浮点运算更快     </li><li><strong>更低的能耗与占用面积</strong><br> <img src="/imgs/Quantization/energy_and_area.jpg" alt="energy_and_area"><br> <em>数据来源：《<a href="https://media.nips.cc/Conferences/2015/tutorialslides/Dally-NIPS-Tutorial-2015.pdf" target="_blank" rel="noopener">High-Performance Hardware for Machine Learning(NIPS2015)</a>》</em><br> 从上图可以看到，FP32乘法的能耗是INT8乘法能耗的18.5倍，芯片占用面积则是27.3倍——对芯片设计和FPGA设计而言，更少的资源占用意味着相同数量的单元下可以设计出更多的计算单元；而更少的能耗意味着更少的发热，和更长久的续航。     </li><li><strong>尚可接受的精度损失</strong><br> 量化相当于对模型权重引入噪声，所幸CNN本身对噪声不敏感（甚至在训练过程中，模拟量化所引入的权重加噪还有利于防止过拟合），在合适的比特数下量化后的模型并不会带来很严重的精度损失。按照<a href="https://gluon-cv.mxnet.io/model_zoo/classification.html" target="_blank" rel="noopener">gluoncv</a>提供的报告，经过int8量化之后，ResNet50_v1和MobileNet1.0_v1在ILSVRC2012数据集上的准确率仅分别从77.36%、73.28%下降为76.86%、72.85%。</li><li><strong>趋势</strong><br> 无论是移动端还是服务器端，都可以看到新的计算设备正不断迎合量化技术。比如NPU/APU/AIPU等基本都是支持int8（甚至更低精度的int4）计算的，并且有相当可观的TOPs，而Mali GPU开始引入int8 dot支持，Nvidia也不例外——<br> <img src="/imgs/Quantization/GTX-vs-RTX-and-NPU.jpg" alt="GTX-vs-RTX-and-NPU"></li></ol><h3 id="线性量化"><a href="#线性量化" class="headerlink" title="线性量化"></a>线性量化</h3><p>常见的线性量化过程可以用以下数学表达式来表示：<br>$$r = Round(S(q - Z))$$<br>其中，<br>$q$ 是float32的原始值；<br>$Z$ 是float32的偏移量，也可以量化为int32；<br>$S$ 是float32的缩放因子；<br>$Round(\cdot)$ 是四舍五入近似取整的数学函数，除了四舍五入，向上、向下取整也是可以的；<br>$r$ 是量化后的一个整数值。       </p><p>而我们需要做的，就是确定合适的 $S$ 和 $Z$        </p><h4 id="对称和非对称"><a href="#对称和非对称" class="headerlink" title="对称和非对称"></a>对称和非对称</h4><p>参考：<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Algorithms - Quantization | Distiller</a><br>根据参数 $Z$ 是否为零可以将线性量化分为两类——对称和非对称。      </p><ul><li><p><strong>对称量化</strong><br>  <center><img src="/imgs/Quantization/symmetric-mode.png" alt="symmetric-mode"></center><br>  在对称量化中，$r$ 是用有符号的整型数值来表示的，此时 $Z=0$，且 $q=0$ 时恰好有 $r=0$。<br>  比如简单地取，<br>  $$S = \frac{2^{n-1} - 1}{max(|x|)}$$<br>  $$Z = 0$$<br>  其中，<br>  $n$ 是用来表示该数值的位宽，<br>  $x$ 是数据集的总体样本。       </p><p>  对称量化比较简单，不仅实现简单，而且由于$Z=0$运算也变得非常简单。      </p></li><li><p><strong>非对称量化</strong><br>  <center><img src="/imgs/Quantization/asymmetric-mode.png" alt="asymmetric-mode"></center><br>  比如简单地取，<br>  $$S = \frac{2^{n-1} - 1}{max(x)-min(x)}$$<br>  $$Z = min(x)$$<br>  非对称量化比较灵活，通常 $r$ 是用无符号的整型数值来表示，此时 $Z \neq 0$。        </p></li></ul><h4 id="逐层、逐组和逐通道"><a href="#逐层、逐组和逐通道" class="headerlink" title="逐层、逐组和逐通道"></a>逐层、逐组和逐通道</h4><p>按照量化的粒度（共享量化参数的范围）可以分为逐层、逐组和逐通道——     </p><ul><li><strong>逐层</strong>量化以一个层为单位，整个layer的权重共用一组缩放因子$S$和偏移量$Z$；      </li><li><strong>逐组</strong>量化以组为单位，每个group使用一组$S$和$Z$；</li><li><strong>逐通道</strong>量化则以通道为单位，每个channel单独使用一组$S$和$Z$。</li></ul><p>当 $group=1$ 时，逐组量化与逐层量化等价；<br>当 $group=num\_filters$ （即dw卷积）时，逐组量化逐通道量化等价      </p><h4 id="在线和离线"><a href="#在线和离线" class="headerlink" title="在线和离线"></a>在线和离线</h4><p>按照激活值的量化方式，可以分为在线（online）量化和离线（offline）量化。       </p><ul><li>在线量化指激活值的 $S$ 和 $Z$ 在实际推断过程中根据实际的激活值动态计算；    </li><li>离线量化指提前确定好激活值的 $S$ 和 $Z$      </li></ul><p>由于不需要动态计算量化参数，通常离线量化的推断速度更快些，通常有三种方法来确定相关的量化参数——      </p><ol><li><strong>指数平滑平均</strong><br> 将校准数据集投喂给模型，收集每个量化的层的输出特征图，计算每个batch的 $S$ 和 $Z$，并通过指数平滑平均更新 $S$ 和 $Z$     </li><li><strong>直方图截断</strong><br> 以$S = \frac{2^n - 1}{max(|x|)}$为例，由于有的特征图会出现偏离较远的奇异值，导致max非常大，所以可以通过直方图截取的形式，比如抛弃最大的前1%数据，以前1%分界点的数值作为max计算量化参数      </li><li><strong>KL散度校准</strong><br> 参考：《<a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="noopener">8-bit Inference with TensorRT</a>》<br> TensorRT的校准方案，通过KL散度（也称为相对熵，用以描述两个分布之间的差异）来评估量化前后分布的差异，搜索并选取KL散度最小的量化参数     </li></ol><h3 id="模拟量化实验"><a href="#模拟量化实验" class="headerlink" title="模拟量化实验"></a>模拟量化实验</h3><p>我在Gluon上用mobilenet和resnet50做了简单的模拟量化实验（量化卷积和全连接）：<a href="https://github.com/hey-yahei/Quantization.MXNet#simulate-quantization" target="_blank" rel="noopener">Quantization.MXNet | github</a>          </p><table><thead><tr><th style="text-align:center">IN dtype</th><th style="text-align:center">IN offline</th><th style="text-align:center">WT dtype</th><th style="text-align:center">WT qtype</th><th style="text-align:center">Merge BN</th><th style="text-align:center">w/o 1st conv</th><th style="text-align:center">M-Top1 Acc</th><th style="text-align:center">R-Top1 Acc</th></tr></thead><tbody><tr><td style="text-align:center">float32</td><td style="text-align:center">/</td><td style="text-align:center">float32</td><td style="text-align:center">/</td><td style="text-align:center"></td><td style="text-align:center">/</td><td style="text-align:center">73.28%</td><td style="text-align:center">77.36%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">44.57%</td><td style="text-align:center">55.97%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.84%</td><td style="text-align:center">76.92%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.92%</td><td style="text-align:center">76.90%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.72%</td><td style="text-align:center">77.00%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.58%</td><td style="text-align:center">76.81%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.66%</td><td style="text-align:center">76.71%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">15.21%</td><td style="text-align:center">76.62%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">32.70%</td><td style="text-align:center">76.61%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">14.70%</td><td style="text-align:center">76.60%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">47.80%</td><td style="text-align:center">56.21%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.93%</td><td style="text-align:center">77.33%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.85%</td><td style="text-align:center">77.31%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.68%</td><td style="text-align:center">77.35%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.63%</td><td style="text-align:center">77.22%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.68%</td><td style="text-align:center">77.08%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">72.75%</td><td style="text-align:center">77.11%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">72.04%</td><td style="text-align:center">76.69%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">72.67%</td><td style="text-align:center">77.07%</td></tr></tbody></table><ol><li>由于大部分网络使用relu作为激活函数，所以往往使用uint能获得更小的量化误差，不过也有的平台只支持int8的量化（比如ncnn）；     </li><li>离线量化的校准数据集从训练集抽取（每个类别抽取5张图片，共计5000张），naive校准采用指数平滑平均的方式收集最大值并计算量化参数，KL校准搜索KL散度最小的量化参数；</li><li>由于合并BN层后，不同通道的权重的分布差异变得更加显著，这对冗余网络（如resnet）跟逐通道量化的影响较小，但逐层量化紧凑网络（如mobilent）就直接崩溃（对dw卷积影响显著）；</li><li>大部分情况下naive校准和KL校准效果都挺接近，不过在合并BN后的逐通道量化下，KL校准则明显由于naive校准；</li></ol><p>为了进一步比较naive校准和KL校准，在cifar_resnet56_v1模型上又补充了几个简单实验：     </p><table><thead><tr><th style="text-align:center">IN dtype</th><th style="text-align:center">WT dtype</th><th style="text-align:center">WT qtype</th><th style="text-align:center">Merge BN</th><th style="text-align:center">w/o 1st conv</th><th style="text-align:center">Top1 Acc@naive</th><th style="text-align:center">Top1 Acc@KL</th></tr></thead><tbody><tr><td style="text-align:center">float32</td><td style="text-align:center">float32</td><td style="text-align:center">/</td><td style="text-align:center"></td><td style="text-align:center">/</td><td style="text-align:center">93.60%</td><td style="text-align:center">93.60%</td></tr><tr><td style="text-align:center">uint6</td><td style="text-align:center">int6</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">93.09%</td><td style="text-align:center">93.83%</td></tr><tr><td style="text-align:center">uint5</td><td style="text-align:center">int5</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">92.71%</td><td style="text-align:center">93.29%</td></tr><tr><td style="text-align:center">uint4</td><td style="text-align:center">int4</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">91.62%</td><td style="text-align:center">89.27%</td></tr><tr><td style="text-align:center">uint3</td><td style="text-align:center">int3</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">81.75%</td><td style="text-align:center">55.98%</td></tr></tbody></table><p>可以看到，    </p><ul><li>在比特数还比较高（量化误差比较小）的情况下，KL校准明显优于naive校准；     </li><li>但当比特数比较低的时候（量化误差变大）的情况下，KL校准的优势逐渐缩小，甚至最后崩溃            </li></ul><h3 id="重训练量化实验"><a href="#重训练量化实验" class="headerlink" title="重训练量化实验"></a>重训练量化实验</h3><p>当量化的比特数比较大的情况下（如8bits），逐通道量化、KL校准已经能取得很不错的效果；但当比特数非常小的时候，量化带来的精度损失还是比较明显的，此时我们可以按照《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2017)</a>》做简单的恢复训练：<a href="https://github.com/hey-yahei/Quantization.MXNet#quantization-aware-training" target="_blank" rel="noopener">Quantization.MXNet | github</a>          </p><p><strong>BN层的处理</strong>：<br>通常我们会把训练好的模型的BN层融合到卷积层当中来减少推断时间，量化卷积权重之前也需要先把BN层融合进去，但是在训练过程中我们又需要BN层来抑制过拟合，那么可以在训练过程中引入伪BN——<br>正常的BN是对卷积的输出特征图进行操作，而按照合并BN的思路，伪BN则是直接对权重进行操作。先用BN的参数与原始权重进行计算取得“融合”后的权重，再用新的权重进行卷积，在训练过程中正常更新原始的卷积权重和BN的参数。         </p><center><img src="/imgs/Quantization/fake_bn.jpg" alt="fake_bn"></center>      <p>使用伪BN后，每次前向传播需要两次卷积运算——     </p><ol><li>用原始权重卷积，卷积输出用于更新伪BatchNorm的平均值、标准差，这次卷积运算不需要反向传播；</li><li>另外一次用量化过的权重卷积，卷积输出的结果作为下一层的输入，这一次的卷积运算需要反向传播</li></ol><p>这样一来，训练的速度会明显降低，但却保留的BN本身的效果。      </p><p><strong>4bits量化的恢复训练效果如下</strong>：    </p><table><thead><tr><th style="text-align:center">DataType</th><th style="text-align:center">QuantType</th><th style="text-align:center">Offline</th><th style="text-align:center">Retrain</th><th style="text-align:center">FakeBN</th><th style="text-align:center">Top-1 Acc</th></tr></thead><tbody><tr><td style="text-align:center">fp32/fp32</td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">93.60%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">layer</td><td style="text-align:center">naive</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">84.95%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">layer</td><td style="text-align:center">KL</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">73.36%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">90.77%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">channel</td><td style="text-align:center">naive</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">91.62%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">channel</td><td style="text-align:center">KL</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">89.27%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">93.19%</td></tr></tbody></table><ol><li>由于比特数较低，KL校准的效果变得非常差；    </li><li>相比于naive校准，恢复训练总能取得更低一些的精度损失    </li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>2019中兴捧月·总决赛</title>
      <link href="/2019/05/25/zte_challenge_final/"/>
      <url>/2019/05/25/zte_challenge_final/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=618661&auto=0&height=32"></iframe><h2 id="赛题背景"><a href="#赛题背景" class="headerlink" title="赛题背景"></a>赛题背景</h2><p>与<a href="/2019/05/22/zte_challenge_preliminary/">初赛</a>类似，不过初赛更多关注的是<strong>加速</strong>，而总决赛更关注的是<strong>压缩</strong>。<br>原始模型是一个简单的3x112x112输入大小的resnet18，人脸识别项目，主办方提供了两万张无标签的校准数据集，和两千张带标签的本地验证数据集，同时主办方保留两千张私有、不公开的测试数据集。<br>评分规则如下：<br>$$ score = \left( \left( \frac { M - m } { M } \right) \times 20 + \left( \frac { S - s } { s } \right) \times 80 \right) \times A ( z ) \times B ( z )$$<br>$$A ( z ) = \left\{ \begin{array} { l l } { 1 , } &amp; { z \geq 0.97 } \\ { 0.9 , } &amp; { 0.965 \leq z &lt; 0.97 } \\ { 0 , } &amp; { z &lt; 0.965 } \end{array} \right.$$<br>$$B ( z ) = \left\{ \begin{array} { l l } { 1 , } &amp; { s \leq 40 M B } \\ { 0.9 , } &amp; { 40 M B &lt; s \leq 50 M B } \\ { 0.8 , } &amp; { 50 M B &lt; s \leq 63 M B } \\ { 0 , } &amp; { s &gt; 63 M B } \end{array} \right.$$      </p><p>其中，$M$、$S$分别为原始模型的内存占用大小、模型文件大小，$m$、$s$、$z$分别是压缩后的内存占用大小、模型文件大小、万分之一误检率下的正检率。主要参照本地验证数据集（一千个人，每人两张图片）的z值，私有测试集只用于模型泛化能力的参考（防止选手故意过拟合于验证集）。       </p><h2 id="压缩方案"><a href="#压缩方案" class="headerlink" title="压缩方案"></a>压缩方案</h2><p>总体思路参考论文《<a href="http://www.arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding(2016)</a>》，采用剪枝-量化-哈夫曼编码三步走的压缩策略。       </p><center><img src="/imgs/ZTE_Challenge/DeepCompression.png" alt="DeepCompression"></center>      <h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><h4 id="粒度"><a href="#粒度" class="headerlink" title="粒度"></a>粒度</h4><p>按照论文《<a href="http://arxiv.org/pdf/1705.08922" target="_blank" rel="noopener">Exploring the Regularity of Sparse Structure in Convolutional Neural Networks(2017)</a>》，剪枝的粒度按不规则（非结构）到规则（结构）可以分为fine-grained（精细的神经元层面的剪枝，也称为netron）、vector(向量，以卷积核的某一行或列为单位)、kernel（核，以输入通道为单位）、filter（滤波器，以输出通道为单位），除此之外其实还有以layer或block为单位进行裁剪，但相对比较暴力，实用中比较少见。    </p><center><img src="/imgs/ZTE_Challenge/PruningLevel.png" alt="PruningLevel"></center>     <p>在上述粒度中，最为常见的是<strong>fine-grained(netron)</strong>和<strong>filter</strong>，前者因为采用精细化的裁剪，往往可以取得更高的压缩率，但需要搭配稀疏存储、稀疏计算技术使用；后者有比较高的结构性，往往配合以<strong>kernel</strong>为单位的裁剪（前一层裁掉filter后，后一层可以相应的裁剪kernel），不依赖于额外的存储和计算技术，而且有直接、明显的速度提升，适合加速网络。<br>本次赛题以<strong>压缩</strong>为主，故自然而然地采用<strong>fine-grained(netron)</strong>的剪枝方案。         </p><h4 id="剪枝标准"><a href="#剪枝标准" class="headerlink" title="剪枝标准"></a>剪枝标准</h4><p>在<strong>fine-grained(netron)</strong>级别的剪枝中通常采用某个阈值作为剪枝标准，最简单的阈值可以通过人为设置，也可以设置一个剪枝的百分比。而论文《<a href="http://arxiv.org/pdf/1506.02626" target="_blank" rel="noopener">Learning both Weights and Connections for Efficient Neural Networks(2015)</a>》则采用<strong>敏感度（sensitivity）</strong>作为剪枝的标准——<br>$$sensitivity = std ( \text {weight} ) * s$$      </p><p>计算十分简单，直接统计一个层里权重的标准差，然后乘以一个人为设定的系数$s$作为剪枝的阈值。         </p><center><img src="/imgs/ZTE_Challenge/WeightsVolume.jpg" alt="WeightsVolume"></center>     <p>众所周知，     </p><ol><li>神经网络的第一层卷积比较敏感；    </li><li>全连接层的冗余性远高于卷积层         </li></ol><p>所以我简单的分了三档系数，以普通的卷积层的剪枝系数为$s$，分别设定第一层卷积、最后的全连接层为$0$和$2s$。<br><em>事实上，第一层卷积主要是对量化比较敏感，还是可以轻裁的，只不过我后续把稀疏存储和哈弗曼编码写在了一起，如果只裁剪不量化就需要拆解代码，加上第一层卷积只有1728个参数（相比之下全连接层可是有足足8,388,608个参数），剪枝的压缩空间非常小，所以这里索性不对第一层卷积进行裁剪。</em>    </p><h4 id="恢复训练"><a href="#恢复训练" class="headerlink" title="恢复训练"></a>恢复训练</h4><p>如果能对剪枝后的模型进行简单的训练，模型可以有效的恢复精度。而本次比赛只给了两万张无标签的校准数据，常规的训练是行不通的，但既然有原始模型，我们不妨采用<strong>知识蒸馏</strong>的策略对剪枝后的模型进行恢复训练。       </p><p>向原始模型依次投喂这两万张数据，并保存其输出作为恢复训练的标签；<br>loss可以采用简单的距离度量，比如L2、cosine，还可以采用KL散度（又称为相对熵）——实践表明，<strong>KL散度的效果略优于L2和cosine</strong>。决赛时也有大佬融合了L2和cosine作为loss，我自己尚未做过测试，不知道相比之下效果如何。        </p><p>恢复训练通常有两种形式，   </p><ol><li>直接一刀剪枝，然后一次性fine-tune到最佳效果；    </li><li>逐层剪枝，每次剪枝后都进行fine-tune到最佳效果再进行下一次剪枝；      </li></ol><center><img src="/imgs/ZTE_Challenge/PruningRetrain.png" alt="PruningRetrain"></center>      <p>前一种方式简单粗暴，但无疑第二种方式往往可以取得比较好的结果。可第二种方式往往也是最费时的，比赛时间有限，所以我采取了论文《<a href="http://arxiv.org/pdf/1710.01878" target="_blank" rel="noopener">To prune, or not to prune: exploring the efficacy of pruning for model compression(2017)</a>》用的折中方案——每次多剪一点点，然后简单的fine-tune（但不fine-tune到最佳效果），最后达到目标剪枝结果后再进行彻底的fine-tune。       </p><h4 id="稀疏存储"><a href="#稀疏存储" class="headerlink" title="稀疏存储"></a>稀疏存储</h4><p>netron级别的剪枝往往需要搭配稀疏存储和稀疏运算来实现，比如对于密集的矩阵数据存储方式，每个非零数值可以改为<strong>(行序, 列序, 数值)</strong>的三元组进行存储，甚至可以展平后按<strong>(索引, 数值)</strong>的二元组进行存储，只要稀疏度足够高，这种存储方式就能获得收益。<br>论文《<a href="http://www.arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding(2016)</a>》甚至对该方式进行了简单的改进，采用二元组存储，但使用<strong>相对索引</strong>而不是<strong>绝对索引</strong>——        </p><center><img src="/imgs/ZTE_Challenge/SparseStorage-RelativeIndex.png" alt="SparseStorage-RelativeIndex"></center>     <p>当非零元素之间的距离超过最大值时，通过补0值的方式来保证相对索引的正常工作。         </p><h3 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h3><p>按量化后数值的分布进行简单地划分，量化可以分为<strong>均匀分布的量化</strong>和<strong>非均匀分布的量化</strong>，前者因为可以将浮点运算转换为整型运算而大幅提高模型推理速度，所以更为常见；后者不得不依赖查表运算，对推理速度的提升毫无帮助，但由于量化过程中聚类中心（可以把量化看成一种权重共享，聚集成$2^n$类）不再需要“均匀分布”这一约束，往往能对量化后的模型造成更小的损失，也意味着可以采用更低位数的量化方式。        </p><center><img src="/imgs/ZTE_Challenge/UniformQuantization.jpg" alt="UniformQuantization"></center>      <p>与剪枝类似，在训练过程中融入模拟量化有助于减少量化造成的模型精度损失，也即论文《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2018)</a>》提到的<strong>Quantization-Aware Training</strong>，先前在《<a href="/2019/01/23/MXNet-RT_Quantization/">MXNet上的重训练量化 | Hey~YaHei!</a>》一文中有所提及，这里就不再赘述。       </p><p>非均匀分布的量化的训练过程则有些不同，如论文《<a href="http://www.arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding(2016)</a>》采用KMEANS进行聚类，训练过程中用量化后的权重前向传播，反向传播时则将所有梯度按类别分组求和，最后乘以学习率（也即SGD方式）来更新聚类中心。     </p><center><img src="/imgs/ZTE_Challenge/QuantizationAwareTraining.png" alt="QuantizationAwareTraining"></center>     <p><em>原本在参加比赛前已经写好了训练代码，在有硬标签的ImageNet上工作正常，但到了决赛现场换成知识蒸馏的方式后训练就不断出现问题，最后比赛时间有限也没来得及解决，所以量化这一块由于没用上重训练，也没有做出很好的效果。</em>           </p><h3 id="哈夫曼编码"><a href="#哈夫曼编码" class="headerlink" title="哈夫曼编码"></a>哈夫曼编码</h3><p>哈夫曼编码是根据数值出现的频次分配不等长的位数进行表示的压缩编码方式，与量化乃是天然的技术组合，也广泛应用在各类文件压缩技术当中。     </p><center><img src="/imgs/ZTE_Challenge/HuffmanConstruction.gif" alt="HuffmanConstruction"></center>     <p>以数值<strong>0、1、2</strong>为例，假设<strong>0</strong>值的出现频率远高于<strong>1、2</strong>，那么如果构建如下图所示的哈夫曼树：      </p><center><img src="/imgs/ZTE_Challenge/HuffmanTree.png" width="200"></center>       <p>将0编码为0b0，将1编码为0b10，将2编码为0b11；此时0值只需要一个bit就能表示，当0值出现频率足够高时，则整体的数据串具有压缩的效果，如——   </p><center><img src="/imgs/ZTE_Challenge/HuffmanEncode.jpg" width="300"></center>       <h2 id="压缩效果"><a href="#压缩效果" class="headerlink" title="压缩效果"></a>压缩效果</h2><p>应用剪枝、量化、哈夫曼编码后，模型大小从<strong>74.5MB</strong>减少到<strong>6.9MB</strong>，验证集上精度（万分之一误检率下的正检率）仅从<strong>97.3%</strong>下降为<strong>97.2%</strong>，各层剪枝情况、量化位数、哈夫曼编码后的平均位数、整体压缩率如下表所示：         </p><table><thead><tr><th style="text-align:center">Layer</th><th style="text-align:center">Sparsity</th><th style="text-align:center">Weight Bits</th><th style="text-align:center">Weight Bits(H)</th><th style="text-align:center">Index Bits</th><th style="text-align:center">Index Bits(H)</th><th style="text-align:center">Rate(P+Q)</th><th style="text-align:center">Rate(P+Q+H)</th></tr></thead><tbody><tr><td style="text-align:center">conv1</td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center">–　</td><td style="text-align:center">–　</td><td style="text-align:center">–　</td><td style="text-align:center">–</td><td style="text-align:center">–</td></tr><tr><td style="text-align:center">res2a_1</td><td style="text-align:center">77.56%</td><td style="text-align:center">7</td><td style="text-align:center">8.21</td><td style="text-align:center">5</td><td style="text-align:center">3.35</td><td style="text-align:center">91.59%</td><td style="text-align:center">91.89%</td></tr><tr><td style="text-align:center">res2a_2a</td><td style="text-align:center">82.18%</td><td style="text-align:center">7</td><td style="text-align:center">6.69</td><td style="text-align:center">5</td><td style="text-align:center">2.65</td><td style="text-align:center">93.32%</td><td style="text-align:center">94.79%</td></tr><tr><td style="text-align:center">res2a_2b</td><td style="text-align:center">67.48%</td><td style="text-align:center">7</td><td style="text-align:center">6.18</td><td style="text-align:center">5</td><td style="text-align:center">2.36</td><td style="text-align:center">87.81%</td><td style="text-align:center">91.32%</td></tr><tr><td style="text-align:center">res2b_2a</td><td style="text-align:center">55.63%</td><td style="text-align:center">7</td><td style="text-align:center">6.23</td><td style="text-align:center">5</td><td style="text-align:center">2.13</td><td style="text-align:center">83.36%</td><td style="text-align:center">88.40%</td></tr><tr><td style="text-align:center">res2b_2b</td><td style="text-align:center">53.96%</td><td style="text-align:center">7</td><td style="text-align:center">6.18</td><td style="text-align:center">5</td><td style="text-align:center">1.96</td><td style="text-align:center">82.73%</td><td style="text-align:center">88.28%</td></tr><tr><td style="text-align:center">res3a_1</td><td style="text-align:center">58.36%</td><td style="text-align:center">7</td><td style="text-align:center">6.83</td><td style="text-align:center">5</td><td style="text-align:center">2.40</td><td style="text-align:center">84.39%</td><td style="text-align:center">87.99%</td></tr><tr><td style="text-align:center">res3a_2a</td><td style="text-align:center">48.08%</td><td style="text-align:center">7</td><td style="text-align:center">5.97</td><td style="text-align:center">5</td><td style="text-align:center">1.91</td><td style="text-align:center">80.53%</td><td style="text-align:center">87.21%</td></tr><tr><td style="text-align:center">res3a_2b</td><td style="text-align:center">52.51%</td><td style="text-align:center">7</td><td style="text-align:center">5.80</td><td style="text-align:center">5</td><td style="text-align:center">2.04</td><td style="text-align:center">82.19%</td><td style="text-align:center">88.35%</td></tr><tr><td style="text-align:center">res3b_2a</td><td style="text-align:center">52.68%</td><td style="text-align:center">7</td><td style="text-align:center">5.85</td><td style="text-align:center">5</td><td style="text-align:center">2.07</td><td style="text-align:center">82.25%</td><td style="text-align:center">88.28%</td></tr><tr><td style="text-align:center">res3b_2b</td><td style="text-align:center">56.35%</td><td style="text-align:center">7</td><td style="text-align:center">5.76</td><td style="text-align:center">5</td><td style="text-align:center">2.04</td><td style="text-align:center">83.63%</td><td style="text-align:center">89.37%</td></tr><tr><td style="text-align:center">res4a_1</td><td style="text-align:center">54.75%</td><td style="text-align:center">7</td><td style="text-align:center">6.17</td><td style="text-align:center">5</td><td style="text-align:center">2.21</td><td style="text-align:center">83.03%</td><td style="text-align:center">88.14%</td></tr><tr><td style="text-align:center">res4a_2a</td><td style="text-align:center">47.52%</td><td style="text-align:center">7</td><td style="text-align:center">5.53</td><td style="text-align:center">5</td><td style="text-align:center">1.90</td><td style="text-align:center">80.32%</td><td style="text-align:center">87.82%</td></tr><tr><td style="text-align:center">res4a_2b</td><td style="text-align:center">52.11%</td><td style="text-align:center">7</td><td style="text-align:center">6.01</td><td style="text-align:center">5</td><td style="text-align:center">2.06</td><td style="text-align:center">82.04%</td><td style="text-align:center">87.92%</td></tr><tr><td style="text-align:center">res4b_2a</td><td style="text-align:center">48.99%</td><td style="text-align:center">7</td><td style="text-align:center">5.70</td><td style="text-align:center">5</td><td style="text-align:center">1.96</td><td style="text-align:center">80.87%</td><td style="text-align:center">87.80%</td></tr><tr><td style="text-align:center">res4b_2b</td><td style="text-align:center">51.80%</td><td style="text-align:center">7</td><td style="text-align:center">5.49</td><td style="text-align:center">5</td><td style="text-align:center">1.99</td><td style="text-align:center">81.93%</td><td style="text-align:center">88.73%</td></tr><tr><td style="text-align:center">res5a_1</td><td style="text-align:center">53.95%</td><td style="text-align:center">7</td><td style="text-align:center">5.92</td><td style="text-align:center">5</td><td style="text-align:center">2.14</td><td style="text-align:center">82.73%</td><td style="text-align:center">88.41%</td></tr><tr><td style="text-align:center">res5a_2a</td><td style="text-align:center">48.01%</td><td style="text-align:center">7</td><td style="text-align:center">5.75</td><td style="text-align:center">5</td><td style="text-align:center">1.92</td><td style="text-align:center">80.51%</td><td style="text-align:center">87.54%</td></tr><tr><td style="text-align:center">res5a_2b</td><td style="text-align:center">46.93%</td><td style="text-align:center">7</td><td style="text-align:center">5.64</td><td style="text-align:center">5</td><td style="text-align:center">1.88</td><td style="text-align:center">80.10%</td><td style="text-align:center">87.51%</td></tr><tr><td style="text-align:center">res5b_2a</td><td style="text-align:center">48.41%</td><td style="text-align:center">7</td><td style="text-align:center">5.67</td><td style="text-align:center">5</td><td style="text-align:center">1.94</td><td style="text-align:center">80.65%</td><td style="text-align:center">87.73%</td></tr><tr><td style="text-align:center">res5b_2b</td><td style="text-align:center">49.19%</td><td style="text-align:center">7</td><td style="text-align:center">5.57</td><td style="text-align:center">5</td><td style="text-align:center">1.94</td><td style="text-align:center">80.95%</td><td style="text-align:center">88.08%</td></tr><tr><td style="text-align:center">fc5</td><td style="text-align:center">73.97%</td><td style="text-align:center">4</td><td style="text-align:center">3.33</td><td style="text-align:center">5</td><td style="text-align:center">3.19</td><td style="text-align:center">92.68%</td><td style="text-align:center">94.69%</td></tr><tr><td style="text-align:center">total</td><td style="text-align:center">59.79%</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">85.97%</td><td style="text-align:center">90.81%</td></tr></tbody></table><p>注意到<strong>res2a_1</strong>的<strong>Weigths Bits(H)</strong>，其哈夫曼编码后占用的空间反而比直接的紧凑存储（七位紧凑存储，而非按字节存储）高，这是因为其编码前的数值出现频次相对均衡造成的（构建的哈夫曼树会是一棵平衡树或相对平衡的树）。      </p><h2 id="关于内存"><a href="#关于内存" class="headerlink" title="关于内存"></a>关于内存</h2><p>起初一听说决赛会考量运行时的内存占用大小，立马就想起了<strong>直接卷积</strong>——天下没有免费的午餐，任何加速算法都需要额外的代价，而这个代价往往就是额外的占用空间，卷积也是如此——还有什么比最朴素的for循环卷积更省空间的吗？        </p><center><img src="/imgs/ZTE_Challenge/ConvolutionAlgorithm.png" width="500"></center>      <p>再加上KMEANS的量化方式不得不采用查表法实现推理，在比赛前我就用纯C++写好了一个神经网络……甚至越陷越深，试图取巧地用紧凑存储的方式把权重存储在内存上（本来想做稀疏存储的，但时间来不及）。       </p><p>最后评委也没认可我这种查表法的处理方式（摊手.jpg），而且只看“加载权重后的内存占用”而不看“前向推理的内存占用”，所以决赛时内存部分也没拿到几分……唉~       </p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>答辩PPT：<a href="/imgs/ZTE_Challenge/阿尔法-总决赛答辩-YaHei.pdf">点击下载</a>      </p><p>一直都是一个人瞎捣鼓着模型压缩的东西，碰巧看到有这么一个比赛所以想去试试，起初也没想过能拿奖，摸着摸着初赛竟然拿到第一，决赛虽然有些遗憾<!--（如果早点跟评委沟通清楚，早些知道查表法不被接受，及时止损改用常规的均匀分布的量化方式，兴许还能跟jh竞争一下——还是太追求极致、不切实际的压缩，没有考虑实际工程应用吧）-->但也已经远远超出最初的预期，而且在比赛过程中学习效率极高，又认识了非常多可爱的人儿，专家组、HR、还有所有的选手都相当棒！！还认识了非常优秀的一等奖大佬（我大三那会儿可啥都不会，jh真是太强了！），——已经十分满足了哈~(๑¯∀¯๑)        </p><p><em>这里的6.9MB也不是最优的结果——首先决赛疏忽大意，剪枝前忘记做一下SVD；此外，剪枝部分给量化预留了太多的空间，事实上还能多裁几刀；按照经验，即使是用上均匀分布的量化，通过重训练应该也能用更少的bit位数（而不是7bits和4bits）来进一步压缩——个人估计压缩到5MB以内应该也没啥问题。</em>    </p><p><strong><em>应主办方要求，总决赛这部分只能公开一下文档，代码就不便开源啦。</em></strong><br>DeepCompression的实现可以稍微参考一下<a href="https://github.com/mightydeveloper/Deep-Compression-PyTorch" target="_blank" rel="noopener">Deep-Compression-PyTorch | Github, mightydeveloper</a>，但他有些部分做的不够完美（比如稀疏存储部分没有使用论文的相对索引导致最终的模型偏大，而且也没实现量化的重训练）。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>2019中兴捧月·初赛</title>
      <link href="/2019/05/22/zte_challenge_preliminary/"/>
      <url>/2019/05/22/zte_challenge_preliminary/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=514375&auto=0&height=32"></iframe>       <h2 id="赛题背景"><a href="#赛题背景" class="headerlink" title="赛题背景"></a>赛题背景</h2><p>自从 Alex Krizhevsky 夺得 ILSVRC 2012 ImageNet 图像分类竞赛的冠军后，深度卷积神经网络在图像分类、物体检测、语义分割、目标跟踪等多个计算机视觉任务中均取得优秀表现，更有甚者在某些领域超越人眼的精度，但在其不断逼近计算机视觉任务的精度极限的同时，其深度和尺寸也在成倍增长，随之而来的较高的设备资源占用、较低计算效率等问题，使其只能在有限的平台下使用，难以移植到移动端和嵌入式芯片当中，因此模型小型化与加速成了亟待解决的问题。本竞赛旨在针对一个给定的深度卷积网络，在不进行深度卷积网络调优训练的条件下，不限定优化的方式，输出新的网络模型，达到降低模型大小，降低运行态设备资源占用，提高运行速度的效果。        </p><p><strong>评分细则</strong>：<br>$$Score = {\frac{X-x}{X} \times 20 + \frac{Y-y}{Y} \times 80} \times A(z)$$<br>$$\mathbf { A } ( \mathbf { z } ) = \left\{ \begin{array} { c l } { \mathbf { 1 } , } &amp; { \mathbf { 0 . 9 8 } \leq \mathbf { z } \leq \mathbf { 1 } } \\ { 0.95 , } &amp; { 0.95 \leq \mathbf { z } &lt; 0.98 } \\ { 0.9 , } &amp; { 0.9 \leq \mathbf { z } &lt; 0.95 } \\ { \mathbf { 0 } , } &amp; { \mathbf { z } &lt; \mathbf { 0 } .9 } \end{array} \right.$$       </p><p>其中，$X$、$Y$分别为原始模型的显存占用大小和推理时间，$x$、$y$、$z$分别为优化后的显存占用大小、推理时间、与原始输出的平均余弦距离（1000张测试图片）。      </p><h2 id="压缩方案"><a href="#压缩方案" class="headerlink" title="压缩方案"></a>压缩方案</h2><h3 id="安全操作"><a href="#安全操作" class="headerlink" title="安全操作"></a>安全操作</h3><p>首先进行一些简单的、接近无损的压缩优化操作。         </p><h4 id="层融合"><a href="#层融合" class="headerlink" title="层融合"></a>层融合</h4><p>通常而言，两个连续的线性操作是可以合并的，而Convolution和BatchNorm的融合是最典型的例子，只需要简单的数学计算就能将BathchNorm的计算过程融合到Convolution中。更多细节可以参见：《<a href="/2018/08/08/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析 - BN层合并 | Hey~YaHei!</a>》<br>具体代码参考：<a href="https://github.com/hey-yahei/ZTE_Challenge2019_MOA/blob/master/merge_bn.ipynb" target="_blank" rel="noopener">merge_bn | github, YaHei</a>     </p><h4 id="清除闲置、无效的Filter"><a href="#清除闲置、无效的Filter" class="headerlink" title="清除闲置、无效的Filter"></a>清除闲置、无效的Filter</h4><p>论文《<a href="https://arxiv.org/pdf/1608.08710.pdf" target="_blank" rel="noopener">Pruning Filters for Efficient Convnets(2017-ICLR)</a>》指出，可以通过计算Filter的L1范数（即绝对值之和）来衡量Filter的重要性。       </p><center><img src="/imgs/ZTE_Challenge/PruneChannel.jpg" alt="PruneChannel"></center>        <p>在这个思路的基础上，我们可以计算每个Filter绝对值的平均值（规范化数值，避免Filter大小不一带来的影响），并设置一个极小的阈值（如1e-5）来对Filter进行裁剪，同时由于卷积的输出通道减少，后续卷积层对应的Kernel也可以随之剔除（全连接层类似）。<br>具体代码参考：<a href="https://github.com/hey-yahei/ZTE_Challenge2019_MOA/blob/master/clear_idle_filters.ipynb" target="_blank" rel="noopener">clear_idle_filters | github, YaHei</a>         </p><h4 id="更换Caffe的Backend"><a href="#更换Caffe的Backend" class="headerlink" title="更换Caffe的Backend"></a>更换Caffe的Backend</h4><p>BLVC/Caffe-gpu为Convolution、Pooling、ReLU等提供了Caffe、cuDNN两种Backend，可以通过prototxt的<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto#L619" target="_blank" rel="noopener">engine参数</a>来指定，而比赛模型默认使用的是cuDNN。首先可以将所有engine替换为Caffe，借助caffe-time工具比较替换前后各层的推理速度，可以发现<strong>大多数卷积和ReLU用Caffe作为Backend推理耗时更短，但也存在例外</strong>。      </p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>从<a href="https://github.com/hey-yahei/ZTE_Challenge2019_MOA/blob/master/models/origin/TestModel.prototxt" target="_blank" rel="noopener">模型</a>上看，Conv1*是一个类似inception的结构；Conv2*是一个类似VGG的卷积堆叠同时加上了Concat的跨层特征融合；Conv3*、Conv4*、COnv5*均为残差结构。但值得注意的是，<strong>Conv5*</strong>竟然全都是1x1卷积，可以猜测Conv5*很可能没学到多少东西。        </p><h4 id="耗时概览"><a href="#耗时概览" class="headerlink" title="耗时概览"></a>耗时概览</h4><p>本次比赛显存优化、速度优化分数占比分别为20%和80%，显然速度优化更为重要，先用caffe-time工具测试各层的推理速度，然后用python解析caffe-time工具的输出获取模型各结构的耗时情况：        </p><center><img src="/imgs/ZTE_Challenge/TimeCost.jpg" alt="TimeCost"></center>       <h4 id="Filters分析"><a href="#Filters分析" class="headerlink" title="Filters分析"></a>Filters分析</h4><p>将所有卷积核沿Filter方向求取绝对均值，绘制成直方图，观察卷积<strong>输出通道</strong>的重要性；     </p><h4 id="Kernels分析"><a href="#Kernels分析" class="headerlink" title="Kernels分析"></a>Kernels分析</h4><p>将所有卷积核沿Kernel方向求取绝对均值，绘制成直方图，观察卷积<strong>输入通道</strong>的重要性；        </p><h4 id="Feature-Maps分析"><a href="#Feature-Maps分析" class="headerlink" title="Feature Maps分析"></a>Feature Maps分析</h4><p>由于本次比赛没有提供数据集，但告知了数据的处理方式（减去127.5并除以128），故向模型投喂随机样张，统计各通道的失活率（输出值为0的概率）；<br>参考论文：《<a href="https://arxiv.org/pdf/1607.03250.pdf" target="_blank" rel="noopener">Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures(2016)</a>》     </p><h3 id="有损的压缩优化"><a href="#有损的压缩优化" class="headerlink" title="有损的压缩优化"></a>有损的压缩优化</h3><p>从先前的分析可以了解到，残差由于其结构的特殊性不易裁剪压缩，但有趣的是 conv5*使用了大量的 1x1 卷积，而且从filters上看conv5*存在大量低效的通道，而 feature maps 也显示 conv5*各输出通道普遍有极高的失活率， 加上 conv5*与全连接层相邻，显然 conv5*具备极大的压缩优化空间；而 conv3*、 conv4*在 filters、 kernels、 feature maps的分析上则显得很紧凑，似乎可操作空间非常小。除了残差外， conv1*、 conv2*这些只由 Convolution 和 Concat 组成的结构还是有一些压缩优化空间的。         </p><h4 id="Block-Prune-conv5"><a href="#Block-Prune-conv5" class="headerlink" title="Block Prune - conv5*"></a>Block Prune - conv5*</h4><p>按理说，不经过重训练直接丢弃整个block是很危险的，但从先前的分析，尤其是在Feature Map上可以看到，Conv5*的存在相当的多余。     </p><center><img src="/imgs/ZTE_Challenge/Conv5APoZ.jpg" alt="Conv5APoZ"></center>         <p>可以看到res_conv5_4_2和res_conv5_6_2的输出通道都有极高的失活率，而res_conv5_1_2和res_conv5_2_2的失活率也不低。加之丢弃block的操作十分诱人——不仅少了一整个block的计算量，又少了层与层之间的延迟；不仅如此，Conv5*之后紧连着全连接层，如果能够同时丢弃conv5_3*、conv5_4*或者conv5_5*、conv5_6*，那么全连接层的神经元数量也可以大幅减小。         </p><ol><li>首先尝试丢去conv5_3*、conv5_4*、conv5_5*、conv5_6*，可以看到线上的z值仅下降到0.983（此时甚至还没有被惩罚）；      </li><li>继续丢去conv5_2*，线上z值还有0.972（低于0.980，有5%的总分惩罚）；      </li><li>再继续丢去conv5_1*，线上z值则骤减；但如果只保留conv5_1_1b，线上z值依旧还有0.947；       </li></ol><p>显然丢弃conv5_3*、conv5_4*、conv5_5*、conv5_6*是极其有利的，此时由于残差的shortcut结构，Concat的三个bottom全指向了res_conv5_2_2，那么不妨直接取消Concat，并将全连接层的三组权重（分别对应每一个bottom）直接叠加得到新的权重，极大地减小了全连接层的计算量和参数量；    </p><p>虽然只丢弃conv5_3*、conv5_4*、conv5_5*、conv5_6*所取得的z值可以避免惩罚，但考虑到其他有损的压缩优化还能进一步提升，死守0.980似乎没有必要，因此conv5_2*也可以果断丢弃，而conv5_1*损失较大，不宜随便丢弃。       </p><h4 id="Layer-Prune-conv2"><a href="#Layer-Prune-conv2" class="headerlink" title="Layer Prune - conv2*"></a>Layer Prune - conv2*</h4><p>受先前conv5*的block prune启发，发现其实conv2*也明显存在类似的情况。      </p><center><img src="/imgs/ZTE_Challenge/Conv2APoZ.jpg" alt="Conv2APoZ"></center>         <p>失活率整体靠右， 同时在后述的逐通道试裁和随机搜索中发现， conv2_7 和 conv2_8确实表现的十分多余， 尝试丢弃后线上 z 值也仅从 0.983 下降到 0.977，不仅丢弃了两层的计算量， Concat 后的 conv3_1_1 和 conv3_1_1b 也获益不少。         </p><h4 id="奇异值分解（SVD）-全连接层"><a href="#奇异值分解（SVD）-全连接层" class="headerlink" title="奇异值分解（SVD） - 全连接层"></a>奇异值分解（SVD） - 全连接层</h4><p>用 SVD 将一层全连接层分解为两个全连接层串联，通过对权重作奇异值分解可以绘制出 sigma 的累加曲线——        </p><center><img src="/imgs/ZTE_Challenge/SVD_Sigma.jpg" alt="SVD_Sigma"></center>         <p>可以看到当 r 值比较大的时候， sigma 累加值占整体求和值的比例都很高，当低于某个值后则开始迅速下降，大致推测最佳收益的 r 值大概在 180-200 附近（实际操作中如果有多余的 z 值可供挥霍，下降至 140、 150 附近也是可行的）。           </p><h4 id="Channel-Prune"><a href="#Channel-Prune" class="headerlink" title="Channel Prune"></a>Channel Prune</h4><p>通道裁剪的关键在于找到合适的衡量通道重要性的标准，如——    </p><ol><li>从输出通道的角度，计算Filter的绝对均值；      </li><li>从输入通道的角度，计算Kernel的绝对均值；       </li><li>输入样张，统计输出特征图的失活率；      </li><li>用SVD的U矩阵作为作为通道重要性的参考；      </li><li>逐通道试裁：逐一删除通道，并在本地测试对模型精度的影响；      </li><li>随机搜索可裁剪通道：每次挑选一个或一组通道裁剪，并在本地测试对精度的影响，若输出损失足够小，则使裁剪生效，否则回溯        </li><li>……        </li></ol><p>注意方法3、5、6依赖于测试数据，由于比赛中实际的数据集不明确，其裁剪结果只具备一定的参考价值，不宜全盘接受。       </p><h3 id="一些提升不明显的小手脚"><a href="#一些提升不明显的小手脚" class="headerlink" title="一些提升不明显的小手脚"></a>一些提升不明显的小手脚</h3><h4 id="不同分支卷积合并计算后切片"><a href="#不同分支卷积合并计算后切片" class="headerlink" title="不同分支卷积合并计算后切片"></a>不同分支卷积合并计算后切片</h4><p>观察 conv1*可以发现 data 被 split 成三份分别送往 conv1_1_1、 conv1_2_1 和conv1_3_1，而且 conv1_1_1 和 conv1_3_1 是完全相同的卷积计算（kernel_size=3,stride=2, pad=1），故可以将两者合并运算后再切片分别送往下一级，虽然 slice 增加了额外的开销，但由于卷积合并运算总体耗时反而有少量的下降：          </p><center><img src="/imgs/ZTE_Challenge/Conv1Merge.jpg" alt="Conv1Merge"></center>        <h4 id="相同分支的卷积合并"><a href="#相同分支的卷积合并" class="headerlink" title="相同分支的卷积合并"></a>相同分支的卷积合并</h4><p>做过空闲通道裁剪之后， conv1_2_1 的卷积输出通道仅为 2，显然计算过程中 GPU 上会有大量的 ALU 空闲，事实上并不友好。有趣的是， conv1_2_1 的 feature maps显示，其失活率极低（两个通道的失活率分别为 0.1%和 0.37%），也就是说非线性层 ReLU 在这作用不大，可以直接丢弃——那么 conv1_2_1 和 conv1_2_2 两个线性层就直接相连了。众所周知，相邻线性计算是可合并的（典型的如 Convolution 和 BatchNorm），故将 conv1_2_1 和 conv1_2_2 合并为 5x5 卷积，虽然表面上 MAC 增加了（2.5M=&gt;4.6M），但由于层延迟的减小和并行度的增加，反而在整体上有了微小的提升。这里注意到两个 3x3 卷积都是带 padding 的，合并到 5x5 卷积后边缘部分的计算结果会和原始结果有较大出入，但所幸此处特征图尺寸高达 128x128，而且层的位置较浅，该误差就几乎可以忽略不计了。conv1*变为——</p><center><img src="/imgs/ZTE_Challenge/Conv1Merge2.jpg" alt="Conv1Merge2"></center>         <h4 id="推迟ReLU"><a href="#推迟ReLU" class="headerlink" title="推迟ReLU"></a>推迟ReLU</h4><p>conv1_2_2和conv1_3_3都需要ReLU后再传递给Concat，既然如此为什么不直接把ReLU直接推迟到Concat之后一口气做呢？         </p><center><img src="/imgs/ZTE_Challenge/Conv1DeferReLU.jpg" alt="Conv1DeferReLU"></center>     <h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>就模型优化学习而言，我也算是个新手（在这方面，以前只做过简单的<a href="https://github.com/hey-yahei/Quantization.MXNet" target="_blank" rel="noopener">线性量化</a>），一开始我只懂得层融合，经过不断地查阅资料、尝试，慢慢挤进第一梯队，再后来运气好通过修改engine一跃到榜首。之后不断有人私戳我与我交流，从大佬们那学来了不少小技巧，学会SVD、学会裁剪等等……整个初赛阶段通过交流、实践使自己的能力和成绩不断提升，受益匪浅。</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>漫谈卷积层</title>
      <link href="/2019/03/28/Conv-Family/"/>
      <url>/2019/03/28/Conv-Family/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=760533&auto=0&height=32"></iframe>         <p>去年刚入手深度学习的时候写过一篇《<a href="/2018/04/18/CNN/">卷积神经网络CNN | Hey~YaHei!</a>》，简单介绍过卷积层——不过经过一年的学习和实践，对卷积又有了新的认识，原本讲道理应该直接更新修改去年的那篇博文的。但是那篇博文涉及面较广，不单单讲卷积，而且之后还写过几篇引用了那篇博文的内容，拾起来魔改似乎会打乱博客的结构，想想还是新开一篇，希望各位看官别嫌弃我炒冷饭。       </p><p>本文要点：    </p><ol><li>提出卷积层的背景；</li><li>标准卷积的过程和原理；</li><li>标准卷积的计算原理（im2）；</li><li>高效卷积：分组卷积、深度向分解卷积、点向卷积；</li><li>其他卷积的变种：空洞卷积和变形卷积</li></ol><hr><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>参考：《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>》Chap13       </p><p>卷积的起源有点仿生的味道，这基于生理心理学上的研究，人在感知视觉的时候具有     </p><ol><li><strong>层次性</strong>，底层的视神经往往能感知简单的图像结构，随着层次的提高，高层的视神经可以感知到更加抽象的图形结构最后乃至图形的概念。举个栗子，最底层的视神经能够感知简单的点，随着层次的提高，逐渐能感受线、简单的线段组合、乃至有概念的图像如鱼、房子等；       <center><img width="600" src="/imgs/Conv-Family/bg-hierachy.png"></center>     </li><li><strong>局部性</strong>，视神经具备有限的感受野，能够将注意力集中的视觉上的局部区域，感受野随着神经网络的深入逐步扩大；     <center><img width="400" src="/imgs/Conv-Family/bg-local.png"></center>     </li></ol><h2 id="标准卷积（Standard-Convolution）"><a href="#标准卷积（Standard-Convolution）" class="headerlink" title="标准卷积（Standard Convolution）"></a>标准卷积（Standard Convolution）</h2><p>参考：    </p><ul><li>《<a href="/2018/04/18/CNN/#卷积层（Conv）">卷积神经网络CNN - 卷积层（Conv）| Hey~YaHei!</a>》     </li><li>《<a href="/2019/01/07/MXNet-OpSummary/#卷积">模型参数与计算量 - 卷积 | Hey~YaHei!</a>》</li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li></ul><h3 id="过程和原理"><a href="#过程和原理" class="headerlink" title="过程和原理"></a>过程和原理</h3><p>卷积层并不是使用严格数学意义的卷积运算，而是使用保留卷积性质但抛弃可交换性的互相关函数；<br><em>卷积运算具有可交换性，这在数学证明上是很有用的，但在神经网络的应用中却是一个比较鸡肋的性质</em><br>卷积操作选用一定大小的卷积核（下图黄色区域）在原始数据上移动，与重合部分数据做乘和运算；<br>依次类推，最终输出一张特征图（Feature Map）     </p><center><img width="400" src="/Handson-ML/12Convolutional_Kernel.gif"></center>       <p>卷积核的作用相当一个滤波器，其参数是经过学习得到的，可以用于提取图片中的特征；<br>由于核参数是随机初始化的，所以它们很可能会提取出不同的特征；<br>由低层的卷积层提取简单特征，然后逐层堆叠卷积层，将简单特征逐渐抽象为更高层次的语义概念；      </p><h3 id="参数和计算量"><a href="#参数和计算量" class="headerlink" title="参数和计算量"></a>参数和计算量</h3><p>首先考虑一个单通道输入输出，输出图大小为 $m_o \times n_o$，核大小为 $k_m \times k_n$，带偏置，步长1，不补零的卷积，       </p><p><center><img height="300" src="/imgs/MXNet-OpSummary/convolution.gif"></center><br>$$<br>\begin{aligned} O _ { 11 } = &amp; w _ { 11 } I _ { 11 } + w _ { 12 } I _ { 12 } + w _ { 13 } I _ { 13 } + \\ &amp; w _ { 21 } I _ { 21 } + w _ { 22 } I _ { 22 } + w _ { 23 } I _ { 23 } + \\ &amp; w _ { 31 } I _ { 31 } + w _ { 32 } I _ { 32 } + w _ { 33 } I _ { 33 } + b \end{aligned}<br>$$<br>其参数数量为 $k_m \times k_n + 1$，乘加次数为 $m_o \times n_o \times k_m \times k_n$。       </p><p>推广到$c_i$通道输入，$c_o$通道输出的情况，<br>其参数数量为 $(k_m \times k_n + 1) \times c_i \times c_o$，乘加次数为 $m_o \times n_o \times k_m \times k_n \times c_i \times c_o$。       </p><h3 id="卷积核大小"><a href="#卷积核大小" class="headerlink" title="卷积核大小"></a>卷积核大小</h3><p>通常来说，大核卷积可以被多层小核卷积所替代；<br>比如用三层3x3卷积核的卷积层可以提取到一层7x7卷积核的卷积层一样的特征——<br><img src="/Handson-ML/12Multi_Conv_Layers.png" alt="12Multi_Conv_Layers"><br>而且，使用多层小核卷积层由以下优势：    </p><ol><li>减少参数<br> 7x7卷积核有 $7 \times 7 = 49$ 个参数，而三层3x3卷积核只有 $3 \times 3 \times 3 = 27$ 个参数      </li><li>增加网络深度<br> 增加网络容量和复杂度    </li></ol><p>博文《<a href="/2019/02/28/bag-of-tricks2/#改进2：拆解大核卷积">深度学习小技巧（二）：模型微调 - 改进2：拆解大核卷积 | Hey~YaHei!</a>》也曾介绍过，改进版的Inception和ResNet目前都已经将7x7卷积层替换成了三层3x3卷积层的堆叠，这里就不再赘述。        </p><h2 id="卷积的计算原理"><a href="#卷积的计算原理" class="headerlink" title="卷积的计算原理"></a>卷积的计算原理</h2><p>参考：《<a href="/2019/02/28/bag-of-tricks2/#高效的1x1卷积">深度学习小技巧（二）：模型微调 - 高效的1x1卷积 | Hey~YaHei!</a>》      </p><p>卷积的计算方式可以分为两种——一是<strong>直接按照原理计算</strong>，二是采用<strong>im2col技术进行矩阵乘法</strong>；     </p><h3 id="直接卷积的潜在问题"><a href="#直接卷积的潜在问题" class="headerlink" title="直接卷积的潜在问题"></a>直接卷积的潜在问题</h3><p>直接卷积非常直观易懂，但它对目前流行的计算设备来说极其不友好。     </p><p>首先考虑3x3的单通道特征图，以及k2s1的卷积核——      </p><div><center><img src="/imgs/bag-of-tricks/conv_raw.jpg" alt="conv_raw"></center></div>       <p>按照卷积计算，<br>$$y_{11} = w_{11}x_{11} + w_{12}x_{12} + w_{21}x_{21} + w_{22}x_{22}$$<br>$$y_{12} = w_{11}x_{12} + w_{12}x_{13} + w_{21}x_{22} + w_{22}x_{23}$$<br>$$y_{21} = w_{11}x_{21} + w_{12}x_{22} + w_{21}x_{31} + w_{22}x_{32}$$<br>$$y_{22} = w_{11}x_{22} + w_{12}x_{23} + w_{21}x_{32} + w_{22}x_{33}$$</p><p>按照“行先序”，特征图和卷积核在内存中是这样排列的——       </p><div><center><img src="/imgs/bag-of-tricks/conv_store.jpg" alt="conv_store"></center></div>       <p>我们用不同的颜色标注出卷积计算中的访存过程（相同颜色的数据相乘）——       </p><div><center><img src="/imgs/bag-of-tricks/conv_access_ram.jpg" alt="conv_access_ram"></center></div>      <p>众所周知，由于程序的<strong>局部性原理</strong>（通常相邻代码段会访问相邻的内存块），现代处理器通常会按块从内存中读取数据到高速缓存中以缓解访存速度和计算速度的巨大差异导致的“<strong>内存墙</strong>”问题。换句话说，如果计算需要从内存中读取<code>x12</code>的数据，那么往往相邻的<code>x11</code>、<code>x13</code>等数据也会被一起读取到高速缓存上，当下次计算需要用到<code>x11</code>或<code>x13</code>时处理器就可以快速地从高速缓存中取出数据而不需要从内存中调取，大大提高了程序的速度。<br><em>注：L1缓存的读取速度是RAM的50-100倍！（数据来源：《<a href="https://book.douban.com/subject/7006537/" target="_blank" rel="noopener">计算机体系结构：量化研究方法</a>》）</em>       </p><p>而从上边展示出来的访存过程中可以看到，直接对于特征图数据的访问过程十分散乱，直接用行先序存储的特征图参与计算是非常愚蠢的选择。      </p><h3 id="im2col"><a href="#im2col" class="headerlink" title="im2col"></a>im2col</h3><p><center><img src="/imgs/bag-of-tricks/conv_im2col.png" alt="conv_im2col"></center><br>其实思路非常简单：把每一次循环所需要的数据都排列成列向量，然后逐一堆叠起来形成矩阵（按通道顺序在列方向上拼接矩阵）。<br>比如$C_i \times W_i \times H_i$大小的输入特征图，$K \times K$大小的卷积核，输出大小为$C_o \times W_o \times H_o$，<br>输入特征图将按需求被转换成$(K*K)\times(C_i*W_o*H_o)$的矩阵，卷积核将被转换成$C_o\times(K*K)$的矩阵，调用GEMM库两矩阵相乘也就完成了所谓的卷积计算。由于按照计算需求排布了数据顺序，每次计算过程中总是能够依次访问特征图数据，迎合了局部性原理，极大地提高了计算卷积的速度！     </p><p>我用NDArry（<em>MXNet提供的一个类似numpy的数学计算库</em>）实现了一个基于im2col卷积层，除了支持基本的标准卷积外，还支持分组卷积以及量化卷积的模拟运算（int8权重、uint8输入、int32的累积）——<br>实现代码：<a href="https://github.com/hey-yahei/Quantization.MXNet/blob/master/nn/quantized_conv.py#L13" target="_blank" rel="noopener">Quantization.MXNet/nn/quantized_conv.py#L13 | github, hey-yahei</a><br>测试代码：<a href="https://github.com/hey-yahei/Quantization.MXNet/blob/master/tests/test_quantized_conv.py" target="_blank" rel="noopener">Quantization.MXNet/tests/test_quantized_conv.py | github, hey-yahei</a>        </p><p>不过要注意的是，<strong>im2col也不一定就比直接卷积好</strong>！！     </p><ol><li>首先，im2col需要额外的数据重组过程，当一个卷积层非常小（主要是通道比较窄）的时候，直接卷积反而可能会比im2col快（尽管现代网络一般都会用比较宽的通道）；    </li><li>im2col在重组数据的时候需要对数据产生大量的副本（约 $K^2$ 倍的内存占用）；</li><li>当采用1x1卷积的时候，im2col和直接卷积的过程其实是等价的；      </li><li>im2col的目的是迎合程序局部性原理，如果使用了定制化的硬件（比如用FPGA直接实现3x3的卷积运算单元），那么im2col就失去了意义而且反而增加了开销        </li></ol><p><em>卷积实现的方式还有很多，暂且只介绍im2col，之后有空会独立开一篇文章细谈。</em><br>《<a href="/2019/08/21/winograd_convolution/">Winograd卷积原理 | Hey~YaHei!</a>》      </p><h2 id="高效卷积"><a href="#高效卷积" class="headerlink" title="高效卷积"></a>高效卷积</h2><p>早在去年的《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》一文中，我们就分解了标准卷积的工作方式——      </p><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><div><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center></div>     <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用M个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>其中 Ⅲ 阶段野蛮地将 $M$ 张特征图拍扁成一张特征图，似乎显得不那么优雅且存在大量的冗余，于是开始有人研究如何让卷积过程变得高效，就出现了<strong>分组卷积</strong>和<strong>深度向分解卷积</strong>。       </p><h3 id="分组卷积（Groupwise-Convolution）"><a href="#分组卷积（Groupwise-Convolution）" class="headerlink" title="分组卷积（Groupwise Convolution）"></a>分组卷积（Groupwise Convolution）</h3><p>分组卷积最初是《<a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks(2012)</a>》提出，作者最初是出于并行目的（作者有三张仅3G显存的GTX580）把网络拆解到多张显卡上进行训练。顾名思义，<strong><em>分组卷积是对输入通道进行分组，分别施加卷积，最后将卷积结果堆叠起来——也可以理解为将原来的一层卷积等分成若干并行的支路，最后再堆叠汇合</em></strong>。<br><img src="/imgs/Conv-Family/groupwise convolution.jpg" alt="groupwise convolution"><br>如图所示，传统卷积的 Ⅲ 被拆解成两部分（分组卷积示意图中的Ⅲ和Ⅳ），不再简单的拍扁成一张特征图，而是分成G组分别拍扁，最后堆叠得到G张特征图。这样做有两个好处：    </p><ol><li>传统卷积中Ⅲ阶段的输入包含Ⅱ阶段的所有输出，而分组卷积Ⅲ阶段被分割为G组，每组的输入都是独立的——简而言之，Ⅲ阶段可以分散到多个不同的计算设备上完成而不需要频繁的通信（最终Ⅳ阶段再汇总到主设备上，甚至可以暂存在不同设备上，数据汇总推迟到Ⅴ阶段）<br><em>不过目前主流框架似乎没有针对分组卷积做充分的优化，而是简单地对输入、卷积核分组，然后逐一使用标准卷积来实现；一般使用分组卷积的时候也就分个两三组，问题倒是不大，不过到了把分组做到极致的深度向卷积，这个问题就凸显出来，在GPU上专门优化过的深度向卷积要比用分组方式实现的卷积快10倍左右，而CPU并行能力有限，这一问题并不明显，这一点早前已经在《<a href="/2018/08/24/mssd-try2/#Depthwise-Convolution和Standard-Convolution-Group-的比较">基于MobileNet-SSD的目标检测Demo（一） | Hey~YaHei!</a>》一文中有所提及。</em>      </li><li>参数数量和乘加次数均下降为标准卷积的1/G         </li></ol><p>简单的实现方法可以参考我的 <a href="https://github.com/hey-yahei/Quantization.MXNet/blob/master/nn/quantized_conv.py#L57" target="_blank" rel="noopener">Quantization.MXNet/nn/quantized_conv.py#L57 | github, hey-yahei</a>。     </p><h3 id="深度向卷积分解（Depthwise-Separable-Convolution）"><a href="#深度向卷积分解（Depthwise-Separable-Convolution）" class="headerlink" title="深度向卷积分解（Depthwise Separable Convolution）"></a>深度向卷积分解（Depthwise Separable Convolution）</h3><p>标准卷积同时聚合了所有通道信息和所有空间信息，分组卷积则同时聚合了所有通道信息和部分空间信息，深度向卷积分解直接割裂了通道信息和空间信息的聚合过程，从形式上看则将标准卷积分解为一次深度向卷积和一次点向卷积——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>这一过程又称为深度向卷积（Depthwise Convoluon），是分组卷积的一种特例（每个通道作为一组）；<br>①、②阶段与标准卷积①阶段的前两个阶段完全相同，③阶段比标准卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>这一过程称又称为点向卷积（Pointwise Convolution）；<br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上         </li></ol><p>此时参数数量和乘加次数均下降为标准卷积的 $\frac{1}{D_K^2} + \frac{1}{N}$，不仅如此，由于深度向卷积分解中有 $\frac{N}{D_K^2 +N}$ 的乘加计算集中在1x1卷积上，而通常 $N&gt;&gt;D_K^2$ 也即绝大多数乘加计算由1x1卷积贡献（比如Mobilenet中有94.86%的乘加计算集中在1x1卷积上），所以其计算效率是非常高的。         </p><h3 id="异构卷积（HetConv）"><a href="#异构卷积（HetConv）" class="headerlink" title="*异构卷积（HetConv）"></a>*异构卷积（HetConv）</h3><p>论文：《<a href="https://arxiv.org/pdf/1903.04120.pdf" target="_blank" rel="noopener">HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs(2019)</a>》<br>印度三哥提出来的一种卷积形式，论文里的实验结果非常可观，但似乎没啥影响力（有人说三哥论文数据爱造假？），作者没有开源也没见人复现过，姑且再放着看看吧。      </p><p><center><img src="/imgs/Conv-Family/HetConv.jpg" alt="HetConv"></center><br>思路倒也简单，有点不定长数组的味道。     </p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>高效卷积具有较强的特征提取效率、较高的并行能力，基于这些特点，高效卷积通常被应用于两种场合：     </p><ol><li>注重<strong>速度</strong>，搭建参数数量、乘加次数均比较少的网络，如MobileNet等；    </li><li>注重<strong>性能</strong>，用高效卷积替代标准卷积的同时扩张卷积的数量（直到接近标准卷积的规模），如Xception、ResNeXt等       </li></ol><h2 id="其他卷积变种"><a href="#其他卷积变种" class="headerlink" title="其他卷积变种"></a>其他卷积变种</h2><h3 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h3><p>论文：《<a href="https://arxiv.org/pdf/1511.07122.pdf" target="_blank" rel="noopener">Multi-scale Context Aggregation by Dilated Convolutions(2015)</a>》    </p><p>为卷积核增加空洞来扩大感受野，比如      </p><ul><li>3x3的卷积，增加1的空洞，可以在不增加参数数量和运算量的情况下，取得等同5x5卷积的感受野；    </li><li>同理，3x3的卷积，增加2的空洞，可以取得等同7x7卷积的感受野       </li></ul><div><center><img src="/imgs/Conv-Family/Dilated Convolution.jpg" alt="Dilated Convolution"></center></div>       <p>简而言之，就是通过牺牲卷积的“分辨率”，来获取更大的“视野”；<br>原理很简单，却很有效，目前广泛应用在语义分割领域。       </p><h3 id="变形卷积"><a href="#变形卷积" class="headerlink" title="变形卷积"></a>变形卷积</h3><p>论文：《<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable Convolutional Networks(2017)</a>》     </p><p>这两年特别火的一种卷积，利用一个辅助的标准卷积根据input拟合出offset，然后根据offset在input上进行带偏移的采样，最后施加卷积运算——与名字不同，并不是对卷积核进行变形，而是对采样点进行偏移。       </p><p><div><center><img src="/imgs/Conv-Family/Deformable Convolution.jpg" alt="Deformable Convolution"></center></div><br>这种卷积广泛存在于各种目标检测、语义分割比赛的冠军方案中。     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>快如闪电的人脸检测——Tengine+libfacedetection</title>
      <link href="/2019/03/21/Tengine-libfacedetection/"/>
      <url>/2019/03/21/Tengine-libfacedetection/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1337935927&auto=0&height=32"></iframe>           <p>前几天，深圳大学于仕琪老师突然开源了libfacedetection——号称最快的人脸检测项目（<a href="https://mp.weixin.qq.com/s/yd0pwTMq6epaCfAz6lSRTw" target="_blank" rel="noopener">超越OpenCV，史上最快人脸检测系统开源 | 新智元</a>）吸引了一大波人脸检测应用开发者围观，紧接着Tengine也立马对libfacedetection模型增加了支持，让我们一起来看看，在Tengine加持下的libfacedetection能快到什么程度吧！       </p><h3 id="libfacedetection"><a href="#libfacedetection" class="headerlink" title="libfacedetection"></a>libfacedetection</h3><p>github项目：<a href="https://github.com/ShiqiYu/libfacedetection" target="_blank" rel="noopener">https://github.com/ShiqiYu/libfacedetection</a>          </p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>本次开源的库是基于卷积神经网络实现的，320x240模型如下图所示：<br><img src="/imgs/Tengine-libfacedetection/model.jpg" alt="model"></p><p>其整体框架参照<a href="/2018/08/06/SSD/">SSD</a>，并由若干组类似<a href="/2018/05/02/经典的CNN分类架构/#VGG-Nets">VGG</a>的卷积层组合堆叠而成——     </p><ul><li>和VGG一样，大部分降采样都由2x2最大池化完成，不过libfacedetction的入口部分则与<a href="/2018/05/02/经典的CNN分类架构/#GoogLeNet">Inception</a>、<a href="/2018/05/02/经典的CNN分类架构/#ResNet">ResNet</a>类似——由卷积层完成降采样（Inception、ResNet这种大网络喜欢一上来先来一层7x7/s2的卷积下采样，<em>改进版则用三层3x3卷积替换7x7</em>；像<a href="/2018/08/05/MobileNets_v1/">Mobilenet</a>这些小网络就直接一层3x3/s2的下采样，这里的libfacedetection也是如此）；       </li><li>与VGG类似的卷积层组合，两到三个带激活的卷积层一组，不同的是，VGG用若干3x3卷积堆叠作为一组，而libfacedetection则是采用【3x3 + 1x1】或【3x3 + 1x1 + 3x3】的组合，将其中一个3x3卷积层替换为1x1不仅降低了MAC而且避免了<a href="/2019/02/28/bag-of-tricks2/#im2col">im2col</a>；</li><li>比较特别的是，每一组卷积之后送去回归和分类之前都经过一个Normilization层对特征图进行归一化操作；</li><li>libfacedetection设置了四组锚框，总计$3600+600+140+45=4385$；</li><li>参数数量约660K，乘加次数约80M（作为参照：MobileNet-v1-1.00约4.25M参数和570M次乘加运算）<br>  <em>没细算，参数数量用caffemodel的文件大小简单除了个4；乘加次数则是用gluon随手搭了个主干（实际应该比这个数字更大一些），然后调<a href="/2019/01/07/MXNet-OpSummary/">mxop</a>测算了一下MAC，粗糙数据，仅供参考。</em>      </li></ul><h4 id="检测效果"><a href="#检测效果" class="headerlink" title="检测效果"></a>检测效果</h4><p>原图：<br><img src="/imgs/Tengine-libfacedetection/test_org.jpg" alt="test">       </p><p>FP32精度输出：<br><img src="/imgs/Tengine-libfacedetection/test_fp32.jpg" alt="test">       </p><p>INT8精度输出：<br><img src="/imgs/Tengine-libfacedetection/test_int8.jpg" alt="test">       </p><h4 id="性能表现"><a href="#性能表现" class="headerlink" title="性能表现"></a>性能表现</h4><!--曾有人在[FDDB](http://vis-www.cs.umass.edu/fddb/)上进行测试过libfacedetection的表现（召回率）——      | 误检数 | libfacedetection | haarcascade-alt2 | MTCNN | | :---: | :---:  | :---: |    :---:    || 100   | 0.8236 | 0.6723 | 0.9083     || 200   | 0.8334 | 0.7045 | 0.9225      || 500   | 0.8387 | 0.7358 | 0.9354      || 1000  | 0.8429 | 0.7405 | 0.9435      || 2000  | 0.8513 | 0.7605 | 0.9505      |***数据来源：《[人脸检测背景介绍和发展现状 | 知乎, YaqiLYU](https://zhuanlan.zhihu.com/p/32702868)》***     **注意：此处libfacedetection指的是以前的非深度学习版本，开源的CNN版本还没看谁做过测试=。=最近比较懒，过阵子要是闲着没事自己测一个玩玩。**        --><p>至于速度，于老师的github上已经有了与OpenCV Haar+AdaBoost的比较数据，引用至此而不再赘述：<br><img src="/imgs/Tengine-libfacedetection/experiment_github.png" alt="experiment">        </p><h3 id="加持Tengine的libfacedetection"><a href="#加持Tengine的libfacedetection" class="headerlink" title="加持Tengine的libfacedetection"></a>加持Tengine的libfacedetection</h3><p>用开发者版的<strong>Tengine</strong>分别在EAIDK-610（<code>RK3399,4A53@1.4GHz+2A72@1.8GHz</code>）和树莓派3B（<code>BCM2837,4A53@1.2GHz</code>）、树莓派3B+（<code>BCM2837,4A53@1.4Hz</code>）平台上测试<strong>libfacedetection</strong>——<br><em>使用于老师github上的caffe模型，输入大小为320x240。</em>       </p><h4 id="EAIDK-610"><a href="#EAIDK-610" class="headerlink" title="EAIDK-610"></a>EAIDK-610</h4><table><thead><tr><th style="text-align:center">CPU-Core</th><th style="text-align:center">Time(fp32)</th><th style="text-align:center">FPS(fp32)</th><th style="text-align:center">Time(int8)</th><th style="text-align:center">FPS(int8)</th><th style="text-align:center">FPS(int8)/FPS(fp32)</th></tr></thead><tbody><tr><td style="text-align:center">1A53</td><td style="text-align:center">68.26ms</td><td style="text-align:center">14.65</td><td style="text-align:center">59.25ms</td><td style="text-align:center">16.88</td><td style="text-align:center">1.1520</td></tr><tr><td style="text-align:center">2A53</td><td style="text-align:center">50.66ms</td><td style="text-align:center">19.74</td><td style="text-align:center">49.61ms</td><td style="text-align:center">20.16</td><td style="text-align:center">1.0213</td></tr><tr><td style="text-align:center">3A53</td><td style="text-align:center">45.97ms</td><td style="text-align:center">21.75</td><td style="text-align:center">47.90ms</td><td style="text-align:center">20.88</td><td style="text-align:center">0.9598</td></tr><tr><td style="text-align:center">4A53</td><td style="text-align:center">42.97ms</td><td style="text-align:center">23.27</td><td style="text-align:center">46.03ms</td><td style="text-align:center">21.73</td><td style="text-align:center">0.9335</td></tr><tr><td style="text-align:center">1A72</td><td style="text-align:center">34.70ms</td><td style="text-align:center">28.82</td><td style="text-align:center">24.56ms</td><td style="text-align:center">40.71</td><td style="text-align:center">1.4126</td></tr><tr><td style="text-align:center">2A72</td><td style="text-align:center">26.41ms</td><td style="text-align:center">37.86</td><td style="text-align:center">20.59ms</td><td style="text-align:center">48.57</td><td style="text-align:center">1.2828</td></tr></tbody></table><p><strong><em>由于大小核共同运算时速度比单个大核还慢，所以此处不列出大小核测试的数据。</em></strong>       </p><ul><li>最快可以达到<strong>48.57FPS</strong>！      </li><li>2A53相对于1A53有相对可观的提升，3A53、4A53就只有少量的提升了；     </li><li>相比于A53，大核的A72量化后能有比较明显的提升          </li></ul><h4 id="树莓派3B"><a href="#树莓派3B" class="headerlink" title="树莓派3B"></a>树莓派3B</h4><table><thead><tr><th style="text-align:center">CPU-Core</th><th style="text-align:center">Time(fp32)</th><th style="text-align:center">FPS(fp32)</th><th style="text-align:center">Time(int8)</th><th style="text-align:center">FPS(int8)</th><th style="text-align:center">FPS(int8)/FPS(fp32)</th></tr></thead><tbody><tr><td style="text-align:center">1A53</td><td style="text-align:center">93.96ms</td><td style="text-align:center">10.64</td><td style="text-align:center">104.11ms</td><td style="text-align:center">9.61</td><td style="text-align:center">0.9025</td></tr><tr><td style="text-align:center">2A53</td><td style="text-align:center">66.74ms</td><td style="text-align:center">14.98</td><td style="text-align:center">62.90ms</td><td style="text-align:center">15.90</td><td style="text-align:center">1.0611</td></tr><tr><td style="text-align:center">3A53</td><td style="text-align:center">58.03ms</td><td style="text-align:center">17.23</td><td style="text-align:center">50.73ms</td><td style="text-align:center">19.71</td><td style="text-align:center">1.1438</td></tr><tr><td style="text-align:center">4A53</td><td style="text-align:center">56.57ms</td><td style="text-align:center">17.68</td><td style="text-align:center">43.69ms</td><td style="text-align:center">22.89</td><td style="text-align:center">1.2947</td></tr></tbody></table><h4 id="树莓派3B-1"><a href="#树莓派3B-1" class="headerlink" title="树莓派3B+"></a>树莓派3B+</h4><table><thead><tr><th style="text-align:center">CPU-Core</th><th style="text-align:center">Time(fp32)</th><th style="text-align:center">FPS(fp32)</th><th style="text-align:center">Time(int8)</th><th style="text-align:center">FPS(int8)</th><th style="text-align:center">FPS(int8)/FPS(fp32)</th></tr></thead><tbody><tr><td style="text-align:center">1A53</td><td style="text-align:center">89.90ms</td><td style="text-align:center">11.12</td><td style="text-align:center">94.61ms</td><td style="text-align:center">10.57</td><td style="text-align:center">0.9502</td></tr><tr><td style="text-align:center">2A53</td><td style="text-align:center">65.87ms</td><td style="text-align:center">15.18</td><td style="text-align:center">58.34ms</td><td style="text-align:center">17.14</td><td style="text-align:center">1.1290</td></tr><tr><td style="text-align:center">3A53</td><td style="text-align:center">59.76ms</td><td style="text-align:center">16.73</td><td style="text-align:center">47.99ms</td><td style="text-align:center">20.84</td><td style="text-align:center">1.2451</td></tr><tr><td style="text-align:center">4A53</td><td style="text-align:center">58.22ms</td><td style="text-align:center">17.18</td><td style="text-align:center">41.26ms</td><td style="text-align:center">24.24</td><td style="text-align:center">1.4111</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tengine </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习小技巧（三）：训练技巧</title>
      <link href="/2019/03/01/bag-of-tricks3/"/>
      <url>/2019/03/01/bag-of-tricks3/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1320572967&auto=0&height=32"></iframe>       </p><blockquote><p>傻乎乎~傻乎乎~傻乎乎~~~~~~~                          </p></blockquote><p>呼~终于到了解读论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》的最后一部分。         </p><hr><h2 id="余弦学习率衰减（Cosine-Learning-Rate-Decay）"><a href="#余弦学习率衰减（Cosine-Learning-Rate-Decay）" class="headerlink" title="余弦学习率衰减（Cosine Learning Rate Decay）"></a>余弦学习率衰减（Cosine Learning Rate Decay）</h2><p>论文：《<a href="https://arxiv.org/pdf/1608.03983.pdf" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts(2016)</a>》           </p><p>先前《<a href="/2019/02/23/bag-of-tricks1/">深度学习小技巧（一）：高效训练 - 学习率衰减 | Hey~YaHei!</a>》已经讨论过学习率衰减的问题啦，它本质上是一种模拟退火策略，通过逐步降低振荡幅度来更好的逼近最优点。而早在《<a href="/2018/04/10/优化器/#学习计划（learning-schedules）">优化器 - 学习计划（learning schedules） | Hey~YaHei!</a>》我们也讨论过各种各样的衰减策略，其中指数衰减在很长一段时间中都是非常受欢迎的。      </p><p>不过近年来大家突发奇想——光是逼近局部最优点哪里够用，不如想办法让它时不时来一次大震动，把它甩到另外一个位置开荒去？于是热重启技术就诞生了！<br><em>关于学习率的事情近两年其实有很多有意思的进展，比如学习率测试、周期性学习率、热重启技术，以及传统Adam的各种改进（比如最近北大和浙大本科生提出的AdamRound），这部分本文先按下不细讲，之后有时间应该会专门开一篇文章讨论学习率的问题。</em>           </p><p><center><img src="/imgs/bag-of-tricks/cos_lr_decay.png" alt="cos_lr_decay"></center><br>在每个周期的开始和末尾学习率变化缓慢，分别让模型有充分的时间跳出局部最优点、更逼急局部最优点；而中间部分则呈现出接近线性的下降趋势，这与周期性学习率的思想是类似的；同时每个周期开始学习率都会反弹到较高的水平，这是一种典型的模拟退火策略。        </p><h2 id="平滑标签（Label-Smoothing-Regularization-LSR）"><a href="#平滑标签（Label-Smoothing-Regularization-LSR）" class="headerlink" title="平滑标签（Label Smoothing Regularization, LSR）"></a>平滑标签（Label Smoothing Regularization, LSR）</h2><p>该正则化技巧最初由《<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision(2015)</a>》提出（Chapter 7），并应用在inception-v2的训练上。      </p><p>首先考虑传统的softmax和交叉熵——<br>$$q _ { i } = \frac { \exp \left( z _ { i } \right) } { \sum _ { j = 1 } ^ { K } \exp \left( z _ { j } \right) }$$<br>$$\mathrm { H } ( \mathrm { p } , \mathrm { q } ) = - \sum _ { k=1 } ^ { K } p_k \cdot \log q_k $$        </p><p>通常来说，对模型最后一层全连接层的输出 $z_i$ 按第一个公式进行归一化作为各个分类的预测概率 $q_i$；训练时则应用第二个公式，对实际概率 $p_k$ 和预测概率 $q_k$ 的对数相乘求和得到损失 $H(p,q)$，训练的目标即是最小化损失 $H(p,q)$。<br>一般我们已知了实际分类，所以自然地用独热码的方式选取——<br>$$p = \delta _ { k , y } = \left\{ \begin{array} { l } { 1 , i = y } \\ { 0 , i \neq y } \end{array} \right.$$<br>此时实际的概率分布是一个冲激函数，论文《<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision(2015)</a>》指出，用冲激函数的实际概率分布进行训练是有问题的——<strong><em>模型会趋向去用极高的概率来输出一个分类结果（或者说，模型会倾向于“武断”），这将不利于泛化（容易发生过拟合）</em></strong>。      </p><p>为了解决这一问题，文中提出了平滑标签LSR，将实际概率函数改造为<br>$$p ^ { \prime } (k) = ( 1 - \epsilon ) \delta _ { k , y } + \epsilon u ( k )$$<br>其中，$u(k)$ 是人为引入的一个固定分布（可以看作是为概率分布引入固定分布的噪声），并且由参数 $\epsilon$ 控制相对权重。那么对应的，交叉熵公式就可以展开为<br>$$H \left( p ^ { \prime } , q \right) = - \sum _ { k = 1 } ^ { K } p ^ { \prime } ( k ) \log q ( k )  = ( 1 - \epsilon ) H ( p , q ) + \epsilon H ( u , q )$$<br>从损失函数的角度上看，LSR相当于为损失函数增加了人为引入的先验概率 $u(k)$ 和预测概率 $q(k)$ 之间的惩罚项，并且赋予了相对权重 $\epsilon$。         </p><p>inception-v2选用了均匀分布的 $u(k)$ 作为先验概率，即 $u(k) = 1 / 1000$ 并且设置相对权重 $\epsilon = 0.1$，使得top-1和top-5错误率都下降了约0.2%。           </p><h2 id="知识蒸馏（Knowledge-Distillation）"><a href="#知识蒸馏（Knowledge-Distillation）" class="headerlink" title="知识蒸馏（Knowledge Distillation）"></a>知识蒸馏（Knowledge Distillation）</h2><p>论文：《<a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network(2015)</a>》     </p><p>知识蒸馏也是模型压缩技术的一部分，以后应该会花时间细致地学习它，这里暂时不展开讲。知识蒸馏的总体思想，就是用一个训练好的大容量模型去指导一个小容量模型训练，从而得到更好的表现。      </p><p>假设已经训练好一个大容量模型（比如ResNet-152），然后用它去指导另一个小容量模型（比如ResNet-50）训练，他们最后一层全连接层的输出（也就是各个分类的得分）分别为 $r$ 和 $z$，设实际标签的概率分布为 $p$，那么可以用如下损失函数进行训练，<br>$$H(p,z,r) = \ell ( p , \operatorname { softmax } ( z ) ) + T ^ { 2 } \ell ( \operatorname { softmax } ( r / T ) , \operatorname { softmax } ( z / T ) )$$<br>其中，$\ell(\cdot)$ 是基本的损失函数（如交叉熵），<br>$T$ 是一个超参数（Temperature），可以看作是用来设定“向老师学习的热情程度”。         </p><h2 id="数据混合（Mixup）"><a href="#数据混合（Mixup）" class="headerlink" title="数据混合（Mixup）"></a>数据混合（Mixup）</h2><p>论文：《<a href="https://arxiv.org/pdf/1710.09412.pdf" target="_blank" rel="noopener">mixup: Beyond Empirical Risk Minimization(2017)</a>》       </p><h3 id="经验风险最小化（Empirical-Risk-Minimization-ERM）"><a href="#经验风险最小化（Empirical-Risk-Minimization-ERM）" class="headerlink" title="经验风险最小化（Empirical Risk Minimization, ERM）"></a>经验风险最小化（Empirical Risk Minimization, ERM）</h3><p>这是当前深度学习训练中最常见的思路，假设有一个学习任务，其特征为 $X$，标签为 $Y$，其实际的联合分布为 $P(X,Y)$，为任务建模 $f(x)$，其损失为 $\ell (f(x), y)$，那么可以定义期望风险<br>$$R ( f ) = \int \ell ( f ( x ) , y ) \mathrm { d } P ( x , y )$$<br>注意到分布 $P$ 在实际应用中是未知的，所以我们往往采用数据驱动的方式，投喂训练集 $\mathcal { D } = \left\{ \left( x _ { i } , y _ { i } \right) \right\} _ { i = 1 } ^ { n }$，从而估计出 $P$ 的经验分布<br>$$P _ { \delta } ( x , y ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \delta \left( x = x _ { i } , y = y _ { i } \right)$$<br>其中，$\delta \left( x = x _ { i } , y = y _ { i } \right)$ 是一个冲激函数（也叫狄拉克函数）<br>$$\delta \left( x = x _ { i } , y = y _ { i } \right) = \left\{ \begin{array} { l } { 1 , (x,y) = (x_i,y_i) } \\ { 0 , others } \end{array} \right.$$<br>用经验分布计算出来的风险即为经验风险<br>$$R _ { \delta } ( f ) = \int \ell ( f ( x ) , y ) \mathrm { d } P _ { \delta } ( x , y ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \ell \left( f \left( x _ { i } \right) , y _ { i } \right)$$<br>相应地，ERM就是通过最小化经验风险的一种训练方式。       </p><h3 id="邻域风险最小化（Vicinal-Risk-Minimization-VRM）"><a href="#邻域风险最小化（Vicinal-Risk-Minimization-VRM）" class="headerlink" title="邻域风险最小化（Vicinal Risk Minimization, VRM）"></a>邻域风险最小化（Vicinal Risk Minimization, VRM）</h3><p>ERM用有限的数据来训练模型，直观地看，模型只要拥有足够的“记忆力”，把整个训练集的分布都原原本本地记下就可以得到一个看似“非常棒”的模型，这就是深度学习中典型的过拟合问题。<br>针对这一问题，论文《<a href="http://papers.nips.cc/paper/1876-vicinal-risk-minimization.pdf" target="_blank" rel="noopener">Vicinal risk minimization(2000)</a>》提出了邻域风险最小化，其实思路非常简单——不再直接用训练集上的经验分布来估计实际分布，而是在经验分布的基础上引入高斯噪声，使得数据量变为“无限”（简单的数据生成），以此来缓解过拟合问题。<br>$$P _ { \nu } ( \tilde { x } , \tilde { y } ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \nu ( \tilde { x } , \tilde { y } | x _ { i } , y _ { i } )$$<br>$$\nu ( \tilde { x } , \tilde { y } | x _ { i } , y _ { i } ) = \mathcal { N } \left( \tilde { x } - x _ { i } , \sigma ^ { 2 } \right) \delta \left( \tilde { y } = y _ { i } \right)$$<br>也即<br>$\tilde { x }$ 在正态分布 $\mathcal { N } \left( \tilde { x } - x _ { i } , \sigma ^ { 2 } \right)$ 上随机抽样；<br>$\tilde { y } = y_i$ 与ERM相同。      </p><h3 id="mixup"><a href="#mixup" class="headerlink" title="mixup"></a>mixup</h3><p>VRM只是给特征 $X$ 引入高斯噪声，取得了不错的成效，但好像还不够合理。mixup则提出了另一种邻域分布——<br>$$\tilde { x } = \lambda x _ { i } + ( 1 - \lambda ) x _ { j }$$<br>$$\tilde { y } = \lambda y _ { i } + ( 1 - \lambda ) y _ { j }$$<br>其中，$\lambda \sim Beta(\alpha, \alpha)$ 且 $\alpha \in (0, \infty)$。<br>简而言之，<strong>mixup将多个数据用加权的方式混合在一起，以此来估计实际分布</strong>；而当 $\alpha \rightarrow 0$时，mixup退化为ERM。     </p><ul><li>用三个及三个以上的数据做mixup并不会有额外的提升，反而会在训练过程中增加计算复杂度</li><li>在一个batch内做mixup和不同batch间做mixup带来的效果很接近，而前者对IO更友好</li><li>只在特征上做mixup的效果不如同时在特征和标签上都做mixup</li><li>mixup提高了对坏样本（比如标签错误）的容忍性、对对抗样例的鲁棒性、同时也能使GAN训练过程变得稳定        </li><li>若$\alpha \rightarrow \infty$，CIFAR-10上可以取得很低的错误率，而ImageNet上却反而会提高错误率</li><li>模型容量越大，训练效果对参数 $\alpha$ 就越不敏感，从而mixup能取得越好的效果</li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h4><p>基本的数据增强：特定比例尺缩放、随机裁剪、水平翻转（无色域扰动）         </p><p><center><img src="/imgs/bag-of-tricks/mixup_classification1.jpg" alt="mixup_classification1"></center>        </p><p><center><img src="/imgs/bag-of-tricks/mixup_classification2.jpg" alt="mixup_classification2"></center>         </p><ul><li>$\alpha$ 越大，正则化效果越明显，甚至会出现欠拟合       </li><li>容量越大的网络mixup的效果越好，而且mixup的效果往往要在训练后期才体现出来</li></ul><h4 id="对坏样本的容忍性测试"><a href="#对坏样本的容忍性测试" class="headerlink" title="对坏样本的容忍性测试"></a>对坏样本的容忍性测试</h4><p>参考 <a href="https://github.com/pluskid/fitting-random-labels" target="_blank" rel="noopener">fitting-random-labels | github, pluskid</a> 的实验，随机将一定比例的训练数据的标签替换为随机噪声生成带坏样本的数据集进行训练。       </p><p><center><img src="/imgs/bag-of-tricks/mixup_corruption.jpg" alt="mixup_corruption"></center><br><em>其中，Best和Last分别指最佳的测试结果、最后一个epoch的测试结果。</em>          </p><ul><li>mixup的效果比dropout的效果更好    </li><li>mixup和dropout是互为补充的，可以同时使用</li></ul><h4 id="对对抗样例的鲁棒性测试"><a href="#对对抗样例的鲁棒性测试" class="headerlink" title="对对抗样例的鲁棒性测试"></a>对对抗样例的鲁棒性测试</h4><p><strong><em>第一次看到这种测试实验，具体还不是很理解，先放着等之后啃啃相关论文</em></strong>          </p><p>参照论文《<a href="https://arxiv.org/pdf/1312.6199.pdf" target="_blank" rel="noopener">Intriguing properties of neural networks(2013)</a>》生成对抗样例。      </p><blockquote><p>Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations to legitimate examples in order to deteriorate the performance of the model. The adversarial noise is generated by ascending the gradient of the loss surface with respect to the legitimate example.        </p></blockquote><p>参照论文《<a href="https://arxiv.org/pdf/1412.6572.pdf" target="_blank" rel="noopener">Explaining and Harnessing Adversarial Examples(2014)</a>》设计测试，并且得出结论：mixup有助于提高模型对对抗样例的鲁棒性。<br><em>一些指标反正现在也看不懂，就不列在这了。</em>             </p><h4 id="切除研究（Ablation-Study）"><a href="#切除研究（Ablation-Study）" class="headerlink" title="切除研究（Ablation Study）"></a>切除研究（Ablation Study）</h4><p>参考《<a href="https://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study" target="_blank" rel="noopener">In the context of deep learning, what is an ablation study? | Quora</a>》和《<a href="https://www.zhihu.com/question/60170398/answer/207709956" target="_blank" rel="noopener">什么是 ablation study？ | 知乎</a>》。<br>说白了切除研究就是对照实验，控制单一变量来对比有无某一结构的结果。     </p><p><center><img src="/imgs/bag-of-tricks/mixup_ablation_study.jpg" alt="mixup_ablation_study"></center><br>其中，<br>AC表示在所有分类之间做mixup，SC表示只在同一分类内做mixup；<br>RP表示在随机数据对之间做mixup，KNN表示在k近邻（k=200）之间做mixup。       </p><ul><li>随机数据对之间做mixup比k近邻效果好</li><li>对浅层表示做mixup似乎有点作用？但不明显，而对深层表示做mixup效果较差</li><li>mixup的效果比标签平滑和加高斯噪声的效果好</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>深度学习小技巧（二）：模型微调</title>
      <link href="/2019/02/28/bag-of-tricks2/"/>
      <url>/2019/02/28/bag-of-tricks2/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=4898499&auto=0&height=32"></iframe>       <p>接着上一篇文章《<a href="/2019/02/23/bag-of-tricks1/">深度学习小技巧（一）：高效训练 | Hey~YaHei!</a>》继续解读论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》，论文中以ResNet为例提出了一些简单的微调技巧，并且取得了一定的成果。且不说准确率如何，论文中除了分析准确率有着怎样怎样的提升之外，还关注了产生了额外开销，并且通过分析、实验量化了这些开销，这是值得肯定的（比那些不考虑开销，盲目微调，通过牺牲很多速度来提高那一点点准确率的论文，不知道要高到哪里去！）      </p><hr><h2 id="以ResNet为例"><a href="#以ResNet为例" class="headerlink" title="以ResNet为例"></a>以ResNet为例</h2><p>原始的ResNet模型可以参考《<a href="/2018/05/02/经典的CNN分类架构/#ResNet">经典的CNN分类架构 - ResNet | Hey~YaHei!</a>》，其核心在于应用了shortcut（原文称为skip connection）技术使得深层网络也能够被有效训练，具体细节这里就不再赘述。      </p><center><img src="/imgs/bag-of-tricks/resnet_a.jpg" alt="resnet_a"></center>           <h3 id="改进1：推迟下采样"><a href="#改进1：推迟下采样" class="headerlink" title="改进1：推迟下采样"></a>改进1：推迟下采样</h3><p>该改进方法最初是在Torch上提出的，目前这一改进也已经被广泛地应用。<br>首先观察原始模型的下采样模块——       </p><center><img src="/imgs/bag-of-tricks/resnet_downsample.jpg" alt="resnet_downsample"></center><br>其PathA依次经过<br>1. 1x1的卷积，完成通道的收缩，并且步长为2以实现下采样<br>2. 3x3的卷积，通道数量不变，主要用于提取特征<br>3. 1x1的卷积，完成通道的扩张<br><br>其中第一个卷积用来作为下采样，所以步长设为了1——但你仔细想想会发现，核大小1x1、步长2的卷积会造成3/4的信息丢失！以6x6的特征图为例，如下图所示，只有红色部分的信息能够传递到下一层去，非红色部分均不参与卷积计算。<br><center><img src="/imgs/bag-of-tricks/conv_k1s2.jpg" alt="conv_k1s2"></center>           <p>由此可见，在1x1的卷积层作下采样是不明智的，更好的做法是把下采样过程挪到3x3的卷积上，如下图所示，由于卷积核宽度大于步长，卷积核在移动过程中能够遍历输入特征图上的所有信息（甚至还能有重叠）：      </p><center><img src="/imgs/bag-of-tricks/conv_k3s2.jpg" alt="conv_k3s2"></center>           <p>下采样模块就变为——     </p><center><img src="/imgs/bag-of-tricks/resnet_b_downsample.jpg" alt="resnet_b_downsample"></center>           <h3 id="改进2：拆解大核卷积"><a href="#改进2：拆解大核卷积" class="headerlink" title="改进2：拆解大核卷积"></a>改进2：拆解大核卷积</h3><p>如《<a href="/2018/04/18/CNN/#卷积层（Conv）">卷积神经网络CNN - 卷积层（Conv） | Hey~YaHei!</a>》所述，大核卷积层可以由多层小核卷积替代实现，这不仅可以减少参数，还能加深网络深度以增加网络容量和复杂度。<br>Inception也早在《<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision(2015)</a>》一文中对Inceptionv1做出改进，分别用三个和两个3x3卷积的级联去替代7x7和5x5的卷积。<br>这一技巧同样适用于ResNet——      </p><center><img src="/imgs/bag-of-tricks/resnet_c_input.jpg" alt="resnet_c_input"></center>           <h3 id="改进3：用平均池化替代1x1卷积做下采样"><a href="#改进3：用平均池化替代1x1卷积做下采样" class="headerlink" title="改进3：用平均池化替代1x1卷积做下采样"></a>改进3：用平均池化替代1x1卷积做下采样</h3><p>下采样模型的PathA和PathB都需要做下采样才能正确地加和，改进1只针对PathA做了改进，其实PathB也用了1x1的卷积做下采样。为此，论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》用平均池化接替了PathB中的降采样工作：       </p><center><img src="/imgs/bag-of-tricks/resnet_d_downsample.jpg" alt="resnet_d_downsample"></center>           <h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><center><img src="/imgs/bag-of-tricks/model_tweak_results.jpg" alt="model_tweak_results"></center><br>其中A、B、C、D分别代表原始、改进1、改进2、改进3的模型。       </p><p>经过改进之后，最终的ResNet-50-D准确率提高了0.95%。但也不得不承认，以上的改进都增加了模型的运算复杂度，FLOPs增加了约13%，但实测速度只下降了3%。       </p><h2 id="关于FLOPs和实测速度"><a href="#关于FLOPs和实测速度" class="headerlink" title="关于FLOPs和实测速度"></a>关于FLOPs和实测速度</h2><p>你可能会意外，为什么运算量明明增加了13%，可实测速度却只下降了3%呢？？      </p><h3 id="多分支网络"><a href="#多分支网络" class="headerlink" title="多分支网络"></a>多分支网络</h3><p>首先要注意到，ResNet由于应用了shortcut技术，相比于传统的直筒式网络增加了分支，不同分支是可以并行计算的，而计算FLOPs时却是把不同分支的运算量依次累加起来。        </p><h3 id="高效的1x1卷积"><a href="#高效的1x1卷积" class="headerlink" title="高效的1x1卷积"></a>高效的1x1卷积</h3><p>早在《<a href="/2018/08/05/MobileNets_v1/">MobileNets v1模型解析 | Hey~YaHei!</a>》一文中就提及过——      </p><blockquote><p>深度向分解的卷积中绝大多数参数和运算都集中在 1×1 的pointwise卷积运算当中，这种运算恰恰是能够被 GEneral Matrix Multiply(GEMM) 函数高度优化的。</p></blockquote><p>为什么1x1卷积能够被高度优化？首先要先从卷积计算的实现讲起——      </p><h4 id="卷积的原理"><a href="#卷积的原理" class="headerlink" title="卷积的原理"></a>卷积的原理</h4><p>详细原理可以参阅《<a href="/2018/04/18/CNN/#卷积层（Conv）">卷积神经网络CNN - 卷积层（Conv） | Hey~YaHei!</a>》和《<a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/conv-layer.html" target="_blank" rel="noopener">5.1二维卷积层 | 动手学深度学习</a>》。<br><img src="/imgs/bag-of-tricks/convolutional_kernel.gif" alt="convolutional_kernel"></p><p>首先考虑3x3的单通道特征图，以及k2s1的卷积核——<br><img src="/imgs/bag-of-tricks/conv_raw.jpg" alt="conv_raw">       </p><p>按照卷积计算，<br>$$y_{11} = w_{11}x_{11} + w_{12}x_{12} + w_{21}x_{21} + w_{22}x_{22}$$<br>$$y_{12} = w_{11}x_{12} + w_{12}x_{13} + w_{21}x_{22} + w_{22}x_{23}$$<br>$$y_{21} = w_{11}x_{21} + w_{12}x_{22} + w_{21}x_{31} + w_{22}x_{32}$$<br>$$y_{22} = w_{11}x_{22} + w_{12}x_{23} + w_{21}x_{32} + w_{22}x_{33}$$</p><p>按照“行先序”，特征图和卷积核在内存中是这样排列的——<br><img src="/imgs/bag-of-tricks/conv_store.jpg" alt="conv_store">       </p><p>我们用不同的颜色标注出卷积计算中的访存过程（相同颜色的数据相乘）——<br><img src="/imgs/bag-of-tricks/conv_access_ram.jpg" alt="conv_access_ram">      </p><p>众所周知，由于程序的<strong>局部性原理</strong>（通常相邻代码段会访问相邻的内存块），现代处理器通常会按块从内存中读取数据到高速缓存中以缓解访存速度和计算速度的巨大差异导致的“<strong>内存墙</strong>”问题。换句话说，如果计算需要从内存中读取<code>x12</code>的数据，那么往往相邻的<code>x11</code>、<code>x13</code>等数据也会被一起读取到高速缓存上，当下次计算需要用到<code>x11</code>或<code>x13</code>时处理器就可以快速地从高速缓存中取出数据而不需要从内存中调取，大大提高了程序的速度。<br><em>注：L1缓存的读取速度是RAM的50-100倍！（数据来源：《<a href="https://book.douban.com/subject/7006537/" target="_blank" rel="noopener">计算机体系结构：量化研究方法</a>》）</em>       </p><p>而从上边展示出来的访存过程中可以看到，直接对于特征图数据的访问过程十分散乱，直接用行先序存储的特征图参与计算是非常愚蠢的选择。<br>因此深度学习框架往往通过牺牲空间的手段（约扩增$K \times K$倍），将特征图转换成庞大的矩阵来进行卷积计算，这就是常说的im2col操作。       </p><h4 id="im2col"><a href="#im2col" class="headerlink" title="im2col"></a>im2col</h4><p>参考：<br>《<a href="https://blog.csdn.net/dwyane12138/article/details/78449898" target="_blank" rel="noopener">im2col的原理和实现 | CSDN, dwyane12138</a>》<br>《<a href="https://www.zhihu.com/question/28385679?sort=created" target="_blank" rel="noopener">在Caffe中如何计算卷积？ | 知乎, 贾扬清</a>》        </p><p><center><img src="/imgs/bag-of-tricks/conv_im2col.png" alt="conv_im2col"></center><br>其实思路非常简单：把每一次循环所需要的数据都排列成列向量，然后逐一堆叠起来形成矩阵（按通道顺序在列方向上拼接矩阵）。<br>比如$C_i \times W_i \times H_i$大小的输入特征图，$K \times K$大小的卷积核，输出大小为$C_o \times W_o \times H_o$，<br>输入特征图将按需求被转换成$(K*K)\times(C_i*W_o*H_o)$的矩阵，卷积核将被转换成$C_o\times(K*K)$的矩阵，调用GEMM库两矩阵相乘也就完成了所谓的卷积计算。由于按照计算需求排布了数据顺序，每次计算过程中总是能够依次访问特征图数据，迎合了局部性原理，极大地提高了计算卷积的速度！     </p><h4 id="特别的1x1"><a href="#特别的1x1" class="headerlink" title="特别的1x1"></a>特别的1x1</h4><p>回到1x1的卷积，它的im2col非常特殊——其原始存储结构跟im2col的重排列矩阵是完全相同的！！也就是说，1x1卷积甚至不需要im2col的过程，拿起来就能直接算，节省了数据重排列的时间和空间，所以哪怕是在相同FLOPs的前提下，1x1卷积也要比3x3卷积快速、高效得多。<br><em>当然，这是建立在局部性原理和冯诺依曼结构的基础之上，对于非冯结构的计算体系可能就不适用了。</em>      </p><p>这也是为什么MobileNet在论文最后要大肆鼓吹说他94.86%的运算量都集中1x1的卷积运算上，它的快速可不仅仅体现在“少参数，少运算量”上！     </p><p>同理，前文中改进1和改进3看似增加了很多运算量，但这些运算量都是负担在1x1卷积上的，这就使得实测速度的下降远没有运算量增加那么明显！     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>深度学习小技巧（一）：高效训练</title>
      <link href="/2019/02/23/bag-of-tricks1/"/>
      <url>/2019/02/23/bag-of-tricks1/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=638400&auto=0&height=32"></iframe>        <p>最近无意在MXNet论坛上翻出一篇不错的综述性（实验性）论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》，作者主要从<strong>高效训练</strong>、<strong>模型微调</strong>、<strong>训练技巧</strong>三个方面列举了一些常见的深度学习小技巧并附上了丰富的比较实验。<br>之后我将抽空按照这三个方面，以该论文为起点，配以相关的资料和个人理解，连更三篇相关的博文(*^__^*)             </p><hr><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><ul><li>训练      <ul><li>随机抽样并将uint8转换为float32</li><li>随机裁剪出比例尺在$[\frac{3}{4}, \frac{4}{3}]$之间、尺寸大小在$[8\%, 100\%]$之间的图像块，并resize为224x224的图像     </li><li>以0.5的概率水平翻转       </li><li>随机使用$[0.6, 1.4]$的系数对亮度、饱和度、对比度进行扰动       </li><li>用正态分布$\mathcal { N } ( 0,0.1 )$的随机系数为图像添加PCA噪声</li><li>图像数值去均值，除以方差</li></ul></li><li>预测      <ul><li>保持比例尺，将短边缩放到256</li><li>从中央裁剪出224x224的图像</li><li>与训练减去相同的均值，除以相同的方差</li></ul></li></ul><h2 id="高效训练"><a href="#高效训练" class="headerlink" title="高效训练"></a>高效训练</h2><p>为了提高训练效率（偶尔训练结果甚至还能有一些微小的提升），目前主流的技巧包括<strong>增大训练批次</strong>、<strong>降低训练精度</strong>两个方向，其主要目的在于<strong>减少复杂度</strong>和<strong>提高并行度</strong>。           </p><h3 id="大批次训练"><a href="#大批次训练" class="headerlink" title="大批次训练"></a>大批次训练</h3><p>参考：《<a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size(2017)</a>》         </p><p>我们往往直观地认为，加大批次能够让模型在同一次迭代中见识更多的样本，能使得学习更加稳定并取得更好的效果（比如最极端的批梯度下降法），但近年来的各种实验研究表明事实并非如此——<strong>在相同学习率、相同epoch下，过大的批大小训练效果反而会比较小的批大小训练结果更差</strong>。一般研究认为，这是因为随着批次的增加，样本整体噪声均值不变但方差却被减小，而多数研究认为样本的噪声有助于SGD规避局部最优点（避开尖锐的最优点，提高整体的泛化能力）。论文《<a href="https://arxiv.org/pdf/1609.04836.pdf" target="_blank" rel="noopener">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima(2016)</a>》将这一现象称为“<strong>泛化差距（generalization gap）</strong>”并展开了详细的讨论。同时论文《<a href="https://arxiv.org/pdf/1710.06451.pdf" target="_blank" rel="noopener">A bayesian perspective on generalization and stochastic gradient descent(2017)</a>》经过一系列实验还指出一个经验关系——$B _ { \text {opt} } \propto \epsilon N$，也即最佳的批大小 $B_{opt}$ 随着学习率 $\epsilon$ 和数据集样本总数 $N$ 的乘积等比例增大（缩小）。     </p><h4 id="随机梯度下降（SGD）"><a href="#随机梯度下降（SGD）" class="headerlink" title="随机梯度下降（SGD）"></a>随机梯度下降（SGD）</h4><p>《<a href="/ML-Andrew/ML-Andrew-notes1.html#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88gradient-descent%EF%BC%89%E7%AE%97%E6%B3%95">吴恩达《机器学习》笔记 - 1绪论、线性回归与逻辑回归 - 梯度下降（gradient descent）算法 | Hey~YaHei</a>》已经讨论过梯度下降法，这是机器学习中常用的基本优化方法。传统的梯度下降法的更新方式分为两种，分别是批梯度下降（即每次循环先计算整个数据集上的损失，然后统一更新权重）和增量梯度下降（又称随机梯度下降，每次循环只计算一个数据样本的损失，频繁地更新权重）。前者收敛缓慢，后者受噪声影响大难以收敛到一个很好的最优点。<br>如今大家常说的“<strong>随机梯度下降（SGD）</strong>”其实指的是小批次随机梯度下降，该方法介于批梯度下降和增量梯度下降之间，每次循环用若干个样本（数量往往远小于数据集总样本数）的损失来更新权重。      </p><h4 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h4><p>一般认为，要达到强凸函数的最小点需要在搜索过程中不断衰减学习率，比如论文《<a href="http://xueshu.baidu.com/usercenter/paper/show?paperid=266744235943375e790f06d44a334efa&amp;site=xueshu_se" target="_blank" rel="noopener">A stochastic approximation method(1951)</a>》提出学习率应当满足的收敛条件：<br>（<em>前提：批大小batch_size固定</em>）<br>$$\sum _ { i = 1 } ^ { \infty } \epsilon _ { i } = \infty$$<br>$$\sum _ { i = 1 } ^ { \infty } \epsilon _ { i } ^ { 2 } &lt; \infty$$<br>其中，$\epsilon_i$ 为第i次更新权重所使用的学习率。       </p><ul><li>前者保证了经过无穷次更新，无论权重如何初始化，最终都能更新到任意点；      </li><li>后者要求有足够的学习率衰减速率，使得模型能够收敛到最优点而不是在最优点附近反复震荡       </li></ul><p>注意上述两式是以批大小固定为前提进行讨论的，而《<a href="https://arxiv.org/pdf/1710.06451.pdf" target="_blank" rel="noopener">A Bayesian Perspective on Generalization and Stochastic Gradient Descent(2017)</a>》引入变化的批大小这一因素，重新思考了SGD的收敛条件：<br>对于梯度下降的更新公式 $\omega_t = \omega_{t-1} - \epsilon \frac{dC}{d\omega_{t-1}}$，从连续时间的角度上看可以得到<br>$$\frac { d \omega } { d t } = - \epsilon \frac { d C } { d \omega } + \eta ( t )$$<br>其中，C为无噪声的损失，$\eta(t)$是一个高斯分布的随机噪声，同时论文中将其建模为 $mean(\eta(t))=0$、$variance(\eta(t))=gF(\omega)\delta(t-t’)$ 的高斯随机噪声，其中$F(\omega)$是权重间梯度波动的协方差，$g=\epsilon(\frac{N}{B}-1)$描述了噪声的波动范围，N、B分别是数据集大小和批大小。<br>通常来说$B &lt;&lt; N$，所以有$g \approx \epsilon \frac{N}{B}$；由此可见，在满足$B&lt;&lt;N$的前提下等比例放大（缩小）$\epsilon$和$B$效果是相当的。      </p><p><strong>学习率衰减实际是一种模拟退火策略。</strong>       </p><blockquote><p>模拟退火来源于物理上的降温退火问题——一个处于很高温度的物体，现在要给它降温，使物体内能降到最低。我们常规的思维是，越快越好，让它的温度迅速地降低。然而，实际上，过快地降温使得物体来不及有序地收缩，难以形成结晶。而结晶态，才是物体真正内能降到最低的形态。正确的做法，是徐徐降温，也就是退火，才能使得物体的每一个粒子都有足够的时间找到自己的最佳位置并紧密有序地排列。开始温度高的时候，粒子活跃地运动并逐渐找到一个合适的状态。在这过程中温度也会越降越低，温度低下来了，那么粒子也渐渐稳定下来，相较于以前不那么活跃了。这时候就可以慢慢形成最终稳定的结晶态了。     </p></blockquote><p>这一过程演化到优化问题上就成为一种模拟退火策略（实际上深度学习中的<strong>退火</strong>可能更加广泛，大概一切有助于跳脱局部最优点，寻找更佳的全局最优点的策略都能称为退火了吧）。训练一开始使用较大的学习率，在比较大的空间上搜索较优的区域，待权重相对稳定后衰减学习率缩小搜索空间，从而取得更好的训练结果并且提高了模型的鲁棒性。       </p><h4 id="大批次训练技巧"><a href="#大批次训练技巧" class="headerlink" title="大批次训练技巧"></a>大批次训练技巧</h4><p>前文已经提到，盲目增大批大小其实无益于提升训练效果，但却有各种小技巧——      </p><h5 id="1-等比例增大学习率"><a href="#1-等比例增大学习率" class="headerlink" title="1.等比例增大学习率"></a>1.等比例增大学习率</h5><p>前边提到在满足$B&lt;&lt;N$的前提下等比例放大（缩小）$\epsilon$和$B$效果是相当的，既然如此，我们在增大批大小的同时等比例增大学习率不仅可以避开泛化差距问题，还有助于加速收敛速度。<br>而对于带动量的优化器，等比例增大 $\epsilon / ( 1 - m )$ 也能取得相同的结果（其中m为动量系数）。      </p><h5 id="2-学习率预热"><a href="#2-学习率预热" class="headerlink" title="2.学习率预热"></a>2.学习率预热</h5><p>训练之初由于参数随机初始化（尤其是非预训练模型），数值上与目标参数组相去甚远，如果采用太大的学习率会出现参数数值的不稳定，不利于模型收敛。因此，从较低的学习率开始训练会比较好，比如：     </p><ul><li>《<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition(2015)</a>》先用较低的学习率（0.01）预热，直到训练错误率低于80%之后再恢复到正常的学习率（0.1）。              <blockquote><p>In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.       </p></blockquote></li><li>《<a href="https://arxiv.org/pdf/1706.02677.pdf" target="_blank" rel="noopener">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour(2017)</a>》2.2节指出常数学习率预热会在学习率突变阶段对模型收敛产生不良印象，并提出了线性学习率预热，用斜坡来代替学习率的突变。           <center><img src="/imgs/bag-of-tricks/lr_warmup.jpg" alt="lr_warmup"></center>        </li></ul><h5 id="3-零gamma初始化"><a href="#3-零gamma初始化" class="headerlink" title="3.零gamma初始化"></a>3.零gamma初始化</h5><p>众所周知，<a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/batch-norm.html" target="_blank" rel="noopener">批归一化BN</a>通常将参数$\gamma$和$\beta$分别初始化为1和0，也就是初始状态下BN只做归一化而不做拉伸和偏移。<br>但对于残差单元来说，将$\gamma$初始化为1其实不一定是最佳选择，残差单元的结构如下图所示，每个卷积层之后都会接一个BN层。      </p><center><img src="/imgs/bag-of-tricks/residual_unit.jpg" alt="residual_unit"></center>       <p>论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》指出<strong>将第二个BN层的$\gamma$初始化为0能取得更好的结果</strong>！此时初始状态下相当于屏蔽掉了卷积层的输出，使得初始模型更加简单，有利于初始阶段模型的训练。      </p><center><img src="/imgs/bag-of-tricks/zero_gamma.jpg" alt="zero_gamma"></center>     <p>该技巧同样适用于其他类似的shortcut结构。       </p><h5 id="4-无偏置衰减"><a href="#4-无偏置衰减" class="headerlink" title="4.无偏置衰减"></a>4.无偏置衰减</h5><p>作为正则化手段的一种，<a href="http://zh.gluon.ai/chapter_deep-learning-basics/weight-decay.html" target="_blank" rel="noopener">权重衰减</a>经常被用来约束权重过度增长，缓解过拟合现象的发生。         </p><p>除了衰减权重之外，其他参数（如偏置）能否也应用衰减技术呢？《<a href="https://arxiv.org/pdf/1807.11205.pdf" target="_blank" rel="noopener">Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes(2018)</a>》作者在ResNet-50和AlexNet上做了实验，结果表明，<strong>只有全连接层和卷积层的权重适合做衰减，偏置以及BN层的$\gamma$和$\beta$参数都不适合做衰减！</strong>           </p><center><img src="/imgs/bag-of-tricks/bn_decay_resnet50.jpg" alt="bn_decay_resnet50"></center><br><br><br><center><img src="/imgs/bag-of-tricks/bn_bias_decay_alexnet.jpg" alt="bn_bias_decay_alexnet"></center>       <p>另外，《<a href="https://openreview.net/pdf?id=rJ4uaX2aW" target="_blank" rel="noopener">Large batch training of convolutional networks with layer-wise adaptive rate scaling(2018)</a>》研究发现，在训练初期，不同层的权重的原始更新项 $\nabla L \left( w _ { t } \right)$ 和 惩罚项 $||\omega||$ 相差悬殊，比如迭代一次后的AlexNet——      </p><center><img src="/imgs/bag-of-tricks/lars_alexnet_ratio.jpg" alt="lars_alexnet_ratio"></center>        <p>这使得训练初期模型的收敛方向对初始权重和初始学习率十分敏感，而导致模型发散、不容易收敛。于是论文提出了<strong>层级自适应比例缩放（Layer-wise Adaptive Rate Scaling, LARS）</strong>，其每层的在每次更新时的学习率都是根据梯度和权重的比例计算出来的：<br>将原始的SGD更新项<br>$$\Delta w _ { t } = \lambda * \nabla L \left( w _ { t } \right)$$     </p><p>改造为<br>$$\triangle w _ { t } ^ { l } = \gamma * \lambda ^ { l } * \nabla L ( w _ { t } ^ { l } )$$       </p><p>其中，$w_t^l$第$l$层的权重，$\gamma$ 用于学习率变化的全局策略（比如整体衰减等），$\lambda^l$ 表示第$l$层的学习率，且<br>$$\lambda ^ { l } = \eta \times \frac { \left| w ^ { l } \right| } { \left| \nabla L \left( w ^ { l } \right) \right| }$$<br>其中，$\eta &lt; 1$ 参数反映了我们对“随机梯度$\nabla L \left( w _ { t } ^ { l } \right)$与真实梯度很接近”的信任程度，越接近（比如批大小越大时，随机梯度就跟真实梯度越接近），那么$\eta$参数就可以设置越大。       </p><p>应用LARS之后，模型收敛速度变得更快——      </p><center><img src="/imgs/bag-of-tricks/alexnet_lars.jpg" alt="alexnet_lars"></center>        <p>而且对于分类任务，batch_size在16K以内都不会有明显的负面影响——      </p><center><img src="/imgs/bag-of-tricks/googlenet_lars.jpg" alt="googlenet_lars"></center>         <h3 id="低精度训练"><a href="#低精度训练" class="headerlink" title="低精度训练"></a>低精度训练</h3><p>现在新出的GPU陆续开始支持低精度FP16计算，并且计算速率远高于FP32，比如V100就提供了100TFLOPS的FP16计算，而其FP32只有14TFLOPS。<br>论文《<a href="https://arxiv.org/pdf/1710.03740.pdf" target="_blank" rel="noopener">Mix Precision Training(2017)</a>》提出一种混合精度的训练过程——    </p><ul><li>用FP16存储所有的参数和激活      </li><li>用FP16计算梯度     </li><li>每份参数都保留一份FP32副本用于更新（更新后再转为FP16）</li><li>为损失乘以一个缩放因子使得FP32的梯度可以更好得跟FP16数据对齐（论文3.2节）<br>  由于FP32比FP16拥有更多的数据位保存指数部分，使得相对于FP16，FP32可以保存小得多或大得多的数值，那么当数值超出FP16范围时就会被截断而导致训练过程中梯度消失或扩散，最终模型发散无法收敛。论文经过实验表明，给FP16的梯度乘以8（即增加三位的指数部分）就足以使训练结果与纯FP32的结果相当。      </li></ul><p>论文经过实验证明，即使使用混合精度也可以训练出跟纯FP32接近的效果——      </p><center><img src="/imgs/bag-of-tricks/mixed_precision_result.jpg" alt="mixed_precision_result"></center>         <p>而且在检测任务上，如果不对损失进行缩放对齐，有可能会导致训练发散——     </p><center><img src="/imgs/bag-of-tricks/mixed_precision_detection.jpg" alt="mixed_precision_detection"></center>         <h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>baseline：批大小256，FP32<br>efficient：批大小1024，FP16    </p><center><img src="/imgs/bag-of-tricks/efficient_results.jpg" alt="efficient_results"></center>         ]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MXNet上的重训练量化</title>
      <link href="/2019/01/23/MXNet-RT_Quantization/"/>
      <url>/2019/01/23/MXNet-RT_Quantization/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=820284&auto=0&height=32"></iframe> <p>本月初随手写了个简单的pip包，用来计算mxnet-gluon模型的参数数量和运算开销，从《<a href="/2019/01/07/MXNet-OpSummary/#结果">模型参数与运算量 - 结果 | Hey~YaHei!</a>》可以看到我们常用的CNN网络的大小和运算开销都是参差不齐的——比如常用的MobileNetv1虽然比ResNet50v1少了约6%的精度，但参数数量和运算量上看，ResNet50v1竟然是MobileNetv1的七倍左右！    </p><p>而从《<a href="/2018/08/05/MobileNets_v1/">MobieleNets v1模型解析 | Hey~YaHei!</a>》可以看到，与ResNet不同的是，MobileNets采用的是一种非常紧凑、高效的卷积计算。除了这种方式，还有许多模型压缩的技巧，比如按《<a href="https://www.jiqizhixin.com/articles/2018-05-22-9" target="_blank" rel="noopener">当前深度神经网络模型压缩和加速都有哪些方法？ - 机器之心</a>》，可以把模型压缩分为<strong>参数修剪和共享</strong>、<strong>低秩因子分解</strong>、<strong>转移/紧凑卷积滤波器</strong>、<strong>知识蒸馏</strong>四个大类。      </p><p>目前应用的比较多的，应该是属于参数修剪和共享类的<strong>裁剪</strong>和<strong>量化</strong>技术。模型压缩的水还很深，我只是这一两个月才开始入的门，不敢瞎扯。本文接下来只简单讨论一下<strong><em>量化</em></strong>技术。     </p><h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p>参考：《<a href="https://www.jiqizhixin.com/articles/2018-06-01-11" target="_blank" rel="noopener">超全总结：神经网络加速之量化模型 | PaperWeekly</a>》<br>简单的说，量化就是降低模型运算的精度，比如把32位的浮点运算变为8位的定点运算（甚至在二值网络或三值网络中乘法运算还变成了简单的加减运算），从而达到大幅度的压缩和加速模型。<br>常见的线性量化过程可以用以下数学表达式来表示：<br>$$r = Round(S(q - Z))$$<br>其中，<br>$q$ 是float32的原始值，<br>$Z$ 是float32的偏移量，<br>$S$ 是float32的缩放因子，<br>$Round(\cdot)$ 是四舍五入近似取整的数学函数，<br>$r$ 是量化后的一个整数值       </p><p>$S$ 和 $Z$ 是量化的两个参数，如何找到合适的 $S$ 和 $Z$ 正是大家研究量化技术的最终目标。<br>或者可以换一个角度看，量化研究的是<strong>表示范围与精确度的权衡</strong>。        </p><h3 id="零点位置：对称量化和非对称量化"><a href="#零点位置：对称量化和非对称量化" class="headerlink" title="零点位置：对称量化和非对称量化"></a>零点位置：对称量化和非对称量化</h3><p>参考：<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Algorithms - Quantization | Distiller</a><br>$Z$ 参数的选择可以分为两类——对称和非对称。      </p><h4 id="对称量化"><a href="#对称量化" class="headerlink" title="对称量化"></a>对称量化</h4><p><center><img src="/imgs/MXNet-RT_Quantization/symmetric-mode.png" alt="symmetric-mode"></center><br>在对称量化中，$r$ 是用有符号的整型数值来表示的，此时 $Z=0$，且$r=0$ 正好是 $\frac{max(q)-min(q)}{2}$ 的量化结果。<br>比如简单地取，<br>$$S = \frac{2^{n-1} - 1}{max(|x|)}$$<br>$$Z = 0$$<br>其中，<br>$n$ 是用来表示该数值的位宽，<br>$x$ 是数据集的总体样本。       </p><p>对称量化比较简单，不仅实现简单，而且由于$Z=0$运算也变得非常简单。      </p><h4 id="非对称量化"><a href="#非对称量化" class="headerlink" title="非对称量化"></a>非对称量化</h4><p><center><img src="/imgs/MXNet-RT_Quantization/asymmetric-mode.png" alt="asymmetric-mode"></center><br>比如简单地取，<br>$$S = \frac{2^{n-1} - 1}{max(x)-min(x)}$$<br>$$Z = min(x)$$<br>非对称量化比较灵活，通常 $r$ 是用无符号的整型数值来表示，此时 $Z \neq 0$。          </p><h3 id="线性量化和截断量化【占坑】"><a href="#线性量化和截断量化【占坑】" class="headerlink" title="线性量化和截断量化【占坑】"></a>线性量化和截断量化【占坑】</h3><p>还没看DoReFa和PACT的论文……        </p><h3 id="逐层量化和逐通道量化"><a href="#逐层量化和逐通道量化" class="headerlink" title="逐层量化和逐通道量化"></a>逐层量化和逐通道量化</h3><p><strong>逐层量化</strong>在整一层上使用同一对量化参数；<br><strong>逐通道量化</strong>则是对每一个通道使用单独的量化参数；<br><em>后文中的重训练量化实现采用的是逐层量化</em>            </p><h3 id="训练后量化和重训练量化"><a href="#训练后量化和重训练量化" class="headerlink" title="训练后量化和重训练量化"></a>训练后量化和重训练量化</h3><h4 id="训练后量化"><a href="#训练后量化" class="headerlink" title="训练后量化"></a>训练后量化</h4><ul><li>最简单的方式是直接利用<strong>最大最小值</strong>，比如量化权重时直接计算每一层里的权重的最大最小值然后代入上述的式子中计算S和Z，计算比较简单，即使在实际应用当中在加载模型时计算量化参数也不会有很大开销；      </li><li>更为复杂的则是采用<strong>聚类</strong>算法，而不是简单的确定S和Z，此时运算会变得比较复杂——相邻两个数值间的差距不再是固定的，需要通过查表的方式来实现；     </li><li>由于输入和激活是动态的，不像静态的权重可以直接事先计算好S和Z，所以在训练后量化中经常会将为输入动态计算S和Z；     </li><li>为了让模型计算更快，也有一些技巧可以静态量化输入和激活，比如<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">Quantization - MXNet | github</a> 允许模型通过平滑平均的方式为每一个输入和激活确定静态的S和Z；          </li></ul><p>除此之外，一些论文以及《<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Algorithms - Quantization | Distiller</a>》指出——      </p><ol><li>量化位宽小于8时，模型精度会出现比较严重的下降；      </li><li>对于ResNet等冗余网络，训练后量化已经可以取得不错的效果；但对于MobileNet等紧凑的网络，训练后量化会对精度造成比较大的伤害；    </li><li>对第一层卷积、最后一层全连接层进行量化会对精度造成比较大的伤害……      </li></ol><h4 id="重训练量化"><a href="#重训练量化" class="headerlink" title="重训练量化"></a>重训练量化</h4><p>参考：《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2017)</a>》      </p><p><center><img src="/imgs/MXNet-RT_Quantization/retrain-quantize.jpg" alt="retrain-quantize"></center><br>由于训练后量化会对MobileNet等紧凑网络的精度造成比较大的伤害，所以有人开始提出要进行重训练量化……<br>思路非常简单，<strong>把量化参数塞入网络当中，并在训练过程中模拟量化过程（缩放、偏移、截断、复原），用32位浮点数进行训练，最终将训练得到的量化参数固化到网络当中</strong>。       </p><p>根据<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize#quantized-accuracy-results" target="_blank" rel="noopener">Tensorflow - Quantize</a>模块的描述，Mobilenet-v1-224-1经过重训练后在ImageNet上的准确率从70.9%下降到69.7%（降幅1.2%）。<br>关于量化对准确率的影响，还有两件事情让人费解：        </p><ol><li>按照<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">MXNet - Quantization</a>给出的数据，仅仅使用校准而非重训练实现的静态量化，MobieleNet-1.0的准确率居然只下降了<strong>0.15%</strong>？！？！？！着实让人吃惊，不过要注意到它的单精度推断准确率也只有69.76%、而且测试数据集是他自己提供的一个<code>val_256_q90.rec</code>的1.4G数据集——对，我试着探索了一番始终没查到这个数据集从哪来的，难道是MXNet自己做的？<br> <img src="/imgs/MXNet-RT_Quantization/mxnet_quantize.jpg" alt="mxnet_quantize">     </li><li><p>按照<a href="http://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html" target="_blank" rel="noopener">Gluon Model zoo</a>的数据，MobileNetv1_224_1.0和MobileNetv2_224_1.0的准确率分别为<strong>71.05%</strong>和<strong>71.92%</strong>（文档没说测试集是哪，但一般应该用的是ILSVRC2012的验证集吧）；另外按照<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize#quantized-accuracy-results" target="_blank" rel="noopener">Tensorflow - Quantize</a>的数据，MobileNetv1_224_1.0和MobileNetv2_224_1.0的准确率分别为<strong>70.9%</strong>和<strong>71.8%</strong>（依旧没有指明测试集，应该也是ILSVRC2012验证集吧，不过这不是重点）——但到了<a href="https://gluon-cv.mxnet.io/model_zoo/classification.html" target="_blank" rel="noopener">Gluon CV</a>的数据，MobileNetv1_224_1.0和MobileNetv2_224_1.0的准确率分别为<strong>73.28%</strong>和<strong>71.92%</strong>（其实他没指明输入图片尺寸，不过inception指明是299了，那其他网络应该也是默认的224吧）——v1竟然反而比v2多了一个百分点还多？？（黑人问号……）       </p><p> <strong>Gluon Model Zoo：</strong><br> <img src="/imgs/MXNet-RT_Quantization/gluon_model_zoo_mobilenet.jpg" alt="gluon_model_zoo_mobilenet">      </p><p> <strong>Tensorflow - Quantize：</strong><br> <img src="/imgs/MXNet-RT_Quantization/tensorflow_quantize.jpg" alt="tensorflow_quantize">     </p><p> <strong>Gluon CV：</strong><br> <img src="/imgs/MXNet-RT_Quantization/gluon_cv_mobilenet.jpg" alt="gluon_cv_mobilenet">      </p></li></ol><h2 id="重训练量化的MXNet实现"><a href="#重训练量化的MXNet实现" class="headerlink" title="重训练量化的MXNet实现"></a>重训练量化的MXNet实现</h2><p>MXNet提供了简单的量化——<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">Quantization - MXNet | github</a>，不过可惜只支持简单的训练后量化。所以我决定参考Intel基于Pytorch实现的<a href="https://github.com/NervanaSystems/distiller" target="_blank" rel="noopener">Distiller</a>和谷歌的论文，实现一个简单的MXNet网络的重训练量化。       </p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>项目已开源至<a href="https://github.com/hey-yahei/Quantization.MXNet" target="_blank" rel="noopener">github</a>，这里只写一下大体的思路：     </p><ul><li>总体思路就是进行模拟量化，然后把量化参数作为可训练参数进行训练；      </li><li>将relu都替换为relu6，因为我发现量化后如果还用relu，冗余网络影响不大，但对紧凑网络来说精度会骤减5%-6%，伤害很大；    </li><li>改造gluon网络的思路跟我先前写的 <a href="https://github.com/hey-yahei/OpSummary.MXNet" target="_blank" rel="noopener">OpSummary.MXNet | github</a> 类似，往相关的Block塞Parameter、重写hybrid_forward对象函数、借助钩子（forward_hook和forward_pre_hook）来收集更新某些平滑参数；     </li><li>通过解析参数名来匹配对应的参数（主要是在合并BN阶段）；      </li><li>利用MXNet提供的底层库函数（MXQuantizeSymbol）固化量化模型的结构，然后将对应的参数解算或映射到最终的参数名和数据类型（官方的底层库不支持静态量化，所幸这个改动不算麻烦，自己写了个脚本去把动态量化的max、min改为静态的数值就行）；    </li><li>训练前期输入和激活的量化用动态实现，同时用指数平滑平均（EMA）累积训练集上的最大最小值，当累积结果相对稳定（比如迭代一万次后）再改为静态量化对其他参数进行微调；        </li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><h4 id="量化前后"><a href="#量化前后" class="headerlink" title="量化前后"></a>量化前后</h4><p>调试过程比较繁琐，所以为简单起见还没用ImageNet进行实验而使用了CIFAR100数据集（20分类，50000训练集，10000验证集），统一不使用数据增强来进行训练。分别训练出准确率分别为<strong>83.45%</strong>、<strong>84.20%</strong>、<strong>89.35%</strong>的<strong>MobileNet_1_0</strong>、<strong>MobileNet_1_0_ReLU6</strong>和<strong>ResNet50_v1</strong>作为baseline——     </p><table><thead><tr><th style="text-align:center">Quantization</th><th style="text-align:center">MobileNet_1_0_ReLU</th><th style="text-align:center">MobileNet_1_0_ReLU6</th><th style="text-align:center">ResNet50_v1</th></tr></thead><tbody><tr><td style="text-align:center">FP32</td><td style="text-align:center">83.45%</td><td style="text-align:center">84.20%</td><td style="text-align:center">89.35%</td></tr><tr><td style="text-align:center">UINT8_ONLINE</td><td style="text-align:center">76.61%</td><td style="text-align:center">77.66%</td><td style="text-align:center">89.11%</td></tr><tr><td style="text-align:center">UINT8_OFFLINE_CALIB</td><td style="text-align:center">72.10%</td><td style="text-align:center">77.44%</td><td style="text-align:center">88.96%</td></tr><tr><td style="text-align:center">UINT8_OFFLINE_RETRAIN</td><td style="text-align:center">80.72%</td><td style="text-align:center">83.03%</td><td style="text-align:center">/</td></tr><tr><td style="text-align:center">UINT8_OFFLINE_FAKEBN</td><td style="text-align:center">80.52%</td><td style="text-align:center">83.00%</td><td style="text-align:center">/</td></tr></tbody></table><p>其中，<br><strong>FP32</strong> 为单精度浮点下的模型；<br><strong>UINT8_ONLINE</strong> 为8位非对称量化、动态量化激活的模型；<br><strong>UINT8_OFFLINE_CALIB</strong> 为8位非对称量化、静态量化激活（在整个训练集上前向传播一次后用EMA校准最大最小值）的模型；<br><strong>UINT8_OFFLINE_RETRAIN</strong> 为8位非对称量化、重训练（不合并BN层）的模型；<br><strong>UINT8_OFFLINE_FAKEBN</strong> 为8位非对称量化、重训练（伪BN操作）的模型；<br><em>由于即使不重训练，ResNet精度也没有很明显的下降，所以调试过程中没有尝试对ResNet做重训练。</em>        </p><p>可以看到，即使没有重训练，量化后的Resnet50_v1也只有少量的精度下降；<br>但MobileNet_1_0精度下降非常明显，静态量化激活甚至会导致精度下降11%左右，不过经过重训练后静态量化激活的模型精度只下降了3.5%左右（其实依旧很多）……后来发现这是激活函数的缘故，把ReLU都替换成ReLU6之后，重训练静态量化的模型精度仅仅下降1.2%！<br>关于RELU6可以参考：<a href="https://stackoverflow.com/questions/47220595/why-the-6-in-relu6/47220765" target="_blank" rel="noopener">Why the 6 in relu6? | stackoverflow</a>，简单来说，ReLU6可以将定点数的整数部分限制在6以内，防止量化误差在传播过程中过分扩大。      </p><h4 id="伪·批归一化"><a href="#伪·批归一化" class="headerlink" title="伪·批归一化"></a>伪·批归一化</h4><p>不过吧，细心的你可能会想起来为了加快推断速度，往往会将BN层融到卷积、全连接等线性层中去（参考《<a href="/2018/08/08/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析 - BN层合并 | Hey~YaHei!</a>》）。现在BN层和卷积层之间隔着一个非线性的量化（因为有截断、取整、取最值的过程），这可怎么办？既然量化后不方便合并BN层，那在量化前（重训练前），提前把BN层融合掉好了，这就是论文《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2017)</a>》提到的伪BatchNorm操作，把BN层的参数都丢给卷积层，训练过程中卷积层既要做伪归一化，又要做伪量化！       </p><p><center><img src="/imgs/MXNet-RT_Quantization/fake_bn.jpg" alt="fake_bn"></center><br>当然，这是有额外代价的，训练的前向传播过程中，需要做两次卷积运算——      </p><ol><li>用原始权重卷积，卷积输出用于更新伪BatchNorm的平均值、标准差，这次卷积运算不需要反向传播；       </li><li>另外一次用量化过的权重卷积，卷积输出的结果作为下一层的输入，这一次的卷积运算需要反向传播         </li></ol><p>这样一来，训练速度大概会下降30%-40%左右（实测）；<br>另外，不引入伪·批归一化的情况下，重训练收敛非常快！几乎算是迭代几百次（哦，我用的是Adam）就接近收敛，引入后精度下降特别厉害（起初甚至以为我程序有问题，然后反复验证我伪BN的输出结果），需要训一段时间才能得到最后的结果：    </p><p><center><img src="/imgs/MXNet-RT_Quantization/mobilenet_quant_relu6.jpg" alt="mobilenet_quant_relu6"></center>      </p><p><center><img src="/imgs/MXNet-RT_Quantization/mobilenet_quant_relu6_fakebn.jpg" alt="mobilenet_quant_relu6_fakebn"></center><br><em>图(上)：不合并BN的学习曲线（前一万次迭代采用动态量化(lr=1e-5)，之后转为静态量化做微调(lr=1e-5)）</em><br><em>图(下)：合并BN后的学习曲线（前一万次迭代采用动态量化(lr=1e-5)，之后转为静态量化做微调(lr=1e-6)）</em>     </p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ol><li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize" target="_blank" rel="noopener">Tensorflow - Quantize</a>：Tensorflow的重训练量化模块      </li><li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization" target="_blank" rel="noopener">Tensorflow - Quantization</a>：Tensorflow的量化OP       </li><li><a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">MXNet - Quantization</a>：MXNet的训练后量化模块     </li><li><a href="https://nervanasystems.github.io/distiller/" target="_blank" rel="noopener">Neural Network Distiller</a>：Intel的开源模型压缩库（基于Pytorch）      </li><li>《<a href="https://arxiv.org/pdf/1806.08342.pdf" target="_blank" rel="noopener">Quantizing deep convolutional networks for efficient inference: A whitepaper</a>》<br> 谷歌Tensorflow官方发布的量化白皮书，译文可参考 <a href="https://blog.csdn.net/guvcolie/article/details/81286349" target="_blank" rel="noopener">CSDN上Colie-Li的翻译</a>      </li><li>《<a href="https://www.jiqizhixin.com/articles/2018-05-22-9" target="_blank" rel="noopener">当前深度神经网络模型压缩和加速都有哪些方法？ | PaperWeekly, 小一一</a>    </li><li>《<a href="https://www.jiqizhixin.com/articles/2018-05-18-4" target="_blank" rel="noopener">让机器“删繁就简”：深度神经网络加速与压缩 | 深度学习大讲堂, 程健</a>》    </li><li>《<a href="https://www.jiqizhixin.com/articles/2018-06-01-11" target="_blank" rel="noopener">超全总结：神经网络加速之量化模型 | PaperWeekly, 郝泽宇</a>》    </li><li>《<a href="https://blog.csdn.net/daniaokuye/article/details/82746661" target="_blank" rel="noopener">模型压缩开源库整理 | CSDN, 库页</a>》      </li></ol><h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><ol><li>编了freeze的代码，输出来的静态图结构看起来也符合预期，但目前还没实际加载运行过 <del>（因为不巧，MXNet对量化的支持还不够好，官方的<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">Quantization - MXNet</a>分为普通CPU推断和Xeon系列的CPU推断，前者竟然不支持带group的卷积，实验室的电脑又正好都是i系列的，十分头疼= =等开了学我再想办法找台Intel Xeon处理器的电脑测试一下）</del> 好像不是处理器的缘故，mkldnn应该是支持所有的intel处理器的吧，现在关键是<code>Symbol</code>居然没有<code>get_backend_symbol</code>方法，我装的应该是最新版本才对啊；    </li><li>除了验证freeze结果之外，还需要测一下量化后的模型推断速度有多大提升；      </li><li>尝试逐通道量化；      </li><li>训完的模型目前只能在台式机跑跑，想再看看有没有可能在Tengine或ncnn上跑跑我的量化模型；      </li><li>看起来量化的效果还不错，但我比较好奇量化后的模型跨数据集时会不会出现严重的精度下降，可以做个实验比较一下~     </li><li>目前只对分类网络进行量化，想试一下对检测网络MobileNet-SSD和人脸识别网络MobileFaceNet网络的量化结果；      </li><li>继续看看DoReFa和PACT的量化方式；       </li></ol><p>代码还比较粗糙，等着开了学再进一步完善吧！    </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>模型参数与计算量</title>
      <link href="/2019/01/07/MXNet-OpSummary/"/>
      <url>/2019/01/07/MXNet-OpSummary/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=449578848&auto=0&height=32"></iframe>     <p>这一两个月比较忙，没什么时间空下来写写博文，加上最近处于摸索阶段，各种思路还没有理清，不敢瞎写。     </p><p>这两天看到Lyken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>，萌生了一个写一个MXNet的计数器的想法，项目已经开源到<a href="https://github.com/hey-yahei/OpSummary.MXNet" target="_blank" rel="noopener">github</a>上，并且做个pip的包，嘻嘻……第一次做包，虽然只是一个简单的工具，还是截图留个念——<br><img src="/imgs/MXNet-OpSummary/mxop-pip.png" alt="mxop-pip">         </p><h3 id="参数量与计算量"><a href="#参数量与计算量" class="headerlink" title="参数量与计算量"></a>参数量与计算量</h3><ul><li><strong>参数量</strong> 是指模型含有多少参数，直接决定模型文件的大小，也影响模型推断时对内存的占用量     </li><li><strong>计算量</strong> 是指模型推断时需要多少计算次数，通常是以<strong><em>MAC(Multiply ACcumulate，乘积累加)</em></strong>次数来表示    </li></ul><p>这两者其实是评估模型时非常重要的参数，<strong>一个实际要应用的模型不应当仅仅考虑它在准确率上有多出色的表现，还应该要考虑它的鲁棒性、扩展性以及对资源的依赖程度</strong>，但事实上很多论文都不讨论他们模型需要多少计算力，一种可能是他们的定位还是纯学术研究——提出一种新的思路，即使这种思路不便于应用，但未来说不定计算力上来了，或者有什么飞跃性的改进方法来改进这一问题，或者提出自己的思路来启发其他研究者的研究（抛砖引玉）；另一种可能就是……他们在有意识地回避这一问题，我总觉得很多人是在回避这一问题，无论是论文还是各种AI比赛的解决方案（比赛本身只关注准确率指标本身也不够合理）。      </p><p>接下来，我们试着用不同的视角重新审视以前那些常用的CNN OP——       </p><h4 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h4><p>首先考虑一个3输入、3输出、有偏置的全连接层（Layer2），       </p><p><center><img height="300" src="/imgs/MXNet-OpSummary/fc.png"></center><br>$$<br>\begin{array} { l } { a _ { 1 } ^ { ( 2 ) } = x _ { 0 } + w _ { 11 } x _ { 1 } + w _ { 12 } x _ { 2 } + w _ { 13 } x _ { 3 } } \\ { a _ { 2 } ^ { ( 2 ) } = x _ { 0 } + w _ { 21 } x _ { 1 } + w _ { 22 } x _ { 2 } + w _ { 23 } x _ { 3 } } \\ { a _ { 3 } ^ { ( 2 ) } = x _ { 0 } + w _ { 31 } x _ { 1 } + w _ { 32 } x _ { 2 } + w _ { 33 } x _ { 3 } } \end{array}<br>$$<br>其参数数量为 $3\times3$，乘加次数为 $3\times3$。<br>这是一个典型的矩阵和向量之间的乘法运算，<br>推广到n输入、m输出、有偏置的全连接层，其参数数量为 $m \times n$，乘加次数为 $m \times n$。      </p><h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><p>首先考虑一个单通道输入输出，输出图大小为 $m_o \times n_o$，核大小为 $k_m \times k_n$，带偏置，步长1，不补零的卷积，       </p><p><center><img height="300" src="/imgs/MXNet-OpSummary/convolution.gif"></center><br>$$<br>\begin{aligned} O _ { 11 } = &amp; w _ { 11 } I _ { 11 } + w _ { 12 } I _ { 12 } + w _ { 13 } I _ { 13 } + \\ &amp; w _ { 21 } I _ { 21 } + w _ { 22 } I _ { 22 } + w _ { 23 } I _ { 23 } + \\ &amp; w _ { 31 } I _ { 31 } + w _ { 32 } I _ { 32 } + w _ { 33 } I _ { 33 } + b \end{aligned}<br>$$<br>其参数数量为 $k_m \times k_n + 1$，乘加次数为 $m_o \times n_o \times k_m \times k_n$。<br>推广到$c_i$通道输入，$c_o$通道输出的情况，其参数数量为 $(k_m \times k_n + 1) \times c_i \times c_o$，乘加次数为 $m_o \times n_o \times k_m \times k_n \times c_i \times c_o$。       </p><p>这是标准卷积的情况，如果是深度向分解的卷积，参考博文《<a href="/2018/08/05/MobileNets_v1/#效率比较">MobileNets v1模型解析/深度向卷积分解/效率比较 | Hey~YaHei!</a>》可以知道，其参数数量为 $(k_m \times k_n + c_o) \times c_i$，乘加次数为 $m_o \times n_o \times (k_m \times k_n + c_o) \times c_i$，这里为简化运算忽略了偏置。      </p><h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><p>池化跟卷积的操作比较相近，最大池化仅仅是比较操作，其计算量往往可以忽略不计；平均池化则会涉及到 $m_o \times m_i \times (k_m \times k_n - 1) \times c$ 次加法和 $m_o \times n_o \times c$ 次除法（输入输出通道均为 $c$），两者都没有参数。      </p><h4 id="批归一化BN"><a href="#批归一化BN" class="headerlink" title="批归一化BN"></a>批归一化BN</h4><p>假设输入数量为N，<br>$$bn[j] = \frac{\gamma (conv[j] - mean)}{\sqrt{variance}} + \beta$$<br>可以很容易看到，其参数数量为 $2N$，运算包含 $2N$ 次加法（包括减法）和 $N$ 次乘法。<br>这里要注意，在推断过程中，variance和mean都是已知的，所以 $\frac{\gamma}{\sqrt{variance}}$ 可以直接合并为一个值，<br>甚至，博文《<a href="/2018/08/08/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析/BN层合并 | Hey~YaHei!</a>》提到过BN层可以直接融入前边的线性层（如卷积和全连接），此时BN层不会造成任何开销。     </p><h3 id="OpSummary"><a href="#OpSummary" class="headerlink" title="OpSummary"></a>OpSummary</h3><p><a href="https://github.com/hey-yahei/OpSummary.MXNet" target="_blank" rel="noopener">hey-yahei/OpSummary.MXNet | github</a><br>知道了如何计算各层的参数数量和运算次数，我们就可以编写一个小工具来为MXNet模型统计参数数量和计算量。     </p><h4 id="钩子"><a href="#钩子" class="headerlink" title="钩子"></a>钩子</h4><p>为了计算运算量，必须能够取得每一层的参数、输入和输出大小，当然可以根据各层的参数一层层的推算，但这似乎太麻烦了。受Lyken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>启发，我们可以为每个Block注册一个hook，每次Block经过前向传播后都会调用这个hook（准确的说，有两种hook，pre_hook在前向传播前调用，hook在前向传播后调用）。      </p><h4 id="超参数获取"><a href="#超参数获取" class="headerlink" title="超参数获取"></a>超参数获取</h4><p>MXNet中想读取一个Block的超参数实在有些麻烦，因为它把超参数全都存在私有属性里了！——<a href="https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/nn/conv_layers.py#L105" target="_blank" rel="noopener">mxnet/gluon/nn/conv_layers.py#L105 | github</a>，不像Pytorch的Module是直接把超参数放在公共属性上——<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L20" target="_blank" rel="noopener">torch/nn/modules/conv.py#L20 | github</a>。<br>有两种思路来获取超参数——     </p><ol><li>从输入输出、公共变量（如Conv的weight和bias）的shape来推断    </li><li>解析字符串<br> MXNet的Block都重载了 <code>__repr__</code> 方法，比如<a href="https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/nn/conv_layers.py#L143" target="_blank" rel="noopener">mxnet/gluon/nn/conv_layers.py#L143 | github</a>，用于打印Block的超参数。那……我们其实可以用 <code>str(nn.Block)</code> 的方式来取得这个字符串，然后进行解析= =好麻烦啊      </li></ol><h4 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h4><p>yken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>在统计各个模块的参数数量和运算次数时，是注册了一个公共的缓冲区来进行累加（参考<a href="https://github.com/Lyken17/pytorch-OpCounter/blob/master/thop/utils.py#L28" target="_blank" rel="noopener">pytorch-OpCounter/thop/utils | github</a>），而MXNet并没有提供这样的缓冲区（<em>或许只是我不知道？</em>），我的解决办法是——<br>写一个拥有静态变量的函数作为累加函数，调用Block的apply方法，让每个Children把自己的统计结果依次累加给这个静态变量，最后从静态变量取出统计结果。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>用OpSummary把MXNet提供的model_zoo里所有的模型都测试了一遍，结果如下表所示：<br>Top1 Acc和Top5 Acc数据来源于 <a href="http://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html" target="_blank" rel="noopener">MXNet文档</a>    </p><table><thead><tr><th style="text-align:right">Model</th><th style="text-align:right">Params(M)</th><th style="text-align:right">Muls(G)</th><th style="text-align:right">*Params(M)</th><th style="text-align:right">*Muls(G)</th><th style="text-align:right">Top1 Acc</th><th style="text-align:right">Top5 Acc</th></tr></thead><tbody><tr><td style="text-align:right">AlexNet</td><td style="text-align:right">61.10</td><td style="text-align:right">0.71</td><td style="text-align:right">2.47</td><td style="text-align:right">0.66</td><td style="text-align:right">0.5492</td><td style="text-align:right">0.7803</td></tr><tr><td style="text-align:right">VGG11</td><td style="text-align:right">132.86</td><td style="text-align:right">7.61</td><td style="text-align:right">9.22</td><td style="text-align:right">7.49</td><td style="text-align:right">0.6662</td><td style="text-align:right">0.8734</td></tr><tr><td style="text-align:right">VGG13</td><td style="text-align:right">133.04</td><td style="text-align:right">11.30</td><td style="text-align:right">9.40</td><td style="text-align:right">11.18</td><td style="text-align:right">0.6774</td><td style="text-align:right">0.8811</td></tr><tr><td style="text-align:right">VGG16</td><td style="text-align:right">138.63</td><td style="text-align:right">15.47</td><td style="text-align:right">14.71</td><td style="text-align:right">15.35</td><td style="text-align:right">0.7323</td><td style="text-align:right">0.9132</td></tr><tr><td style="text-align:right">VGG19</td><td style="text-align:right">143.67</td><td style="text-align:right">19.63</td><td style="text-align:right">20.02</td><td style="text-align:right">19.51</td><td style="text-align:right">0.7411</td><td style="text-align:right">0.9135</td></tr><tr><td style="text-align:right">VGG11_bn</td><td style="text-align:right">132.87</td><td style="text-align:right">7.62</td><td style="text-align:right">9.23</td><td style="text-align:right">7.49</td><td style="text-align:right">0.6859</td><td style="text-align:right">0.8872</td></tr><tr><td style="text-align:right">VGG13_bn</td><td style="text-align:right">133.06</td><td style="text-align:right">11.32</td><td style="text-align:right">9.42</td><td style="text-align:right">11.20</td><td style="text-align:right">0.6884</td><td style="text-align:right">0.8882</td></tr><tr><td style="text-align:right">VGG16_bn</td><td style="text-align:right">138.37</td><td style="text-align:right">15.48</td><td style="text-align:right">14.73</td><td style="text-align:right">15.36</td><td style="text-align:right">0.7310</td><td style="text-align:right">0.9176</td></tr><tr><td style="text-align:right">VGG19_bn</td><td style="text-align:right">143.69</td><td style="text-align:right">19.65</td><td style="text-align:right">20.05</td><td style="text-align:right">19.52</td><td style="text-align:right">0.7433</td><td style="text-align:right">0.9185</td></tr><tr><td style="text-align:right">Inception_v3</td><td style="text-align:right">23.87</td><td style="text-align:right">5.72</td><td style="text-align:right">21.82</td><td style="text-align:right">5.72</td><td style="text-align:right">0.7755</td><td style="text-align:right">0.9364</td></tr><tr><td style="text-align:right">ResNet18_v1</td><td style="text-align:right">11.70</td><td style="text-align:right">1.82</td><td style="text-align:right">11.19</td><td style="text-align:right">1.82</td><td style="text-align:right">0.7093</td><td style="text-align:right">0.8992</td></tr><tr><td style="text-align:right">ResNet34_v1</td><td style="text-align:right">21.81</td><td style="text-align:right">3.67</td><td style="text-align:right">21.3</td><td style="text-align:right">3.67</td><td style="text-align:right">0.7437</td><td style="text-align:right">0.9187</td></tr><tr><td style="text-align:right">ResNet50_v1</td><td style="text-align:right">25.63</td><td style="text-align:right">3.87</td><td style="text-align:right">23.58</td><td style="text-align:right">3.87</td><td style="text-align:right">0.7647</td><td style="text-align:right">0.9313</td></tr><tr><td style="text-align:right">ResNet101_v1</td><td style="text-align:right">44.70</td><td style="text-align:right">7.59</td><td style="text-align:right">42.65</td><td style="text-align:right">7.58</td><td style="text-align:right">0.7834</td><td style="text-align:right">0.9401</td></tr><tr><td style="text-align:right">ResNet152_v1</td><td style="text-align:right">60.40</td><td style="text-align:right">11.30</td><td style="text-align:right">58.36</td><td style="text-align:right">11.30</td><td style="text-align:right">0.7900</td><td style="text-align:right">0.9438</td></tr><tr><td style="text-align:right">ResNet18_v2</td><td style="text-align:right">11.70</td><td style="text-align:right">1.82</td><td style="text-align:right">11.18</td><td style="text-align:right">1.82</td><td style="text-align:right">0.7100</td><td style="text-align:right">0.8992</td></tr><tr><td style="text-align:right">ResNet34_v2</td><td style="text-align:right">21.81</td><td style="text-align:right">3.67</td><td style="text-align:right">21.30</td><td style="text-align:right">3.67</td><td style="text-align:right">0.7440</td><td style="text-align:right">0.9208</td></tr><tr><td style="text-align:right">ResNet50_v2</td><td style="text-align:right">25.60</td><td style="text-align:right">4.10</td><td style="text-align:right">23.55</td><td style="text-align:right">4.10</td><td style="text-align:right">0.7711</td><td style="text-align:right">0.9343</td></tr><tr><td style="text-align:right">ResNet101_v2</td><td style="text-align:right">44.64</td><td style="text-align:right">7.82</td><td style="text-align:right">42.59</td><td style="text-align:right">7.81</td><td style="text-align:right">0.7853</td><td style="text-align:right">0.9417</td></tr><tr><td style="text-align:right">ResNet152_v2</td><td style="text-align:right">60.33</td><td style="text-align:right">11.54</td><td style="text-align:right">58.28</td><td style="text-align:right">11.53</td><td style="text-align:right">0.7921</td><td style="text-align:right">0.9431</td></tr><tr><td style="text-align:right">DenseNet121</td><td style="text-align:right">8.06</td><td style="text-align:right">2.85</td><td style="text-align:right">7.04</td><td style="text-align:right">2.85</td><td style="text-align:right">0.7497</td><td style="text-align:right">0.9225</td></tr><tr><td style="text-align:right">DenseNet161</td><td style="text-align:right">28.90</td><td style="text-align:right">7.76</td><td style="text-align:right">26.69</td><td style="text-align:right">7.76</td><td style="text-align:right">0.7770</td><td style="text-align:right">0.9380</td></tr><tr><td style="text-align:right">DenseNet169</td><td style="text-align:right">14.31</td><td style="text-align:right">3.38</td><td style="text-align:right">12.64</td><td style="text-align:right">3.38</td><td style="text-align:right">0.7617</td><td style="text-align:right">0.9317</td></tr><tr><td style="text-align:right">DenseNet201</td><td style="text-align:right">20.24</td><td style="text-align:right">4.32</td><td style="text-align:right">18.32</td><td style="text-align:right">4.31</td><td style="text-align:right">0.7732</td><td style="text-align:right">0.9362</td></tr><tr><td style="text-align:right">MobileNet_v1_1.00</td><td style="text-align:right">4.25</td><td style="text-align:right">0.57</td><td style="text-align:right">3.23</td><td style="text-align:right">0.57</td><td style="text-align:right">0.7105</td><td style="text-align:right">0.9006</td></tr><tr><td style="text-align:right">MobileNet_v1_0.75</td><td style="text-align:right">2.60</td><td style="text-align:right">0.33</td><td style="text-align:right">1.83</td><td style="text-align:right">0.33</td><td style="text-align:right">0.6738</td><td style="text-align:right">0.8782</td></tr><tr><td style="text-align:right">MobileNet_v1_0.50</td><td style="text-align:right">1.34</td><td style="text-align:right">0.15</td><td style="text-align:right">0.83</td><td style="text-align:right">0.15</td><td style="text-align:right">0.6307</td><td style="text-align:right">0.8475</td></tr><tr><td style="text-align:right">MobileNet_v1_0.25</td><td style="text-align:right">0.48</td><td style="text-align:right">0.04</td><td style="text-align:right">0.22</td><td style="text-align:right">0.04</td><td style="text-align:right">0.5185</td><td style="text-align:right">0.7608</td></tr><tr><td style="text-align:right">MobileNet_v2_1.00</td><td style="text-align:right">3.54</td><td style="text-align:right">0.32</td><td style="text-align:right">2.26</td><td style="text-align:right">0.32</td><td style="text-align:right">0.7192</td><td style="text-align:right">0.9056</td></tr><tr><td style="text-align:right">MobileNet_v2_0.75</td><td style="text-align:right">2.65</td><td style="text-align:right">0.19</td><td style="text-align:right">1.37</td><td style="text-align:right">0.19</td><td style="text-align:right">0.6961</td><td style="text-align:right">0.8895</td></tr><tr><td style="text-align:right">MobileNet_v2_0.50</td><td style="text-align:right">1.98</td><td style="text-align:right">0.10</td><td style="text-align:right">0.70</td><td style="text-align:right">0.09</td><td style="text-align:right">0.6449</td><td style="text-align:right">0.8547</td></tr><tr><td style="text-align:right">MobileNet_v2_0.25</td><td style="text-align:right">1.53</td><td style="text-align:right">0.03</td><td style="text-align:right">0.25</td><td style="text-align:right">0.03</td><td style="text-align:right">0.5074</td><td style="text-align:right">0.7456</td></tr><tr><td style="text-align:right">SqueezeNet1_0</td><td style="text-align:right">1.25</td><td style="text-align:right">0.82</td><td style="text-align:right">0.74</td><td style="text-align:right">0.73</td><td style="text-align:right">0.5611</td><td style="text-align:right">0.7909</td></tr><tr><td style="text-align:right">SqueezeNet1_1</td><td style="text-align:right">1.24</td><td style="text-align:right">0.35</td><td style="text-align:right">0.72</td><td style="text-align:right">0.26</td><td style="text-align:right">0.5496</td><td style="text-align:right">0.7817</td></tr></tbody></table><p>由于分类网络经常用作其他框架（如目标检测的SSD）的backbone，所以这里增加了<strong>*Params</strong>列和<strong>*Muls</strong>列用于表示除去最后几个分类的Layer之后的结果。具体丢弃的层参见 <a href="https://github.com/hey-yahei/OpSummary.MXNet/blob/master/tests/test_gluon_utils.py" target="_blank" rel="noopener">mxop/tests/test_gluon_utils.py | github</a> 文件的 <code>dropped_layers</code> 变量。    </p><p><img src="/imgs/MXNet-OpSummary/Parameters.jpg" alt="Parameters"></p><p><img src="/imgs/MXNet-OpSummary/Multiplication.jpg" alt="Multiplication">   </p><p>emmm还是比较直观的嘛！希望上述的图表对大家挑选backbone的时候能有所帮助~    </p><h4 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h4><p>目前我只模仿Lyken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>实现了简单的参数与计算量计数，并且制作了pip包（你可以按照我github页面上的说明用pip安装 <strong>mxop</strong> 包）；<br>等之后有时间，我想继续      </p><ol><li>依次输出各个层的参数与计算量而不是整个模型，分析各个层的比例     </li><li>支持MXNet的静态图模型（根据json文件解析参数并推算参数量和计算量，而不是用动态图的hook）</li><li>支持MXNet的量化模型</li><li>……      </li></ol><!-----------------------------------------------------     **【几句闲话】**       从去年大四开始学习深度学习就一直在想，深度学习虽然表现很好，但计算量也非常大，这种东西真的能在实际中应用么？比如说我本科毕设用了谷歌的im2txt，一个为静态图片生成一句话描述的模型，在我的PC上用CPU跑，大概是2.5秒钟能完成一张图片的描述——如果作为一个[PC端的图片管理软件](https://github.com/hey-yahei/image-management)（为图库生成描述方便索引）勉强还是能够接受，但如果说想把这种东西应用到嵌入式上（比如我在论文中提到的，辅助盲人导航或者了解周遭世界），实时性会是一个很大的考验——说到底，从学术研究到实际应用终究还有太多路要走（我觉得关键在于方案的**鲁棒性、扩展性和小型化**，而不是那1%的准确率提升，当然，这也看领域）。【[如何看待张潼老师离职腾讯？放浪者的回答 | 知乎](https://www.zhihu.com/question/307359849/answer/565900443?from=timeline&utm_medium=social&utm_oi=75499502043136&utm_source=wechat_session)】       > 然而这跟能否应用到具体业务上具有不亚于天壤之别的鸿沟-----这个鸿沟不仅仅是国内很多企业领袖意识不到，就算是大部分研究人员自己也意识不到。在国外由于有大批量PHD水平的高级工程师甘作码农，还能够尽力去弥补research -> product/service的这个鸿沟（与其说是去弥补这个鸿沟，不如说是尽己所能去做出取舍和折中）。而在国内学术研究和工程开发严重脱节的情况下，这个鸿沟比国外要大很多很多。这是行业现状和人员储备的问题。没错，我们需要绘制蓝图创造未来的研究者、科学家，看起来光鲜亮丽、高大上的科学家，但我们还需要数十数百倍于科学家的工程师来弥补学术和业务的鸿沟，时常在想——***我们是不是真的需要这么多“画饼子”的人？***（个人的浅薄之见，说不定只是我嵌入式学多了太过实用主义，欢迎交流）    去年暑期带着疑问去了Open AI Lab实习（hhh还有幸促成了学院和Open AI Lab、Arm China在教学上的合作），第一次了解到MobileNet、ShuffleNet这些轻量化的模型以及Tengine这种可以充分有效“榨干”系统资源的前向推断引擎，并且看到他们优异的性能表现，顿时觉得深度学习在嵌入式上的应用还是很有希望的。实习回来后，又经过几个月的研究思考，了解裁剪、量化、知识蒸馏等模型压缩技术，以及Tengine、NNAPI、ncnn等优化加速的前向推理引擎和PocketFlow等自动压缩工具、TVM等优化策略搜索工具……发现其实就这两年，大家都纷纷开始重视端侧的部署这一块上。     总而言之，算是大致确定了自己的研究方向——**模型压缩**，    不如先从量化开始——定个小目标，试着在MXNet上实现重训练量化，并且能一步步解决问题，成功把模型部署到Tengine上！      -->]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>树莓派也能玩转深度学习——Tengine推断引擎</title>
      <link href="/2018/10/13/RasPi-Tengine/"/>
      <url>/2018/10/13/RasPi-Tengine/</url>
      
        <content type="html"><![CDATA[<p>一直以来，树莓派以其良好的社区生态，广受嵌入式爱好者、创客欢迎。在一些相关的社区上（比如<a href="http://shumeipai.nxez.com/what-raspi-used-for" target="_blank" rel="noopener">树莓派实验室</a>），我们可以看到非常丰富的应用示例及其教程。但在树莓派上的深度学习应用并不常见，这主要是受到树莓派计算力的限制，比如之前看到过有人把yolov2原原本本生硬地部署到树莓派上，结果每一帧检测耗时高达<strong>6分钟</strong>！！作一帧目标检测花费6分钟这实在是无法忍受的！<br><em>如果是用yolov2-tiny的话会快很多，但耗时依旧接近<strong>40秒</strong>，参考<a href="https://blog.csdn.net/wjbwjbwjbwjb/article/details/77688625" target="_blank" rel="noopener">树莓派3B上测试YOLO效果 | CSDN</a></em>     </p><p>那树莓派只能跟深度学习无缘了么？那可未必！    </p><h3 id="Tengine"><a href="#Tengine" class="headerlink" title="Tengine"></a>Tengine</h3><p><a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">OADI/Tengine | github</a>       </p><blockquote><p>Tengine 是OPEN AI LAB为嵌入式设备开发的一个轻量级、高性能并且模块化的引擎。<br>Tengine在嵌入式设备上支持CPU，GPU，DLA/NPU，DSP异构计算的计算框架，实现异构计算的调度器，基于ARM平台的高效的计算库实现，针对特定硬件平台的性能优化，动态规划计算图的内存使用，提供对于网络远端AI计算能力的访问支持，支持多级别并行，整个系统模块可拆卸，基于事件驱动的计算模型，吸取已有AI计算框架的优点，设计全新的计算图表示。    </p></blockquote><h3 id="编译安装开源版Tengine"><a href="#编译安装开源版Tengine" class="headerlink" title="编译安装开源版Tengine"></a>编译安装开源版Tengine</h3><h4 id="安装相关工具"><a href="#安装相关工具" class="headerlink" title="安装相关工具"></a>安装相关工具</h4><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span> cmake</code></pre><ul><li><strong>git</strong> 是一个版本控制系统，稍后将用来从 <a href="http://github.com" target="_blank" rel="noopener">github</a> 网站上下载Tengine的源码     </li><li><strong>cmake</strong> 是一个编译工具，用来产生make过程中所需要的Makefile文件      </li></ul><h4 id="安装支持库"><a href="#安装支持库" class="headerlink" title="安装支持库"></a>安装支持库</h4><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libprotobuf-dev protobuf-compiler libboost-all-dev libgoogle-glog-dev libopencv-dev libopenblas-dev</code></pre><ul><li><strong>protobuf</strong> 是一种轻便高效的数据存储格式，这是caffe各种配置文件所使用的数据格式</li><li><strong>boost</strong> 是一个c++的扩展程序库，稍后Tengine的编译依赖于该库</li><li><strong>google-glog</strong> 是一个google提供的日志系统的程序库</li><li><strong>opencv</strong> 是一个开源的计算机视觉库</li><li><strong>openblas</strong> 是一个开源的基础线性代数子程序库</li></ul><h4 id="下载-amp-编译"><a href="#下载-amp-编译" class="headerlink" title="下载&amp;编译"></a>下载&amp;编译</h4><ol><li>从github上下载最新的开源版Tengine源码     <pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/OAID/Tengine.git</code></pre></li><li>切换工作目录到Tengine      <pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> Tengine</code></pre></li><li>准备好配置文件<br>Tengine目录下提供了配置模板 <code>makefile.config.example</code> 文件        <pre class=" language-bash"><code class="language-bash"><span class="token function">cp</span> makefile.config.example makefile.config</code></pre></li><li>修改配置文件 <code>makefile.config</code><br>由于开源版的Tengine不支持针对armv7的优化，所以需要用openblas替代实现；<br>将 <code>CONFIG_ARCH_ARM64=y</code> 这一行注释掉（行首加井号 <code>#</code>）以关闭ARM64架构的优化实现；<br>解除 <code>CONFIG_ARCH_ARM32=y</code> 和<code>CONFIG_ARCH_BLAS=y</code> 这一行解除注释（删除行首的井号 <code>#</code>）以开启BLAS计算库的实现方式      </li><li>编译并安装      <pre class=" language-bash"><code class="language-bash"><span class="token function">make</span> -j4<span class="token function">make</span> <span class="token function">install</span></code></pre>这里的 <code>-j4</code> 表示开启四个线程进行编译       </li></ol><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ol><li>下载mobilenet-ssd模型并放置在 <code>Tengine/models</code> 目录下<br>下载链接（提取码为57vb）：<a href="https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g" target="_blank" rel="noopener">https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g</a>     </li><li>将工作目录切换到mobilenet-ssd示例程序的目录下      <pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> ~/Tengine/examples/mobilenet_ssd</code></pre></li><li>编译示例程序    <pre class=" language-bash"><code class="language-bash">cmake -DTENGINE_DIR<span class="token operator">=</span>/home/pi/Tengine <span class="token keyword">.</span><span class="token function">make</span></code></pre>这里 <code>-DTENGINE_DIR</code>用于为cmake指定环境变量TENGINE_DIR，该变量可以在CMakeLists.txt文件中找到       </li><li>运行示例程序     <pre class=" language-bash"><code class="language-bash">./MSSD</code></pre>可以看到对一张照片进行目标检测，总共耗时1148.32ms<br><img src="/imgs/RasPi-Tengine/mssd-opensource.png" alt="mssd-opensource">     </li></ol><h3 id="树莓派开发者版Tengine"><a href="#树莓派开发者版Tengine" class="headerlink" title="树莓派开发者版Tengine"></a>树莓派开发者版Tengine</h3><p>开发者版下载地址：<a href="http://www.tengine.org.cn/info.php?class_id=105105" target="_blank" rel="noopener">Tengine下载 - Tengine开发者中心</a>     </p><ol><li>用树莓派开发者版Tengine的动态链接库覆盖掉原先的开源版<br>动态链接库路径为：<code>Tengine/install/lib/libtengine.so</code><br><em>编译时，make会在build目录下产生libtengine.so动态链接库，而make instll将动态链接库、头文件等拷贝到install目录下</em><br><img src="/imgs/RasPi-Tengine/replace.png" alt="replace">      </li><li>重新运行mobilenet-ssd的示例程序<br>可以看到，单帧耗时从1148.32ms下降为<strong>286.136ms</strong>，速度有了非常明显的提升！<br><img src="/imgs/RasPi-Tengine/mssd-education.png" alt="mssd-education"></li></ol><h3 id="小试牛刀"><a href="#小试牛刀" class="headerlink" title="小试牛刀"></a>小试牛刀</h3><p>用上高性能的树莓派开发者版Tengine，看看mobilenet-ssd在树莓派上能表现如何——     </p><p>为了方便，视频流直接从mp4文件读取，原始视频如下：     </p><iframe height="500" width="700" src="http://player.youku.com/embed/XMzkyNjYzNTIyNA==" frameborder="0" 'allowfullscreen'=""></iframe> <ol><li>从 <a href="https://github.com/hey-yahei/my_blog/tree/master/RasPi-Tengine/mobilenet_ssd" target="_blank" rel="noopener">hey-yahei/my_blog/RasPi-Tengine/mobilenet-ssd | github</a> 上下载源码，并放置在 <code>Tengine/example</code> 目录下    </li><li>检查 <code>CMakeLists.txt</code> 文件中TENGINE_DIR变量是否正确指向Tengine路径</li><li>执行 <code>cmake .</code> 生成Makefile</li><li>执行 <code>make</code> 编译程序</li><li>执行 <code>./MSSD</code> 运行程序         </li></ol><p>实际效果如下：       </p><p><iframe height="500" width="700" src="http://player.youku.com/embed/XMzkyNjYzNTc4OA==" frameborder="0" 'allowfullscreen'=""></iframe><br>由于一部分cpu资源被用于视频的解码工作（对于支持硬解码的平台来说不存在这个问题），可以看到单帧耗时有所下降（400ms-700ms），但对于多数应用场景来说这个帧率是绰绰有余的。      </p><hr><p>本文开头我们说道，<br>直接在树莓派上配置darknet部署的yolo网络，yolov2单帧耗时接近<strong>6分钟</strong>，yolov2-tiny单帧耗时接近<strong>40秒</strong>；<br>而在树莓派上配置Tengine部署的yolov2网络，在blas实现下单帧耗时不到<strong>8秒</strong>（参考<a href="https://songrbb.github.io/2018/08/17/%E5%88%A9%E7%94%A8Tengine%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E8%B7%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">利用Tengine在树莓派上跑深度学习网络 | songrbb</a>），在针对armv7优化实现的教育版下单帧耗时甚至不到<strong>2秒</strong>！     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RK3399 </tag>
            
            <tag> RasPi </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于MobileNet-SSD的目标检测Demo（二）</title>
      <link href="/2018/09/10/mssd-try3/"/>
      <url>/2018/09/10/mssd-try3/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=31352588&auto=0&height=32"></iframe>      <p>上一篇文章《<a href="/2018/08/24/mssd-try2/">基于MobileNet-SSD的目标检测Demo（一）</a>》介绍了如何在VOC数据集的基础上削减分类训练出自己的分类器，并且尝试着进一步把SSD改为SSDLite。但作为一个Demo，在RK3399上MobileNet-SSD每秒钟只能检测6-7帧，如果每次检测后再把视频内容展现出来，那么展示的视频也只有6-7帧，这样的展示效果似乎不太好。在本篇文章中，我们将尝试把视频的获取与展示和检测任务分离开来，分别放在两个不同的线程上工作，同时将不同的线程绑定到不同的cpu核上，使得两者的工作不会冲突。         </p><hr><h3 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h3><p>进程和线程是操作系统中的两个重要概念。</p><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>考虑在51单片机或是STM32上开发程序，通常这些程序都是串行结构。打个比方，      </p><ol><li>写个数码管的动态驱动，让四个个数码管持续显示数值<code>1217</code>       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token number">1217</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li>用一个超声波模块进行测距，并且用数码管显示结果       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token function">ultrasonicGetDatum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li>超声波数量少还没关系，数码管还是能正常驱动，如果多来几个呢？（简化一下，数码管只显示求和的结果）       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span><span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">ultrasonicgetDatum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>CPU大部分时间都用去等超声波信号了呀，数码管根本就不能驱动起来，那换种方式——      <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>sum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        sum<span class="token operator">+</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        segment<span class="token operator">=</span>sum<span class="token punctuation">;</span>        i<span class="token operator">=</span>sum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>这样一来，如果超声波数据无效就继续驱动数码管，不会让CPU空等。    </li><li>那如果不是简单的求和运算呢？        <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ultras<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        ultras<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        segment<span class="token operator">=</span><span class="token function">process</span><span class="token punctuation">(</span>ultras<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 变成了其他复杂的运算</span>        i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>万一<code>process</code>函数运算过程很复杂，占用了很多CPU的时间，导致数码管不能及时刷新，那数码管依旧驱动不起来。当然，你可以去推算<code>process</code>运算的复杂程度，人为地去拆解运算，变成<code>process[0]</code>、<code>process[1]</code>、……、<code>process[n]</code>最后再由<code>combine</code>把中间结果整合起来（注意这里要保证每个操作都足够小，不会占用太多运算时间）。     <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i_process<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ultras<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span>tmp<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        ultras<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        tmp<span class="token punctuation">[</span>i_process<span class="token punctuation">]</span><span class="token operator">=</span>process<span class="token punctuation">[</span>i_process<span class="token punctuation">]</span><span class="token punctuation">(</span>ultras<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>i_process<span class="token operator">==</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            segment<span class="token operator">=</span><span class="token function">combine</span><span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">;</span>            i_process<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>            i_process<span class="token operator">++</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>看起来确实可行，但是手工拆解运算，这也太恶心了吧，而且这一点都不优雅，万一我程序开发着开发着，这个运算过程发生改变了怎么办？重新拆解运算？那不得炸毛？！     </li><li>那换个思路，我们让两个任务分时进行吧，每个任务轮流运算10ms，超时就带上你的中间结果滚蛋            <pre class=" language-c"><code class="language-c"><span class="token comment" spellcheck="true">// 设置定时器，每10ms中产生一次中断</span><span class="token function">timer_handler</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// 中断处理</span>    <span class="token function">save_metadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 触发中断后保存数据</span>    <span class="token function">change_task</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 切换到另一个任务</span>    <span class="token function">load_metadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 取出新换入任务的中间数据</span>    <span class="token function">task_run</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">// 继续任务</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// ……省略任务定义</span></code></pre>这样虽然增加了额外的开销（任务的调度），但形成了一个通用化的任务调度功能，无论具体任务怎么改变都能够适用，减少CPU闲置的机会，可以更好的压榨CPU     </li></ol><p>实际应用当中，我们经常都能碰见这种多任务的情况，人为地分配任务给处理器需要大量的推算和分解，费时费力还不易调整。在这里，我们形成了一个简单的通用的任务调度功能，其实这也是现代操作系统的基本功能之一，对操作系统而言，这些任务就是一个个的“<strong>进程</strong>”，中间数据被称为“<strong>进程上下文</strong>”，而操作系统有一个专门的模块负责“<strong>进程调度</strong>”的工作，这里举例的固定时间片是一种最简单的调度方式。         </p><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p><center><img src="/imgs/mssd-try/try3/process.png" alt="process"></center><br>这是一张进程状态转换图，         </p><ul><li>新建一个进程时处于“<strong>新建态</strong>”，至于加载到“就绪态”还是“就绪/挂起态”就取决于操作系统翻不翻你牌子；      </li><li>“<strong>就绪态</strong>”指的是进程已经就绪的状态，比如进程执行所需要的资源（比如键盘、鼠标、显卡等外设）可用，并且已经被操作系统翻牌，在等待执行的队伍里排着队；   </li><li>“<strong>阻塞态</strong>”指的是进程未就绪，比如执行的所需资源还未到位，进程本身处于等待的状态；      </li><li>“就绪态”和“阻塞态”还分别有对应的“<strong>就绪/挂起态</strong>”和“<strong>阻塞/挂起态</strong>”，这是进程本身处于就绪或未就绪的状态，但操作系统还没有翻他们牌子；     </li><li>“<strong>运行态</strong>”和“<strong>退出态</strong>”不难理解，就是进程运行中的状态，以及进程完全运行结束而将退出的状态         </li></ul><p>进程上下文保存在一个特殊的称为<strong>进程控制块（PCB）</strong>的结构里，其中包含        </p><ol><li>进程标识信息（各种标识符）</li><li>进程状态信息（寄存器、栈指针等）</li><li>进程控制信息（调度和状态的相关信息，比如进程状态、优先级、事件等）     </li></ol><p>常见的用户进程创建有两种，       </p><ol><li>用户运行一个程序，这个程序会放到一个进程上<br>比如直接运行一个编译好的c程序，或是一个python程序；       </li><li>由现有进程派生<br>比如在c程序中调用fork函数来派生出一个新的进程——        <pre class=" language-c"><code class="language-c"> <span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span> <span class="token keyword">int</span> <span class="token function">main</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>     <span class="token keyword">int</span> pid<span class="token punctuation">,</span> ppid<span class="token punctuation">;</span>     pid <span class="token operator">=</span> <span class="token function">fork</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// fork函数将派生出一个相同的进程，返回新进程的id（对于原始进程返回0）</span>     <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d first output from both processes\n"</span><span class="token punctuation">,</span> pid<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token function">sleep</span> <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"This is the child's pid,  output by the parent process\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"is printed inside the child process if the fork succeeds \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         pid <span class="token operator">=</span> <span class="token function">getpid</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// getpid函数可以获取当前进程的id</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"is the child pid printed by the child, obtained by getpid()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token keyword">else</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"fork failed\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span></code></pre></li></ol><p>进程间的通信通常由信号量、中断和共享空间实现，简单地说，      </p><ul><li><strong>信号量</strong>，事实上就是一个整型变量。操作系统负责维护一个信号量池，进程可以在该池注册带有名称的信号量，多个进程间可以对同一个信号量进行加法或减法操作（通常称为PV操作，该操作由操作系统管理，不会读写冲突），但减法的结果不能小于0，否则进程就会阻塞挂起，等待有足够的信号量可以减去。信号量有点像资源的指示标志，进程占用资源的时候作减法，释放资源的时候做加法。          </li><li><strong>中断</strong>，是Linux提供的一套机制，事实上<code>kill</code>指令就是对目标进程发送一个特定的中断信号，进程接收到中断之后会跳转到指定的中断处理函数进行处理（当然进程也可以设置忽略某一些信号）。在Linux中可以通过指令<code>kill -l</code>查看可用信号，如下：       <pre class=" language-bash"><code class="language-bash">  <span class="token comment" spellcheck="true"># kill -l</span>   1<span class="token punctuation">)</span> SIGHUP       2<span class="token punctuation">)</span> SIGINT       3<span class="token punctuation">)</span> SIGQUIT      4<span class="token punctuation">)</span> SIGILL       5<span class="token punctuation">)</span> SIGTRAP   6<span class="token punctuation">)</span> SIGABRT      7<span class="token punctuation">)</span> SIGBUS       8<span class="token punctuation">)</span> SIGFPE       9<span class="token punctuation">)</span> SIGKILL     10<span class="token punctuation">)</span> SIGUSR1  11<span class="token punctuation">)</span> SIGSEGV     12<span class="token punctuation">)</span> SIGUSR2     13<span class="token punctuation">)</span> SIGPIPE     14<span class="token punctuation">)</span> SIGALRM     15<span class="token punctuation">)</span> SIGTERM  16<span class="token punctuation">)</span> SIGSTKFLT   17<span class="token punctuation">)</span> SIGCHLD     18<span class="token punctuation">)</span> SIGCONT     19<span class="token punctuation">)</span> SIGSTOP     20<span class="token punctuation">)</span> SIGTSTP  21<span class="token punctuation">)</span> SIGTTIN     22<span class="token punctuation">)</span> SIGTTOU     23<span class="token punctuation">)</span> SIGURG      24<span class="token punctuation">)</span> SIGXCPU     25<span class="token punctuation">)</span> SIGXFSZ  26<span class="token punctuation">)</span> SIGVTALRM   27<span class="token punctuation">)</span> SIGPROF     28<span class="token punctuation">)</span> SIGWINCH    29<span class="token punctuation">)</span> SIGIO       30<span class="token punctuation">)</span> SIGPWR  31<span class="token punctuation">)</span> SIGSYS      34<span class="token punctuation">)</span> SIGRTMIN    35<span class="token punctuation">)</span> SIGRTMIN+1  36<span class="token punctuation">)</span> SIGRTMIN+2  37<span class="token punctuation">)</span> SIGRTMIN+3  38<span class="token punctuation">)</span> SIGRTMIN+4  39<span class="token punctuation">)</span> SIGRTMIN+5  40<span class="token punctuation">)</span> SIGRTMIN+6  41<span class="token punctuation">)</span> SIGRTMIN+7  42<span class="token punctuation">)</span> SIGRTMIN+8  43<span class="token punctuation">)</span> SIGRTMIN+9  44<span class="token punctuation">)</span> SIGRTMIN+10 45<span class="token punctuation">)</span> SIGRTMIN+11 46<span class="token punctuation">)</span> SIGRTMIN+12 47<span class="token punctuation">)</span> SIGRTMIN+13  48<span class="token punctuation">)</span> SIGRTMIN+14 49<span class="token punctuation">)</span> SIGRTMIN+15 50<span class="token punctuation">)</span> SIGRTMAX-14 51<span class="token punctuation">)</span> SIGRTMAX-13 52<span class="token punctuation">)</span> SIGRTMAX-12  53<span class="token punctuation">)</span> SIGRTMAX-11 54<span class="token punctuation">)</span> SIGRTMAX-10 55<span class="token punctuation">)</span> SIGRTMAX-9  56<span class="token punctuation">)</span> SIGRTMAX-8  57<span class="token punctuation">)</span> SIGRTMAX-7  58<span class="token punctuation">)</span> SIGRTMAX-6  59<span class="token punctuation">)</span> SIGRTMAX-5  60<span class="token punctuation">)</span> SIGRTMAX-4  61<span class="token punctuation">)</span> SIGRTMAX-3  62<span class="token punctuation">)</span> SIGRTMAX-2  63<span class="token punctuation">)</span> SIGRTMAX-1  64<span class="token punctuation">)</span> SIGRTMAX</code></pre>  不同的信号有不同的含义，其中10号的<code>SIGUSR1</code>和12号的<code>SIGUSR2</code>是两个可以用户自定义的中断信号。       </li><li><strong>共享空间</strong>，类似进程内部的全局变量，由操作系统负责维护，跟信号量一样各个共享空间拥有自己的标识符，不同进程都可以访问相同的共享空间，但是要注意防止访问冲突（比如一个进程对空间写操作，同时又有另一个进程对空间进程读或写操作），这通常是通过信号量来实现的。并且在共享的过程要注意避免进程死锁（各个进程各自占有一部分但并不充足资源，导致进程同时陷入无休止的阻塞状态），有一些专门用于检测死锁和防止死锁的算法，此处不展开讨论。       </li></ul><p>具体的实现不展开讲，因为我们接下来要用到的是线程而不是进程。<br>如果你对进程间通信感兴趣，也可以参考我本科期间的一个课程作业，一个 <a href="/others/chat.zip">简单的本地聊天程序</a> （具体使用方法参见<code>chat.c</code>里的注释）。      </p><h4 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h4><p>在单一程序中，进程的粒度似乎还是太大了，如果把一个程序的大任务细分多个小任务，明明大家都是同一个目标，却要使用独立的上下文，上下文频繁地保存和加载，这样似乎不太方便。于是就产生了粒度更小的<strong>线程</strong>，一个进程可以拥有多个线程，这些线程共享一个上下文环境。<br>相比于进程，       </p><ol><li>创建一个线程的速度要快得多；       </li><li>终止一个线程的速度也要快得多；     </li><li>线程间的切换也比进程间切换快，因为不需要交换上下文；     </li><li>线程的通信效率更高，因为线程可以直接通过共享全局变量来实现通信。            </li></ol><p>线程的通信方式比进程简单，这跟进程的“信号量+共享空间”的组合有些类似，      </p><ul><li><strong>线程锁</strong>，类似进程通信中的信号量，但这个变量只有两种状态——“上锁”和“解锁”，用于保护共享的内存空间不会出现读写冲突；     </li><li><strong>全局变量</strong>，类似进程通信中的共享空间。         </li></ul><p>简单的实现方式——      </p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span></span>pthread_mutex_t mutex<span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">// 线程锁（用于保护shared_variable变量）</span><span class="token keyword">int</span> shared_variable <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 线程间共享的全局变量</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">thread_write</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 写入前上锁（如果mutex锁住，则阻塞等待）</span>    shared_variable<span class="token operator">++</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 写入后解锁</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">thread_read</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 读取前上锁（如果mutex锁住，则阻塞等待）</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> shared_variable<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 读取后解锁</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    pthread_t id1<span class="token punctuation">,</span> id2<span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 初始化线程锁</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> thread_write<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 创建写线程</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> thread_read<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 创建读线程</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 阻塞主线程，等待线程id1执行完毕</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 阻塞主线程，等待线程id2执行完毕</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 销毁线程锁</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p><code>pthread_create</code>、<code>pthread_join</code>、<code>pthread_mutex_init</code>还可以传递其他参数，其他复杂的用法可以自行查阅资料。     </p><h4 id="处理器调度"><a href="#处理器调度" class="headerlink" title="处理器调度"></a>处理器调度</h4><p>在背景中我们提到了一种规定时间的调度方法，接下来我们也简单介绍其他一些处理器的调度策略。<br>假设有A-E五个进程集合，他们的启动时间和CPU占用总时间如下表所示——       </p><table><thead><tr><th style="text-align:center">进程</th><th style="text-align:center">启动时间</th><th style="text-align:center">CPU占用总时间</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">t=0</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">t=2</td><td style="text-align:center">6</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">t=4</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">t=6</td><td style="text-align:center">5</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">t=8</td><td style="text-align:center">2</td></tr></tbody></table><p><strong>先来先服务策略（First Come First Served, FCFS）</strong><br>非抢占策略，进程依次进入等待队列，先进入队列的先使用CPU，直到进程结束再从队列取出下一个进程。<br><img src="/imgs/mssd-try/try3/process-FCFS.png" alt="process">       </p><p><strong>轮转策略（Round Robin, RR）</strong><br>一种基于时钟的抢占策略，又称为时间片策略，设定一个时间片q，每次从进程等待队列中取出一个进程执行一个时间片，如果没执行完就放回等待队列的队尾，然后从队首取出下一个进程出来执行一个时间片。假设q=1，则<br><img src="/imgs/mssd-try/try3/process-RR.png" alt="process"><br><em>注意：这里假设如果同时发生“中断”和“新服务入队”，则先将新服务入队，再交换进程。</em>             </p><p><strong>最短进程优先（Shortest Process Next, SPN）</strong><br>一种基于预计处理时间的非抢占策略，每次从等待队列中取出预计处理时间最短的进程出来执行，直到进程结束再从进程取出下一个预计处理时间最短的进程。<br><img src="/imgs/mssd-try/try3/process-SPN.png" alt="process">       </p><p><strong>最短剩余时间（Shortest Remaining Time, SRT）</strong><br>一种基于预计剩余处理时间的抢占策略，在SPN的基础上，当有新进程加入队列时总会估算各个进程的剩余时间，然后选择预计剩余处理时间最短的进程出来执行。<br><img src="/imgs/mssd-try/try3/process-SRT.png" alt="process">       </p><p><strong>最高响应比优先（Highest Response Ratio Next, HRRN）</strong><br>非抢占策略，定义一个响应比参数，$响应比=\frac{等待时间+预计处理时间}{预计处理时间}$，每次从等待队列中挑选响应比最高的进程出来执行，直到进程执行结束再从队列中取出下一个响应比最高的进程。<br><img src="/imgs/mssd-try/try3/process-HRRN.png" alt="process">       </p><p><strong>反馈调度</strong><br>可以注意到FCFS、RR策略相对简单，不太能很好的利用CPU，而SPN、SRT、HRRN策略虽然不错，但依赖于处理时间和剩余处理时间的估计，而现实应用中这种时间估计往往是难以实现的。因此产生了反馈调度策略，对进程进行分级（而不是简单的一个等待队列），进程运行时间长调度的优先级越低，每被抢占一次就下降一级。<br><img src="/imgs/mssd-try/try3/process-feedback.png" alt="process"><br>反馈调度往往会和前述的简单调度（比如FCFS或RR）结合使用，同时长进程周转时间会出现惊人的增加的现象（长进程多次被抢占，优先级不断下降，而长期得不到调度），所以会有一些相应的补偿措施（比如设置允许被抢占的次数，每当超过这个次数才会对进程进行降级操作）。           </p><p><strong>实时调度</strong><br>嵌入式开发中还常见一些实时调度策略，与前述策略不同的时间，这些任务有最后期限的限制，这通常分为两种——一种是“硬实时”，要求必须满足最后期限的限制，否则将给系统带来不可接受的破坏或致命的错误（任务超时完成是无意义的）；另一种是“软实时”，希望能够满足最后期限的限制，但并非强制（即使超时完成任务也有意义）。    </p><h3 id="拆解目标检测demo"><a href="#拆解目标检测demo" class="headerlink" title="拆解目标检测demo"></a>拆解目标检测demo</h3><p>前边介绍了进程、线程以及处理器调度的概念和简单的使用，接下来我们考虑如何将我们的目标检测demo拆解成两个线程以提高展示的流畅性。       </p><p>我们的目标是将demo拆解成 <strong>目标检测</strong> 和 <strong>视频流获取和展示</strong> 两个线程，其中需要共享的数据包括 <strong>图像数据</strong> 和 <strong>检测结果</strong> 两部分（为了响应退出按钮，后续的示例程序还会额外增加一个作为退出标志的共享数据），每部分数据需要配备一把线程锁进行读写保护。       </p><p>最终程序如下——<br><strong><em>注意：这里不仅划分了线程，还针对不同线程的任务分配了cpu，比如RK3399上CPU0-3是四个小核，我们用来做视频流的获取、检测结果的标注和视频流的展示；CPU4-5是大核，我们用来做核心的目标检测任务。两个不同的线程使用不同的CPU核，互不冲突，合理地分配CPU对应用程序也会有一定的提升。</em></strong>     </p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/imgproc/imgproc.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/highgui/highgui.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"tengine_c_api.h"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"common.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span>    </span><span class="token comment" spellcheck="true">// 包含线程控制相关的库</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_PROTO "models/MobileNetSSD_deploy.prototxt"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_MODEL "models/MobileNetSSD_deploy.caffemodel"</span><span class="token keyword">struct</span> Box<span class="token punctuation">{</span>    <span class="token keyword">float</span> x0<span class="token punctuation">;</span>    <span class="token keyword">float</span> y0<span class="token punctuation">;</span>    <span class="token keyword">float</span> x1<span class="token punctuation">;</span>    <span class="token keyword">float</span> y1<span class="token punctuation">;</span>    <span class="token keyword">int</span> class_idx<span class="token punctuation">;</span>    <span class="token keyword">float</span> score<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> input_data<span class="token punctuation">,</span> <span class="token keyword">int</span> img_h<span class="token punctuation">,</span>  <span class="token keyword">int</span> img_w<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        std<span class="token operator">::</span>cerr <span class="token operator">&lt;&lt;</span> <span class="token string">"Failed to read image from camera.\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span><span class="token function">resize</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    img<span class="token punctuation">.</span><span class="token function">convertTo</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> CV_32FC3<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>img_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>img<span class="token punctuation">.</span>data<span class="token punctuation">;</span>    <span class="token keyword">int</span> hw <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w<span class="token punctuation">;</span>    <span class="token keyword">float</span> mean<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> img_h<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> w <span class="token operator">&lt;</span> img_w<span class="token punctuation">;</span> w<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                input_data<span class="token punctuation">[</span>c <span class="token operator">*</span> hw <span class="token operator">+</span> h <span class="token operator">*</span> img_w <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.007843</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">*</span>img_data <span class="token operator">-</span> mean<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                img_data<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span> threshold<span class="token punctuation">,</span><span class="token keyword">float</span><span class="token operator">*</span> outdata<span class="token punctuation">,</span><span class="token keyword">int</span> num<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> class_names<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"background"</span><span class="token punctuation">,</span>                            <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span>                            <span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span>                            <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span>                            <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span>                            <span class="token string">"sheep"</span><span class="token punctuation">,</span> <span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_h <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>height<span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_w <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>width<span class="token punctuation">;</span>    std<span class="token operator">::</span>vector<span class="token operator">&lt;</span>Box<span class="token operator">></span> boxes<span class="token punctuation">;</span>    <span class="token keyword">int</span> line_width<span class="token operator">=</span>raw_w<span class="token operator">*</span><span class="token number">0.002</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// printf("detect ruesult num: %d \n",num);</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>=</span>threshold<span class="token punctuation">)</span><span class="token punctuation">{</span>            Box box<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>class_idx<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>score<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            boxes<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>box<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// printf("%s\t:%.0f%%\n", class_names[box.class_idx], box.score * 100);</span>            <span class="token comment" spellcheck="true">// printf("BOX:( %g , %g ),( %g , %g )\n",box.x0,box.y0,box.x1,box.y1);</span>        <span class="token punctuation">}</span>        outdata<span class="token operator">+</span><span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span>boxes<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        Box box<span class="token operator">=</span>boxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x1<span class="token operator">-</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>y1<span class="token operator">-</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>line_width<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>ostringstream score_str<span class="token punctuation">;</span>        score_str<span class="token operator">&lt;&lt;</span>box<span class="token punctuation">.</span>score<span class="token punctuation">;</span>        std<span class="token operator">::</span>string label <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">string</span><span class="token punctuation">(</span>class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> score_str<span class="token punctuation">.</span><span class="token function">str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> baseLine <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span>Size label_size <span class="token operator">=</span> cv<span class="token operator">::</span><span class="token function">getTextSize</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>baseLine<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token operator">-</span> label_size<span class="token punctuation">.</span>height<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>label_size<span class="token punctuation">.</span>width<span class="token punctuation">,</span> label_size<span class="token punctuation">.</span>height <span class="token operator">+</span> baseLine<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CV_FILLED<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">putText</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> label<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">,</span>                    cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">float</span> outdata<span class="token punctuation">[</span><span class="token number">15</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 线程间共享变量——检测结果</span>cv<span class="token operator">::</span>Mat frame<span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 线程间共享变量——图像数据</span><span class="token keyword">int</span> detect_num<span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 线程间共享变量——检测结果</span><span class="token keyword">bool</span> quit_flag <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 程序间共享变量——退出标志</span>graph_t graph<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 与共享变量对应的线程锁</span>pthread_mutex_t m_frame<span class="token punctuation">,</span> m_outdata<span class="token punctuation">,</span> m_quit<span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">th_vedio</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 将线程绑定到cpu0-3上</span>    cpu_set_t mask<span class="token punctuation">;</span>    <span class="token function">CPU_ZERO</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">sched_setaffinity</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cpu_set_t<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error: setaffinity()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span>VideoCapture <span class="token function">capture</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_WIDTH<span class="token punctuation">,</span> <span class="token number">960</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_HEIGHT<span class="token punctuation">,</span> <span class="token number">540</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span><span class="token function">namedWindow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> CV_WINDOW_NORMAL<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">cvResizeWindow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> <span class="token number">1280</span><span class="token punctuation">,</span> <span class="token number">720</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        capture <span class="token operator">>></span> frame<span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">float</span> show_threshold<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 上锁</span>        <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> show_threshold<span class="token punctuation">,</span> outdata<span class="token punctuation">,</span> detect_num<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 解锁</span>        cv<span class="token operator">::</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 解锁</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> cv<span class="token operator">::</span><span class="token function">waitKey</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'q'</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 上锁</span>            quit_flag <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>            <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 解锁</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token function">usleep</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 注意必须sleep（不然太过频繁地取帧会影响检测线程的调度）</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">th_detect</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 将该线程绑定到cpu4-5上</span>    cpu_set_t mask<span class="token punctuation">;</span>    <span class="token function">CPU_ZERO</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">sched_setaffinity</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cpu_set_t<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error: setaffinity()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// input</span>    <span class="token keyword">int</span> img_h <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_w <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_size <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>input_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> img_size<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> node_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> tensor_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    tensor_t input_tensor <span class="token operator">=</span> <span class="token function">get_graph_input_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> node_idx<span class="token punctuation">,</span> tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_tensor_valid</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Get input node failed : node_idx: %d, tensor_idx: %d\n"</span><span class="token punctuation">,</span>node_idx<span class="token punctuation">,</span>tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> dims<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">set_tensor_shape</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dims<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">prerun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> repeat_count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>repeat <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">"REPEAT_COUNT"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>repeat<span class="token punctuation">)</span>        repeat_count <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">strtoul</span><span class="token punctuation">(</span>repeat<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> out_dim<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    tensor_t out_tensor<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 上锁</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>quit_flag<span class="token punctuation">)</span>  <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 解锁</span>        <span class="token keyword">struct</span> timeval t0<span class="token punctuation">,</span> t1<span class="token punctuation">;</span>        <span class="token keyword">float</span> total_time <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>f<span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> repeat_count<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token comment" spellcheck="true">// 上锁</span>            <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span>  img_w<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 解锁</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t0<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">set_tensor_buffer</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">run_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">float</span> mytime <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t1<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span>t0<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t0<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span><span class="token punctuation">;</span>            total_time <span class="token operator">+</span><span class="token operator">=</span> mytime<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"--------------------------------------\n"</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"repeat "</span> <span class="token operator">&lt;&lt;</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" times, avg time per run is "</span> <span class="token operator">&lt;&lt;</span> total_time <span class="token operator">/</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" ms\n"</span><span class="token punctuation">;</span>        out_tensor <span class="token operator">=</span> <span class="token function">get_graph_output_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">get_tensor_shape</span><span class="token punctuation">(</span> out_tensor<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 上锁</span>        detect_num <span class="token operator">=</span> out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">15</span> <span class="token operator">?</span> out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">:</span> <span class="token number">15</span><span class="token punctuation">;</span>        <span class="token function">memcpy</span><span class="token punctuation">(</span>outdata<span class="token punctuation">,</span> <span class="token function">get_tensor_buffer</span><span class="token punctuation">(</span>out_tensor<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>detect_num<span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 解锁</span>    <span class="token punctuation">}</span>    <span class="token function">free</span><span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> std<span class="token operator">::</span>string root_path <span class="token operator">=</span> <span class="token function">get_root_path</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>string proto_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string model_file<span class="token punctuation">;</span>    <span class="token keyword">int</span> res<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token punctuation">(</span> res<span class="token operator">=</span><span class="token function">getopt</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">,</span><span class="token string">"p:m:h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">switch</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token string">'p'</span><span class="token operator">:</span>                proto_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'m'</span><span class="token operator">:</span>                model_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'h'</span><span class="token operator">:</span>                std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"[Usage]: "</span> <span class="token operator">&lt;&lt;</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" [-h]\n"</span>                          <span class="token operator">&lt;&lt;</span> <span class="token string">"   [-p proto_file] [-m model_file]\n"</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token keyword">default</span><span class="token operator">:</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>model_name <span class="token operator">=</span> <span class="token string">"mssd_300"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>proto_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        proto_file <span class="token operator">=</span> DEF_PROTO<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"proto file not specified,using "</span><span class="token operator">&lt;&lt;</span> proto_file <span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>model_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        model_file <span class="token operator">=</span> DEF_MODEL<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"model file not specified,using "</span><span class="token operator">&lt;&lt;</span> model_file <span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// init tengine</span>    <span class="token function">init_tengine_library</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">request_tengine_version</span><span class="token punctuation">(</span><span class="token string">"0.1"</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">load_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> <span class="token string">"caffe"</span><span class="token punctuation">,</span> proto_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"load model done!\n"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// create graph</span>    graph <span class="token operator">=</span> <span class="token function">create_runtime_graph</span><span class="token punctuation">(</span><span class="token string">"graph"</span><span class="token punctuation">,</span> model_name<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_graph_valid</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"create graph0 failed\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 初始化线程锁</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 创建线程</span>    pthread_t id1<span class="token punctuation">,</span> id2<span class="token punctuation">;</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> th_vedio<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> th_detect<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 等待线程</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 销毁线程锁</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">postrun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">destroy_runtime_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">remove_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><hr><p>在本文中，我们将尝试把视频的获取与展示和检测任务分离开来，分别放在两个不同的线程上工作，同时将不同的线程绑定到不同的cpu核上，使得两者的工作不会冲突。但在实际使用中你会发现，模型对于小目标的检测能力还是有所欠缺，下一篇文章我们将探究如何改善检测模型的小目标检测能力。</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>基于MobileNet-SSD的目标检测Demo（一）</title>
      <link href="/2018/08/24/mssd-try2/"/>
      <url>/2018/08/24/mssd-try2/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=536622304&auto=0&height=32"></iframe>    <p>上一篇文章《<a href="/2018/08/21/mssd-try1/">训练MobileNet-SSD | Hey~YaHei!</a>》介绍了如何训练自己的MobileNet-SSD模型并部署在Tengine平台上。<br>本文将继续尝试根据实际情况删减多余类别进行训练，并用Depthwise Convolution进一步替换Standard Convolution。         </p><hr><h3 id="削减类别"><a href="#削减类别" class="headerlink" title="削减类别"></a>削减类别</h3><p>VOC数据集包含二十个类别的物体，分别是——aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, foa, train, tvmonitor，有时候我们想用VOC数据集训练，但并不需要这么多类别，而caffe-ssd提供的数据处理工具<code>create_list.sh</code>和<code>create_data.sh</code>默认是处理所有的20个分类的。如果我们不想重写这些数据处理工具，可以从根源入手，也就是直接修改数据集里的标注信息，把多余分类的信息删去。          </p><h4 id="处理数据集"><a href="#处理数据集" class="headerlink" title="处理数据集"></a>处理数据集</h4><p>首先观察一下VOC数据集的结构——<br><img src="/imgs/mssd-try/try1/dataset_tree.png" width="350">       </p><ul><li>Annotations：存放图片的标注信息，每张图片对应一个xml文件      </li><li>ImageSets：存放图片的分类列表，包含三个子目录：       <ul><li>Layout：存放与人体部位有关的图片列表文件</li><li>Main：存放物体分类中每一个分类的图片列表文件</li><li>Segmentation：存放与图像分别有关的图片列表文件</li></ul></li><li>JPEGImages：存放所有的图片</li><li><del><em>NewAnnotations：忽略吧……是我自己生成的目录</em></del></li><li>SegmentationClass：存放类别分割任务的蒙版文件</li><li>SegmentationObject：存放实体分割任务的蒙版文件</li></ul><p>JPEGImages目录下每张图片都包含一到多个物体，这些物体的位置、类别信息都记录再Annotations目录下的同名xml文件中，文件内容类似：     </p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>folder</span><span class="token punctuation">></span></span>VOC2007<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>folder</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filename</span><span class="token punctuation">></span></span>008973.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filename</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>database</span><span class="token punctuation">></span></span>The VOC2007 Database<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>database</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>PASCAL VOC2007<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>image</span><span class="token punctuation">></span></span>flickr<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>image</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flickrid</span><span class="token punctuation">></span></span>335707085<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flickrid</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>owner</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flickrid</span><span class="token punctuation">></span></span>kjmurray<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flickrid</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>Katherine Murray<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>owner</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>size</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>width</span><span class="token punctuation">></span></span>500<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>width</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>height</span><span class="token punctuation">></span></span>333<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>height</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>depth</span><span class="token punctuation">></span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>depth</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>size</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>segmented</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>segmented</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>cow<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">></span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">></span></span>271<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">></span></span>43<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">></span></span>444<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">></span></span>279<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span></code></pre><p>而caffe-ssd的数据处理工具正是根据这些xml文件提供的标记进行处理的，所以说，<strong>我们可以通过遍历xml文件，判断object的类别，如果是我们不需要的，则把对应的object标签删去来达到削减类别的目的</strong>，除此之外还要处理对应的图片路径列表和图片大小列表，删除多余的项。     </p><p>根据这一思路，可以写一个简单的py脚本来实现：       </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true">#-*- coidng: utf-8 -*-</span><span class="token keyword">import</span> xml<span class="token punctuation">.</span>etree<span class="token punctuation">.</span>ElementTree <span class="token keyword">as</span> ET<span class="token keyword">import</span> osVOC_ROOT <span class="token operator">=</span> <span class="token string">"/home/zhengkai/data/VOCdevkit/"</span>CLASS2KEEP <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'background'</span><span class="token punctuation">,</span>            <span class="token string">'aeroplane'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span>            <span class="token string">'bottle'</span><span class="token punctuation">,</span> <span class="token string">'bus'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'chair'</span><span class="token punctuation">,</span>            <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'bicycle'</span><span class="token punctuation">,</span> <span class="token string">'motorbike'</span><span class="token punctuation">,</span>            <span class="token string">'boat'</span><span class="token punctuation">,</span> <span class="token string">'sofa'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">process_xml</span><span class="token punctuation">(</span>src_path<span class="token punctuation">,</span> dst_path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    解析并处理xml文件——    解析源文件，删除无关object标签，    如果存在有效object，则写入到目标文件，并返回True；    否则，直接返回False。    """</span>    tree <span class="token operator">=</span> ET<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>src_path<span class="token punctuation">)</span>    root <span class="token operator">=</span> tree<span class="token punctuation">.</span>getroot<span class="token punctuation">(</span><span class="token punctuation">)</span>    no_objs <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">for</span> obj <span class="token keyword">in</span> root<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">"object"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        cls <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        <span class="token keyword">if</span> cls <span class="token operator">not</span> <span class="token keyword">in</span> CLASS2KEEP<span class="token punctuation">:</span>            root<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>obj<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            no_objs <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token keyword">if</span> <span class="token operator">not</span> no_objs<span class="token punctuation">:</span>        tree<span class="token punctuation">.</span>write<span class="token punctuation">(</span>dst_path<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 记录有效的xml文件名</span>    valid_lst <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 处理xml文件，新的xml文件写入到NewAnnotations目录下</span>    <span class="token keyword">for</span> dataset <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"VOC2007/"</span><span class="token punctuation">,</span> <span class="token string">"VOC2012/"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        raw_anno_dir <span class="token operator">=</span> VOC_ROOT <span class="token operator">+</span> dataset <span class="token operator">+</span> <span class="token string">"Annotations/"</span>        dst_anno_dir <span class="token operator">=</span> VOC_ROOT <span class="token operator">+</span> dataset <span class="token operator">+</span> <span class="token string">"NewAnnotations/"</span>        <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>dst_anno_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Create a new dir: "</span> <span class="token operator">+</span> dst_anno_dir<span class="token punctuation">)</span>            os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>dst_anno_dir<span class="token punctuation">)</span>        <span class="token keyword">for</span> xml_filename <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>raw_anno_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> process_xml<span class="token punctuation">(</span>raw_anno_dir <span class="token operator">+</span> xml_filename<span class="token punctuation">,</span> dst_anno_dir <span class="token operator">+</span> xml_filename<span class="token punctuation">)</span><span class="token punctuation">:</span>                valid_lst<span class="token punctuation">.</span>append<span class="token punctuation">(</span>xml_filename<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 处理图片路径列表txt文件，根据valid_lst筛选有效的图片路径</span>    <span class="token keyword">for</span> filename <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"test.txt"</span><span class="token punctuation">,</span> <span class="token string">"trainval.txt"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"new_"</span> <span class="token operator">+</span> filename<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> nf<span class="token punctuation">:</span>            <span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> of<span class="token punctuation">:</span>                <span class="token keyword">for</span> line <span class="token keyword">in</span> of<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> valid_lst<span class="token punctuation">:</span>                        nf<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"Annotations"</span><span class="token punctuation">,</span> <span class="token string">"NewAnnotations"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 处理图片大小列表txt文件，根据valid_lst筛选有效的图片大小</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"new_test_name_size.txt"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> nf<span class="token punctuation">:</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"test_name_size.txt"</span><span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> of<span class="token punctuation">:</span>            <span class="token keyword">for</span> line <span class="token keyword">in</span> of<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> valid_lst<span class="token punctuation">:</span>                    nf<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"Annotations"</span><span class="token punctuation">,</span> <span class="token string">"NewAnnotations"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="生成lmdb文件"><a href="#生成lmdb文件" class="headerlink" title="生成lmdb文件"></a>生成lmdb文件</h4><p>修改caffe-ssd数据处理工具中的标签映射文件<code>labelmap_voc.prototxt</code>，该文件由若干个类似下边的<code>item</code>组成：      </p><pre><code>item {    name: &quot;none_of_the_above&quot;    label: 0    display_name: &quot;background&quot;}</code></pre><ul><li>name：物体类别在xml文件中出现的名称</li><li>label：标签对应的数值（为方便处理，建议序号从0递增）</li><li>display_name：该类别最后要展示出来的名称</li></ul><p>删除映射文件中多余类别对应的<code>item</code>，然后按顺序重新为各个类别编号（修改label项）；     </p><p>修改<code>create_list.sh</code>脚本，将<strong>第29行</strong>的<code>Annotations</code>改为<code>NewAnnotations</code>——       </p><pre class=" language-bash"><code class="language-bash"><span class="token punctuation">[</span>27<span class="token punctuation">]</span> label_file<span class="token operator">=</span><span class="token variable">$bash_dir</span>/<span class="token variable">$dataset</span><span class="token string">"_label.txt"</span><span class="token punctuation">[</span>28<span class="token punctuation">]</span> <span class="token function">cp</span> <span class="token variable">$dataset_file</span> <span class="token variable">$label_file</span><span class="token punctuation">[</span>29<span class="token punctuation">]</span> <span class="token function">sed</span> -i <span class="token string">"s/^/<span class="token variable">$name</span>\/NewAnnotations\//g"</span> <span class="token variable">$label_file</span><span class="token punctuation">[</span>30<span class="token punctuation">]</span> <span class="token function">sed</span> -i <span class="token string">"s/$/.xml/g"</span> <span class="token variable">$label_file</span></code></pre><p>然后跟上一篇文章一样，依次执行脚本<code>create_list.sh</code>和<code>create_data.sh</code>即可。     </p><h4 id="训练和部署"><a href="#训练和部署" class="headerlink" title="训练和部署"></a>训练和部署</h4><p>训练和部署过程与 《<a href="/2018/08/21/mssd-try1/#训练">训练MobileNet-SSD/开始训练MobileNet-SSD/训练(部署) | Hey~YaHei!</a>》 基本相同；<br>微小的区别在于，     </p><ol><li>生成模型文件时<br> <code>./gen_model.sh 21</code>中<code>21</code>要换成实际的类别数量（含背景background）；      </li><li>要使用新的标签映射文件<code>labelmap.prototxt</code>；</li><li>应用程序中标签要对应修改<br> 如《<a href="/2018/08/04/RK3399-Tengine/#目标检测网络MobileNet-SSD">RK3399上Tengine平台搭建/目标检测网络MobileNet-SSD | Hey~YaHei!</a>》最后列出的代码中，<code>post_process_ssd</code>函数里的<code>class_names</code>数组常量要对应修改（索引号与<code>labelmap.prototxt</code>文件里的<code>label</code>标签一一对应）。         </li></ol><h3 id="Depthwise-Convolution和Standard-Convolution-Group-的比较"><a href="#Depthwise-Convolution和Standard-Convolution-Group-的比较" class="headerlink" title="Depthwise Convolution和Standard Convolution(Group)的比较"></a>Depthwise Convolution和Standard Convolution(Group)的比较</h3><p>观察chuanqi305的 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/deploy.prototxt" target="_blank" rel="noopener">MobileNet-SSD模型文件deploy.prototxt</a> 可以发现，其中的Depthwise Convolution都是使用特殊的caffe原生卷积层（group参数与num_output参数相等）来实现的。     </p><p>查阅caffe官方文档，</p><blockquote><p>group (g) [default 1]: If g &gt; 1, we restrict the connectivity of each filter to a subset of the input. Specifically, the input and output channels are separated into g groups, and the ith output group channels will be only connected to the ith input group channels.</p></blockquote><p>可以知道，group参数强制输入通道和输出通道分为若干组，一组输入通道卷积运算得到组序相同的输出通道，所以使 <code>group == num_output</code> 确实可以得到与Depthwise Convolution相同的结果。但是，“分组”？从字面的意思上看，不免让人怀疑，这个group是不是通过简单的循环实现的，如果真是如此，不同的group还会并行的计算吗？我们不妨做个简单的实验——<br>我在github上找到第三方实现的 <a href="https://github.com/yonghenglh6/DepthwiseConvolution" target="_blank" rel="noopener">caffe - Depthwise Convolution层</a>，根据其README的说明将其放到caffe源码下重新编译caffe-ssd得到专门实现的<code>DepthwiseConvolution</code>。<br>在单卡GTX1080Ti、Intel E5-2683环境下测试结果如下（分别重复运行100次，单位：秒/百次）：     </p><table><thead><tr><th style="text-align:center">caffe-mode</th><th style="text-align:center">Standard Convolution(Group)</th><th style="text-align:center">Depthwise Convolution</th></tr></thead><tbody><tr><td style="text-align:center">cpu-only</td><td style="text-align:center">26.13568</td><td style="text-align:center">23.40233</td></tr><tr><td style="text-align:center">gpu</td><td style="text-align:center">6.93938</td><td style="text-align:center">0.53499</td></tr><tr><td style="text-align:center">gpu-cudnn</td><td style="text-align:center">6.86799</td><td style="text-align:center">0.53779</td></tr></tbody></table><p>很显然，在cpu-only模式下，两者没有太大区别；在gpu模式下（无论是caffe自身的加速库还是cudnn加速库），专门实现的DepthwiseConvolution都要<strong>快10倍左右</strong>！<br>除此之外，<a href="https://github.com/yonghenglh6/DepthwiseConvolution" target="_blank" rel="noopener">Depthwise Convolution Layer | github</a>的README也给出了一些测试数据。      </p><p>那Tengine有专门实现的DepthwiseConvolution层吗？<br>从<a href="https://github.com/OAID/Tengine/blob/master/doc/operator_ir.md" target="_blank" rel="noopener">官方的文档</a>上看，确实没有专门的DepthwiseConvolution层，如果试着在模型文件里使用<code>DepthwiseConvolution</code>也会看到报错。不过！在源码 <a href="https://github.com/OAID/Tengine/blob/master/executor/operator/arm64/conv/conv_2d_dw.cpp#L222" target="_blank" rel="noopener">executor/operator/arm64/conv/conv_2d_dw.cpp | github, Tengine</a>可以看到有一个 <code>isDepthwiseSupported</code> 函数——      </p><pre class=" language-cpp"><code class="language-cpp"><span class="token keyword">static</span> <span class="token keyword">bool</span> <span class="token function">isDepthwiseSupported</span><span class="token punctuation">(</span><span class="token keyword">const</span> ConvParam <span class="token operator">*</span> param<span class="token punctuation">,</span> <span class="token keyword">const</span> TShape<span class="token operator">&amp;</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> input_c<span class="token operator">=</span>input_shape<span class="token punctuation">.</span><span class="token function">GetC</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> group<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>group<span class="token punctuation">;</span>    <span class="token keyword">int</span> kernel_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>kernel_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> kernel_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>kernel_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> stride_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>stride_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> stride_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>stride_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> dilation_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>dilation_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> dilation_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>dilation_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_h0<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_w0<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_h1<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_w1<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>group <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">||</span> input_c <span class="token operator">!=</span> group <span class="token operator">||</span> kernel_h <span class="token operator">!=</span> <span class="token number">3</span> <span class="token operator">||</span> kernel_w <span class="token operator">!=</span> <span class="token number">3</span> <span class="token operator">||</span>       pad_h0 <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> pad_w0 <span class="token operator">!=</span><span class="token number">1</span> <span class="token operator">||</span> pad_h0 <span class="token operator">!=</span> pad_h1 <span class="token operator">||</span> pad_w0 <span class="token operator">!=</span> pad_w1 <span class="token operator">||</span>       dilation_h <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> dilation_w <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> stride_w <span class="token operator">!=</span> stride_h<span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>也就是说，<strong>Tengine会自行判断<code>Convolution</code>层是否属于<code>DepthwiseConvolution</code>并相应作出优化</strong>，对应的汇编实现为 <a href="https://github.com/OAID/Tengine/blob/master/executor/operator/arm64/conv/dw_k3s1p1.S" target="_blank" rel="noopener">executor/operator/arm64/conv/dw_k3s1p1.S | github, Tengine</a>。     </p><h3 id="进一步替换Depthwise-Convolution"><a href="#进一步替换Depthwise-Convolution" class="headerlink" title="进一步替换Depthwise Convolution"></a>进一步替换Depthwise Convolution</h3><p>从《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》一文中可以看到，chuanqi305在设计MobileNet-SSD时还是保守地在<code>Conv14_1</code>到<code>Conv17_2</code>使用Standard Convolution，我们不妨进一步把这一部分也替换为深度向分解的卷积，替换的方式也很简单，举个例子：<br>对于某个传统的Convolution层        </p><pre><code>layer {  name: &quot;conv14_2&quot;  type: &quot;Convolution&quot;  bottom: &quot;conv14_1&quot;  top: &quot;conv14_2&quot;  param {    lr_mult: 1.0    decay_mult: 1.0  }  param {    lr_mult: 2.0    decay_mult: 0.0  }  convolution_param {    num_output: 512    pad: 1    kernel_size: 3    stride: 2    weight_filler {      type: &quot;msra&quot;    }    bias_filler {      type: &quot;constant&quot;      value: 0.0    }  }}layer {  name: &quot;conv14_2/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2&quot;  top: &quot;conv14_2&quot;}</code></pre><p>修改为</p><pre><code>layer {  name: &quot;conv14_2_new/dw&quot;  type: &quot;DepthwiseConvolution&quot;  bottom: &quot;conv14_1_new&quot;  top: &quot;conv14_2_new/dw&quot;  param {    lr_mult: 0.1    decay_mult: 0.1  }  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 2    group: 256    engine: CAFFE    weight_filler {      type: &quot;msra&quot;    }  }}layer {  name: &quot;conv14_2_new/dw/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2_new/dw&quot;  top: &quot;conv14_2_new/dw&quot;}layer {  name: &quot;conv14_2_new&quot;  type: &quot;Convolution&quot;  bottom: &quot;conv14_2_new/dw&quot;  top: &quot;conv14_2_new&quot;  param {    lr_mult: 0.1    decay_mult: 0.1  }  convolution_param {    num_output: 512    bias_term: false    kernel_size: 1    weight_filler {      type: &quot;msra&quot;    }  }}layer {  name: &quot;conv14_2_new/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2_new&quot;  top: &quot;conv14_2_new&quot;}</code></pre><p>要注意几点，    </p><ol><li>修改后的层要给个新的名字，避免初始化权重的时候从预训练好的模型误导入权重；      </li><li>训练模型<code>train.prototxt</code>和测试模型<code>test.prototxt</code>可别忘了加上BN层；      </li><li>部署在Tengine上的时候要记得把type从<code>DepthwiseConvolution</code>替换为<code>Convolution</code>。            </li></ol><p>……其他层也做类似的修改即可。<br>替换前后比较——      </p><table><thead><tr><th style="text-align:center">VOC2007-test</th><th style="text-align:center">MobileNet-SSD</th><th style="text-align:center">MobileNet-SSDLite</th></tr></thead><tbody><tr><td style="text-align:center">mAP</td><td style="text-align:center">0.727</td><td style="text-align:center">0.718</td></tr><tr><td style="text-align:center">FPS(1080Ti)</td><td style="text-align:center">258</td><td style="text-align:center">278</td></tr><tr><td style="text-align:center">caffemodel</td><td style="text-align:center">23MB</td><td style="text-align:center">16MB</td></tr></tbody></table><hr><p>本文介绍了如何削减VOC数据集上多余类别进行训练，并且尝试用深度向分解的卷积层进一步替换传统的卷积层，同时比较了专门优化加速的DepthwiseConvolution和Convolution(Group)在效率上的差别。<br>下一篇文章《<a href="/2018/09/10/mssd-try3">基于MobileNet-SSD的目标检测Demo（二）</a>》将介绍如何把<strong>目标检测</strong>和<strong>视频解码与显示</strong>分别放到两个线程上，来提高目标检测demo的流畅性。        </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>训练MobileNet-SSD</title>
      <link href="/2018/08/21/mssd-try1/"/>
      <url>/2018/08/21/mssd-try1/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=26142483&auto=0&height=32"></iframe>     <p>《<a href="/2018/08/04/RK3399-Tengine">RK3399上Tengine平台搭建 | Hey~YaHei!</a>》一文介绍了RK3399和Tengine并且尝试跑通了MobilNet-SSD网络，而随后又分别用《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》、《<a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>》《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》三篇文章分别介绍了MobileNet v1、SSD和MobileNet-SSD。<br>接下来，本文将尝试训练自己的MobileNet-SSD并且部署在Tengine平台上。     </p><hr><h3 id="安装配置cuda、caffe"><a href="#安装配置cuda、caffe" class="headerlink" title="安装配置cuda、caffe"></a>安装配置cuda、caffe</h3><p>cuda的安装网上有非常多的教程，比如《<a href="https://www.cnblogs.com/iloveblog/p/7683349.html" target="_blank" rel="noopener">Ubuntu16.04+cuda9.0安装教程 | 贝多芬的悲桑, cnblogs</a>》和《<a href="https://blog.csdn.net/huang826336127/article/details/78754767" target="_blank" rel="noopener">安装cuda-8.0 | 代码小哥, csdn</a>》，过程也很简单，在官网下载你需要的版本对应的<code>.run</code>文件，直接运行按提示安装即可。          </p><p>caffe由于要使用SSD框架，所以要编译安装caffe的ssd分支——       </p><h4 id="下载caffe-ssd源码"><a href="#下载caffe-ssd源码" class="headerlink" title="下载caffe-ssd源码"></a>下载caffe-ssd源码</h4><ul><li>直接用git把仓库克隆到本地并切换到ssd分支       <pre class=" language-bash"><code class="language-bash">  <span class="token function">git</span> clone https://github.com/weiliu89/caffe.git  <span class="token function">cd</span> caffe  <span class="token function">git</span> checkout ssd</code></pre></li><li>github服务器在海外，网络不是很稳定，你可以试着挂vpn下载源码。<br>  如果你的linux没有配置vpn，但windows或mac有，那也可以直接在 <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">weiliu89/caffe at ssd | github</a> 上下载zip压缩包，再拷贝到linux用<code>unzip</code>指令解压；       </li><li>如果你没有vpn，也可以到我的网盘上下载：<a href="https://pan.baidu.com/s/1wR0iJcvTgT7c4vwJF1pVUQ#list/path=%2Fblog-share%2Fmobilenet_ssd" target="_blank" rel="noopener">blog-share/mobilenet_ssd/caffe-ssd.zip | 百度网盘</a></li></ul><h4 id="编译caffe-ssd"><a href="#编译caffe-ssd" class="headerlink" title="编译caffe-ssd"></a>编译caffe-ssd</h4><p>编译过程可以参照 <a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener">Caffe | Installation</a> 来进行；     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> CAFFE_ROOT<span class="token operator">=</span>/your/caffe/root/path<span class="token comment" spellcheck="true"># 进入caffe源码的根目录</span><span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span><span class="token comment" spellcheck="true"># 从模板拷贝一份编译的配置文件</span><span class="token function">cp</span> Makefile.config.example Makefile.config<span class="token comment" spellcheck="true"># 按需要修改编译配置</span><span class="token comment" spellcheck="true"># vim Makefile.config</span><span class="token comment" spellcheck="true"># 开始编译（参数j表示编译使用的线程数量，一般数值越大越快，取决于你cpu支持的线程数）</span><span class="token function">make</span> -j8<span class="token comment" spellcheck="true"># 修改环境变量</span><span class="token keyword">echo</span> <span class="token string">"export PYTHONPATH=<span class="token variable">$CAFFE_ROOT</span>/python:<span class="token variable">$PYTHONPATH</span>"</span> <span class="token operator">>></span> ~/.bashrc<span class="token function">source</span> ~/.bashrc<span class="token comment" spellcheck="true"># 编译python包</span><span class="token function">make</span> py<span class="token comment" spellcheck="true"># 编译测试程序</span><span class="token function">make</span> <span class="token function">test</span> -j8<span class="token comment" spellcheck="true"># 测试</span><span class="token function">make</span> runtest -j8</code></pre><p>关于配置文件，一般直接用默认配置就行，       </p><ul><li>如果你想用cudnn加速而不是caffe自己提供的加速库（caffe不建议用cudnn），给<strong>第5行</strong>解除注释。      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>4<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># cuDNN acceleration switch (uncomment to build with cuDNN).</span>  <span class="token punctuation">[</span>5<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># USE_CUDNN := 1</span></code></pre></li><li>如果你不想用gpu，给<strong>第8行</strong>解除注释，      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>7<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CPU-only switch (uncomment to build without GPU support).</span>  <span class="token punctuation">[</span>8<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CPU_ONLY := 1</span></code></pre></li><li>如果你的cuda没有安装在默认位置，你可能需要在<strong>第28行</strong>修改变量<code>CUDA_DIR</code>       <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>27<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CUDA directory contains bin/ and lib/ directories that we need.</span>  <span class="token punctuation">[</span>28<span class="token punctuation">]</span> CUDA_DIR :<span class="token operator">=</span> /usr/local/cuda  <span class="token punctuation">[</span>29<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># On Ubuntu 14.04, if cuda tools are installed via</span>  <span class="token punctuation">[</span>30<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span>  <span class="token punctuation">[</span>31<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CUDA_DIR := /usr</span></code></pre></li><li>如果你用的是anaconda，或者你要用python3接口，你可能需要修改      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>64<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># NOTE: this is required only if you will compile the python interface.</span>  <span class="token punctuation">[</span>65<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># We need to be able to find Python.h and numpy/arrayobject.h.</span>  <span class="token punctuation">[</span>66<span class="token punctuation">]</span> PYTHON_INCLUDE :<span class="token operator">=</span> /usr/include/python2.7 \  <span class="token punctuation">[</span>67<span class="token punctuation">]</span>         /usr/lib/python2.7/dist-packages/numpy/core/include  <span class="token punctuation">[</span>68<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Anaconda Python distribution is quite popular. Include path:</span>  <span class="token punctuation">[</span>69<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Verify anaconda location, sometimes it's in root.</span>  <span class="token punctuation">[</span>70<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># ANACONDA_HOME := $(HOME)/anaconda2</span>  <span class="token punctuation">[</span>71<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span>  <span class="token punctuation">[</span>72<span class="token punctuation">]</span>         <span class="token variable"><span class="token variable">$(</span>ANACONDA_HOME<span class="token variable">)</span></span>/include/python2.7 \  <span class="token punctuation">[</span>73<span class="token punctuation">]</span>         <span class="token variable"><span class="token variable">$(</span>ANACONDA_HOME<span class="token variable">)</span></span>/lib/python2.7/site-packages/numpy/core/include \  <span class="token punctuation">[</span>74<span class="token punctuation">]</span>   <span class="token punctuation">[</span>75<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Uncomment to use Python 3 (default is Python 2)</span>  <span class="token punctuation">[</span>76<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_LIBRARIES := boost_python3 python3.5m</span>  <span class="token punctuation">[</span>77<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_INCLUDE := /usr/include/python3.5m \</span>  <span class="token punctuation">[</span>78<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></code></pre></li></ul><p><strong><em>编译过程中如果出现 <code>google::protobuf</code> 或者 <code>google::protoc</code> 相关的报错，你可能需要到 <a href="https://github.com/google/protobuf/releases" target="_blank" rel="noopener">google/protobuf | github</a> 下载合适版本的protobuf到本地编译并且配置环境变量（可以用<code>protoc --version</code>指令查看当前使用的protobuf版本）</em></strong>    </p><h3 id="开始训练MobileNet-SSD"><a href="#开始训练MobileNet-SSD" class="headerlink" title="开始训练MobileNet-SSD"></a>开始训练MobileNet-SSD</h3><p>首先，先跑通默认的 <a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">MobileNet-SSD</a>——      </p><h4 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h4><p>MobileNet-SSD默认使用<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">Pascal VOC</a>的2007和2012数据集，<br>下载以下数据集，并解压到同一个目录下：     </p><ul><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar" target="_blank" rel="noopener">VOC2007 - training/validation data</a></li><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar" target="_blank" rel="noopener">VOC2007 - test data</a></li><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar" target="_blank" rel="noopener">VOC2012 - training/validation data</a></li><li><a href="http://host.robots.ox.ac.uk:8080/" target="_blank" rel="noopener">VOC2012 - test data</a>    </li></ul><p>解压后目录如下图所示：<br><img src="/imgs/mssd-try/try1/dataset_tree.png" width="350">       </p><p>然后用caffe-ssd提供的 <a href="https://github.com/weiliu89/caffe/tree/ssd/data/VOC0712" target="_blank" rel="noopener">VOC数据集处理工具</a> 对数据集进行处理——    </p><ol><li>按实际情况修改并执行脚本<code>$CAFFE_ROOT/data/VOC0712/create_list.sh</code><br> 将<strong>第3行</strong>的<code>root_dir</code>变量修改为你的VOC数据集目录，比如按照我的目录树，则设置为<code>$HOME/data/VOCdevkit</code>       <pre class=" language-bash"><code class="language-bash"> <span class="token punctuation">[</span>1<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#!/bin/bash</span> <span class="token punctuation">[</span>2<span class="token punctuation">]</span>  <span class="token punctuation">[</span>3<span class="token punctuation">]</span> root_dir<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/ <span class="token punctuation">[</span>4<span class="token punctuation">]</span> sub_dir<span class="token operator">=</span>ImageSets/Main <span class="token punctuation">[</span>5<span class="token punctuation">]</span> bash_dir<span class="token operator">=</span><span class="token string">"$(cd "</span><span class="token punctuation">$(</span>dirname <span class="token string">"<span class="token variable">${BASH_SOURCE[0]}</span>"</span><span class="token punctuation">)</span><span class="token string">" &amp;&amp; pwd)"</span></code></pre></li><li>按实际情况修改并执行脚本<code>$CAFFE_ROOT/data/VOC0712/create_data.sh</code><br> 将<strong>第7行</strong>的<code>data_root_dir</code>变量修改为你的VOC数据集目录，比如按照我的目录树，则设置为<code>$HOME/data/VOCdevkit</code>       <pre class=" language-bash"><code class="language-bash"> <span class="token punctuation">[</span>1<span class="token punctuation">]</span> cur_dir<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">cd</span> <span class="token punctuation">$(</span> <span class="token function">dirname</span> $<span class="token punctuation">{</span>BASH_SOURCE<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">}</span> <span class="token variable">)</span></span> <span class="token operator">&amp;&amp;</span> <span class="token function">pwd</span> <span class="token punctuation">)</span> <span class="token punctuation">[</span>2<span class="token punctuation">]</span> root_dir<span class="token operator">=</span><span class="token variable">$cur_dir</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span> <span class="token punctuation">[</span>3<span class="token punctuation">]</span>  <span class="token punctuation">[</span>4<span class="token punctuation">]</span> <span class="token function">cd</span> <span class="token variable">$root_dir</span> <span class="token punctuation">[</span>5<span class="token punctuation">]</span>  <span class="token punctuation">[</span>6<span class="token punctuation">]</span> redo<span class="token operator">=</span>1 <span class="token punctuation">[</span>7<span class="token punctuation">]</span> data_root_dir<span class="token operator">=</span><span class="token string">"<span class="token variable">$HOME</span>/data/VOCdevkit"</span> <span class="token punctuation">[</span>8<span class="token punctuation">]</span> dataset_name<span class="token operator">=</span><span class="token string">"VOC0712"</span> <span class="token punctuation">[</span>9<span class="token punctuation">]</span> mapfile<span class="token operator">=</span><span class="token string">"<span class="token variable">$root_dir</span>/data/<span class="token variable">$dataset_name</span>/labelmap_voc.prototxt"</span></code></pre></li></ol><p>执行完毕后将会自动在<code>$data_root_dir</code>目录下生成<code>VOC0712</code>子目录，里边包含了从数据集VOC2007和VOC2012提取的图片和标记信息，并构建caffe能够高效读取的lmdb文件。<br><code>VOC0712</code>子目录结构如下图所示：<br><img src="/imgs/mssd-try/try1/lmdb_tree.png" width="350">       </p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p><strong>下载MobileNet-SSD源码：</strong>          </p><ul><li>直接用git克隆仓库到examples目录下     <pre class=" language-bash"><code class="language-bash">  <span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span>/examples  <span class="token function">git</span> clone https://github.com/chuanqi305/MobileNet-SSD.git</code></pre></li><li>或者到我的网盘上下载并解压到caffe的examples目录下：<a href="https://pan.baidu.com/s/1wR0iJcvTgT7c4vwJF1pVUQ#list/path=%2Fblog-share%2Fmobilenet_ssd" target="_blank" rel="noopener">blog-share/mobilenet_ssd/MobileNet-SSD.zip | 百度网盘</a>         </li></ul><p><strong>创建数据集软链接：</strong>       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> TRAINVAL_LMDB<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb/<span class="token function">export</span> TEST_LMDB<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb/<span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span>/examples/MobileNet-SSD<span class="token function">ln</span> -s <span class="token variable">$TRAINVAL_LMDB</span> ./trainval_lmdb<span class="token function">ln</span> -s <span class="token variable">$TEST_LMDB</span> ./test_lmdb</code></pre><p><strong>把VOC的标签映射文件复制过来：</strong>        </p><pre class=" language-bash"><code class="language-bash"><span class="token function">cp</span> <span class="token variable">$CAFFE_ROOT</span>/data/VOC0712/labelmap_voc.prototxt <span class="token variable">$CAFFE_ROOT</span>/examples/MobileNet-SSD/labelmap.prototxt</code></pre><p><strong>生成模型文件：</strong>     </p><pre class=" language-bash"><code class="language-bash">./gen_model.sh 21</code></pre><p>这里21指的是VOC的21个类别（含负样本），生成的模型文件默认放置在<code>example</code>目录下；          </p><p><strong>如果需要修改训练参数和测试参数</strong>，可以分别修改目录下的<code>solver_train.protxt</code>和<code>sovler_test.protxt</code>文件，<br>默认使用<code>example</code>目录下的训练模型和测试模型；     </p><p><strong>如果需要指定GPU和初始化权重</strong>，可以修改目录下的<code>train.sh</code>或<code>test.sh</code>文件，以<code>train.sh</code>为例：       </p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span><span class="token keyword">if</span> <span class="token operator">!</span> <span class="token function">test</span> -f example/MobileNetSSD_train.prototxt <span class="token punctuation">;</span><span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"error: example/MobileNetSSD_train.prototxt does not exist."</span>    <span class="token keyword">echo</span> <span class="token string">"please use the gen_model.sh to generate your own model."</span>        <span class="token keyword">exit</span> 1<span class="token keyword">fi</span><span class="token function">mkdir</span> -p snapshot<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/build/tools/caffe train -solver<span class="token operator">=</span><span class="token string">"solver_train.prototxt"</span> \-weights<span class="token operator">=</span><span class="token string">"mobilenet_iter_73000.caffemodel"</span> \-gpu 0 </code></pre><p><code>weights</code>参数指定初始化的权重文件，这里用了chuanqi305预训练迭代了73000次的模型；<br><code>gpu</code>参数指定使用的gpu，多个gpu可以用逗号隔开；<br>除此之外，如果需要继续之前中断的训练，还可以指定<code>snapshot</code>参数，<br>比如我想从最近的快照继续训练，可以这样修改<code>train.sh</code>——      </p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>latest<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">ls</span> -t snapshot/*.caffemodel <span class="token operator">|</span> <span class="token function">head</span> -n 1<span class="token variable">)</span></span><span class="token keyword">if</span> <span class="token operator">!</span> <span class="token function">test</span> -f example/MobileNetSSD_train.prototxt <span class="token punctuation">;</span><span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"error: example/MobileNetSSD_train.prototxt does not exist."</span>    <span class="token keyword">echo</span> <span class="token string">"please use the gen_model.sh to generate your own model."</span>        <span class="token keyword">exit</span> 1<span class="token keyword">fi</span><span class="token function">mkdir</span> -p snapshot<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/build/tools/caffe train -solver<span class="token operator">=</span><span class="token string">"solver_train.prototxt"</span> \-snapshot<span class="token operator">=</span><span class="token variable">$latest</span> \-gpu 0 </code></pre><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p><strong>合并BN层：</strong><br>训练后会在<code>snapshot</code>目录下产生一个相应的<code>caffemodel</code>文件；<br>按实际情况修改<code>merge_bn.py</code>文件并执行：      </p><pre class=" language-python"><code class="language-python"><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np  <span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">import</span> sys<span class="token punctuation">,</span>os  <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">]</span> caffe_root <span class="token operator">=</span> <span class="token string">'/your/caffe/root/path/'</span><span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">]</span> sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> caffe_root <span class="token operator">+</span> <span class="token string">'python'</span><span class="token punctuation">)</span>  <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">]</span> <span class="token keyword">import</span> caffe  <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">]</span> train_proto <span class="token operator">=</span> <span class="token string">'example/MobileNetSSD_train.prototxt'</span>       <span class="token comment" spellcheck="true"># 训练时所用的模型文件</span><span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">]</span> train_model <span class="token operator">=</span> <span class="token string">'mobilenet_iter_73000.caffemodel'</span>           <span class="token comment" spellcheck="true"># 训练后产生的caffemodel文件</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> deploy_proto <span class="token operator">=</span> <span class="token string">'example/MobileNetSSD_deploy.prototxt'</span>     <span class="token comment" spellcheck="true"># 部署时所要用的模型文件（去掉BN层）</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span> save_model <span class="token operator">=</span> <span class="token string">'MobileNetSSD_deploy.caffemodel'</span>             <span class="token comment" spellcheck="true"># 最终生成的caffemodel文件（合并BN层参数）</span></code></pre><p>生成的合并BN层后的caffemodel就在MobileNet-SSD项目的根目录下；<br>编辑<code>example/MobileNetSSD_deploy.prototxt</code>修改输入层，即把</p><pre><code>input: &quot;data&quot;input_shape {    dim: 1    dim: 3    dim: 300    dim: 300}</code></pre><p>改为     </p><pre><code>layer {    name: &quot;input&quot;    type: &quot;Input&quot;    top: &quot;data&quot;    input_param {        shape {            dim: 1            dim: 3            dim: 300            dim: 300        }    }}</code></pre><p>把 <code>example/MobileNetSSD_deploy.prototxt</code> 和 <code>MobileNetSSD_deploy.caffemodel</code> 拷贝到<strong>Tengine</strong>平台的<code>models</code>目录下，此时运行<code>mobilenet_ssd/MSSD</code>用的就是新训练好的模型啦！      </p><hr><p>本文简单介绍了如何用chuanqi305的MobileNet-SSD训练出自己的网络。<br>下一篇文章《<a href="/2018/08/24/mssd-try2/">基于MobileNet-SSD的目标检测Demo（一） | Hey~YaHei!</a>》将继续尝试根据实际情况删减多余类别进行训练。还可以注意到，        </p><ul><li>chuanqi305的MobileNet-SSD模型除了基础网络部分之外依旧保守的使用了Standard Conv，可以尝试将这一部分也改造为Depthwise Conv；      </li><li>同时，MobileNet-SSD使用带group的caffe原生Conv来进行Depthwise Conv操作，这是非常低效率的，下篇文章还将进一步比较Depthwise Conv和带group的原生Conv的效率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNet-SSD网络解析</title>
      <link href="/2018/08/08/MobileNets-SSD/"/>
      <url>/2018/08/08/MobileNets-SSD/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28941713&auto=0&height=32"></iframe>     <p>上一篇文章《<a href="/2018/08/06/SSD/#网络结构">SSD框架解析 - 网络结构| Hey~YaHei!</a>》和上上篇文章《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》我们分别解析了SSD目标检测框架和MobileNet v1分类模型。<br>在本文中将会把两者综合起来，一起分析<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合得到MobileNet-SSD网络的。         </p><hr><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>参照 <a href="https://github.com/chuanqi305/MobileNet-SSD/tree/master/template" target="_blank" rel="noopener">MobileNet-SSD(chuanqi305)的caffe模型（prototxt文件） | github</a>，绘制出MobileNet-SSD的整体结构如下（忽略一些参数细节）：<br><img src="/imgs/MobileNet-SSD/mobilenet-ssd.jpg" alt="mobilenet-ssd">    </p><p>图片中从上到下分别是MobileNet v1模型（统一输入大小为300x300）、chuanqi305的Mobilenet-SSD网络、VGG16-SSD网络。且默认都是用3x3大小的卷积核，除了MobileNet-SSD的Conv14_1、Conv15_1、Conv16_1、Conv17_1和VGG16-SSD的Conv8_1、Conv9_1、Conv10_1、Conv11_1用的是1x1大小的卷积核。<br>图中每个立方体代表对应层的<strong>输出</strong>特征图；        </p><ul><li>首先观察基础网络部分<br>  MobileNet-SSD从Conv0到Conv13的配置与MobileNet v1模型是完全一致的，相当于只是去掉MobileNet v1最后的全局平均池化、全连接层和Softmax层；     </li><li>再看SSD部分       <ul><li>在VGG16-SSD的方案中，用Conv6和Conv7分别替代了原VGG16的FC6和FC7；       </li><li>MobileNet-SSD和VGG16-SSD都是从六个不同尺度的特征图上提取特征来做Detections，它们的大小为：      <pre><code>  MobileNet-SSD   |   VGG16-SSD  ----------------+-----------------  19 x 19 x  512  |   38 x 38 x  512  10 x 10 x 1024  |   19 x 19 x 1024   5 x  5 x  512  |   10 x 10 x  512   3 x  3 x  256  |   5  x  5 x  256   2 x  2 x  256  |   3  x  3 x  256   1 x  1 x  128  |   1  x  1 x  128</code></pre><ul><li>从通道数量上看，两者是完全一致的</li><li>从特征图分辨率上看，MobileNet-SSD都只有VGG16-SSD的一半     <ul><li><strong>这意味着什么？</strong><br>  打个比方，假设对于那个分辨率最大的特征图，都能用4x4的感受野检测出一只猫，如下图所示，黑色是头，红色是身体，棕色是腿，黄色是尾巴。<br>  <center><img src="/imgs/MobileNet-SSD/cat.png" width="200"></center><br>  那用MobileNet-SSD可以检测出占原图$\frac{4}{19} \approx 0.211$大小的猫，而VGG16-SSD却可以检测出占原图$\frac{4}{38} \approx 0.105$大小的猫；       </li><li><strong>那为什么MobileNet-SSD为什么不和VGG16-SSD一样，从38x38分辨率的特征图开始做Detections呢？</strong><br>  回到上一篇博文《<a href="/2018/08/06/SSD/#网络结构">SSD框架解析 - 网络结构| Hey~YaHei!</a>》，VGG16是从Conv4_3也就是第10层卷积层取出38x38分辨率的特征图；<br>  再观察一下MobileNet v1-300的模型，想要取出38x38分辨率的特征图，最深也只能从Conv5也就是第6层卷积层取出，这个位置比较浅，实在很难保证网络提取出了足够有用的特征可以使用；         </li><li><strong>那可以通过增加最初输入图片的分辨率来解决这个问题吗？</strong><br>  倒也可以，比如把输入图片大小扩大到512x512，那么Conv11的输出就变为32x32，按上上一点的描述，可以检测出占原图$\frac{4}{32} = 0.125$大小的猫；<br>  但要付出相应的代价，仅考虑基础网络部分（Conv0到Conv13），参数数量和乘加运算量均提高为原来的 $(\frac{512}{300})^2 \approx 2.913$ 倍（不考虑padding的影响，计算方式可以参考《<a href="/2018/08/05/MobileNets_v1/#效率比较">MobileNets v1模型解析 - 效率比较 | Hey~YaHei!</a>》），MobileNet本身小模型的低参数量、低运算量优势变得不再明显。           </li></ul></li></ul></li><li>还有一个小细节，观察特征图到Detections的路径<br>  VGG16-SSD中用的都是3x3大小的卷积核，缺省框数量依次是<font color="red">4</font>、6、6、6、<font color="red">4</font>、<font color="red">4</font>；<br>  MobileNet-SSD中用的都是1x1大小的卷积核，缺省框数量依次是<font color="red">3</font>、6、6、6、<font color="red">6</font>、<font color="red">6</font>；<br>  <em>这一部分的改动不是很能理解，3x3卷积改1x1卷积可能是实践中发现改动后效果差不多但可以减少运算量；缺省框数量改动的原因就不得而知了~</em>      </li></ul></li></ul><h3 id="BN层合并"><a href="#BN层合并" class="headerlink" title="BN层合并"></a>BN层合并</h3><p>对比chuanqi305的 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/template/MobileNetSSD_train_template.prototxt" target="_blank" rel="noopener">train模型</a> 和 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/template/MobileNetSSD_deploy_template.prototxt" target="_blank" rel="noopener">deploy模型</a> 还能发现一件有趣的事情——<br><strong>deploy模型中的BN层和scale层都不见啦！！！</strong><br>BN层是这样随随便便就能丢弃的么？没道理啊！         </p><p>几经辗转，查阅资料之后发现，原来BN层是可以合并进前一层的卷积层或全连接层的，而且这还有利于减少预测用时。<br>参考《<a href="http://machinethink.net/blog/object-detection-with-yolo/#converting-to-metal" target="_blank" rel="noopener">Real-time object detection with YOLO - Converting to Metal</a>》    </p><p>合并的原理：卷积层、全连接层和BN层都是纯粹的线性转换。       </p><p>数学推导也很简单：<br><em>假设图片为 $x$ ，卷积层权重为 $w$ 。</em><br>那么对于卷积运算有，<br>$$ conv[j] = x[i]w[0] + x[i+1]w[1] + x[i+2]w[2] + … + x[i+k]w[k] + b $$<br>BN层运算为，<br>$$ bn[j] = \frac{\gamma (conv[j] - mean)}{\sqrt{variance}} + \beta = \frac{\gamma \cdot conv[j]}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} + \beta $$<br>代入$conv[j]$变为，<br>$$ bn[j] = x[i] \frac{\gamma \cdot w[0]}{\sqrt{variance}} + x[i+1] \frac{\gamma \cdot w[1]}{\sqrt{variance}} + … + x[i+k] \frac{\gamma \cdot w[k]}{\sqrt{variance}} + \frac{\gamma \cdot b}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} + \beta $$<br>两式对比可以得到，<br>$$ w_{new} = \frac{\gamma \cdot w}{\sqrt{variance}} $$<br>$$ b_{new} = \beta + \frac{\gamma \cdot b}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} = \beta + \frac{\gamma (b-mean)}{\sqrt{variance}} $$<br>注意，其中 $\gamma$、$mean$、$variance$、$\beta$ 都是训练出来的量，在预测阶段相当于一个常量。       </p><p>原文摘录如下：    </p><center><img src="/imgs/MobileNet-SSD/Converting to Metal.png" alt="Converting to Metal"></center>    <hr><p>本文介绍了chuanqi305的MobileNet-SSD网络是如何组成的以及实用的MergeBN技术，在下一篇博文中我们将尝试用该网络进行训练并部署在RK3399的Tengine平台上，并且进一步对该网络进行改进以满足我们实际场景的需要。        </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>SSD框架解析</title>
      <link href="/2018/08/06/SSD/"/>
      <url>/2018/08/06/SSD/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=38689090&auto=0&height=32"></iframe>       <p>上一篇文章《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》介绍了MobileNets v1的核心思想和网络结构，本文将解析MobileNet SSD网络的另外一部分——SSD框架。SSD的主要贡献，一方面在于从多个不同尺度的特征图获取特征信息进而预测目标的位置和类别，使网络同时对输入图片上的大小物体都比较敏感；另一方面在于其训练技巧值得借鉴，这在论文中有一定的阐述，更加详细的训练技巧可以结合作者开源的代码学习。         </p><hr><p>论文：《<a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector(2016)</a>》      </p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>与分析MobileNets v1模型不同，分析框架我们先从整体入手。          </p><p><img src="/imgs/SSD/SSD_architecture.png" alt="SSD architeture">     </p><ul><li>用预训练好的分类网络作为特征提取器（论文里使用的是VGG16）<br>  VGG16原模型如下图所示：<br>  <img src="/imgs/SSD/vgg16_raw.png" alt="vgg16"><br>  由于SSD论文里输入是 $300 \times 300$，我们重写一下VGG16模型的各层输出大小：<br>  <img src="/imgs/SSD/vgg16_300.png" alt="vgg16">     </li><li>论文中，SSD丢掉了VGG16最后的全局池化和全连接层（FC6和FC7）<br>  并且分别用 $3 \times 3 \times 1024$ 的卷积层Conv6和 $1 \times 1 \times 1024$ 的卷积层Conv7替代FC6和FC7作基础分类网络的最终特征抽取。         </li><li>随后是一系列不同尺度的卷积层在不同尺度上做特征提取      </li><li>融合不同尺度特征信息<br>  分别用卷积操作从 $38 \times 38$ 的Conv4_3、$19 \times 19$ 的Conv7、$10 \times 10$ 的Conv8_2、$5 \times 5$ 的Conv9_2、$3 \times 3$ 的Conv10_2、$1 \times 1$ 的Conv11_2抽取特征（直接回归出后述的预测框的位置以及各分类的置信度），各自Flatten之后拼接成“长条”状特征向量。       </li><li>非极大值抑制（Non-Maximum Suppression，NMS）      <ul><li>从置信度最高的框开始，如果其他预测框和该框的jaccard重叠率超过阈值，则丢弃     </li><li>从剩下的框找到置信度最高的框，如果其他预测框和该框的jaccard重叠率超过阈值，则丢弃     </li><li>……重复直到遍历所有的框</li></ul></li></ul><h3 id="感受野和缺省框"><a href="#感受野和缺省框" class="headerlink" title="感受野和缺省框"></a>感受野和缺省框</h3><center><img src="/imgs/SSD/bounding box.png" alt="bounding box"></center>       <h4 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h4><p>卷积是对特定小区域的特征提取，比如一张 $300 \times 300$ 的原图经过一定卷积操作之后得到 $8 \times 8$ 的特征图，特征图上的每个“像素点”其实对应原图的一个“感受野”，在这里一个“感受野”的大小为 $\frac{300}{8} \times \frac{300}{8}$ 也即 $37.5 \times 37.5$ （实际上不可能有$.5$，是$37$还是$38$要看卷积过程是否padding）。      </p><p>换句话说，特征图左上角的一个感受野其实是原图左上角一个 $37.5 \times 37.5$ 大小区域的一个特征抽取（或者说是这个区域的一个抽象化，融合了这个区域的信息）。     </p><h4 id="缺省框"><a href="#缺省框" class="headerlink" title="缺省框"></a>缺省框</h4><p><em>论文里称为default box，而源码里称为prior box.</em>          </p><p>在SSD中，为每个向Detection贡献特征的特征图上的每个感受野分配若干不同大小和长宽比（aspect ratio）的绑定框（Bounding Box），比如在作者开源的源码中，这些绑定框是这样生成的——<br>参考<a href="https://github.com/weiliu89/caffe/blob/ssd/src/caffe/layers/prior_box_layer.cpp" target="_blank" rel="noopener">SSD - PriorBoxLayer源码 | github</a><br>（其中，$D_{min}、x_{offset}、y_{offset}、AR、D_{max}$ 都是用户指定的超参数，而且 $D_{max}$ 是可选的）</p><ol><li>首先有一个最小的绑定框，其尺寸为 $D_{min} \times D_{min}$，绑定框中心相对感受野中心偏移 $(x_{offset}, y_{offset})$；       </li><li>根据用户指定的长宽比列表 $AR$ 生成若干绑定框<br> 对于某一特定长宽比 $AR_i$，生成尺寸分别为 $\frac{D_{min}}{\sqrt{AR_i}} \times D_{min}\sqrt{AR_i}$ 和 $D_{min}\sqrt{AR_i} \times \frac{D_{min}}{\sqrt{AR_i}}$ 的绑定框，其框中心同1；      </li><li>最后还生成一个尺寸为 $D_{max} \times D_{max}$ 的绑定框，其框中心同1；     </li></ol><p>事实上网络输出的预测框位置参数就是这每一个缺省框的中心点偏移量和长、宽偏移量（每个框共计4个参数）！<br><em>对于一个方框其实有两种不同的表示方法，都是四元组，即（中心点x坐标，中心点y坐标，长度，宽度）或（左上角x坐标，左下角x坐标，右下角x坐标，右下角y坐标）</em>    </p><h4 id="D-min-和-D-max-的设置建议"><a href="#D-min-和-D-max-的设置建议" class="headerlink" title="$D_{min}$ 和 $D_{max}$ 的设置建议"></a>$D_{min}$ 和 $D_{max}$ 的设置建议</h4><p>论文中也给出了超参数 $D_{min}$ 和 $D_{max}$ 的设置建议——      </p><ul><li>首先确定框的最大最小归一化尺寸 $scale_{min}$ 、 $scale_{max}$<br>  $$ scale = \frac{BoxSize}{ImageSize} $$<br>  比如按论文的设计 $300 \times 300$ 的输入图片，设 $scale_{min} = 0.2、scale_{max} = 0.9$，那框的最大尺寸就是 $270 \times 270$，最小尺寸为 $60 \times 60$；     </li><li>然后最浅层的特征图对应的bbox设 $\text{min_size} = scale_{min} \times ImageSize$；        </li><li>随后按等差数量设置各特征对应的bbox的 min_size，而 max_size 可以取下一层的 min_size（论文中默认不设置max_size）；      </li></ul><p>写成数学公式为：<br>$$s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1} (k-1), k \in [1,m]$$<br>其中，<br>$s_k$，是第k个特征图的 $min_size$ 参数；<br>$s_{min}$，是设计好的最小归一化尺寸，即前述 $scale_{min}$；<br>$s_{max}$，是设计好的最大归一化尺寸，即前述 $scale_{max}$；<br>$m$，是特征图的总数；<br><em>注意：上述“特征图”指的是对Detection有贡献的特征图，序号从浅层到深层递增</em>        </p><p>回过头再看看网络结构，注意从不同尺度融合特征信息的那一部分，用的都是一个 $3 \times 3$ 的卷积核，输出通道要么是 num_class + 4 的四倍要么是六倍，这里 num_class 就是最终分类的数量，数值 $4$ 其实指的是预测框的位置参数的数量，而四倍或六倍指的是特征图上每个感受野对应的绑定框数量。<br>按照论文，浅层的分辨率比较高，所以使用了 $AR=\{1,2,3\}$ 的分辨率组合，对于 $AR_i = 2$ 和 $AR_i = 3$ 分别会生成两个绑定框，而 $AR_i = 1$ 除了本身之外还会额外产生一个归一化尺寸为 $s’_k = \sqrt{s_k s_{k+1}}$ 的绑定框；而浅层分辨率较低，2或3的长宽比其实没太大区别或必要，所以只取了 $AR=\{1,2\}$。       </p><p>源码prototxt参数示例：       </p><pre><code>layer {  name: &quot;conv11_mbox_priorbox&quot;  type: &quot;PriorBox&quot;  bottom: &quot;conv11&quot;  # bottom[0]：特征向量  bottom: &quot;data&quot;    # bottom[1]：原始输入（主要提供宽、高参数）  top: &quot;conv11_mbox_priorbox&quot;  prior_box_param {    min_size: 30.0      # 最小的基本Box大小（可根据论文计算）    aspect_ratio: 2.0   # 所要用的长宽比（除了1.0）    flip: true          # 是否以0.5的概率翻转    clip: false         # 是否截断Box（若截断表示不允许Box超出图片范围）    variance: 0.1       # xmin的偏差因子    variance: 0.1       # ymin的偏差因子    variance: 0.2       # xmax的偏差因子    variance: 0.2       # ymax的偏差因子    offset: 0.5         # Box中心相对于感受野中心的x、y偏移量  }}</code></pre><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="框配对（Matching）"><a href="#框配对（Matching）" class="headerlink" title="框配对（Matching）"></a>框配对（Matching）</h4><p>前边讲到了感受野和缺省框，我们现在看看这些缺省框是如何跟实际标注的真实框（Ground Truth, GT）配对起来的。        </p><p>介绍一个重叠率计算公式，jaccard重叠率：<br>$$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|} $$</p><p>对于每个GT，      </p><ul><li>首先，在所有缺省框中挑选出jaccard重叠率最高的一个作为配对正样本，确保每个GT都有唯一一个配对的缺省框；       </li><li>然后，用其他缺省框挨个与GT计算jaccard，超过阈值（论文中设定为0.5）的都作为配对正样本；       </li><li>其他未配对的都作为负样本，但显然数量上负样本远多于正样本，这会导致网络过于重视负样本而loss不稳定；<br>  所以论文采取了Hard Negative Mining的策略，训练时按照分类的置信度为各个负样本框排序；<br>  挑选置信度高的一批作为实际训练的负样本，同时控制负样本的数量大概为正样本的三倍左右      </li></ul><h4 id="损失函数（MultiBoxLoss）"><a href="#损失函数（MultiBoxLoss）" class="headerlink" title="损失函数（MultiBoxLoss）"></a>损失函数（MultiBoxLoss）</h4><p>损失函数由位置损失 $L_{loc}$ 和分类损失 $L_{conf}$ 加权求和获得；<br>完整公式如下：<br>$$L(x,c,l,g) = \frac{1}{N} (L_{conf}(x,c) + \alpha L_{loc}(x,l,g))$$<br>其中，<br>$N$，是匹配正样本的总量（如果N=0，则令L=0）；<br>$x$、$c$，分别是分类的指示量和置信度；<br>$l$、$g$，分别是预测框和GT框；<br>$\alpha$，是位置损失的权重。        </p><p>位置损失是预测框l和真实框g之间的Smooth L1损失，<br>$$ L_{loc} = \sum^N_{i \in Pos} \sum_{m \in {cx, cy, w, h} x^k_{ij}} smooth_{L1} (l^m_i - \hat{g}_j^m) $$<br>$$ \hat{g}^{cx}_j = \frac{g^{cx}_j - d^{cx}_i}{d_i^w}，\hat{g}^{cy}_j = \frac{g^{cy}_j - d^{cy}_i}{d_i^h}，\hat{g}^w_j = log(\frac{g_j^w}{d_i^w})，\hat{g}^h_j = log(\frac{g_j^h}{d_i^h}) $$<br>其中，<br>$x^k_{ij}$，是指示量，当第i个匹配框和分类p的第j个真实框配对时值为1，否则为0；<br>$cx$、$cy$、$w$、$h$，分别是框的中心点x坐标、中心点y坐标，宽度、高度；<br>$d$，是绑定框（网络本身预设的绑定框）；<br>$l$，是预测框（网络输出的加上预测偏移量的框）；<br>$g$，是GT框（数据集标注的真实框）。     </p><p>分类损失是分类置信度之间的softmax损失，<br>$$ L_{conf}(x,c) = -\sum^N_{i \in Pos} x^p_{ij} log(\hat{c}^p_i) - \sum_{i \in Neg}log(\hat{c}^0_i) $$<br>$$ \hat{c}^p_i = \frac{exp(c^p_i)}{\sum_p exp(c_i^p)} $$        </p><p>源码prototxt参数示例：     </p><pre><code>layer {  name: &quot;mbox_loss&quot;  type: &quot;MultiBoxLoss&quot;  bottom: &quot;mbox_loc&quot;        # 用于预测位置的特征向量  bottom: &quot;mbox_conf&quot;       # 用于预测分类的特征向量  bottom: &quot;mbox_priorbox&quot;   # 若干PriorBox的输出连接  bottom: &quot;label&quot;           # 训练用的标签  top: &quot;mbox_loss&quot;  include {    phase: TRAIN  }  propagate_down: true      # bottom[0] : mbox_loc，需要训练（反向传播）  propagate_down: true      # bottom[1] : mbox_conf，需要训练  propagate_down: false     # bottom[2] : mbox_priorbox，不需要训练  propagate_down: false     # bottom[3] : label，不需要训练  # 损失计算参数组  loss_param {    normalization: VALID    # 损失的归一化方式                            #    FULL：除以batch_size                            #    VALID：除以有效的数量（排除ignore_label参数指定的标签）                            #    NONE  }  # multibox损失参数组  multibox_loss_param {    loc_loss_type: SMOOTH_L1    # 位置预测的损失函数    conf_loss_type: SOFTMAX     # 分类预测的损失函数    loc_weight: 1.0             # loc_loss的权重，论文中的alpha    num_classes: 12             # 输出类别    share_location: true        # 是否让所有的预测框共享参数    match_type: PER_PREDICTION  #     overlap_threshold: 0.5      # 重叠阈值（训练时超过该阈值的Box作为正样本）    use_prior_for_matching: true    # 是否使用先验框（即前边是否有PriorBox）    background_label_id: 0      # 背景（background）标签的id值（与labelmap.prototxt文件匹配）    use_difficult_gt: true      # 是否使用difficult的Ground Truth（？）    neg_pos_ratio: 3.0          # 负样本:正样本 的比例    neg_overlap: 0.5            # 负样本阈值（低于该阈值的Box作为负样本）    code_type: CENTER_SIZE      # bounding box的编码方式    ignore_cross_boundary_bbox: false   #     mining_type: MAX_NEGATIVE   # 挖掘类型                                #   NONE：什么都不用，会发生正负样本步均衡                                #   MAX_NEGATIVE：为负样本排序，选择分类得分最高的一批作为训练的负样本                                #   HARD_EXAMPLE：选择基于“在线硬示例挖掘的基于训练区域的对象探测器”的硬实例（？）  }}</code></pre><h4 id="数据增强（Data-Augmentation）"><a href="#数据增强（Data-Augmentation）" class="headerlink" title="数据增强（Data Augmentation）"></a>数据增强（Data Augmentation）</h4><p>目标检测任务和图片分类任务不同，      </p><ul><li>图片分类任务可以随意对分类对象（也即整张图片）作伸缩变换       </li><li>目标检测任务只能对整张图片作伸缩变换，却不能对分类对象直接作伸缩变换      </li></ul><p>基本的数据增强，每张图片在输入会等概率地选取以下一种变换：      </p><ol><li>原始图片      </li><li>随机裁剪    </li><li>带jaccard重叠率约束的随机裁剪（论文给出的是0.1, 0.3, 0.5, 0.7, 0.9五种重叠率设置）        </li></ol><p>随机裁剪是有条件的，限制最低归一化尺寸（论文中为0.1），以及长宽比（论文中为$[1/2,2]$）；<br>实际操作中是先生成一系列满足条件的框，在从中挑选若干框来裁剪图像输入训练。         </p><p>仔细思考一下会发现，上述的数据增强方式只能对某个目标进行放大（zoom in）操作，这会导致小目标数据集缺失。<br>于是论文中在执行上述数据增强前又增加了一步缩小（zoom out）的增强措施——      </p><ol><li>首先生成一张画布，画布的长宽是原图片的1-4倍（随机数）；        </li><li>将原图放在画布的随机位置上；     </li><li>……接下来再送给前述的基本数据增强步骤       </li></ol><p><em>据论文介绍，该额外操作为mAP提升了两到三个百分点。</em>       </p><p>对于画布，从源码<a href="https://github.com/weiliu89/caffe/blob/ssd/src/caffe/data_transformer.cpp#L489" target="_blank" rel="noopener">data_transform.cpp/ExpandImage()函数（第489行） | github</a>上看，是用空格符初始化了一个字符串缓冲区来作为画布，这似乎意味着是用RGB(32,32,32)的固定颜色填充了画布（空格符的assic码为32）；<br>这种做法有些残暴，或许用原图的平均值、或者一个随机值来作固定颜色填充效果会更好一些；<br>甚至，用实际图片作为背景图可能会取得更好的效果，具体结论还有待实验。      </p><p>源码prototxt参数示例：     </p><pre><code>layer {  name: &quot;data&quot;  type: &quot;AnnotatedData&quot;  top: &quot;data&quot;  top: &quot;label&quot;  include {    phase: TRAIN  }  # 图像转换参数组  transform_param {    scale: 0.007843     # 归一化，1/127.5    mirror: true        # 0.5的概率镜像翻转    mean_value: 127.5   # 去均值（R通道）    mean_value: 127.5   # 去均值（G通道）    mean_value: 127.5   # 去均值（B通道）    # 缩放参数组    resize_param {      prob: 1.0                 # 缩放操作的概率      resize_mode: WARP         # 缩放模式                                #   WARP：放大或缩小以适应(width, height)                                #   FIT_SMALL_SIZE：                                #   FIT_LARGE_SIZE_AND_PAD：      height: 300               # 缩放后的高      width: 300                # 缩放后的宽      # 插值模式同opencv      interp_mode: LINEAR       # 线性      interp_mode: AREA         # 像素区域重采样      interp_mode: NEAREST      # 最近邻      interp_mode: CUBIC        # 三次样条      interp_mode: LANCZOS4     # Lanczos    }    emit_constraint {      emit_type: CENTER    }    # 色彩扭曲参数组    distort_param {      brightness_prob: 0.5      brightness_delta: 32.0      contrast_prob: 0.5      contrast_lower: 0.5      contrast_upper: 1.5      hue_prob: 0.5      hue_delta: 18.0      saturation_prob: 0.5      saturation_lower: 0.5      saturation_upper: 1.5      random_order_prob: 0.0    }    # 图像扩展参数组（zoom-out）    expand_param {      prob: 0.5                 # 概率      max_expand_ratio: 4.0     # 扩大倍数（4x4）    }  }  # 数据源参数组  data_param {    source: &quot;trainval_lmdb/&quot;    # 源文件    batch_size: 24           backend: LMDB               # 数据源类型  }  # AnnotatedData参数组  annotated_data_param {    # 采样器1：原图    batch_sampler {      max_sample: 1      max_trials: 1    }    # 采样器2：随机抠图（overlap=0.1）    batch_sampler {      sampler {        min_scale: 0.15         # 最小尺寸（原图的0.15倍）        max_scale: 1.0          # 最大尺寸        min_aspect_ratio: 0.5   # 最小长宽比        max_aspect_ratio: 2.0   # 最大长宽比      }      sample_constraint {        min_jaccard_overlap: 0.1    # 最小JACCARD_OVERLAP（重叠率）      }      max_sample: 1     # 最大采样数量      max_trials: 50    # 最大尝试数量（产生50个随机框，然后再筛选出1个输出）    }    # [略]采样器3：随机抠图（overlap=0.3）    # [略]采样器4：随机抠图（overlap=0.5）    # [略]采样器5：随机抠图（overlap=0.7）    # [略]采样器6：随机抠图（overlap=0.9）    # [略]采样器7：随机抠图（overlap=1.0）    label_map_file: &quot;labelmap.prototxt&quot;     # 标签映射文件  }}</code></pre><hr><p>本文介绍分析了SSD框架的核心思路和训练技巧，加上上一篇文章对MobileNet模型的解析，基本的前置理论准备完毕。在下一篇博文中，将分析<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合起来的——《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNets v1模型解析</title>
      <link href="/2018/08/05/MobileNets_v1/"/>
      <url>/2018/08/05/MobileNets_v1/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=488388731&auto=0&height=32"></iframe><br>上一篇文章《<a href="/2018/08/04/RK3399-Tengine">RK3399上Tengine平台搭建 | Hey~YaHei!</a>》中在RK3399上搭建了Tengine平台并试运行了MobileNet SSD网络，本文将为你解析MobileNets v1的实现思路。<br><strong><em>下边分解过程是按自己理解画的图，如果理解有误欢迎指正~</em></strong>       </p><hr><p>论文：《<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(2017)</a>》   </p><h3 id="深度向卷积分解（Depthwise-Separable-Convolution）"><a href="#深度向卷积分解（Depthwise-Separable-Convolution）" class="headerlink" title="深度向卷积分解（Depthwise Separable Convolution）"></a>深度向卷积分解（Depthwise Separable Convolution）</h3><h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center>   <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用M个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>可以看到，传统的卷积中用 $N$ 个不同的卷积核不厌其烦地对原图进行特征提取来得到 $N$ 通道的输出，这其中必定从原图中提取到了大量的重复特征。有没有可能<strong>只用单个卷积核来做特征提取，最后依旧能输出多通道的特征图</strong>呢？这就是深度向卷积分解的核心思想。<br>观察①阶段中的第三个子阶段，该阶段将多张单通道特征图按通道堆叠起来之后“拍扁”，如果去掉这个“拍扁”的过程，其实就可以提取得到一张 $M$ 通道的特征图啦，再经过一个 $M$ 维空间到 $N$ 维空间的线性映射，就能够和普通的卷积操作一样得到一张 $N$ 通道的特征图 $O$。完整的卷积过程如下图所示——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>①、②阶段与传统卷积①阶段的前两个阶段完全相同，③阶段比传统卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上</li></ol><h4 id="效率比较"><a href="#效率比较" class="headerlink" title="效率比较"></a>效率比较</h4><p><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><p>对于普通的卷积操作，        </p><ul><li>输出 $N$ 通道特征图需要 $N$ 个卷积核，故参数数量为 $M \times N \times D_K \times D_K$；     </li><li>一个 $D_K \times D_K$ 的卷积核在原图的某个位置的某个通道上需要进行 $D_K \times D_K$ 次乘加操作，输出特征图大小为 $D_O \times D_O$，原图通道数量为 $M$，共有 $N$ 个不同的卷积核，故乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M \times N$           </li></ul><p>对于深度向分解后的卷积操作，       </p><ul><li>特征提取只使用了一个 $D_K \times D_K$ 的卷积核，组合过程为了作线性映射用了 $N$ 个 $1 \times 1$ 的卷积核，故参数数量为 $M \times N + M \times D_K \times D_K$；       </li><li>特征提取过程只有一个卷积核，所以该过程乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M$。同样的，组合过程容易算得需要 $D_O \times D_O \times M \times N$ 次乘加操作。故乘加操作的总数量为 $D_O \times D_O \times M \times ( D_K \times D_K + N)$             </li></ul><p>总的来说，参数数量和乘加操作的运算量均下降为原来的 $\frac{1}{D_K^2} + \frac{1}{N}$，通常使用 $3 \times 3$ 的卷积核，也就是下降为原来的九分之一到八分之一左右。而从论文的实验部分来看，准确率也只有极小的下降。    </p><center><img src="/imgs/MobileNet/dw_vs_full.png" alt="dw_vs_full"></center>    <h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><center><img src="/imgs/MobileNet/v1_body_architecture.png" alt="v1_body_architecture"></center>    </p><ul><li>第一层使用普通的卷积层，最前端的特征提取非常重要，宁可存在重复的特征信息，也不该放掉；     </li><li>随后是一系列的深度向分解卷积层，用于逐层次提取特征，用步长不为1的卷积替代池化做降采样，同时整体也满足通道加深，特征图分辨率降低的CNN一般特点；        </li><li>最后也是常规的全局平均池化、全连接、Softmax；      </li><li>每次卷积操作之后都紧跟一个BN层（在预测阶段可以被合并）和一个RELU层；      <center><img src="/imgs/MobileNet/dw_vs_full_train.png" alt="dw_vs_full_train"></center>    </li><li>而且，深度向分解的卷积中绝大多数参数和运算都集中在 $1 \times 1$ 的pointwise卷积运算当中，这种运算恰恰是能够被 <code>GEneral Matrix Multiply(GEMM)</code> 函数高度优化的（具体参见《<a href="/2019/02/28/bag-of-tricks2/#高效的1x1卷积">深度学习小技巧（二）：模型微调 - 高效的1x1卷积 | Hey~YaHei!</a>》）；    <center><img src="/imgs/MobileNet/resource.png" alt="resource"></center>    </li><li>论文中还提到两个压缩模型的因子，分别用于输入图片分辨率收缩和特征通道数量收缩来进一步精简模型；    </li></ul><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul><li>小模型，不容易过拟合，不需要太多正则化策略和数据增强策略     </li><li>参数数量少，可以为Depthwise Convolution加入一个很小的权重衰减甚至不需要权重衰减         </li></ul><hr><p>本文介绍了MobileNets v1的主要思想——Depthwise Separable Convolution以及网络的完整结构，其实这一思想也不是MobileNet首创，在MobileNets v1之前Xception就已经提出这种思路。此外，MobileNets v1还是只是一个传统的结构，而且没有像Xception一样去RELU来避免卷积后通道下降非线性单元对特征信息造成的损失，在今年Google新发的MobileNets v2就分析和缓解这一问题，并且引入了类似ResNet的shortcut设计。     </p><p>但，我们还是先一步步解剖好chuanqi305的MobileNets-SSD网络，所以暂时不就MobileNets v2的设计展开讨论，下篇文章将继续讨论<strong>目标检测框架SSD</strong>的设计——《<a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>》。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>RK3399上Tengine平台搭建</title>
      <link href="/2018/08/04/RK3399-Tengine/"/>
      <url>/2018/08/04/RK3399-Tengine/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=640866&auto=0&height=32"></iframe><h3 id="Tengine-amp-RK3399介绍"><a href="#Tengine-amp-RK3399介绍" class="headerlink" title="Tengine&amp;RK3399介绍"></a>Tengine&amp;RK3399介绍</h3><h4 id="Tengine"><a href="#Tengine" class="headerlink" title="Tengine"></a>Tengine</h4><p><a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">OADI/Tengine | github</a>       </p><blockquote><p>Tengine 是OPEN AI LAB 为嵌入式设备开发的一个轻量级、高性能并且模块化的引擎。<br>Tengine在嵌入式设备上支持CPU，GPU，DLA/NPU，DSP异构计算的计算框架，实现异构计算的调度器，基于ARM平台的高效的计算库实现，针对特定硬件平台的性能优化，动态规划计算图的内存使用，提供对于网络远端AI计算能力的访问支持，支持多级别并行，整个系统模块可拆卸，基于事件驱动的计算模型，吸取已有AI计算框架的优点，设计全新的计算图表示。    </p></blockquote><h4 id="RK3399"><a href="#RK3399" class="headerlink" title="RK3399"></a>RK3399</h4><p><a href="https://store.t-firefly.com/goods.php?id=44" target="_blank" rel="noopener">Firefly-RK3399 | Firefly</a><br><a href="http://www.t-firefly.com/doc/download/page/id/3.html" target="_blank" rel="noopener">Firefly-RK3399资料下载 | Firefly</a>     </p><blockquote><p>作为Firefly新一代的顶级开源平台，Firefly-RK3399采用了六核64位“服务器级”处理器Rockchip RK3399，拥有2GB/4GB DDR3和16G/32GB eMMC, 并新增DP 1.2、PCIe 2.1 M.2、Type-C、USB3.0 HOST等高性能数据传输和显示接口。Firefly-RK3399强大的性能配置将给VR、全景拍摄、视觉识别、服务器、3D等前沿技术带来里程碑的变革。</p></blockquote><h3 id="RK3399系统烧录"><a href="#RK3399系统烧录" class="headerlink" title="RK3399系统烧录"></a>RK3399系统烧录</h3><p>系统烧录是玩开发板重要的一步，学会如何为开发板烧录系统，就可以无所畏惧地瞎捣鼓——玩坏了大不了就重刷系统！<br>参考<a href="http://dev.t-firefly.com/thread-12613-1-1.html" target="_blank" rel="noopener">RK3399资料 | Firefly论坛</a>      </p><ol><li>下载烧录工具和系统镜像<br> <a href="https://pan.baidu.com/s/1i4AhyJV#list/path=%2F" target="_blank" rel="noopener">烧录工具下载地址 | 百度云</a><br> <a href="https://pan.baidu.com/s/1hsOCOJU#list/path=%2F" target="_blank" rel="noopener">系统镜像下载地址 | 百度云</a><br> 系统镜像选择<strong>Firefly-RK3399-ubuntu16.04-20180416112819</strong>，下载下来是一个tar压缩包，解压后得到一个img镜像文件；<br> 烧录工具的压缩包解压后包含一个<code>AndroidTool</code>的烧录工具以及一个<code>DriverAssitant</code>驱动程序；      </li><li>按照USB驱动<br> 解压<code>DriverAssitant_v4.5</code>的压缩包，运行其中的<code>Driverinstall.exe</code>程序，点击“驱动安装”，按照步骤安装即可； <center><img src="/imgs/RK3399-Tengine/系统烧录-驱动安装.png" alt="系统烧录-驱动安装"></center>      </li><li>使RK3399进入升级模式<br> 用USB线连接PC和RK3399，Type-A端接PC，Type-C端接RK3399；<br> RK3399断电，按住RECOVERY键并接上电源（或在通电情况下，按住RECOVERY然后轻按RESET重启），保持两三秒后松开RECOVERY键，此时启动PC的设备管理器（快捷键Win+X，可以找到设备管理器入口），如果看到多出一个<strong>Class for rockusb devices</strong>设备说明RK3399成功进入升级模式     </li><li>系统烧录<br> 运行<code>AndroidTool.exe</code>，切换到“升级固件”选项卡，点击“固件”并选择下载的镜像文件（扩展名为<code>.img</code>），然后点击“升级”开始烧录，右边的log会输出相关的信息，直到“下载固件成功”以及“重启设备成功”说明成功完成烧录。<br> <img src="/imgs/RK3399-Tengine/镜像烧录成功.png" alt="镜像烧录成功">     </li></ol><h3 id="RK3399远程访问"><a href="#RK3399远程访问" class="headerlink" title="RK3399远程访问"></a>RK3399远程访问</h3><p>有时候专门为RK3399外接显示器和键鼠不大方便，我们可以通过ssh或vnc来远程访问；<br>首先让RK3399连接上网络（有线或无线），然后快捷键<code>ctrl</code>+<code>alt</code>+<code>t</code>呼出终端，输入指令<code>ifconfig</code>查看当前的网络配置——       </p><p><center><img src="/imgs/RK3399-Tengine/ifconfig.png" alt="ifconfig"></center><br>其中<code>eth0</code>和<code>wlan0</code>分别是有线和无线网络的配置信息，我这里连接的是无线网，可以看到<code>wlan0</code>下有一项<code>inet addr</code>，这是设备在无线网络上的ip地址，把后边这串地址<code>192.168.50.176</code>记下来待会用得上。（如果你接的是有线网络，那么也可以在<code>eth0</code>下找到相应的<code>inet addr</code>地址）<br>推荐一个非常实用的免费远程连接工具：<a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener">MobaXterm</a>     </p><h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><p>烧录的系统镜像本身自带一个ssh服务器<code>openssh-server</code>，不需要我们额外安装。直接打开MobaXterm，点击左上角的Session<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh1.png" alt="Mobaxterm_ssh1">    </p><p>按照下图进行配置——<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh2.png" alt="Mobaxterm_ssh2">       </p><p>配置完就可以通过远程连接到RK3399的终端上——<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh3.png" alt="Mobaxterm_ssh3">      </p><p>既可以直接在PC上远程执行指令，也可以方便地在PC和RK3399之间传输文件。     </p><h4 id="vnc"><a href="#vnc" class="headerlink" title="vnc"></a>vnc</h4><p>ssh只能连接到RK3399上的纯文本模式的终端，如果你需要进一步控制RK3399的界面，可以额外安装vnc服务；<br>打开终端，刷新apt源：     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> update</code></pre><p>安装x11vnc：       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> x11vnc</code></pre><p>为vnc服务生成密码（按照提示输入密码，并写入文件）：         </p><pre class=" language-bash"><code class="language-bash">x11vnc -storepasswd</code></pre><p>添加服务：     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> vim /lib/systemd/system/x11vnc.service</code></pre><p>为x11vnc.service添加以下内容然后保存：</p><pre><code>[Unit]Description=Start x11vnc at startup.After=multi-user.target[Service]Type=simpleExecStart=/usr/bin/x11vnc -auth guess -once -loop -noxdamage -repeat -rfbauth /home/firefly/.vnc/passwd -rfbport 5900 -shared[Install]WantedBy=multi-user.target</code></pre><p>加载服务：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl daemon-reload</code></pre><p>启动服务：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">service</span> x11vnc start</code></pre><p>设置开机自启动：       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl <span class="token function">enable</span> x11vnc.service</code></pre><p>这样一来RK3399上的vnc服务就设置完毕，接下来直接用<code>MobaXterm</code>远程控制桌面；<br>和ssd一样点击左上角的Session选项，切换到vnc选项卡，如下图配置：<br><img src="/imgs/RK3399-Tengine/Mobaxterm_vnc.png" alt="Mobaxterm_vnc"><br>配置完毕后双击并输入刚刚在RK3399上设置的密码就可以远程控制桌面~~        </p><h3 id="安装Tengine"><a href="#安装Tengine" class="headerlink" title="安装Tengine"></a>安装Tengine</h3><p>RK3399的基本环境安顿好之后，接下来可以开始搭建Tengine的环境。       </p><ol><li>安装git      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span></code></pre></li><li>用git下载源码     <pre class=" language-bash"><code class="language-bash"> <span class="token function">git</span> clone https://github.com/OAID/tengine</code></pre></li><li>安装编译源码时需要依赖的包      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> apt <span class="token function">install</span> libprotobuf-dev protobuf-compiler libboost-all-dev libgoogle-glog-dev libopenblas-dev libopencv-dev</code></pre></li><li>进入Tengine目录，复制编译的配置文件      <pre class=" language-bash"><code class="language-bash"> <span class="token function">cd</span> ~/tengine <span class="token function">cp</span> makefile.config.example makefile.config</code></pre></li><li>编辑<code>makefile.config</code>文件（如果不需要修改配置，可以直接忽略这一步）      <pre class=" language-bash"><code class="language-bash"> vim makefile.config</code></pre> <em>后续需要用到MobileNet SSD网络，其中包含维度交换的<code>Permute</code>层，该层是ACL暂时不支持的，所以这里暂时不建议开启ACL支持</em>     </li><li>编译     <pre class=" language-bash"><code class="language-bash"> <span class="token function">make</span> <span class="token function">make</span> <span class="token function">install</span></code></pre><!--7. 配置相关环境       ```bash sudo mkdir -p /usr/local/AID/Tengine sudo cp -rpf ~/Tengine/install/* /usr/local/AID/Tengine wget ftp://ftp.openailab.net.cn/tools/script/gen-pkg-config-pc.sh chmod +x ./gen-pkg-config-pc.sh sudo ./gen-pkg-config-pc.sh ```--></li></ol><h3 id="小试牛刀：运行Tengine自带的Demo"><a href="#小试牛刀：运行Tengine自带的Demo" class="headerlink" title="小试牛刀：运行Tengine自带的Demo"></a>小试牛刀：运行Tengine自带的Demo</h3><p>Tengine配置完毕，接下来我们试着运行Tengine自带的几个Demo。       </p><h4 id="分类网络SqueezeNet和MobileNet"><a href="#分类网络SqueezeNet和MobileNet" class="headerlink" title="分类网络SqueezeNet和MobileNet"></a>分类网络SqueezeNet和MobileNet</h4><ol><li>运行SqueezeNet<br> <code>./build/tests/bin/bench_sqz -r1</code>——（-r1 代表重复次数）       </li><li>运行MobileNet<br> <code>./build/tests/bin/bench_mobilenet -r1</code>——（-r1 代表重复次数）     </li></ol><p>运行后即可在终端看到输出结果。         </p><h4 id="目标检测网络MobileNet-SSD"><a href="#目标检测网络MobileNet-SSD" class="headerlink" title="目标检测网络MobileNet SSD"></a>目标检测网络MobileNet SSD</h4><p><a href="https://github.com/OAID/Tengine/tree/master/examples/mobilenet_ssd" target="_blank" rel="noopener">Mobilenet_SSD implementation with Tengine | github</a></p><p>在<code>example</code>目录下有一个<code>mobilenet_ssd</code>的子目录，一般情况下在目录执行      </p><pre class=" language-bash"><code class="language-bash">cmake <span class="token keyword">.</span><span class="token function">make</span></code></pre><p>就可以编译目录下的程序，然而……<br><img src="/imgs/RK3399-Tengine/tengine_cmake_not_found.png" alt="tengine_cmake_not_found"><br>好吧，烧录的系统上没有<code>cmake</code>，安装一下：    </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cmake</code></pre><p>不过make的时候又报了错——<br><img src="/imgs/RK3399-Tengine/tengine_headers_not_found.png" alt="tengine_headers_not_found"><br>看起来是找不到tengine的头文件，打开<code>CMakeLists.txt</code>文件瞧瞧，开头部分是这样的——     </p><pre class=" language-bash"><code class="language-bash">cmake_minimum_required <span class="token punctuation">(</span>VERSION 2.8<span class="token punctuation">)</span>project<span class="token punctuation">(</span>MSSD<span class="token punctuation">)</span>set<span class="token punctuation">(</span> INSTALL_DIR <span class="token variable">${TENGINE_DIR}</span>/install/<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_LIBS tengine<span class="token punctuation">)</span><span class="token punctuation">..</span>.</code></pre><p>好像这里引用了一个变量<code>TENGINE_DIR</code>但却没有提前指定，我们给它设置一下，变为——    </p><pre class=" language-bash"><code class="language-bash">cmake_minimum_required <span class="token punctuation">(</span>VERSION 2.8<span class="token punctuation">)</span>project<span class="token punctuation">(</span>MSSD<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_DIR /home/firefly/Tengine <span class="token punctuation">)</span>set<span class="token punctuation">(</span> INSTALL_DIR <span class="token variable">${TENGINE_DIR}</span>/install/<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_LIBS tengine<span class="token punctuation">)</span><span class="token punctuation">..</span>.</code></pre><p>再make一下，头文件是找到了，但printf好像有点问题——<br><img src="/imgs/RK3399-Tengine/tengine_printf.png" alt="tengine_printf"><br>打开源代码<code>mssd.cpp</code>，添加头文件<br><code>#include &lt;stdio.h&gt;</code><br>搜索一下<code>prinf</code>，如果<code>printf</code>前有<code>std::</code>就去掉（也就是把<code>std::printf</code>替换为<code>printf</code>），保存后再make一下……诶！通过了~~     </p><p>运行一下<br><code>./MSSD</code>     </p><p><img src="/imgs/RK3399-Tengine/tengine_model_not_found.png" alt="tengine_model_not_found"><br>ummmm没有模型文件，下载一个！<br>Tengine提供了一些训练好的模型——<a href="https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g#list/path=%2F" target="_blank" rel="noopener">Tengine_models | 百度云（提取码：57vb）</a><br>找到<code>mobilenet_ssd</code>文件夹把其中的<code>MobileNetSSD_deploy.prototxt</code>和<code>MobileNetSSD_deploy.caffemodel</code>下载下来放到<code>./models</code>目录下就行，再运行一下<code>./MSSD</code>——<br><img src="/imgs/RK3399-Tengine/tengine_mssd.png" alt="tengine_mssd"><br>没报错，有结果<del>，好了，收工</del>！     </p><p>等等，这些输出什么意思呢？      </p><ul><li>从prototxt文件里读出模型<br>  <code>proto file not specified,using /home/firefly/Tengine/models/MobileNetSSD_deploy.prototxt by default</code></li><li>从caffemodel文件里读出模型参数<br>  <code>model file not specified,using /home/firefly/Tengine/models/MobileNetSSD_deploy.caffemodel by default</code></li><li>读一张<code>ssd_dog.jpg</code>的文件作为输入<br>  <code>image file not specified,using /home/firefly/Tengine/tests/images/ssd_dog.jpg by default</code><br>  这张图片长这样：<br>  <img src="/imgs/RK3399-Tengine/tengine_mssd_input.jpg" alt="tengine_mssd_input">     </li><li>检测出了三个物体：     <pre><code>  repeat 1 times, avg time per run is 161.088 ms  detect ruesult num: 3  dog     :100%  BOX:( 138.529 , 209.238 ),( 324.026 , 541.275 )  car     :100%  BOX:( 466.138 , 72.3095 ),( 688.261 , 171.256 )  bicycle :99%  BOX:( 106.674 , 140.974 ),( 573.514 , 415.127 )</code></pre>  分别是狗、小车、自行车，用时161.088ms       </li><li>最后图片输出到了<code>save.jpg</code><br>  <code>[DETECTED IMAGE SAVED]: save.jpg</code><br>  这张图长这样：<br>  <img src="/imgs/RK3399-Tengine/tengine_mssd_output.jpg" alt="tengine_mssd_output">     </li></ul><p>啊就输入一张图片，输出检测好框好图片的结果。好没意思~改成动态检测的吧！<br>以下是修改后的源码，改动也不大，就是调用摄像头获取图片，处理完之后再输出显示（在RK3399上FPS大概为5-6）。      </p><pre class=" language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * License); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * *   http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, * software distributed under the License is distributed on an * AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY * KIND, either express or implied.  See the License for the * specific language governing permissions and limitations * under the License. */</span><span class="token comment" spellcheck="true">/* * Copyright (c) 2018, Open AI Lab * Author: chunyinglv@openailab.com */</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/imgproc/imgproc.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/highgui/highgui.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"tengine_c_api.h"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"common.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_PROTO "models/MobileNetSSD_deploy.prototxt"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_MODEL "models/MobileNetSSD_deploy.caffemodel"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_IMAGE "tests/images/ssd_dog.jpg"</span><span class="token keyword">struct</span> Box<span class="token punctuation">{</span>    <span class="token keyword">float</span> x0<span class="token punctuation">;</span>    <span class="token keyword">float</span> y0<span class="token punctuation">;</span>    <span class="token keyword">float</span> x1<span class="token punctuation">;</span>    <span class="token keyword">float</span> y1<span class="token punctuation">;</span>    <span class="token keyword">int</span> class_idx<span class="token punctuation">;</span>    <span class="token keyword">float</span> score<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// void get_input_data_ssd(std::string&amp; image_file, float* input_data, int img_h,  int img_w)</span><span class="token keyword">void</span> <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> input_data<span class="token punctuation">,</span> <span class="token keyword">int</span> img_h<span class="token punctuation">,</span>  <span class="token keyword">int</span> img_w<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// cv::Mat img = cv::imread(image_file);</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// std::cerr &lt;&lt; "Failed to read image file " &lt;&lt; image_file &lt;&lt; ".\n";</span>        std<span class="token operator">::</span>cerr <span class="token operator">&lt;&lt;</span> <span class="token string">"Failed to read image from camera.\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span><span class="token function">resize</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    img<span class="token punctuation">.</span><span class="token function">convertTo</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> CV_32FC3<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>img_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>img<span class="token punctuation">.</span>data<span class="token punctuation">;</span>    <span class="token keyword">int</span> hw <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w<span class="token punctuation">;</span>    <span class="token keyword">float</span> mean<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> img_h<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> w <span class="token operator">&lt;</span> img_w<span class="token punctuation">;</span> w<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span>            <span class="token punctuation">{</span>                input_data<span class="token punctuation">[</span>c <span class="token operator">*</span> hw <span class="token operator">+</span> h <span class="token operator">*</span> img_w <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.007843</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">*</span>img_data <span class="token operator">-</span> mean<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                img_data<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// void post_process_ssd(std::string&amp; image_file,float threshold,float* outdata,int num,std::string&amp; save_name)</span><span class="token keyword">void</span> <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span> threshold<span class="token punctuation">,</span><span class="token keyword">float</span><span class="token operator">*</span> outdata<span class="token punctuation">,</span><span class="token keyword">int</span> num<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> class_names<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"background"</span><span class="token punctuation">,</span>                            <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span>                            <span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span>                            <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span>                            <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span>                            <span class="token string">"sheep"</span><span class="token punctuation">,</span> <span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// cv::Mat img = cv::imread(image_file);</span>    <span class="token keyword">int</span> raw_h <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>height<span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_w <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>width<span class="token punctuation">;</span>    std<span class="token operator">::</span>vector<span class="token operator">&lt;</span>Box<span class="token operator">></span> boxes<span class="token punctuation">;</span>    <span class="token keyword">int</span> line_width<span class="token operator">=</span>raw_w<span class="token operator">*</span><span class="token number">0.002</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"detect ruesult num: %d \n"</span><span class="token punctuation">,</span>num<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>=</span>threshold<span class="token punctuation">)</span>        <span class="token punctuation">{</span>            Box box<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>class_idx<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>score<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            boxes<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>box<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%s\t:%.0f%%\n"</span><span class="token punctuation">,</span> class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> box<span class="token punctuation">.</span>score <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"BOX:( %g , %g ),( %g , %g )\n"</span><span class="token punctuation">,</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>x1<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y1<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        outdata<span class="token operator">+</span><span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span>boxes<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        Box box<span class="token operator">=</span>boxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x1<span class="token operator">-</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>y1<span class="token operator">-</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>line_width<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>ostringstream score_str<span class="token punctuation">;</span>        score_str<span class="token operator">&lt;&lt;</span>box<span class="token punctuation">.</span>score<span class="token punctuation">;</span>        std<span class="token operator">::</span>string label <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">string</span><span class="token punctuation">(</span>class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> score_str<span class="token punctuation">.</span><span class="token function">str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> baseLine <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span>Size label_size <span class="token operator">=</span> cv<span class="token operator">::</span><span class="token function">getTextSize</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>baseLine<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token operator">-</span> label_size<span class="token punctuation">.</span>height<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>label_size<span class="token punctuation">.</span>width<span class="token punctuation">,</span> label_size<span class="token punctuation">.</span>height <span class="token operator">+</span> baseLine<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CV_FILLED<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">putText</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> label<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">,</span>                    cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// cv::imwrite(save_name,img);</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"======================================\n";</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"[DETECTED IMAGE SAVED]:\t"&lt;&lt; save_name&lt;&lt;"\n";</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"======================================\n";</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> std<span class="token operator">::</span>string root_path <span class="token operator">=</span> <span class="token function">get_root_path</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>string proto_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string model_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string image_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string save_name<span class="token operator">=</span><span class="token string">"save.jpg"</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> res<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token punctuation">(</span> res<span class="token operator">=</span><span class="token function">getopt</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">,</span><span class="token string">"p:m:i:h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">switch</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token string">'p'</span><span class="token operator">:</span>                proto_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'m'</span><span class="token operator">:</span>                model_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'i'</span><span class="token operator">:</span>                image_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'h'</span><span class="token operator">:</span>                std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"[Usage]: "</span> <span class="token operator">&lt;&lt;</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" [-h]\n"</span>                          <span class="token operator">&lt;&lt;</span> <span class="token string">"   [-p proto_file] [-m model_file] [-i image_file]\n"</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token keyword">default</span><span class="token operator">:</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>model_name <span class="token operator">=</span> <span class="token string">"mssd_300"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>proto_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        proto_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_PROTO<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"proto file not specified,using "</span><span class="token operator">&lt;&lt;</span>proto_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>model_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        model_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_MODEL<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"model file not specified,using "</span><span class="token operator">&lt;&lt;</span>model_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>image_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        image_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_IMAGE<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"image file not specified,using "</span><span class="token operator">&lt;&lt;</span>image_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// init tengine</span>    <span class="token function">init_tengine_library</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">request_tengine_version</span><span class="token punctuation">(</span><span class="token string">"0.1"</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">load_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> <span class="token string">"caffe"</span><span class="token punctuation">,</span> proto_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"load model done!\n"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// create graph</span>    graph_t graph <span class="token operator">=</span> <span class="token function">create_runtime_graph</span><span class="token punctuation">(</span><span class="token string">"graph"</span><span class="token punctuation">,</span> model_name<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_graph_valid</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"create graph0 failed\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// input</span>    <span class="token keyword">int</span> img_h <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_w <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_size <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>input_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> img_size<span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span>VideoCapture <span class="token function">capture</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_WIDTH<span class="token punctuation">,</span> <span class="token number">1920</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_HEIGHT<span class="token punctuation">,</span> <span class="token number">1080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span>Mat frame<span class="token punctuation">;</span>    <span class="token keyword">int</span> node_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> tensor_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    tensor_t input_tensor <span class="token operator">=</span> <span class="token function">get_graph_input_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> node_idx<span class="token punctuation">,</span> tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_tensor_valid</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Get input node failed : node_idx: %d, tensor_idx: %d\n"</span><span class="token punctuation">,</span>node_idx<span class="token punctuation">,</span>tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> dims<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">set_tensor_shape</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dims<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">prerun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> repeat_count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>repeat <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">"REPEAT_COUNT"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>repeat<span class="token punctuation">)</span>        repeat_count <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">strtoul</span><span class="token punctuation">(</span>repeat<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>outdata<span class="token punctuation">;</span>    <span class="token keyword">int</span> out_dim<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">struct</span> timeval t0<span class="token punctuation">,</span> t1<span class="token punctuation">;</span>        <span class="token keyword">float</span> total_time <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>f<span class="token punctuation">;</span>        capture <span class="token operator">>></span> frame<span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> repeat_count<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span>  img_w<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t0<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">set_tensor_buffer</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">run_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">float</span> mytime <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t1<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span>t0<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t0<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span><span class="token punctuation">;</span>            total_time <span class="token operator">+</span><span class="token operator">=</span> mytime<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"--------------------------------------\n"</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"repeat "</span> <span class="token operator">&lt;&lt;</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" times, avg time per run is "</span> <span class="token operator">&lt;&lt;</span> total_time <span class="token operator">/</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" ms\n"</span><span class="token punctuation">;</span>        tensor_t out_tensor <span class="token operator">=</span> <span class="token function">get_graph_output_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//"detection_out");</span>        <span class="token function">get_tensor_shape</span><span class="token punctuation">(</span> out_tensor<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        outdata <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">get_tensor_buffer</span><span class="token punctuation">(</span>out_tensor<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> num<span class="token operator">=</span>out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">float</span> show_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">;</span>        <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> show_threshold<span class="token punctuation">,</span> outdata<span class="token punctuation">,</span> num<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> cv<span class="token operator">::</span><span class="token function">waitKey</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'q'</span> <span class="token punctuation">)</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">postrun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">free</span><span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">destroy_runtime_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">remove_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>报错，<br><img src="/imgs/RK3399-Tengine/opengl_error.png" alt="opengl_error"><br>烧录的系统没带opengl，没法调用opencv的imshow，树莓派也有一样的问题，安装 <code>libgl1-mesa-dri</code> 然后重启板子就能解决。       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libgl1-mesa-dri<span class="token function">sudo</span> <span class="token function">reboot</span></code></pre><hr><p>本篇文章中我们在RK3399上搭建了Tengine平台并试运行了MobileNet SSD网络，接下来我们将细致解析MobileNets分类网络和SSD目标检测框架，最后进一步解析源码作者<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合起来的。      </p><ul><li><a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>       </li><li><a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>      </li><li><a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>     </li></ul><p>随后还将结合实际的使用场景，尝试对MobileNet-SSD的网络结构以及训练参数细节进行分析优化~      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RK3399 </tag>
            
            <tag> Tengine </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>漫谈池化层</title>
      <link href="/2018/05/07/%E6%BC%AB%E8%B0%88%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
      <url>/2018/05/07/%E6%BC%AB%E8%B0%88%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=530995191&auto=0&height=32"></iframe><br>BGM：<strong>《革命机valvrave》ED1</strong><br>有点小high，建议先调低音量再播放；<br>还有个现场版，像是中二版的凤凰传奇hhhhh——<a href="https://www.bilibili.com/video/av20638023" target="_blank" rel="noopener">TMRevolution×水樹奈々 - Preserved Roses_革命デュアリズム_ | bilibili</a><br><em>话说水树奈奈是不是有点像隔壁控制学院的ly哈？</em><br>不过现场版的调音好像有问题，重低音效果有点差~     </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li><li>《<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">Deep Learning 深度学习(2017)</a>》        </li></ol><h3 id="基本的池化操作"><a href="#基本的池化操作" class="headerlink" title="基本的池化操作"></a>基本的池化操作</h3><p>简单的聚合操作，取均值、取最值等，分别称为<strong>平均池化</strong>（Average Pooling）和<strong>最大池化</strong>（Max Pooling）；<br>一般使池化核的大小与步长相等，不重叠、全覆盖地进行降采样；      </p><p>平均池化和最大池化的tensorflow实现参见 <a href="/2018/04/18/CNN/#池化层">卷积神经网络CNN / tensorflow实现 / 池化层 | Hey~YaHei!</a></p><h4 id="池化的意义"><a href="#池化的意义" class="headerlink" title="池化的意义"></a>池化的意义</h4><p>既对数据进行降采样（down-sampling）操作，又可以用p范数（p-norm）作非线性映射的“卷积”<br>$$p范数：||A||_p = (\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^p)^{1/p}, p&gt;=1$$<br><em>当 $p \to \infty$ 时即为最大池化</em>      </p><p>具体作用为：       </p><ol><li>特征不变性<br> 使模型更关注包含一定的自由度，能容忍特征微小的位移     </li><li>特征降维<br> 降采样使后续操作的计算量得到减少      </li><li>一定程度防止过拟合     </li></ol><h4 id="平均池化和最大池化的区别"><a href="#平均池化和最大池化的区别" class="headerlink" title="平均池化和最大池化的区别"></a>平均池化和最大池化的区别</h4><p>论文 <a href="https://cs.nyu.edu/~ylan/files/publi/boureau-cvpr-10.pdf" target="_blank" rel="noopener">Learning Mid-Level Features For Recognition(2010)</a> 第5节      </p><p>CNN特征提取的误差主要来自两个方面：    </p><ol><li>邻域大小受限造成的估计值<strong>方差增大</strong><br> 平均池化能有效减少该误差，更多的保留图像的背景信息；<br> 均匀采样的方差只有总体方差的 $\frac{1}{N}$；<br> 但如果模型中杂波方差较大（也即第二个误差明显），最后输出类别的概率分布将出现明显的混叠，导致分类准确率下降     </li><li>卷积层参数误差造成估计值<strong>均值偏移</strong><br> 最大池化能有效减少该误差，更多的保留图像纹理信息；<br> 最大值采样的方差为总体方差的 $\frac{1}{\sqrt{log(N)}}$ （推导过程参见论文），受第一种误差影响较大；    </li></ol><h3 id="改进的池化操作"><a href="#改进的池化操作" class="headerlink" title="改进的池化操作"></a>改进的池化操作</h3><h4 id="重叠池化（Overlapping-Pooling）"><a href="#重叠池化（Overlapping-Pooling）" class="headerlink" title="重叠池化（Overlapping Pooling）"></a>重叠池化（Overlapping Pooling）</h4><p>论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks(2012)</a> 第3.4节——      </p><blockquote><p>3.4 Overlapping Pooling<br>Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z × z centered at the location of the pooling unit. If we set s = z, we obtain traditional local pooling as commonly employed in CNNs. If we set s &lt; z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s = 2; z = 2, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.       </p></blockquote><p><em>卧槽，这篇论文居然有1.3万的被引？！？！？！？</em><br>思路很简单，让池化的步长略小于池化核的大小，使相邻的池化图像块出现一定程度的重叠；<br>据论文介绍，相比于大小2x2、步长2的传统池化操作，3x3、步长2的重叠池化操作使他们的模型的分类错误率top-1和top-5分别降低0.4%和0.3%；<br>并且他们通过实验观察到（有点主观），重叠池化具有一定的正则化作用；         </p><h4 id="随机池化（Stochastic-Pooling）"><a href="#随机池化（Stochastic-Pooling）" class="headerlink" title="随机池化（Stochastic Pooling）"></a>随机池化（Stochastic Pooling）</h4><p>论文：<a href="https://arxiv.org/pdf/1301.3557.pdf" target="_blank" rel="noopener">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks(2013)</a>     </p><p>按一定概率随机选取其中一个元素，介于平均池化和最大池化之间，并且受dropout启发，具有更好的正则化效果；<br>可以看作是，在输入图片的许多副本（有一些小的局部变形）上进行标准的最大池化操作；      </p><h5 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h5><ul><li>训练（随机采样）      <ol><li>归一化卷积层的输出计算，并以此作为每个元素的概率<br> $$ p_i = \frac{a_i}{\sum_{k \in R_j} a_k} $$<br> 其中，<br> $p_i$ 是第i个输出的采样概率，<br> $a_i$ 是前层（卷积层）的第i个输出，<br> $R_j$ 是池化核扫过的第j个区域      </li><li>用基于概率 $p$ 的多项分布的随机数对池化核扫过的区域进行随机采样作为区域池化的输出<br> $$ s_j = a_l \text{ where } l~P(p_1, …, p_{|R_j|})  $$<br> 其中，<br> $s_j$ 是第j个区域的输出，<br> $a_l$ 是随机采样抽中的前层的一个输出        </li><li>反向传播时，将抽中的元素用于梯度计算       </li></ol></li><li>预测（加权平均）<br>  $$ s_j = \sum_{i \in R_j} p_i a_i $$<br>  依旧计算每个前层输出的概率，以概率为权重，对池化核扫过的区域内每个输出作加权平均作为区域的输出    </li></ul><h5 id="分析比较"><a href="#分析比较" class="headerlink" title="分析比较"></a>分析比较</h5><ul><li>与最大池化相比（主要体现在训练上）<br>  最大池化仅仅保留区域内的最大值；而随机池化以各元素值的大小为概率，使非最大值的元素也有一定概率参与训练    </li><li>与平均池化相比（主要体现在预测上）<br>  平均池化不考虑权重，直接对区域内元素作算术平均；而随机池化采用加权平均，经过实验可以得知相对于平均池化有比较大的提升     </li><li>与dropout相比<br>  同样在训练中产生了很多不同的模型；<br>  考虑包含n个大小为d的区域的随机池化，其产生的可能模型有 $n^d$ 种，其中d通常为 $10^4 - 10^6$，且n通常取 $4, 9, 16$（相比之下，dropout中 $n=2$，因为只有激活/失活两种状态）；<br>  再者，dropout似乎不适宜用于卷积层，而随机池化可以；     </li></ul><h5 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h5><ul><li>准确度<br>  根据论文，相比于最大池化、平均池化、dropout，随机池化在CIFAR-10、MNIST、CIFAR-100、SVHN数据集上都取得比较明显的提升     </li><li>数据集规模<br>  论文以MNIST、CIFAR-10数据集为例，随机抽取数据集的子集对不同池化操作进行比较，如下图所示（左MNIST，右CIFAR-10）：<br>  <img src="/Handson-ML/SP_Reduced_Traing_Set_Size.png" alt="SP_Reduced_Traing_Set_Size">      <ul><li>数据集很小时，随机池化与最大池化效果相当，平均池化效果明显要差一些；     </li><li>随着数据集增大，随机池化与最大池化的效果几乎一直比平均池化要好（CIFAR-10在数据集比较大时，平均池化和最大池化效果相当）；      </li><li>随着数据集增大，随机池化与平均池化在MNIST上表现相当；在CIFAR-10上表现更优      </li></ul></li><li>不同训练、预测操作的组合<br>  论文以CIFAR-10数据集为例，对不同的池化方式在训练、预测上的不同组合进行比较，如下图所示：<br>  <img src="/Handson-ML/SP_Method_Combination.png" alt="SP_Method_Combination"><br>  可以看到，<strong>随机池化训练 + 加权平均预测</strong> 是最佳组合方式     </li><li>网络输出可视化<br>  作者用反卷积技术对网络输出进行可视化，以此比较最大池化、平均池化、随机池化以及训练过程中使用多项分布抽样、均匀抽样的不同表现；<br>  如下图所示（FF、UN分别表示多项分布抽样、均匀分布抽样的随机池化）：<br>  <img src="/Handson-ML/SP_Visualization.png" alt="SP_Visualization"><br>  可以看到，最大池化可以比较好的保留图片中的纹理信息，而平均池化已经失去可辨识的纹理信息了；<br>  多项分布抽样的随机池化表现良好，甚至比最大池化保留更多的纹理信息；<br>  随着更多的FF被替换为UN，纹理信息逐步丧失，而且越靠近底层的随机池化影响越严重；     </li></ul><h4 id="空间金字塔池化（Spatial-Pyramid-Pooling-SPP）"><a href="#空间金字塔池化（Spatial-Pyramid-Pooling-SPP）" class="headerlink" title="空间金字塔池化（Spatial Pyramid Pooling, SPP）"></a>空间金字塔池化（Spatial Pyramid Pooling, SPP）</h4><p>论文：<a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition(2015)</a>     </p><p><img src="/Handson-ML/Muti-scale_CNN.png" alt="Muti-scale_CNN">     </p><p>传统CNN中，输入的图片数据必须是特定尺度的，以确保进入FC或SVM前的尺寸固定（连接矩阵的大小固定）；<br>因而往往在输入数据前需要对图片进行尺寸处理，比如裁剪、缩放；<br>而SPP操作不仅可以支持多尺度的图片输入，而且多方面特征提取更具鲁棒性，在目标检测任务上能够明显提高精度；      </p><p><img src="/Handson-ML/Spatial_Pyramid_Pooling.png" alt="Spatial_Pyramid_Pooling">     </p><ul><li><code>convolutional layers</code> 包含卷积、激活函数、池化操作，假设最后一层输出 $N_f$ 个 $a \times b$的特征图<br>  论文中假设 $N_f = 256, a = b = 13$       </li><li><code>spatial pyramid pooling layers</code> 是一个典型三层SPP，包含三组动态参数（核大小、步长由 $a$ 和 $b$ 决定）的最大池化操作<br>  <em>这里的池化都是不padding、允许重叠的</em>       <ul><li>第一组（最右）为每个特征图生成一个1x1的池化图<br>  换言之，采用核大小为 $a \times b$ 、步长为 $[a, b]$ 的最大池化    </li><li>第二组（中间）为每个特征图生成一个2x2的池化图<br>  换言之，采用核大小为 $\lceil a/2 \rceil \times \lceil b/2 \rceil$、步长为 $\lfloor a/2 \rfloor, \lfloor b/2 \rfloor$ 的最大池化    </li><li>第三组（最左）为每个特征图生成一个4x4的池化图<br>  换言之，采用核大小为 $\lceil a/4 \rceil \times \lceil b/4 \rceil$、步长为 $\lfloor a/4 \rfloor, \lfloor b/4 \rfloor$ 的最大池化    </li><li>最后将三组池化图展开拼接起来，得到一个 $(1+4+16) \times N_f$ 的固定大小输出<br>  注意到这里的N是由最后一个卷积层的参数之一，也就是一个网络超参数，不受输入数据的大小影响    </li></ul></li><li>最后跟常规操作相同，将SSP的输出展平之后接FC<br>  比如论文中的fc6的大小为4096       </li></ul><p>tensorflow实现：<br>参考 <a href="http://baijiahao.baidu.com/s?id=1584420462327782297&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">深度学习中空间金字塔池化的TensorFlow实现 | 百度百家号</a>     </p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">spp_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_<span class="token punctuation">,</span> levels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">'SPP_layer'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''Multiple Level SPP layer. Works for levels=[1, 2, 3, 6].'''</span>     shape <span class="token operator">=</span> input_<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>         pool_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>         <span class="token keyword">for</span> l <span class="token keyword">in</span> levels<span class="token punctuation">:</span>             <span class="token comment" spellcheck="true"># 计算池化核大小</span>            kernel_size <span class="token operator">=</span> <span class="token punctuation">[</span>                <span class="token number">1</span><span class="token punctuation">,</span>      <span class="token comment" spellcheck="true"># batch</span>                np<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># row</span>                np<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># col</span>                <span class="token number">1</span>       <span class="token comment" spellcheck="true"># channel</span>            <span class="token punctuation">]</span>            pool_strides <span class="token operator">=</span> <span class="token punctuation">[</span>                <span class="token number">1</span><span class="token punctuation">,</span>                 np<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>                 np<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> l<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">,</span>                 <span class="token number">1</span>            <span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># 最大池化</span>            pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>input_<span class="token punctuation">,</span> ksize<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> strides<span class="token operator">=</span>pool_strides<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'SAME'</span><span class="token punctuation">)</span>             <span class="token comment" spellcheck="true"># print("Pool Level {:}: shape {:}".format(l, pool.get_shape().as_list())) </span>            h <span class="token operator">=</span> int<span class="token punctuation">(</span>pool<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             w <span class="token operator">=</span> int<span class="token punctuation">(</span>pool<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             d <span class="token operator">=</span> int<span class="token punctuation">(</span>pool<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             <span class="token comment" spellcheck="true"># 池化结果的二维池化块拉成一维</span>            pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>pool<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>pool<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> h <span class="token operator">*</span> w<span class="token punctuation">,</span> d<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>             pool_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pool<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 将几个一维池化块拼接得到最终的SPP表示</span>        spp_pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>pool_outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>         <span class="token keyword">return</span> spp_pool</code></pre><h3 id="池化不是必要操作"><a href="#池化不是必要操作" class="headerlink" title="池化不是必要操作"></a>池化不是必要操作</h3><p>比如<strong>Stride Convolutional Layer</strong>可以代替池化层降采样，得到一个只含卷积操作的网络<br>论文：<a href="https://arxiv.org/pdf/1412.6806.pdf" target="_blank" rel="noopener">Striving for Simplicity: The All Convolutional Net(2015)</a>    </p><p>甚至也有论文表示CNN对微小平移和变形的稳定性与池化层无关，<br>参考论文 <a href="https://arxiv.org/pdf/1804.04438.pdf" target="_blank" rel="noopener">Learned Deformation Stability in Convolutional Neural Networks(2018)</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>失眠者自助指南</title>
      <link href="/2018/05/04/%E5%A4%B1%E7%9C%A0%E8%80%85%E8%87%AA%E5%8A%A9%E6%8C%87%E5%8D%97/"/>
      <url>/2018/05/04/%E5%A4%B1%E7%9C%A0%E8%80%85%E8%87%AA%E5%8A%A9%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=519935307&auto=0&height=32"></iframe><br>BGM：<strong>《Just Because!》ED</strong>      </p><hr><p>啊最近又失眠，脑阔疼 (◦`~´◦)<br>从去年年中就开始隔三岔五失个眠，高三的时候也是如此，大概是个易失眠体质了。<br>有的人会安慰失眠的人说“别老胡思乱想就好”，<br>那这跟安慰抑郁症患者说“看开点就好”或者安慰感冒患者“别流鼻涕就好”没啥区别吧，<br>感冒患者也不想流鼻涕，抑郁症患者也想看开啊，我又不是自己想要胡思乱想。<br>说出来你可能不信，是它们先动的手！      </p><p>嘛~<strong><em>The only person that can save you, is you.</em></strong><br>那标题就叫“失眠者自助指南”吧哈哈哈哈哈哈      </p><p>今天睡醒昏昏沉沉的，丝毫不想学习；<br>周六闲来无趣唠唠高三、大四跟“失眠之魔”斗智斗勇的两三事儿；<br>（好像是我第一次在博客上发无关技术、无关笔记的东西？）     </p><p>最常见的办法就是<strong>喝牛奶</strong>，牛奶据说确实含有镇静成份，比如色氨酸。<br>但也有说法是，大脑与血液之间存在“血脑屏障”，大脑所能从牛奶吸收的色氨酸是极其有限的。<br>不过咧，皮肤温度上升确实可以有效地助眠，所以喝杯<strong>热牛奶</strong>多多少少对睡眠还是有帮助的。<br>由此看来，其实<strong>洗热水澡</strong>或者<strong>泡热水脚</strong>的助眠效果应该会更好。<br>可参考 <a href="https://www.zhihu.com/question/23648474/answer/25306001" target="_blank" rel="noopener">牛奶真的助眠么？ | 知乎</a><br>从我个人来看，喝热牛奶几乎对我的睡眠毫无帮助，甚至可能会出现反作用——尤其是在我肠胃不好的时候，难以消化牛奶，对肠胃造成负担。但空腹确实会影响睡眠，睡前<strong>喝热粥</strong>效果会更好，一方面防止空腹，另一方面吃点热食有助睡眠且粥对肠胃的负担要小得多；<br>睡前洗澡需要等头发干才能入睡（<a href="https://zhidao.baidu.com/question/5924661.html" target="_blank" rel="noopener">晚上头发不干就睡觉有什么坏处 | 百度知道</a>），等你头发干了皮肤温度也降得差不多啦，助眠效果甚微；<br>泡热水脚还不错，每次烫完脚上床睡觉挺舒服的，不过偶尔泡太长好像反而不利于睡眠？ummm经验之谈，不知道是不是个例。<br>也可以试试换个枕头，抬高颈部，比如买个护颈枕或者糖果枕——睡眠过程中<strong>头部后仰</strong>（类似做人工呼吸时候患者的状态），一方面可以预防颈椎病，另一方面有助于打开气道，使呼吸顺畅（据说还能防止打鼾）。可能起初会觉得这种枕头不舒服，但是习惯之后会发现睡眠质量有所提升（如果确实习惯不了，那还不如换回你原来的枕头算了）    </p><p>还有一点从《自控力》一书习得的小技巧，这是一种<strong>冥想</strong>技巧，原本是用来训练大脑，扩大“意志力储备”的，不过我发现这有助于把思维引导到“无意义”的方向上，避免胡思乱想。原文如下：    </p><blockquote><p>专心呼吸是一种简单有效的冥想技巧，它不但能训练大脑，还能增强意志力。它能减轻你的压力，指导大脑处理内在的干扰（比如冲动、担忧、欲望）和外在的诱惑（比如声音、画面、气味）。新研究表明，定期的思维训练能帮人戒烟、减肥、戒毒、保持清醒。无论你“要做”和“不要”的是什么，这种5分钟冥想都有助于你增强意志力。<br>让我们开始吧。<br>1．原地不动，安静坐好。<br>坐在椅子上，双脚平放在地上，或盘腿坐在垫子上。背挺直，双手放在膝盖上。冥想时一定不能烦躁，这是自控力的基本保证。如果你想挠痒的话，可以调整一下胳膊的位置，腿交叉或伸直，看自己是否有冲动但能克制。简单的静坐对于意志力的冥想训练至关重要。你将学会，不再屈服于大脑和身体产生的冲动。<br>2．注意你的呼吸。<br>闭上眼睛。要是怕睡着，你可以盯着某处看，比如盯着一面白墙，但不要看家庭购物频道。注意你的呼吸。吸气时在脑海中默念“吸”，呼气时在脑海中默念“呼”。当你发现自己有点走神的时候，重新将注意力集中到呼吸上。这种反复的注意力训练，能让前额皮质开启高速模式，让大脑中处理压力和冲动的区域更加稳定。<br>3．感受呼吸，弄清自己是怎么走神的。<br>几分钟后，你就可以不再默念“呼”、“吸”了。试着专注于呼吸本身。你会注意到空气从鼻子和嘴巴进入和呼出的感觉，感觉到吸气时胸腹部的扩张和呼气时胸腹部的收缩。不再默念“呼”、“吸”后，你可能更容易走神。像之前一样，当你发现自己在想别的事情时，重新将注意力集中到呼吸上。如果你觉得很难重新集中注意力，就在心里多默念几遍“呼”和“吸”。这部分的训练能锻炼你的自我意识和自控能力。<br>刚开始的时候，你每天锻炼5分钟就行。习惯成自然之后，请试着每天做10 15分钟。如果你觉得有负担，那就减少到5分钟。每天做比较短的训练，也比把比较长的训练拖到明天好。这样，你每天都会有一段固定的时间冥想，比如早晨洗澡之前。如果你做不到，可以对时间进行适当的调整。</p></blockquote><p>不需要非坐正凝视冥想，稍作变通，平躺在床上，闭上眼睛，<strong>专注呼吸</strong>即可。<br>这是我高三时候用过的最有效的助眠方法之一，一开始就像《自控力》所说，很快思维就会跑偏，但经过几次的“训练”，专注呼吸的持续时间会逐渐增加，到最后可以避免自己胡思乱想，从而达到助眠的目的。     </p><p>与专注呼吸的冥想法类似的，之前看过某本暗示心理学的书之后自己提炼出另外一个<strong>自我暗示</strong>或者说是自我催眠、自我欺诈的助眠方法。<br>首先你需要有一个点状光源，不能太刺眼，暖色调的柔光灯泡为佳。平躺在床上，放松一下身心（可以结合专注呼吸的方法），然后眼睛盯着准备好的点光源，不断暗示自己“越来越困”，并且眼睛缓缓闭上（很慢很慢的合上，这像是一种欺诈，为之前的暗示佐证自己确实“很困”）。<br>讲真，这种方法挺好用的，但有一个条件限制——需要你开着灯，开一晚上灯（如果你的灯能定时关闭那就更棒了）。平时都住在寝室的我哪有机会实现这样的条件哇，怕是要被室友打死。         </p><p>也是从心理学书上学到的，还有另外一个相对治本的办法——<strong>运动</strong>。       </p><ul><li>睡前运动算是一种<strong>疲劳助眠</strong>，“疲劳”有种和“困”类似的感觉，而且如前所述，运动提升皮肤温度也有助于睡眠，不过不宜做剧烈运动，这个……可能会做噩梦；<br>  之前试过睡前做俯卧撑，emmm小有效果吧    </li><li>日常的运动有助于<strong>减压</strong>，某本书说，散步是最好的减压运动，其次是慢跑<br>  很感谢yang高三的时候每天傍晚吃完饭都拉我到操场上散半小时步（哈，好怀念高中的日子）      </li></ul><p>也有一些简单粗暴的助眠方法，比如<strong>酒精助眠</strong>了解一下~<br><!--*然而，这些方法到大四的时候似乎都不大管用*       于是开始尝试点简单粗暴的助眠方法，**酒精助眠**了解一下~      曹孟德说“何以解忧，唯有杜康”，杜康是怎么解忧的我不知道，或许是文化影响产生的安慰效应，或许是通过麻痹大脑。    酒精助眠也不是说就把自己喝醉，只是喝到微醺，半昏半沉，头脑一定程度的麻痹，以这种状态入睡。       我喝的是啤酒，不同啤酒有不同的度数，像雪花青岛只有2.x度、百威3.x度、蓝带4.x接近5度、前几天喝的奥丁格甚至有8.9度的特度啤酒。酒精摄入少了，就达不到助眠效果；但酒精摄入过多，入睡是舒服了，第二天起床怕是要头疼。另外啤酒利尿，喝完啤酒要留点缓冲时间，排完尿再上床，防止半夜憋尿憋醒（酒醒了重新入睡未必有那么顺利）。每个人的酒量是不同的，需要自己一点点尝试，比如我自己的话，喝一听300ml的蓝带就恰到好处。-      还好大四失眠不像高三频繁，只是间歇性的，每次开始出现失眠，连着三五天睡前喝个小酒，然后美滋滋的入睡；    当身体习惯了这种“按时入睡”的感觉，失眠症状就会暂时解除。-->      </p><p>前阵子在知乎上看到一种很专业的入睡技巧，比较复杂，我还没试过——<br><a href="https://zhuanlan.zhihu.com/p/34952593" target="_blank" rel="noopener">如何在 2 分钟内入睡（二战时期美国飞行员训练法）| 知乎</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>目标函数</title>
      <link href="/2018/05/03/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/"/>
      <url>/2018/05/03/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=529814551&auto=0&height=32"></iframe><br>BGM：<strong>《刻刻》ED</strong><br>小哥哥唱歌听起来像是有点绕舌？      </p><hr><p>参考：《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》Chap9      </p><p>目标函数（target function）、损失函数（loss function）、代价函数（cost function）是一个东西~     </p><h3 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h3><p>记某分类任务共 $N$ 个训练样本，<br>网络最后的分类层第i个样本的输入特征为 $x_i$，<br>真实标记为 $y_i \in {1,2,…,C}$ （C为类别总数），<br>网络最终输出预测结果（指示每种分类的可能性） $h = (h_1, h_2, …, h_C)^T$       </p><h4 id="交叉熵（cross-entropy）"><a href="#交叉熵（cross-entropy）" class="headerlink" title="交叉熵（cross entropy）"></a>交叉熵（cross entropy）</h4><p>又称Sotfmax损失函数，目前分类任务最常用的损失函数；<br>用指数变换的形式，将网络输出转换成概率——<br>$$ L_{cross-entropy-loss} = L_{softmax-loss} = -\frac{1}{N} \sum_{i=1}^N log( \frac{e^{h_{y_i}}}{\sum_{j=1}^C e^{h_j}} ) $$     </p><h4 id="合页（hinge）"><a href="#合页（hinge）" class="headerlink" title="合页（hinge）"></a>合页（hinge）</h4><p>主要在SVM中广泛使用，有时也用于神经网络模型；<br>设计理念为“对错误越大的样本施加越严重的惩罚”；<br>一般在分类任务中，交叉熵效果要略优于合页<br>$$ L_{hinge-loss} = \frac{1}{N} \sum_{i=1}^N max(0, 1-h_{y_i}) $$     </p><h4 id="坡道（ramp）"><a href="#坡道（ramp）" class="headerlink" title="坡道（ramp）"></a>坡道（ramp）</h4><p>论文：<a href="http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=39D2D095B2A9E803DB744CE374E869EE?doi=10.1.1.127.2310&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Trading convexity for scalability(2006)</a><br>合页损失函数对噪声的抵抗能力较差，非凸损失函数的引入可以很好的解决这个问题；<br>而坡道损失函数、Tukey’s biweight损失函数分别是分类任务、回归任务中非凸损失函数的代表，又称“鲁棒损失函数”；<br>它们在误差较大的区域进行截断，使得较大的误差不会大程度地影响整个误差函数；<br>但其非凸性质在传统机器学习中难以优化，但得益于神经网络模型的训练机制，这点非凸性质不成问题：<br>$$ L_{ramp-loss} = L_{hinge-loss} - \frac{1}{N} \sum_{i=1}^N max(0, s-h_{y_i}) $$<br>展开为：<br>$$ L_{ramp-loss} = \frac{1}{N} \sum_{i=1}^N ( max(0, 1-h_{y_i}) - max(0, s-h_{y_i}) ) $$<br>其中，s指定了截断点的位置，如下图所示（s=-0.5）——<br><img src="/Handson-ML/ramp_loss.png" alt="ramp_loss">       </p><p>此外，论文 <a href="http://www.unc.edu/~yfliu/papers/rsvm.pdf" target="_blank" rel="noopener">Robust Truncated Hinge Loss Support Vector Machines(2007)</a> 经过理论推导指出，一般设置 $s = - \frac{1}{C-1}$     </p><h4 id="交叉熵的变体"><a href="#交叉熵的变体" class="headerlink" title="交叉熵的变体"></a>交叉熵的变体</h4><p>基于交叉熵设计的新型损失函数，考虑了增加类间距离、减少类内差异等不同要素，提升网络学习特征的判别能力；       </p><p><strong>大间隔交叉熵</strong>（large-margin softmax）：<br>论文：<a href="https://arxiv.org/pdf/1612.02295.pdf" target="_blank" rel="noopener">Large-Margin Softmax Loss for Convolutional Neural Networks(2017)</a>         </p><p>首先考虑传统的交叉熵损失函数，根据 $h=W^T x_i$ 将其展开为——<br>$$ L_{softmax-loss} = -\frac{1}{N} \sum_{i=1}^N log( \frac{ e^{ W^T_{y_i} x_i } }{ \sum_{j=1}^C e^{ W^T_j x_i } } ) $$<br>根据内积定义进一步展开为——<br>$$ L_{softmax-loss} = -\frac{1}{N} \sum_{i=1}^N log( \frac{ e^{ ||W_{y_i}|| ||x_i|| cos(\theta_{y_i}) } }{ \sum_{j=1}^C e^{ ||W_j|| ||x_i|| cos(\theta_j) } } ) $$<br>比如二分类，传统交叉熵使得学到的参数满足 $W_1^T x_i &gt; W_2^T x_i$，也即<br>$$ ||W_1|| ||x_i|| cos(\theta_1) &gt; ||W_2|| ||x_i|| cos(\theta_2) $$<br>大间隔交叉熵则在上式的基础上，引入超参数m拉开两个分类的差距，即——<br>$$ ||W_1|| ||x_i|| cos(m \theta_1) &gt; ||W_2|| ||x_i|| cos(\theta_2), 0&lt;= \theta_1 &lt;= \frac{\pi}{m} 且 m \in N^+ $$<br>m起到控制间隔大小的作用，m越大，类间间隔（即差距）越大，类间分类的置信度越大；当 $m=1$ 时将退化为传统交叉熵       </p><p>完整定义：<br>$$ L_{large-margin-softmax-loss} = -\frac{1}{N} log( \frac{ e^{ ||W_{y_i}|| ||x_i|| \phi(\theta_{y_i}) } }{ e^{ ||W_{y_i}|| ||x_i|| \phi(\theta_{y_i}) + \sum_{j \ne y_i} e^{ ||W_j|| ||x_i|| cos(\theta_j) } } } ) $$<br>其中，<br>$$<br>\begin{equation}<br>\phi(\theta) = \left\{<br>\begin{aligned}<br>&amp; cos(m \theta),  &amp; 0 &lt;= \theta &lt;= \frac{\pi}{m}\\<br>&amp; D(\theta),      &amp; \frac{\pi}{m} &lt; \theta &lt; \pi<br>\end{aligned}<br>\right.<br>\end{equation}<br>$$<br>式中 $D(\theta)$ 只需满足“<strong>单调递减</strong>”，且 $D(\frac{\pi}{m}) = cos(\frac{\pi}{m})$ 即可；<br>为了简化网络前向和反向计算，论文推荐了一种具体的形式如下——<br>$$ \phi(\theta) = (-1)^k cos(m \theta) - 2k, \theta \in [\frac{k \pi}{m}, \frac{(k+1)\pi}{m}] $$<br>其中，k为整数且满足 $k \in [0, m-1]$      </p><p>二分类情况下的比较——<br><img src="/Handson-ML/large-margin_softmax_loss.png" alt="large-margin-softmax-loss">     </p><p>此时训练目标要比传统交叉熵损失函数更加困难；<br>不过好处是可以起到防止模型过拟合的作用；<br>在分类性能方面，大间隔交叉熵要优于传统交叉熵和合页；         </p><p><strong>中心损失函数</strong>（center loss function）：<br>论文：<a href="https://www.dl.icdst.org/pdfs/files1/c8edf1770d6ac7f5415f8daef7fb9bce.pdf" target="_blank" rel="noopener">A Discriminative Feature Learning Approach for Deep Face Recognition(2016)</a>       </p><p>中心损失函数定义：<br>$$ L_{center-loss} = \frac{1}{2} \sum_{i=1}^N ||x_i c_{y_i} ||^2_2 $$<br>其中， $c_{y_i}$ 为第 $y_i$ 类所有深度特征的均值（中心）；<br>这将迫使样本与中心不要距离太远，否则加大惩罚；      </p><p>中心损失函数只考虑了类内差异，所以通常要与考虑类间举例的损失函数（如交叉熵）配合使用，变化为——<br>$$ L_{final} = L_{cross-entropy-loss} + \lambda L_{center-loss}(h, y_i) $$<br>展开为——<br>$$ L_{final} = -\frac{1}{N} \sum_{i=1}^N log ( \frac{e^{h_{y_i}}}{\sum_{j=1}^C e^{h_j}} ) + \frac{\lambda}{2} \sum_{i=1}^N ||x_i - c_{y_i} ||^2_2 $$<br>式中，$\lambda$ 为两个损失函数之间的权衡因子，$\lambda$越大类内差异占整个目标函数的比重越大；       </p><p>中心损失函数搭配传统交叉熵函数在分类性能上优于单独的传统交叉熵；<br>尤其在人脸识别任务上有较大的提升      </p><h3 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h3><p>回归任务通常用残差衡量预测值和真实值的靠近程度；<br>记回归问题第i个输入特征 $x_i$，<br>真实标记为 $y^i = (y_1, y_2, …, y_M)^T$，M为标记向量的总维度；<br>预测值为 $\hat{y}^i$，<br>$l^i_t = y^i_t - \hat{y}^i_t$ 表示样本i上预测值与真实值在第t维上的预测误差；        </p><p>l1、l2、Tukey’s biweight损失函数如下图所示——<br><img src="/Handson-ML/regression.png" alt="regression"></p><h4 id="l1损失函数"><a href="#l1损失函数" class="headerlink" title="l1损失函数"></a>l1损失函数</h4><p>$$ L_{l_1-loss} = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^M |l^i_T| $$         </p><h4 id="l2损失函数"><a href="#l2损失函数" class="headerlink" title="l2损失函数"></a>l2损失函数</h4><p>$$ L_{l_2-loss} = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^M (l^i_T)^2 $$<br>一般l1和l2在回归精度上所差无几，但一些情况下l2会略优于l1；<br>l2收敛略快于l1；         </p><h4 id="Tukey’s-biweight损失函数"><a href="#Tukey’s-biweight损失函数" class="headerlink" title="Tukey’s biweight损失函数"></a>Tukey’s biweight损失函数</h4><p>论文：<a href="https://arxiv.org/pdf/1505.06606.pdf" target="_blank" rel="noopener">Robust Optimization for Deep Regression(2015)</a>       </p><p>非凸损失函数；<br>定义：<br>$$<br>\begin{equation}<br>L_{Tukey’s-biweights-loss} = \left\{<br>\begin{aligned}<br>&amp; \frac{c^2}{6N} \sum_{i=1}^N \sum_{t=1}^M [1-(1-(\frac{l^i_t}{c})^2)^3] ,  &amp; |l^i_t| &lt;= c\\<br>&amp; \frac{c^2 M}{6},                                                          &amp; |l^i_t| &gt; c<br>\end{aligned}<br>\right.<br>\end{equation}<br>$$<br>其中，常数c指定了函数拐点，通常取 $c=4.6851$，此时该损失函数可以与l2在最小化符合标准正态分布的残差类似的回归效果      </p><h3 id="其他任务：KL散度"><a href="#其他任务：KL散度" class="headerlink" title="其他任务：KL散度"></a>其他任务：KL散度</h3><p>实际问题往往不能简单划为回归任务或者分类任务。<br>如年龄估计中，经常会表达“看起来30岁左右”；<br>此时通常采用一个“标记的分布”来描述，如均值为30的一个正态分布；<br>此外，在头部倾斜角度估计、多标记分类、图像语义分割等问题上也存在类似的问题；     </p><p>通常先将h转化为一个合法的分布（比如用softmax函数）；<br>用KL散度（Kullback-Leibler divergence）来衡量真实标记和预测分布的误差，此时也称为KL损失：<br>$$ L_{KL-loss} = \sum_k y_k log \frac{y_k}{\hat{y}_k} $$<br>因为$y_k$是已知的常量（真实标记），上式等价为：<br>$$ L_{KL-loss} = - \sum_k y_k log \hat{y}_k $$</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>经典的CNN分类架构</title>
      <link href="/2018/05/02/%E7%BB%8F%E5%85%B8%E7%9A%84CNN%E5%88%86%E7%B1%BB%E6%9E%B6%E6%9E%84/"/>
      <url>/2018/05/02/%E7%BB%8F%E5%85%B8%E7%9A%84CNN%E5%88%86%E7%B1%BB%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=600349&auto=0&height=32"></iframe><p>BGM：<strong>《GOSICK》ED2</strong>     </p><hr><p>从原文《<a href="/2018/04/18/CNN/">卷积神经网络CNN | Hey~YaHei!</a>》抽取出来；     </p><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》       </li></ol><p>CNN的典型组合方式是，以 <strong>卷积层+激活函数（一般是relu）+池化层</strong> 作为一组操作；<br>一张图片经过若干组这样的操作的stack之后，变得越来越小，同时越来越深（feature map越来越多）；<br>在stack的顶层，经过一个常规的前馈神经网络（由若干个带激活的全连接层组成）后产生预测结果（通常经过一个softmax层）——<br><img src="/Handson-ML/12Typical_CNN_Architectures.png" alt="Typical_CNN_Architectures">    </p><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>经典的CNN框架，1998年由Yann LeCun提出，被广泛地应用于手写体数字识别（MNIST）；      </p><p>详细配置：<br><img src="/Handson-ML/12LeNet-5.png" alt="LeNet-5">       </p><ul><li>MNIST数据为 $28 \times 28$ ，LeNet-5将其填零补充到 $32 \times 32$ 并在投喂给网络前进行归一化；<br>  而其他层不再进行padding，所以可以看到每经过一层，图片就发生一次萎缩       </li><li>平均池化层除了对输入进行平均之外，还经过一次仿射变换（缩放因子、偏置项都作为可学习的参数），并且在输出前经过一次激活       </li><li>C3层的神经元只与S2层的3到4个输出连接      </li><li>Out层不是简单的将输入和权重进行点乘，而是计算输入和权重的欧式距离的平方<br>  <em>而现在更好的方式是使用交叉熵</em>        </li></ul><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Imagenet Classification with Deep Convolutional Neural Networks(2012)</a>        </p><p>赢得2012年的ImageNet ILSVRC比赛；<br>与LeNet-5很像，只是更大更深（约60万个参数）；<br>他的突出成就是首次<strong>连续叠加卷积层</strong>而不是在每个卷积层之间插入池化层         </p><p>详细配置：<br><img src="/Handson-ML/12AlexNet.png" alt="AlexNet"></p><ul><li>加入了一些正则化技术      <ul><li>为F8、F9加入dropout（dropout rate为50%）       </li><li>数据增强（随机平移图片、水平翻转、改变光照条件）        </li></ul></li><li>在C1、C3层的激活函数之后紧跟一个局部响应归一化操作（Local Response Normalization, LRN）<br>  使得强烈激活的神经元在相邻特征图的相同位置上的神经元被抑制（仿生）；<br>  这促进不同的特征图得到差异化，能够关注到不同的特征。       <ul><li>LRN的具体操作如下：<br>  $$ b_i = a_i (k + \alpha \sum_{j = max(0, i - r/2)}^{min(i+r/2, f_n -1)} a_j^2)^\beta$$<br>  其中，<br>  $a_i, b_i$ 分别是LRN的第i个特征图输入、正则化输出；<br>  $k, \alpha, \beta, r$ 是超参量，$k$ 称为偏置，$r$ 称为深度半径；<br>  $f_n$ 是特征图的数量；<br>  比如，当 $r=2$ 时，强烈激活的神经元会抑制他的上一个和下一个特征图中相同位置的神经元；        </li><li>AlexNet采用的配置为 $r=2, \alpha = 0.00002, \beta = 0.75, k = 1$；<br>  tensorflow提供了相应的 <code>local_response_normalization()</code> 操作；        </li></ul></li></ul><p>变种：<strong>ZFNet</strong><br>论文：<a href="http://www.uvm.edu/~cdanfort/csc-reading-group/zeiler-eccv-2014.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks(2014)</a><br>2013年ILSVRC比赛冠军        </p><h3 id="VGG-Nets"><a href="#VGG-Nets" class="headerlink" title="VGG-Nets"></a>VGG-Nets</h3><p>论文：<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION(2015)</a><br>2014年ILSVRC亚军    </p><p>详细配置：<br><img src="/Handson-ML/12VGGNet1.png" alt="12VGGNet1"><br><img src="/Handson-ML/12VGGNet2.png" alt="12VGGNet2">      </p><p>特点（相比AlexNet）：</p><ol><li>普遍使用小卷积核、以及一些保持输入大小的技巧<br> 增加网络深度时确保各层输入大小随深度增加而不减小     </li><li>网络卷积的通道数逐层增加（$3 \to 64 \to 128 \to 256 \to 512$）    </li></ol><h3 id="NIN（Network-in-Netwrok）"><a href="#NIN（Network-in-Netwrok）" class="headerlink" title="NIN（Network in Netwrok）"></a>NIN（Network in Netwrok）</h3><p>论文：<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network In Network(2014)</a>     </p><p>特点：    </p><ol><li>用多层感知机（多层FC和非线性函数的组合）替代线性卷积层<br> 线性卷积层复杂度有限，层间映射只能将前层特征或输入简单地线性组合成后层特征；<br> 高复杂度的多层感知机增加了卷积层的非线性能力，使得前层特征能有更多的复杂性和可能性映射到后层特征；<br> 该思想随后也被Inception和ResNet借鉴     </li><li>使用全局平均池化（Global Average Pooling）操作替代全连接层作为“分类器”<br> 全局池化分别作用于每张特征图，最后映射到样本的真实标记；<br> 此时每张特征图上的响应将很自然的对应到不同的样本类别，使得NIN更具可解释性；<br> 该技术随后也被Inception借鉴     </li></ol><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>论文：<a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" rel="noopener">Going Deeper with Convolutions(2015)</a><br>2014年ILSVRC冠军；<br>利用子网络inception模块，网络更加高效（大约只有AlexNet的十分之一），使得网络可以更深；       </p><p>Inception模块：<br><img src="/Handson-ML/12Inception_Module.png" alt="Inception_Module"><br>三层结构。其中，$3 \times 3 + 1(S)$ 表示核大小为3x3，步长为1，使用SAME填充方式；        </p><ul><li>每个卷积层都使用relu激活函数；</li><li>第二层分别使用了1x1，3x3，5x5的卷积核，这是为了模块在多尺度上捕捉特征      </li><li>各个卷积层、池化层步长均为1，也就是说他们的输出大小与输入大小相同，这使得他们的输出可以被直接由Depth Concat层连接        </li><li>1x1卷积层的作用：       <ul><li>降采样，1x1卷积层也可以称为瓶颈层（bottleneck layer），在3x3、5x5卷积层之前降采样，可以有效地减小计算开销      </li><li>与3x3、5x5卷积层成对存在，使得图片被一个双层网络扫描而不是一个简单的线性分类器（<em>啊？这在说啥</em>）        <blockquote><p>Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a single,<br>powerful convolutional layer, capable of capturing more complex patterns. Indeed, instead of<br>sweeping a simple linear classifier across the image (as a single convolutional layer does), this<br>pair of convolutional layers sweeps a two-layer neural network across the image</p></blockquote></li></ul></li></ul><p>GoogLeNet框架：<br>包括9个Inception模块，每个卷积层都使用relu激活函数：<br><img src="/Handson-ML/12GoogLeNet.png" alt="GoogLeNet">       </p><ul><li>头两层（卷积层、池化层）步长均为2，相当于图片的长、宽都除以4，整体压缩为原来的十六分之一，以减少计算量        </li><li>LRN保证了前两层捕捉到多尺度的特征       </li><li>1x1和3x3的卷积层对，构成一个比简单的线性分类器表现更好的卷积层      </li><li>又一个LRN保证前层捕捉到多尺度的特征      </li><li>步长为2的池化层进一步把图片压缩为原来的四分之一，减少计算量       </li><li>接下来是连续的九个Inception模块，中间插入了两个最大池化层以进一步减少计算量（每一个池化层将图片压缩为四分之一）</li><li>平均池化层使用了不填充的步长为1的池化核，这种策略称为<strong>全局平均池化（Global Average Pooling）</strong><br>  这有效地令前层产生一个特征图，这个特征图实际上是每个预测分类的置信图（<em>因为其他特征会在平均过程中被摧毁？</em>）；<br>  这样一来，模型在输出前就不再需要多个全连接层，相当于减少了参数的数量并且减少过拟合       </li><li>最后dropout层起正则化的作用，全连接层和softmax层产生预测结果       </li></ul><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>论文：<a href="https://arxiv.org/pdf/1512.03385v1.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition(2015)</a>        </p><p>其他：    </p><ol><li><a href="https://mp.weixin.qq.com/s/7fWh2dovmfbsF8afaX9UOg" target="_blank" rel="noopener">机器之心 - 一文简述ResNet及其多种变体(2018)</a> 【<a href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035" target="_blank" rel="noopener">原文</a>】      </li><li><a href="https://arxiv.org/pdf/1507.06228.pdf" target="_blank" rel="noopener">Training Very Deep Networks(2015)</a><br> 受LSTM中门机制的启发，提出高速公路网络，对前馈神经网络进行修正，使信息能够在多个神经网络层之间高效流动；这一思想也影响了ResNet网络    </li></ol><p>2015年ILSVRC冠军；<br>网络更深（152层），如此深的网络能够被训练主要得益于<strong>Skip Connections（或Shortcut Connections）</strong>技术；      </p><p><strong>残差学习（Residual Learning）</strong>：<br>在常规网络的基础上，网络输出前增加一个与输入加和的操作；<br>这时网络不再直接训练目标函数 $h(x)$，而是训练 $f(x) = h(x) - x$；<br><img src="/Handson-ML/12Residual_Learning.png" alt="Residual_Learning">          </p><p>对于常规网络，权重被初始化为一个接近0的数，网络一开始只能输出一个非常接近于0的结果；<br>但是残差网络输出的却是一个接近输入的结果，而往往我们所要训练的目标函数跟这个输入还是比较接近的，这将大大加速我们的训练过程；       </p><p>而且，如果加入了很多skip connections，网络可以在很早的时候就开始取得进步（有效地学习），即使中间有部分层还没开始进步也不影响，因为信号的传递很容易穿过整个网络；<br>深层残差网络可以看作是很多残差单元（Residual Unit, RU）的堆叠，每个残差单元是一个包含一个skip connection的小网络——<br><img src="/Handson-ML/12Deep_Residual_Network.png" alt="Deep_Residual_Network">        </p><p>ResNet框架：<br>他的最底层和最顶层跟GoogLeNet差不多，<code>ResNet-xxx</code>中数字<code>xxx</code>代表模型中卷积层的数量；<br><img src="/Handson-ML/12ResNet.png" alt="ResNet">        </p><ul><li>头两层与GoogLeNet完全相同         </li><li>但ResNet没有LRN也没有1x1和3x3的卷积层对       </li><li>接下来是一个深层的残差网络，由许多残差单元组成（每个残差单元包含两个卷积层以及BN层、relu激活函数、skip connection，步长为1、补零、保持大小）<br>  每经过一定层数，图片会经过一个步长为2的卷积层而被压缩，然后接下来的残差单元的大小都增加一倍<br>  此时这个残差单元的输入不能直接加到输出上（大小不一致），而应该在加和前让输入经过一个1x1步长2的卷积层——<br>  <img src="/Handson-ML/12Double_Units.png" alt="Double_Units">       </li><li>最后两层也与GoogLeNet完全相同，不过这里不需要dropout       </li></ul><p>各种不同深度的ResNet：<br>（只计算卷积层和全连接层，除去底层的卷积层和顶层的全连接层，不考虑每次倍增的时候多出来的1x1卷积层）       </p><ul><li>ResNet-34     <ul><li>3个64输出的RUs</li><li>4个128输出的RUs</li><li>6个256输出的RUs</li><li>3个512输出的RUs</li></ul></li><li>ResNet-152<br>  RU结构发生变化，使用三层卷积（1x1的64输出卷积层作为瓶颈层，然后一个3x3的64输出卷积层，再紧接一个1x1的 $4 \times 64=256$ 输出卷积层来恢复深度）     <ul><li>3个256输出的新RUs</li><li>8个512输出的新RUs</li><li>36个1024输出的新RUs</li><li>3个2048输出的新RUs   </li></ul></li></ul><h3 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h3><p>网络越来越深，参数越来越少；<br>目前而言，ResNet兼具简单、表现出色两个特点；<br><em>2016年和2017年中国拿了第一，但是好像没有提出什么有趣的东西，是刷榜吗？</em>      </p><p>其他值得研究的框架——                </p><ul><li><a href="https://arxiv.org/pdf/1602.07261v2.pdf" target="_blank" rel="noopener">Inception-v4(2016)</a>：吸收兼并GoogLeNet和ResNet的思想，并提出网络inception-resnet-v1和inception-resnet-v2       </li><li><a href="https://arxiv.org/pdf/1707.07012.pdf" target="_blank" rel="noopener">NASNet(2018)</a>：2017谷歌提的框架，准确率很诱人，但据说规模大的很，吐槽的人倒也不少      </li><li><a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNets(2017)</a>：目前移动端应用地最多的、速度很快同时表现客观的一个网络，具体介绍可以参见《<a href="/2018/08/05/MobileNets_v1/">MobileNets v1模型解析 | Hey~YaHei!</a>》</li></ul><p>最后附一个tensorflow内置的<a href="https://github.com/tensorflow/models/tree/master/research/slim" target="_blank" rel="noopener">常见框架模块及其预训练模型</a>       </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>自编码器</title>
      <link href="/2018/04/25/autoencoder/"/>
      <url>/2018/04/25/autoencoder/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=489970553&auto=0&height=32"></iframe><br>BGM：<strong>《末日时在做什么？有没有空？可以来拯救吗？》第九话插入曲</strong><br>这番名字真是狗血，但剧情和音乐出奇的不错       </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap15<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><p>自编码器工作方式非常简单，就是学习如何模仿输入来产生输出；<br>我们会采取各种约束（比如限制输出大小、加噪等）来避免自编码器纯粹地把输入作为输出，从而得到一个高效的数据表示方式；<br>简而言之，自编码器通过尝试学习某些约束下的特征函数来产生输入的编码，也即一种高效的数据表示；<br>自编码器可以无监督学习输入数据的编码方式、降低数据维度、作为特征检测器、作为生成模型等等……      </p><h3 id="数据的高效表示"><a href="#数据的高效表示" class="headerlink" title="数据的高效表示"></a>数据的高效表示</h3><p>论文 <a href="https://www.sciencedirect.com/science/article/pii/0010028573900042" target="_blank" rel="noopener">Perception in chess(1973)</a> 研究了记忆、概念、模式匹配之间的联系；     </p><p>自编码器可以分为Encoder和Decoder两部分，<br>Encoder也称识别网络，用于将输入转换为某种内部表示；<br>Decoder也称生成网络，用于将内部表示转换成输出；<br>架构跟MLP一样，不过他的<strong>输出神经元数与输入数相等</strong>，<strong>中间层的神经元数小于输入数</strong>；<br>也就是说，中间层的输出必定是输入的一个不完全表示，我们的目的就在于训练出一个输出与输入相近的网络——<br><strong>可以理解为Encoder是对输入的一个有损压缩，Decoder对其进行解压，我们要训练一个损耗率尽可能小的网络</strong><br><img src="/Handson-ML/15Simple_Autoencoder.png" alt="15Simple_Autoencoder">       </p><h3 id="简单的线性自编码器"><a href="#简单的线性自编码器" class="headerlink" title="简单的线性自编码器"></a>简单的线性自编码器</h3><p>无非线性激活，MSE损失函数，可以实现一个PCA；       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> fully_connected<span class="token comment" spellcheck="true"># 三维输入编码为二维表示</span>n_inputs <span class="token operator">=</span> <span class="token number">3</span>n_hidden <span class="token operator">=</span> <span class="token number">2</span>n_outputs <span class="token operator">=</span> n_inputslearning_rate <span class="token operator">=</span> <span class="token number">0.01</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>hidden <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 纯线性，无激活</span>outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># MSE</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>reconstruction_loss<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>X_train<span class="token punctuation">,</span> X_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>     <span class="token comment" spellcheck="true"># 载入数据集</span>n_iterations <span class="token operator">=</span> <span class="token number">1000</span>codings <span class="token operator">=</span> hidden     <span class="token comment" spellcheck="true"># 自编码器的目的是获取编码，也即中间层的输出</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        training_op<span class="token punctuation">.</span>run<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_train<span class="token punctuation">}</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 无监督</span>    codings_val <span class="token operator">=</span> codings<span class="token punctuation">.</span>eval<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_test<span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><h3 id="深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）"><a href="#深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）" class="headerlink" title="深层自编码器（Deep autoencoders, or Stacked autoencoders）"></a>深层自编码器（Deep autoencoders, or Stacked autoencoders）</h3><p>深层自编码器为中心对称的“三明治”结构，最中间的一层产生实际的编码，称编码层（Coding Layer），如——<br><img src="/Handson-ML/15Stacked_Autoencoder.png" alt="15Stacked_Autoencoder">       </p><p>深度学习的训练技术（如<a href="/2018/04/08/梯度消失与梯度爆炸">解决梯度爆炸与梯度消失的技术</a>、<a href="/2018/04/09/复用预训练层">复用预训练层</a>、<a href="/2018/04/10/优化器">优化器</a>、<a href="/2018/04/11/正则化技术">正则化技术</a>等）依旧适用；<br>与之前讲述的网络的区别在于，自编码器没有标注，是无监督学习     </p><h3 id="权重共享"><a href="#权重共享" class="headerlink" title="权重共享"></a>权重共享</h3><p>由于结构是中心对称的，Encoder和Decoder可以直接共享权重，但tensorflow中的 <code>fully_connected()</code> 没法共享权重，所以需要手动书写全连接层——    </p><pre class=" language-python"><code class="language-python">activation <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>eluregularizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>l2_regularizer<span class="token punctuation">(</span>l2_reg<span class="token punctuation">)</span>initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>weights1_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_inputs<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">]</span><span class="token punctuation">)</span>weights2_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">]</span><span class="token punctuation">)</span>weights1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights1_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights1"</span><span class="token punctuation">)</span>weights2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights2_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights2"</span><span class="token punctuation">)</span>weights3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights2<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights3"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>weights4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights1<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights4"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>biases1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden1<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases1"</span><span class="token punctuation">)</span>biases2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden2<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases2"</span><span class="token punctuation">)</span>biases3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden3<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases3"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>biases4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases4"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>hidden1 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> weights1<span class="token punctuation">)</span> <span class="token operator">+</span> biases1<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>hidden3 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> weights3<span class="token punctuation">)</span> <span class="token operator">+</span> biases3<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden3<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 只对weights1和weights2施加正则化</span>loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> reg_lossoptimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="逐层训练"><a href="#逐层训练" class="headerlink" title="逐层训练"></a>逐层训练</h3><p>逐层训练然后将各层堆叠起来，要比直接训练一个堆叠好的自编码器要快得多；<br>比如训练一个三层的自编码器：     </p><ol><li>只保留第一隐藏层进行训练    </li><li>叠加第二、第三隐藏层进行训练（<em>这里第一隐藏层和第三隐藏层的权重不共享？</em>）       </li><li>各层训练完毕，叠加起来得到一个完整的自编码器       </li></ol><p><img src="/Handson-ML/15Training_One_Autoencoder_at_A_Time.png" alt="15Training_One_Autoencoder_at_A_Time">     </p><p>可以按照这个描述，分别构造多个计算图来进行训练；<br>更巧妙的方式是添加一些操作在同一张计算图中区分训练阶段——<br><img src="/Handson-ML/15A_Single_Graph_To_Train_A_Stacked_Autoencoder.png" alt="15A_Single_Graph_To_Train_A_Stacked_Autoencoder">     </p><ol><li>中间部分是一个完整的自编码器   </li><li>左侧是第一训练阶段，旁路了第二和第三隐藏层，<code>Phase 1 Outputs</code> 跟 完整模型中的 <code>Outputs</code> 是共享参数的；<br> 这一阶段目标是使得最终输出与输入接近，训练 <code>Hidden 1</code> 和 <code>Outputs</code> 的权重      </li><li>右侧是第二训练阶段，旁路了输出层，第一隐藏层参数固定，只训练第二和第三隐藏层<br> 这一阶段目标是使得 <code>Hidden 3</code> 的输出与 <code>Hidden 1</code> 的输出接近，训练 <code>Hidden 2</code> 和 <code>Hidden 3</code> 的权重       </li></ol><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Build the whole stacked autoencoder normally.</span><span class="token comment" spellcheck="true"># In this example, the weights are not tied.</span><span class="token comment" spellcheck="true"># [...] </span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 阶段一的输出层，与完整模型的输出层共享参数</span>    phase1_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4    phase1_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>phase1_outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    phase1_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights4<span class="token punctuation">)</span>    phase1_loss <span class="token operator">=</span> phase1_reconstruction_loss <span class="token operator">+</span> phase1_reg_loss    phase1_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase1_loss<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    phase2_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3 <span class="token operator">-</span> hidden1<span class="token punctuation">)</span><span class="token punctuation">)</span>    phase2_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights3<span class="token punctuation">)</span>    phase2_loss <span class="token operator">=</span> phase2_reconstruction_loss <span class="token operator">+</span> phase2_reg_loss    <span class="token comment" spellcheck="true"># 忽略weights1和biases1</span>    train_vars <span class="token operator">=</span> <span class="token punctuation">[</span>weights2<span class="token punctuation">,</span> biases2<span class="token punctuation">,</span> weights3<span class="token punctuation">,</span> biases3<span class="token punctuation">]</span>    phase2_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase2_loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>train_vars<span class="token punctuation">)</span></code></pre><p>由于阶段一已经计算了 <code>hidden 1</code> 的输出，而且阶段二中 <code>hidden 1</code> 参数固定；<br>在内存足够的情况下可以先计算出整个batch上的输出并保留，以减少阶段二的训练时间；<br>这与<a href="/2018/04/09/复用预训练层/#加速训练：复用frozen层的输出结果（牺牲空间）">复用预训练层 - 复用frozen层输出加速训练</a>类似      </p><h4 id="重构效果可视化"><a href="#重构效果可视化" class="headerlink" title="重构效果可视化"></a>重构效果可视化</h4><p>直接显示压缩再解压后的结果，与输入进行直观的比较进行判断；<br>可以初步判断重构的效果       </p><h4 id="特征可视化"><a href="#特征可视化" class="headerlink" title="特征可视化"></a>特征可视化</h4><p>对于高层神经元，尤其是最后一个隐藏层的神经元，可以直接观察特定的输入时哪些神经元激活程度比较高；<br>但底层神经元关注的是更抽象、更小的特征，是我们无法直接理解的特征；   </p><ol><li>用权重分布图来观察每一个神经元的关注点<br> 比如观察第一隐藏层前五个神经元的关注点：     <pre class=" language-python"><code class="language-python"> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># train autoencoder</span>     weights1_val <span class="token operator">=</span> weights1<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>     plot_image<span class="token punctuation">(</span>weights1_val<span class="token punctuation">.</span>T<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre> <img src="/Handson-ML/15Visualzing_The_Feature.png" alt="15Visualzing_The_Feature"><br> <em>【越是关注的像素点，权重越大，在灰度图中就越亮】</em><br> 可以看到前四个神经元关注的都是某些局部的小区域，第五个神经元则似乎更关注竖直的笔画       </li><li>随机输入一个图像，然后用反向传播不断改变图像以最大化某个神经元的激活程度<br> 经过一定的迭代次数之后，图像将被扭曲为明显带有该神经元所关注特征的图像       </li><li>如果用自编码器用于某些任务（如分类任务）的前期无监督预训练<br> 那么可以直接通过观察这些任务的最终表现，来判断自编码器的效果      </li></ol><h3 id="用自编码器作无监督预训练"><a href="#用自编码器作无监督预训练" class="headerlink" title="用自编码器作无监督预训练"></a>用自编码器作无监督预训练</h3><p><a href="/2018/04/09/复用预训练层/#无监督预训练">《Handson-ML》笔记 - 无监督预训练</a><br>论文：<a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep" target="_blank" rel="noopener">Greedy Layer-Wise Training of Deep Networks(2006)</a>   </p><p><img src="/Handson-ML/15Unsupervised_Pretraining.png" alt="15Unsupervised_Pretraining">       </p><h3 id="不同约束下的自编码器"><a href="#不同约束下的自编码器" class="headerlink" title="不同约束下的自编码器"></a>不同约束下的自编码器</h3><p>添加约束，避免自编码器纯粹地把输入作为输出，从而得到一个更加高效的数据表示方式       </p><h4 id="降噪自编码器（Denoising-Autoencoders）"><a href="#降噪自编码器（Denoising-Autoencoders）" class="headerlink" title="降噪自编码器（Denoising Autoencoders）"></a>降噪自编码器（Denoising Autoencoders）</h4><p>论文：<br><a href="https://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf" target="_blank" rel="noopener">Extracting and Composing Robust Features with Denoising Autoencoders(2008)</a> 提出自编码器可以用于特征提取；<br><a href="http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf" target="_blank" rel="noopener">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(2010)</a> 提出降噪自编码器；      </p><p>降噪自编码器通过<strong>对输入加入高斯噪声</strong>或者<strong>在输入之后紧接一个dropout层</strong>，可以有效的避免输入噪声对模型的影响；<br><img src="/Handson-ML/15Denoising_Autoencoders.png" alt="15Denoising_Autoencoders">        </p><p>具体实现：<br>【高斯噪声方案】      </p><pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>X_noisy <span class="token operator">=</span> X <span class="token operator">+</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 其他层</span></code></pre><p>【dropout层方案】    </p><pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.7</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder_with_default<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>X_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 其他层</span></code></pre><h4 id="稀疏自编码器（Sparse-Autoencoders）"><a href="#稀疏自编码器（Sparse-Autoencoders）" class="headerlink" title="稀疏自编码器（Sparse Autoencoders）"></a>稀疏自编码器（Sparse Autoencoders）</h4><p>基本思想是为损失函数添加适当的稀疏度损失项，使得对于每个输入，编码层只有一小部分神经元被显著激活；<br>这使得编码器可以更好地提取出特征；<br><em>如果只让你用一句话描述自己的想法，那你可能会深思熟虑如何更好的表达</em>      </p><p>在每次迭代前，都必须先评估编码层的实际稀疏程度——计算编码层中活跃神经元的平均数量；<br>为了获取比较准确的平均数量，batch的大小一定不能太小；       </p><p>接下来我们为损失函数添加一个关于神经元活跃程度的惩罚项，比如用MSE；<br>但更好的方式是用具有更大梯度的<strong>KL散度（Kullback–Leibler divergence）</strong>；        </p><p><img src="/Handson-ML/15Sparsity loss.png" alt="15Sparsity loss">     </p><p>对于两个离散概率分布P和Q，KL散度表示为——<br>$$ D_{KL}(P||Q) = \sum_i P(i) log \frac{P(i)}{Q(i)} $$       </p><p>具体到稀疏自编码器上，对于评估的激活概率q和目标激活概率p，KL散度为（激活/不激活 是二项分布的）——<br>$$ D_{KL}(p||q) = p log \frac{p}{q} + (1-p) log \frac{1-p}{1-q} $$       </p><p>一旦计算出编码层上每个神经元的稀疏度损失，就可以把他们都加和到损失函数上进行训练；<br>为了权衡稀疏度损失和重构损失的重要性，可以为加和的稀疏度损失额外添加一个数值合适的权重超参数进行训练；       </p><p>具体实现：    </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算KL散度</span><span class="token keyword">def</span> <span class="token function">kl_divergence</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> p <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p <span class="token operator">/</span> q<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span>learning_rate <span class="token operator">=</span> <span class="token number">0.01</span>sparsity_target <span class="token operator">=</span> <span class="token number">0.1</span>sparsity_weight <span class="token operator">=</span> <span class="token number">0.2</span><span class="token comment" spellcheck="true"># [...] # Build a normal autoencoder (in this example the coding layer is hidden1)</span><span class="token comment" spellcheck="true"># 注意：编码层的激活程度必须是在(0,1)区间的</span><span class="token comment" spellcheck="true">#    比如可以用sigmoid函数强制激活程度归一化为(0,1)区间的数值</span><span class="token comment" spellcheck="true">#    hidden1 = tf.nn.sigmoid(tf.matmul(X, weights1) + biases1)</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>hidden1_mean <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 计算整个batch上的平均值</span>sparsity_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kl_divergence<span class="token punctuation">(</span>sparsity_target<span class="token punctuation">,</span> hidden1_mean<span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算稀疏度损失总和</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算重构损失（MSE）</span>loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> sparsity_weight <span class="token operator">*</span> sparsity_loss    <span class="token comment" spellcheck="true"># 计算全局损失</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span></code></pre><p>把重构损失 <code>reconstruction_loss</code> 的计算改为交叉熵可以加速收敛，但要注意交叉熵要求输入归一化，因此——     </p><pre class=" language-python"><code class="language-python">logits <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 训练时outputs不是必要的，只是为了看到重构结果才计算outputs</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span> <span class="token punctuation">)</span></code></pre><h4 id="变分自编码器（Variational-Autoencoders）"><a href="#变分自编码器（Variational-Autoencoders）" class="headerlink" title="变分自编码器（Variational Autoencoders）"></a>变分自编码器（Variational Autoencoders）</h4><p>论文：<a href="https://arxiv.org/pdf/1312.6114v10.pdf" target="_blank" rel="noopener">Auto-Encoding Variational Bayes(2014)</a>     </p><p>主要特点：     </p><ol><li>概率自编码器，输出是有一定偶然性的，即使是训练完之后      </li><li>生成自编码器，能够生成类似他们在训练集上采样的实例     </li></ol><p>这跟RBMs有些类似，但变分自编码器更加容易训练而且采样更快！     </p><p>其结构如下所示，编码层不再直接输出编码，而是在 $\mu$ 附近随机采样——<br><img src="/Handson-ML/15Variational_Autoencoder.png" alt="15Variational_Autoencoder">    </p><p>比如下图的输入数据，编码层将在以 $\mu$ 为中心，半径为 $\sigma$ 的范围内随机采样作为编码结果；<br><img src="/Handson-ML/15Variational_Autoencoder_Instance.png" alt="15Variational_Autoencoder_Instance">    </p><p>损失函数分为两个部分：    </p><ol><li>重构损失    </li><li><p>隐藏损失：即编码层上的在高斯分布下的损失（这一部分用高斯分布的目标分布和实际分布的KL散度来表示）<br> 高斯分布的噪声使传输给编码层的信息数量受限，迫使网络学习一些有意义的特征；<br> 这在数学计算上复杂了不少，不过可以用下列这个式子进行简化——<br> $$ L_l = \frac{1}{2} \sum \sigma^2 + \mu^2 - 1 - log(eps + \sigma^2) $$<br> 通常取 $eps=10^{-10}$，用于防止 $log(0)$ 的情况出现；<br> 有一种变种的损失函数——<br> $$ L_l = \frac{1}{2} \sum e^\gamma + \mu^2 - 1 - \gamma $$<br> 其中 $\gamma = log(\sigma^2)$，即 $\sigma = e^{\gamma / 2}$；<br> 该变种使得不同尺度下的 $\sigma$ 更容易被捕获，从而加速收敛；        </p><pre class=" language-python"><code class="language-python"> <span class="token comment" spellcheck="true"># [...] 超参数</span> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>framework<span class="token punctuation">.</span>arg_scope<span class="token punctuation">(</span>         <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>         activation_fn<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu<span class="token punctuation">,</span>         weights_initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># [...] 前层，hidden3为编码层</span>     hidden3_mean <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 从hidden2习得平均值</span>     hidden3_gamma <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 从hidden2习得gamma（与标准差相关）</span>     hidden3_sigma <span class="token operator">=</span> tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> hidden3_gamma<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算标准差sigma</span>     noise <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>hidden3_sigma<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 高斯分布噪声</span>     hidden3 <span class="token operator">=</span> hidden3_mean <span class="token operator">+</span> hidden3_sigma <span class="token operator">*</span> noise  <span class="token comment" spellcheck="true"># 在以平均值为中心，标准差为半径的范围内随机采样</span>     <span class="token comment" spellcheck="true"># [...] 后层</span>     logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden5<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 用于计算损失函数</span>     outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 实际输出</span> <span class="token comment" spellcheck="true"># 计算重构损失</span> reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>                         tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算隐藏损失</span> latent_loss <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>                         tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>hidden3_gamma<span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3_mean<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> hidden3_gamma<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算全局损失</span> cost <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> latent_loss <span class="token comment" spellcheck="true"># [...] 优化器等训练过程略</span></code></pre></li></ol><h4 id="其他自编码器"><a href="#其他自编码器" class="headerlink" title="其他自编码器"></a>其他自编码器</h4><ul><li>Contractive autoencoder(CAE)<br>  论文：<a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" target="_blank" rel="noopener">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction(2011)</a><br>  关于输入的编码上的导数比较小，使得两个相似的输入得到相似的编码         </li><li>Stacked convolutioncal autoencoders<br>  论文：<a href="http://people.idsia.ch/~ciresan/data/icann2011.pdf" target="_blank" rel="noopener">Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction(2011)</a><br>  通过用卷积层来重构图像的方式来提取图像特征     </li><li>Generative stochastic network(GSN)<br>  论文：<a href="https://arxiv.org/pdf/1503.05571v2.pdf" target="_blank" rel="noopener">GSNs: Generative Stochastic Networks(2015)</a><br>  降噪自编码器的一般化，能够生成数据       </li><li>Winner-take-all(WTA) autoencoder<br>  论文：<a href="https://arxiv.org/pdf/1409.2752v2.pdf" target="_blank" rel="noopener">Winner-Take-All Autoencoders(2015)</a><br>  训练过程中，计算时只保留激活程度前k%的神经元的激活程度，其余都置为0；<br>  这将稀疏化编码，类似的WTA方法也可以应用于生成稀疏化的卷积自编码器；      </li><li>Adversarial autoencoders<br>  论文：<a href="https://arxiv.org/pdf/1511.05644v2.pdf" target="_blank" rel="noopener">Adversarial Autoencoders(2016)</a><br>  分为两个网络，一个训练来重构输入的数据，与此同时另外一个训练来找到前者重构效果不好的输入数据；<br>  以此来迫使前者学习出鲁棒性比较好的编码方式      </li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>循环神经网络RNN</title>
      <link href="/2018/04/22/RNN/"/>
      <url>/2018/04/22/RNN/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=30431340&auto=0&height=32"></iframe><br>BGM：<strong>《Aldnoah Zero》ED1</strong><br>歌词混好几种语言       </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap14<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><blockquote><p>【占坑待填】<br>Handson ML对RNN的介绍比较简略，先占着坑          </p></blockquote><h3 id="Basic-RNNs的tensorflow实现"><a href="#Basic-RNNs的tensorflow实现" class="headerlink" title="Basic RNNs的tensorflow实现"></a>Basic RNNs的tensorflow实现</h3><ul><li><a href="/Handson-ML/14.1Static_Unrolling_Through_Time(Plain).html">Jupyter - Static Unrolling Through Time(Plain)</a><br>  底层操作实现的静态展开</li><li><a href="/Handson-ML/14.2Unrolling_Through_Time(High-level_API).html">Jupyter - Unrolling Through Time(High-level_API)</a><br>  高层操作实现的静态展开和动态展开          </li><li><a href="/Handson-ML/14.3Handling_Variable_Length_Sequences.html">Jupyter - Handling Variable Length Sequences</a><br>  处理变长的输入输出，另外还有一种办法，参见<a href="#机器翻译（Encoder-Decoder）">5.2 机器翻译（Encoder-Decoder）</a>       </li></ul><h3 id="训练RNN（BackproPagation-Through-Time-BPTT）"><a href="#训练RNN（BackproPagation-Through-Time-BPTT）" class="headerlink" title="训练RNN（BackproPagation Through Time, BPTT）"></a>训练RNN（BackproPagation Through Time, BPTT）</h3><p>一般将RNN按时间展开，然后使用常规的反向传播进行训练；<br>如下图所示：      </p><p><img src="/Handson-ML/14BPTT.png" alt="BPTT">        </p><p>注意这里损失函数 <code>C(·)</code> 是由最后几个输出计算得到的，而不是最后一个输出！      </p><h4 id="序列分类器"><a href="#序列分类器" class="headerlink" title="序列分类器"></a>序列分类器</h4><p>类似CNN做MNIST分类器，RNN也可以实现手写数字的识别；<br>按照《Handson-ML》，准确率也可以达到98%以上      </p><h4 id="预测时间序列"><a href="#预测时间序列" class="headerlink" title="预测时间序列"></a>预测时间序列</h4><p>比如股价预测——<br>随机截取一些连续的20个时间点的股价作为mini-batch，并且右移1个时间点作为标注；<br>即训练一个预测下一个时间点的股价的模型；<br><img src="/Handson-ML/14Stock_Price_Prediction.png" alt="14Stock_Price_Prediction">       </p><p>按前述直接用动态展开的BasicRNNCell组成的RNN网络——    </p><pre class=" language-python"><code class="language-python">n_steps <span class="token operator">=</span> <span class="token number">20</span>n_inputs <span class="token operator">=</span> <span class="token number">1</span>n_neurons <span class="token operator">=</span> <span class="token number">100</span>n_outputs <span class="token operator">=</span> <span class="token number">1</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>但是此时的网络输出outputs是一个包含100元素（<code>n_neurons=100</code>）的向量，<br>而我们需要的只是一个元素，即使下一时刻的预测值，最简单的思路是对cell进行包装：<br>其实就是在BasicRNNCell之后再加一个fully connected层（无激活函数）<br><img src="/Handson-ML/14OutputProjectionWrapper.png" alt="OutputProjectionWrapper">           </p><p>具体实现——         </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [...]</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>OutputProjectionWrapper<span class="token punctuation">(</span>basic_cell<span class="token punctuation">,</span> output_size<span class="token operator">=</span>n_outputs<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>此时的outputs就是一个单元素的输出啦~<br>尽管OutputProjectionWrapper可以解决这一问题，但每一步的神经元输出都需要过一层FC，效率不是很高；<br>更好的办法是（只需要过一次FC）：      </p><ol><li>将RNN的输出从 <code>[batch_size, n_steps, n_neurons]</code> 重整为 <code>[batch_size * n_steps, n_neurons]</code>；        </li><li>再通过一个fully connected整合成 <code>[batch_size * n_steps, n_outputs]</code> 大小的输出；        </li><li>最后展开成 <code>[batch_size, n_steps, n_outputs]</code> 大小的最终输出<br><img src="/Handson-ML/14More_Efficient_Method.png" alt="14More_Efficient_Method">       </li></ol><p>具体实现——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [...]</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>basic_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将RNN的输出从 [batch_size, n_steps, n_neurons] 重整为 [batch_size * n_steps, n_neurons]</span>stacked_rnn_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>rnn_outputs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_neurons<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 通过一个fully connected整合成 [batch_size * n_steps, n_outputs] 大小的输出</span>stacked_outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>stacked_rnn_outputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 最后展开成 [batch_size, n_steps, n_outputs] 大小的最终输出</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>stacked_outputs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h3><p>直接堆叠RNN就可以得到一个深层的RNN——<br><img src="/Handson-ML/14Deep_RNN.png" alt="14Deep_RNN">         </p><p>具体实现——<br>借助 <code>tf.contrib.rnn.MultiRNNCell()</code> 可以将多个RNN堆叠起来       </p><pre class=" language-python"><code class="language-python">n_neurons <span class="token operator">=</span> <span class="token number">100</span>n_layers <span class="token operator">=</span> <span class="token number">3</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span>multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>basic_cell<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>此时 <code>states</code> 是一个元组，每个元素都是每一层对应的大小为 <code>[batch_size, n_neurons]</code> 的Tensor；<br>如果为 <code>MultiRNNCell</code> 指定参数 <code>state_is_tuple=False</code> ，那么 <code>states</code> 就只是一个 <code>[batch_size, n_layers * n_neurons]</code> 大小的Tensor；      </p><h4 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h4><p><em>……暂略……</em>    </p><h4 id="使用Dropout"><a href="#使用Dropout" class="headerlink" title="使用Dropout"></a>使用Dropout</h4><p>如果只是在RNN层之前或者之后，可以直接添加Dropout层【可以参见<a href="/2018/04/11/正则化技术/#dropout">正则化技术 - Dropout</a>】；<br>但如果是在RNN层与RNN层之间添加Dropout，就必须使用 <code>DropoutWrapper</code>：        </p><pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.5</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用DropoutWrapper包装BasicRNNCell</span><span class="token comment" spellcheck="true"># ... input_keep_prob参数是输入前dropout层的keep_prob（不指定则步添加dropout）</span><span class="token comment" spellcheck="true"># ... 相应的还有output_keep_prob参数</span>cell_drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 直接堆叠若干个RNN层，层与层之间不能直接使用Dropout层</span>multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>cell_drop<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span>rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>然而头疼的是，<code>DropoutWrapper</code> 并不支持 <code>is_training</code> 参数，也就是说，它在预测是依旧存在dropout层；<br>解决方法有两种——      </p><ol><li>自己重写一个支持 <code>is_training</code> 参数的 <code>DropoutWrapper</code> 类（<em>什么鬼设定</em>）</li><li><p>针对训练和预测构建不同的计算图，具体如下：        </p><pre class=" language-python"><code class="language-python"> <span class="token keyword">import</span> sys is_training <span class="token operator">=</span> <span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"train"</span><span class="token punctuation">)</span> X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span> y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span> cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span> <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># 如果是训练，则用DropoutWrapper包装；如果是预测就拉倒</span>     cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span> multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>cell<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span> rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [...] build the rest of the graph</span> init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span> saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># 如果是训练，那就执行初始化器并进行训练</span>         init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true"># [...] # train the model</span>             save_path <span class="token operator">=</span> saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">"/tmp/my_model.ckpt"</span><span class="token punctuation">)</span>     <span class="token keyword">else</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 如果是预测，那就直接载入模型并使用</span>         saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">"/tmp/my_model.ckpt"</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># [...] # use the model</span></code></pre></li></ol><h4 id="长期依赖问题"><a href="#长期依赖问题" class="headerlink" title="长期依赖问题"></a>长期依赖问题</h4><p><a href="/2018/04/08/梯度消失与梯度爆炸/">梯度消失与梯度爆炸</a> 所述技术对RNN也是有效的；<br>但是使用这些技术之后，随着时间步增多，训练将变得十分缓慢；<br>最简单粗暴的解决方法是<strong>缩短时间步</strong>，但如此一来RNN忽略比较遥远的过去，也即存在长期依赖的问题；<br>为了解决这一问题，LSTM出现啦！       </p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>论文：<br>【起源】：<a href="https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw" target="_blank" rel="noopener">Long Short-Term Memory(2006)</a><br>【改进】：       </p><ol><li><a href="https://arxiv.org/pdf/1402.1128.pdf" target="_blank" rel="noopener">LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION(2014)</a>        </li><li><a href="https://arxiv.org/pdf/1409.2329v5.pdf" target="_blank" rel="noopener">RECURRENT NEURAL NETWORK REGULARIZATION(2015)</a>       </li><li><a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Recurrent Nets that Time and Count(2000)</a> 【提出peephole connection】      </li><li><a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)</a> 【提出GRU和Encoder-Decoder架构】<br>其他：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks(2015)</a>      </li></ol><p>tensorflow中的使用：<br>直接将先前的 <code>tf.contrib.rnn.BasicRNNCell</code> 替换为 <code>tf.contrib.rnn.BasicLSTMCell</code> 即可；<br>区别在于，<code>BasicLSTMCell</code> 的states包含两个向量，如果要合并在一起的话可以指定参数 <code>state_is_tuple=False</code>；<br>如果要使用变种的LSTM，则使用 <code>tf.contrib.rnn.LSTMCell</code>：<br>比如使用带peephole connection的LSTM，则指定参数 <code>use_peepholes=True</code>          </p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>LSTMCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> use_peepholes<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks(2015)</a> 一问介绍了RNN的一些应用；   </p><p>接下来只介绍RNN在自然语言处理（NLP）上的两个应用</p><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>参考：</p><ol><li><a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">Tensorflow Tutorial - Word2Vec</a> 或 中文的<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/word2vec.html" target="_blank" rel="noopener">字词的向量表示</a>   </li><li><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" target="_blank" rel="noopener">Deep Learning, NLP, and Representations(2014)</a>      </li><li><a href="http://ruder.io/word-embeddings-2017/" target="_blank" rel="noopener">Word Embeddings in 2017: Trends and future directions(2017)</a>  </li></ol><p>词表示的方式：       </p><ol><li>独热码：稀疏表示    </li><li>顺序编码：稠密表示          </li><li>词嵌入：前两者的折中，而且可以通过训练，使得两个词的距离表示它们的相似程度      </li></ol><p>tensorflow实现：    </p><pre class=" language-python"><code class="language-python">vocabulary_size <span class="token operator">=</span> <span class="token number">50000</span>embedding_size <span class="token operator">=</span> <span class="token number">150</span><span class="token comment" spellcheck="true"># 创建一个待训练的词向量变量</span>embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>random_uniform<span class="token punctuation">(</span><span class="token punctuation">[</span>vocabulary_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 占位符，输入单词的顺序编码</span>train_inputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用embedding_lookup获取对应单词的顺序编码对应的词向量表示</span>embed <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>embedding_lookup<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> train_inputs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 投喂语料库训练出合适的词向量</span><span class="token comment" spellcheck="true"># 投喂前需要预处理语料库，比如将一些未知的单词都置为&lt;UNK>、将链接都置为&lt;URL></span></code></pre><p>当然，也可以直接下载已经训练好的词向量载入给 <code>embedding</code> 然后直接使用；      </p><h4 id="机器翻译（Encoder-Decoder）"><a href="#机器翻译（Encoder-Decoder）" class="headerlink" title="机器翻译（Encoder-Decoder）"></a>机器翻译（Encoder-Decoder）</h4><p>参考：<a href="https://www.tensorflow.org/tutorials/seq2seq" target="_blank" rel="noopener">Tensorflow Tutorial - Seq2Seq</a> 【<a href="https://github.com/google/seq2seq" target="_blank" rel="noopener">代码</a>】    </p><p>训练时：<br><img src="/Handson-ML/14Translate.png" alt="14Translate">    </p><ul><li>源语句投喂给Encoder，注意应该使第一个单词最先进入Decoder      </li><li>目标语句投喂给Decoder     </li><li>Decoder对每个时间步产生一个评分并交由Softmax层得到单词的输出概率      </li></ul><p>预测时：<br><img src="/Handson-ML/14Translate_Inference.png" alt="14Translate_Inference">     </p><p>Google - Seq2Seq项目的特别之处：     </p><ol><li>独特的处理变长序列的方式<br> 前述提到用 <strong>sequence_length参数+静态展开</strong> 或 <strong>动态展开</strong> 的方式来处理变长序列；<br> 而Seq2Seq采用另一种方式——<br> 先将语句根据不同长度段分别放入不同的桶中（比如有接收长度为1~6的语句桶、接收长度为7~12的语句桶）；<br> 再用符号 <code>&lt;pad&gt;</code> 将桶内的语句填充到同一规格（比如<code>I drink milk &lt;eos&gt;</code>被填充为<code>I drink milk &lt;eos&gt; &lt;pad&gt; &lt;pad&gt;</code>）；<br> （注意，源语句在前面填充符号，目标语句在末尾填充符号）<br> 然后用一个 <code>target_weights</code> 向量表征每个单词的权重（比如<code>I drink milk &lt;eos&gt; &lt;pad&gt; &lt;pad&gt;</code>的权重为<code>[1,1,1,1,0,0]</code>）；<br> 这样，当损失函数与 <code>target_weights</code> 向量相乘时，<code>&lt;pad&gt;</code> 就被忽略了；      </li><li>使用<strong>Sampled Softmax</strong>技术<br> 论文：<a href="https://arxiv.org/pdf/1412.2007v2.pdf" target="_blank" rel="noopener">On Using Very Large Target Vocabulary for Neural Machine Translation</a><br> 当输出字典非常大（比如50000）时，Decoder将产生50000维的向量，使得计算softmax函数时变得非常复杂；<br> 为了避免这个问题，可以使Decoder输出小得多的向量（如1000维），然后用采样技术来评估损失；<br> 这在tensorflow中可以借助函数 <code>sampled_softmax_loss()</code> 实现    </li><li>使用注意力机制<br> 论文：<a href="https://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a>    </li><li>Seq2Seq使用了 <code>tf.nn.legacy_seq2seq</code> 模块，模块中包含了各种各样的Encoder-Decoder模型<br> 比如 <code>embedding_rnn_seq2seq()</code> 函数创建一个与前述机器翻译训练时的图中相同的模型       </li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络CNN</title>
      <link href="/2018/04/18/CNN/"/>
      <url>/2018/04/18/CNN/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=458725210&auto=0&height=32"></iframe><br>BGM：<strong>《四月是你的谎言》ED2</strong>        </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li><li>《<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">Deep Learning 深度学习(2017)</a>》        </li></ol><h3 id="CNN原理"><a href="#CNN原理" class="headerlink" title="CNN原理"></a>CNN原理</h3><p>卷积神经网络主要由<strong>卷积层+激活函数+池化层</strong>组成，并且在最后用全连接层输出——<br><img src="/Handson-ML/12CNN.png" alt="12CNN">     </p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p><a href="/ML-Andrew/ML-Andrew-notes3.html#反向传播算法?_blank">机器学习-吴恩达/3 非线性分类器——神经网络/反向传播算法 | Hey~YaHei!</a><br>论文：<a href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf" target="_blank" rel="noopener">Learning representation by back-propagating errors(1986)</a>   </p><p>前向传播：<br>$$ z_i = \omega_i^T x_{i}  $$<br>其中，<br>$z_i$ 为第i层的损失，即 $z_i = Cost(x_{i+1}, y)$；<br>$x_i$ 为第i层的输入；<br>$\omega_i$ 为第i层的参数；<br>$x_0$ 为原始输入    </p><p>根据链式法则，$z_i$ 对参数 $\omega_i$ 和输入 $x_i$ 求偏导——<br>$$ \frac{\partial z_i}{\partial \omega_i} = \frac{\partial z_i}{\partial x_{i+1}} \frac{\partial x_{i+1}}{\partial \omega_i} $$<br>$$ \frac{\partial z_i}{\partial x_i} = \frac{\partial z_i}{\partial x_{i+1}} \frac{\partial x_{i+1}}{\partial x_i} $$<br>其中，<br>由于 $x_{i+1}$ 由 $x_i$ 经过 $\omega_i$ 的作用得到，则 $\frac{\partial x_{i+1}}{\partial \omega_i}$ 和 $\frac{\partial x_{i+1}}{\partial x_i}$ 可以直接求得；<br>剩下的部分 $\frac{\partial z_i}{\partial x_{i+1}}$ 是由后一层计算得到；    </p><p>总的来说，误差由后往前传播，<br>$\frac{\partial z_i}{\partial \omega_i}$ 用于梯度下降，如 $\omega_i \gets \omega_i - \eta \frac{\partial z}{\partial \omega_i}$；<br>$\frac{\partial z_i}{\partial x_i}$ 用于前层 $\frac{\partial z_{i-1}}{\partial \omega_{i-1}}$ 的计算     </p><p>这里对不同参数的求导操作非常繁琐，Theano、Tensorflow都采用符号微分的方法进行自动求导（编译时就计算得到导数的数学表示）；<br>具体可以参见“花书”《Deep Learning 深度学习》（中文版）P126-139       </p><h4 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h4><p>卷积层并不是使用严格数学意义的卷积运算，而是使用保留卷积性质但抛弃可交换性的互相关函数；<br><em>卷积运算具有可交换性，这在数学证明上是很有用的，但在神经网络的应用中却是一个比较鸡肋的性质</em><br>卷积操作选用一定大小的卷积核（下图黄色区域）在原始数据上移动，与重合部分数据做乘和运算；<br>依次类推，最终输出一张特征图（Feature Map）<br><img src="/Handson-ML/12Convolutional_Kernel.gif" alt="12Convolutional_Kernel">       </p><p>卷积核的作用相当一个滤波器，其参数是经过学习得到的，可以用于提取图片中的特征；<br>由于核参数是随机初始化的，所以它们很可能会提取出不同的特征；<br>由低层的卷积层提取简单特征，然后逐层堆叠卷积层，将简单特征逐渐抽象为更高层次的语义概念；       </p><p>大核的卷积层可以用多层的小核的卷积层实现；<br>比如用三层3x3卷积核的卷积层可以提取到一层7x7卷积核的卷积层一样的特征——<br><img src="/Handson-ML/12Multi_Conv_Layers.png" alt="12Multi_Conv_Layers"><br>而且，使用多层小核卷积层由以下优势：    </p><ol><li>减少参数<br> 7x7卷积核有 $7 \times 7 = 49$ 个参数，而三层3x3卷积核只有 $3 \times 3 \times 3 = 27$ 个参数      </li><li>增加网络深度<br> 增加网络容量和复杂度    </li></ol><p>卷积操作的变体：     </p><ol><li>扩大原有卷积核在前层的感受野<br> 论文：<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable Convolutional Networks(2017)</a>     </li><li>感受野形状可变（而不再是简单的矩形区域）<br> 论文：<a href="https://arxiv.org/pdf/1511.07122.pdf" target="_blank" rel="noopener">Multi-Scale Context Aggregation by Dilated Convolutions(2016)</a>      </li></ol><h4 id="池化层（Pool）"><a href="#池化层（Pool）" class="headerlink" title="池化层（Pool）"></a>池化层（Pool）</h4><p>池化操作与卷积操作类似，但池化层是<strong>不需要参数</strong>的；<br>选用一定大小的池化核在原始数据上移动，与重合部分数据做一定的聚合运算（取均值、取最值、按一定概率随机取值等）；<br>依次类推，最终输出一张特征图；       </p><p>详见 《<a href="/2018/05/07/漫谈池化层?_blank">漫谈池化层 | Hey~YaHei!</a>》     </p><h4 id="全连接层（Fully-Connection）"><a href="#全连接层（Fully-Connection）" class="headerlink" title="全连接层（Fully Connection）"></a>全连接层（Fully Connection）</h4><p>参考：<a href="https://www.zhihu.com/question/41037974" target="_blank" rel="noopener">全连接层的作用是什么？——魏秀参的回答 | 知乎</a>     </p><p>全连接层在CNN中起“分类器”作用，将卷积层、池化层、激活函数学到的特征表示映射到样本的标记空间；<br>由于最后一层卷积层输出一个若干个二维数据（总体为三维），所以输入FC前通常需要将其展平（Flatten）为一维；       </p><p>实际上，全连接层可以用卷积操作代为实现：      </p><ol><li>如果FC前为FC，则该FC可以转换成1 x 1的卷积     </li><li>如果FC前为卷积层，则该FC可以转换为H x W的卷积（H、W为前层输出的高、宽）       </li></ol><p>由于全连接层参数冗余，一些网络如ResNet、GoogLeNet等采用全局平均池化（GAP）取代FC来融合学到的深度特征；    </p><p>近期研究也发现，FC可以在模型表示能力迁移过程中（尤其是原任务与目标任务差异较大时）充当“防火墙”，保证模型表示能力的迁移；     </p><h3 id="tensorflow实现"><a href="#tensorflow实现" class="headerlink" title="tensorflow实现"></a>tensorflow实现</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>借助 <code>tf.nn.conv2d()</code> ——            </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_sample_images<span class="token comment" spellcheck="true"># 读入一些图片</span>dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>load_sample_images<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>batch_size<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 创建两个三维filter</span><span class="token comment" spellcheck="true"># ... 大小7*7，通道数由图片决定</span><span class="token comment" spellcheck="true"># ... 一个水平filter，一个垂直filter（只有某一行或列为1）</span>filters_test <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>filters_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># vertical line</span>filters_test<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># horizontal line</span><span class="token comment" spellcheck="true"># 图片占位符</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加卷积层</span><span class="token comment" spellcheck="true"># ... X为输入</span><span class="token comment" spellcheck="true"># ... filters是所用的一系列filter，列表最后一维是filter的索引</span><span class="token comment" spellcheck="true"># ... stride是步长，针对输入而言的，比如这里batch、行、列、通道的步长分别为1,2,2,1</span><span class="token comment" spellcheck="true"># ... padding为填充方式，SAME表示填零，VALID表示不填零（可能会舍弃结尾的部分元素）</span>convolution <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>X<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"SAME"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    output <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>convolution<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> dataset<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示第一张图片的第二个feature map</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>padding参数，SAME和VALID的区别——<br><img src="/Handson-ML/12Padding.png" alt="padding">          </p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>借助<code>tf.nn.max_pool()</code>、<code>tf.nn.avg_pool()</code> 等——       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_sample_images<span class="token comment" spellcheck="true"># 读入一些图片</span>dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>load_sample_images<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>batch_size<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 图片占位符</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加最大池化层</span><span class="token comment" spellcheck="true"># ... X为输入的数据</span><span class="token comment" spellcheck="true"># ... ksize为池化核大小，这里表示每次一张图片，池化核大小为2x2，池化核每次只应用于一个通道</span><span class="token comment" spellcheck="true"># ...... 注意：tensorflow不支持多对象池化，所以第一维必须是1</span><span class="token comment" spellcheck="true"># .......... 而且，只支持平面池化或者深度池化</span><span class="token comment" spellcheck="true"># .......... 也就是说，要么深度（通道）参数置为1，要么长宽都置为1</span><span class="token comment" spellcheck="true"># ... stride为步长，同卷积层，这里batch、行、列、通道的步长分别为1,2,2,1</span><span class="token comment" spellcheck="true"># ... padding为填充方式，同卷积层，这里表示不填充</span>max_pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>X<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"VALID"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    output <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>max_pool<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> dataset<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示第一张图片池化后的效果</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="内存占用计算（以卷积层为例）"><a href="#内存占用计算（以卷积层为例）" class="headerlink" title="内存占用计算（以卷积层为例）"></a>内存占用计算（以卷积层为例）</h3><p>考虑 $N_f$ 个大小为 $m_f \times n_f \times c$ 的三维filter组成的卷积层，<br>假设输入数据（比如图片）大小为 $m_i \times n_i$，每个batch有 $N_i$ 个instances，<br>那么每个filter输出feature map的大小也为 $m_i \times n_i$；        </p><p>此时,<br>该卷积层包含参数数量为——<br>$$ N_p = (m_f \times n_f \times c \underbrace{+1}_{\text{偏置单元}}) \times N_f $$<br>该卷积层输出的feature maps的变量总数为——<br>$$ N_v = m_i \times n_i \times N_i $$<br>该卷积层每一趟要执行的运算次数为——<br>$$ N_o = \underbrace{m_i \times n_i \times N_i}_{\text{feature maps的变量总数}} \times \underbrace{m_f \times n_f \times c}_{\text{卷积核大小}} $$        </p><p>比如，200个大小为 $5 \times 5 \times 3$ 的filter组成的卷积层，输入大小为 $150 \times 100$ 的三通道图片；<br>那么该层参数数量为 $ (5 \times 5 \times 3 + 1) \times 200 = 15200 $ ；<br>如果每个batch大小为1，使用float32存储变量，那么需要占用内存 $200 \times 150 \times 100 \times 32 = 96,000,000bits$ （约11.4MB）；<br>需要进行 $200 \times 150 \times 100 \times 5 \times 5 \times 3 = 225,000,000$ 次float乘法；      </p><p>解决内存溢出的办法：     </p><ol><li>加大步长达到降维的目的（使feature map比输入小）       </li><li>减少一些层      </li><li>使用占用空间更少的变量类型        </li><li>分布式运算</li></ol><h3 id="经典的CNN分类架构"><a href="#经典的CNN分类架构" class="headerlink" title="经典的CNN分类架构"></a>经典的CNN分类架构</h3><p>目前常见的CNN分类架构有LeNet-5、AlexNet(2012)、NIN(2014)、VGG-Nets(2015)、GoogLeNet(2015)、ResNet(2015)等；<br>详见 《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》     </p><h3 id="目标检测架构"><a href="#目标检测架构" class="headerlink" title="目标检测架构"></a>目标检测架构</h3><p>参见 <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741534&amp;idx=1&amp;sn=02dc164ffcedbf22124b97841ba67fe5&amp;chksm=871adf60b06d567690fa2328b161c012a464687768e50f812a51b5533a7d68b99af1cf8f02b8&amp;scene=0#rd" target="_blank" rel="noopener">从RCNN到SSD，这应该是最全的一份目标检测算法盘点 | 机器之心(2018)</a>【<a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9" target="_blank" rel="noopener">原文</a>】     </p><h3 id="CNN可视化"><a href="#CNN可视化" class="headerlink" title="CNN可视化"></a>CNN可视化</h3><p>论文：    </p><ol><li><a href="https://cs.nyu.edu/~fergus/drafts/deconv_iccv_names.pdf" target="_blank" rel="noopener">Adaptive Deconvolutional Networks for Mid and High Level Feature Learning(2011)</a> 提出反卷积技术     </li><li><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks(2013)</a> 用反卷积技术实现CNN可视化（以AlexNet为例）    </li></ol><h3 id="网络压缩"><a href="#网络压缩" class="headerlink" title="网络压缩"></a>网络压缩</h3><p>深度神经网络面临严峻的<strong>过参数化（over-parameterization）</strong>问题，<br>如论文 <a href="http://export.arxiv.org/pdf/1306.0543" target="_blank" rel="noopener">Predicting Parameters in Deep Learning(2014)</a> 指出只给定很小一部分参数子集（约5%）就可以完整地重构剩余的参数；    </p><p>但事实上，参数的冗余在模型<strong>训练过程中</strong>是十分必要的，因为面临一个极其复杂的非凸优化问题，对现有基于梯度下降的优化算法而言，参数冗余保证了网络能够收敛到一个比较好的最优值。一定程度上，网络越深，参数越多，模型越复杂，最终效果也往往越好；     </p><p>压缩既指体积上的压缩，也指时间上的压缩。<br>绝大多数压缩算法旨在将一个庞大而复杂的<strong>预训练模型</strong>转化为一个精简的小模型；<br>按对网络结构的破坏程度分，可以分为前端压缩和后端压缩——     </p><ul><li><strong>前端压缩</strong><br>  不改变原网络结构，仅仅在原模型基础上减少网络层数或滤波器个数，可以完美适配现有的深度学习库；<br>  主要包括知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝等     </li><li><strong>后端压缩</strong><br>  尽可能减少模型大小，对原始网络造成极大程度的改造（往往不可逆），必须开发相应配套的运行库甚至专门的硬件设备；<br>  主要包括低秩近似、未加限制的剪枝、参数量化、二值网络等    </li><li>前端压缩和后端压缩是互补的关系<br>  通过相互结合，将前端压缩和后端压缩级联起来，可以在最大程度上减少模型复杂度       </li><li>此外，也有人试图设计更加紧凑的网络结构，对新的网络结构进行训练<br>  这也能减小模型复杂度，但不是严格意义上的网络压缩    </li></ul><!-- 妈耶！好难啊，先放一放#### 低秩近似       基本思想：      CNN的卷积操作由矩阵乘法完成，权重矩阵往往稠密巨大，带来计算和存储上的巨大开销；      可以将稠密的矩阵用若干个小规模矩阵近似重构出来，这类算法大多采用低秩近似的技术      对于权重矩阵 $W \in R^{m \times n}$ ，用若干低秩矩阵 $M_i$ 组合来进行表示——      $$ W = \sum^n_{i=1} \alpha_i M_i $$        其中，$M_i \in R^{m \times n}$ 且其秩为 $r_i << min(m,n)$；       并且可以对每个低秩矩阵进一步分解为小规模矩阵的乘积——       $$ M_i = G_i H_i^T $$        其中，$G_i \in R^{m \times r_i}$， $H_i \in R^{n \times r_i}$      当 $r_i$ 很小时，可以大幅度降低总体的存储和计算开销（以全连接层为例）——     * 原始权重矩阵 $W$          参数总数为 $nm$；         计算 $W^T X$          包含 $nm$ 次乘法 和 $n(m-1)$ 次加法；      * 低秩矩阵 $W_i$         参数总数为 $n(m r_i + n r_i + 1)$；        计算 $W^T X = \sum_{i=1}^n \alpha_i M_i^T X = \sum_{i=1}^n \alpha_i H_i G_i^T X$         包含 $r_i(m+n) + n$ 次乘法 和 $r_i((n-1)+(m-1)) + n$ 次加法；         *注意：先计算 $G_i^T X$，再计算 $H_i (G_i^T X)$*           这里看起来参数总数反而变多了（实际上后边有更简单的表示法），但在 $r_i << min(m, n)$ 时可以明显看到计算量减少了-->       <h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p>有效的数据扩充通过扩充训练样本数量，增加样本多样性，一方面可以避免过拟合，另一方面会给模型性能带来提升；       </p><h4 id="简单的数据扩充"><a href="#简单的数据扩充" class="headerlink" title="简单的数据扩充"></a>简单的数据扩充</h4><ul><li>图像水平翻转<br>  数据集扩充一倍    </li><li>随机抠取<br>  一般用较大的正方形（0.8~0.9倍的原图大小）在原图的随机位置抠取图像块；<br>  抠取次数决定数据集的扩充倍数；<br>  同时用设定好的比例抠取图像，避免了图像缩放带来的分辨率失真        </li><li>尺度变换<br>  将原图等比率缩放为原图的0.8、0.9、1.1、1.2、1.3等倍数；<br>  缩放次数决定数据集的扩充倍数；<br>  增加CNN在物体尺度上的鲁棒性       </li><li>旋转<br>  将原图旋转-30度、-15度、15度、30度等角度；<br>  旋转次数决定数据集的扩充倍数；<br>  增加CNN在方向上的鲁棒性        </li><li>色彩抖动<br>  在RGB颜色空间对色彩分布进行轻微扰动，在HSV颜色空间对图像饱和度、明度、色调进行轻微扰动；    </li></ul><p>实践中往往会在上述几种方式叠加使用；<br>相关实践代码可以参见：<a href="https://github.com/aleju/imgaug" target="_blank" rel="noopener">aleju/imgaug | github</a>，一个图像数据集的扩充python库     </p><h4 id="特殊的数据扩充"><a href="#特殊的数据扩充" class="headerlink" title="特殊的数据扩充"></a>特殊的数据扩充</h4><ul><li><p>Fancy PCA<br>  论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Imagenet Classification with Deep Convolutional Neural Networks(2012)</a><br>  和<a href="/2018/05/02/经典的CNN分类架构/#AlexNet">AlexNet</a>网络一同提出；<br>  论文指出，Fancy PCA可以近似捕获自然图像的一个重要特性——<strong>物体特质与光照强度和颜色变化无关</strong>        </p><ol><li>对整个数据集的R、G、B进行PCA操作，得到特征向量 $p_i$ 和特征值 $\lambda_i$，其中 $i=1,2,3$；     </li><li>计算一组随机值$[p_1, p_2, p_3][\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T$，将其作为扰动加到原像素值中；<br> 其中，$\alpha_i$ 为0均值0.1标准差的高斯分布随机值      </li><li>每一个epoch之后，重新选取一次$\alpha_i$进行扰动       </li></ol></li><li><p>监督式数据扩充<br>  海康威视在2016ImageNet竞赛的场景分类任务中提出；<br>  在以物体为中心的图像分类任务中，随机抠取图像块可以取得比较好的效果；<br>  但对于依靠图像整体蕴含的高层语义的场景分类任务中，随机抠取图像块很可能会抠取到关联性比较差的结果（比如“海滩”中抠取到“树”和“天空”）；<br>  可以借助图像标记信息解决这一问题：    </p><ol><li>根据原数据训练一个分类的初始模型     </li><li>利用该模型对每张图生成激活图（activation map）或热力图（heat map）<br> 可以直接将最后一层卷积层特征按深度方向加和得到；<br> 也可以参照论文 <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">Learning Deep Features for Discriminative Localization</a> 生成分类激活图（class activation map）<br> 该热力图可以指示图像区域与场景标记之间的相关概率    </li><li>根据上述概率映射回原图选择较强相关的图像区域作为抠取的图像块</li></ol></li></ul><h3 id="图像预处理：中心式归一化"><a href="#图像预处理：中心式归一化" class="headerlink" title="图像预处理：中心式归一化"></a>图像预处理：中心式归一化</h3><p>在训练集上计算各通道的均值，然后对训练集、验证集、测试集上每一个像素点的各通道都减去该均值；<br>其原理在于，自然图像一般是一类平稳的数据分布，通过减均值操作可以移除图像的共性部分而凸显个体的差异；<br>比如下图通过减均值操作之后，可以发现背景部分被有效“移除”了，而只保留车、建筑等显著区域<br><img src="/Handson-ML/12Image_Preprocess.png" alt="12Image_Preprocess">     </p><h3 id="超参数设定"><a href="#超参数设定" class="headerlink" title="超参数设定"></a>超参数设定</h3><h4 id="输入数据像素大小"><a href="#输入数据像素大小" class="headerlink" title="输入数据像素大小"></a>输入数据像素大小</h4><p>CNN需要对输入的图像统一大小，通常为了便于GPU设备并行，都会统一将图像压缩为 $2^n$ 大小；<br>在设备、时间条件允许的情况下，一般分辨率高的数据有助于网络性能的提升，尤其是对基于注意力模型的网络；<br>一般CNN最后采用FC作为分类器，如果改变了原模型的图像分辨率，通常也需要重新设定FC输入的滤波器大小以及其他相关参数     </p><h4 id="卷积层参数"><a href="#卷积层参数" class="headerlink" title="卷积层参数"></a>卷积层参数</h4><p>包括卷积核大小、卷积步长、卷积核个数（即输出的特征图数量）；      </p><p>实践中通常采用3x3和5x5的小核，小的卷积核有以下作用：      </p><ol><li>增加模型复杂度，防止欠拟合    </li><li>减少参数数量     </li></ol><p>卷积操作可以选择性的搭配填充操作（padding），有以下作用：     </p><ol><li>充分利用和处理输入数据的边缘信息    </li><li>搭配合适的参数可以保持输入、输出大小不变，避免随着网络深度增加输入大小急剧减小<br> 对于fxf的卷积核、步长为1的卷积操作，在边缘各添加 $p=(f-1)/2$ 个像素可以维持输入输出大小不变       </li></ol><p>为了便于GPU设备方便存储，卷积核个数也即输出的特征图数量通常为 $2^n$ ；    </p><p>可参考：<a href="https://www.quora.com/How-can-I-decide-the-kernel-size-output-maps-and-layers-of-CNN?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa" target="_blank" rel="noopener">How can I decide the kernel size, output maps and layers of CNN? | Quora</a>  </p><p>论文：    </p><ol><li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">Efficient BackProp(1998)</a>     </li><li><a href="https://arxiv.org/pdf/1606.02228v2.pdf" target="_blank" rel="noopener">Systematic evaluation of CNN advances on the ImageNet(2017)</a> 比较了ILSVRC上各种技术、模块在不同参数下的表现<br> 对应github评估项目：<a href="https://github.com/ducha-aiki/caffenet-benchmark" target="_blank" rel="noopener">ducha-aiki/caffenet-benchmark | github</a>     </li></ol><p>通常，     </p><ul><li>网络越深越好，但这是以更大的数据集、学习任务复杂度增加为代价的；     </li><li>batch size设为几百，具体视任务而定，批大小约大计算资源的需求就越高，批大小不宜太小（这会导致估计产生较大的偏差）；     </li><li>一开始使用较少的特征图数量，然后逐渐增加并且一边观察误差的变化趋势；    </li><li>小核可以捕捉图像的细节，大核容易丢失图像的细微特征；    </li><li>可以参考类似任务的网络配置；     </li></ul><h4 id="池化层参数"><a href="#池化层参数" class="headerlink" title="池化层参数"></a>池化层参数</h4><ul><li>池化核一般比较小，如2x2、3x3等<br>  为了不丢弃过多输入而损失网络性能，很少使用超过3x3的池化核     </li><li>最常用的是2x2大小、2步长的池化操作<br>  此时输出缩小为原来的四分之一，也即丢弃了75%的响应值     </li></ul><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><p><strong>随机打乱训练数据</strong>：<br>信息论指出——    </p><blockquote><p>从相似的事件中学习总是比从相似事件中学习更具信息量    </p></blockquote><p>在每轮（epoch）训练中随机打乱训练数据，使得模型每次在不同批次看到“不同”的数据，<br>不仅可以提高模型收敛速度，还对模型的泛化性能有略微的提升      </p><p><strong>学习率设定</strong>：     </p><ul><li>初始学习率不宜过大，以0.01和0.001为宜；<br>  如果刚训练几步模型的loss就急剧上升，说明初始学习率过大       </li><li>使用一定的学习计划策略<br>  可参见 <a href="/2018/04/10/优化器/#学习计划（learning-schedules）">优化器/学习计划 | Hey~YaHei!</a>     </li><li>训练过程中观察学习曲线（loss随步数的变化）对学习率进行诊断<br>  <img src="/Handson-ML/learning_rate.png" alt="learning_rate">     </li></ul><p><strong>批规范化操作（BN操作）</strong>：<br>参见 <a href="/2018/04/08/梯度消失与梯度爆炸/#批量归一化（Batch-Normalization-BN）">梯度消失与梯度爆炸/批量归一化 | Hey~YaHei!</a>     </p><p><strong>优化器</strong>：<br>参见 <a href="/2018/04/10/优化器/">优化器 | Hey~YaHei!</a>     </p><p><strong>微调预训练好的神经网络</strong>：<br>用目标任务数据在预训练模型上继续进行训练过程；      </p><ul><li>网络已经在原始数据上收敛，微调时采用更小的学习率（一般在$10^{-4}$及其以下）    </li><li>CNN浅层有更强的泛化特征，深层对应更抽象的语义特征；<br>  微调时往往对前层更新的少，对深层更新的多，故可以设置不同的学习率；      </li><li>微调策略   <ul><li>数据较少且任务非常相似时，可仅微调最后的几层    </li><li>数据较多且任务相似时，可以微调更多甚至全部的网络层    </li><li>当数据较少、差异较大时，可以尝试微调，但不一定能成功<br>  这种情况下还可以借助<strong>部分原始数据与目标数据协同训练</strong>；<br>  论文：<a href="https://arxiv.org/pdf/1702.08690.pdf" target="_blank" rel="noopener">Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning(2017)</a><br>  在浅层特征空间选择目标数据的近邻作为协同训练的原始数据子集；<br>  微调阶段改造为多目标学习任务：将目标任务基于原始数据子集、将目标任务基于全部目标数据；     </li></ul></li></ul><hr><p>2018-05-02<br>将经典的CNN分类架构抽离出来作为单独一篇博文：<br>《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》     </p><hr><p>2018-05-07<br>将池化层原理部分抽离出来作为单独一篇博文：<br>详见 《<a href="/2018/05/07/漫谈池化层?_blank">漫谈池化层 | Hey~YaHei!</a>》         </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>正则化技术</title>
      <link href="/2018/04/11/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
      <url>/2018/04/11/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28160278&auto=0&height=32"></iframe><br>BGM：<strong>《命运石之门：负荷领域的既视感》主题曲</strong>        </p><hr><p>参考：   </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap11<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li></ol><h3 id="提前终止（Early-Stopping）"><a href="#提前终止（Early-Stopping）" class="headerlink" title="提前终止（Early Stopping）"></a>提前终止（Early Stopping）</h3><p>每经过一定迭代次数之后将模型用于验证集上的评估，暂存、更新最近几次在验证集上有一定loss下降的模型；<br>当连续几次在验证集上没有出现明显的loss下降（甚至有可能回升）时终止训练；<br>提前终止通常表现的很好，如果和其他正则化技术共同使用可以获得更好的表现      </p><h3 id="L1、L2正则化"><a href="#L1、L2正则化" class="headerlink" title="L1、L2正则化"></a>L1、L2正则化</h3><p><strong>L2正则化</strong>：<br>又称权重衰减（weight decay）、岭回归（ridge regression）、Tikhonov正则化（Tikhonov regularization）；<br>$$ l_2 = \frac{1}{2} \lambda ||\omega||^2_2 $$<br>其中 $\lambda$ 控制正则项大小，取值越大对模型复杂度的约束程度越大；<br>一般将该l2惩罚项加入到目标函数中，通过目标函数的误差反向传播；        </p><p><strong>L1正则化</strong>：<br>又称Elastic网络正则化；<br>$$ l_1 = \lambda ||\omega||_1 = \sum_i |\omega_i| $$<br>L1正则化不仅能够约束参数量级，还可以使参数稀疏化，使优化后部分参数置为0，并且也有去除噪声的效果；<br>L1和L2惩罚可以联合使用，如 $ \lambda_1 ||\omega||_1 + \lambda_2 ||\omega||_2^2 $     </p><p>具体实现：<br>tensorflow中很多输出变量的函数（如 <code>get_variable</code>、<code>fully_connected</code>）都接受名为 <code>*_regularizer</code> 的参数进行正则化；      </p><pre class=" language-python"><code class="language-python"><span class="token keyword">with</span> arg_scope<span class="token punctuation">(</span>        <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>        weights_regularizer<span class="token operator">=</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>l1_regularizer<span class="token punctuation">(</span>scale<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 为fully_connected指定L1正则化</span>    hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">)</span>    hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">)</span>    logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">,</span>scope<span class="token operator">=</span><span class="token string">"out"</span><span class="token punctuation">)</span></code></pre><p>注意在计算总的loss时要手动把正则化loss加上——       </p><pre class=" language-python"><code class="language-python">reg_losses <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>REGULARIZATION_LOSSES<span class="token punctuation">)</span>loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>add_n<span class="token punctuation">(</span><span class="token punctuation">[</span>base_loss<span class="token punctuation">]</span> <span class="token operator">+</span> reg_losses<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"loss"</span><span class="token punctuation">)</span></code></pre><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><p>论文：<br>【提出】<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">Improving neural networks by preventing co-adaptation of feature detectors(2012)</a><br>【细节讨论】<a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting(2014)</a>           </p><p>核心思想：<br>在每一步训练中，所有神经单元（包含输入神经单元，但不包含输出神经单元）有一定概率p被忽略（但在预测时使用整个网络）；<br>这里p称为<strong>dropout rate</strong>，通常取 $p=0.5$；<br>对应的还有一个<strong>keep probability</strong>，即 $q = 1 - p$；<br>每步训练时都有一部分单元缺失，使得每个单元都有机会在本次训练中占有一定地位，从而使得各个单元可以更好地从训练集中学习，使整个网络在工作时更有弹性；<br>部分“队友”的缺失、部分“输入数据”的缺失，在这种训练下，每个单元有更强的鲁棒性；<br>由于各个单元是一定概率q参与训练的，所以在需要对这些单元进行一定补偿——       </p><ol><li>可以在训练时，对每个单元的输出都除以q        </li><li>也可以在预测时，为每个单元的输出都乘以q<br>这两种方式虽然不完全等价，但实际效果时差不多的          </li></ol><p>具体实现：<br>tensorflow提供专门的dropout层，默认时用第二种补偿方式；         </p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> dropout<span class="token comment" spellcheck="true"># ...</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>bool<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span>keep_prob <span class="token operator">=</span> <span class="token number">0.5</span>X_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X_drop<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">)</span>hidden1_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1_drop<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">)</span>hidden2_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2_drop<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"outputs"</span><span class="token punctuation">)</span></code></pre><blockquote><p>tensorflow提供了两个dropout()，一个在tensorflow.conrtib.layers包里，一个在tensorflow.nn包里——<br>前者在训练时有效，预测时失效；<br>后者在训练和预测时都是有效的；<br>所以一般来说，我们需要的是前者的dropout       </p></blockquote><p>如果说，在训练中加了dropout还是发现过拟合了，可以考虑增加dropout_rate（减小keep_prob）；        </p><p>缺点：       </p><ul><li>dropout减缓了收敛的速度    </li><li>而且，<strong>dropout对卷积层似乎没有作用</strong>（一般只用于全连接层）<br>  参考 <a href="https://www.quora.com/Why-would-I-need-to-apply-a-dropout-layer-before-a-convolutional-layer" target="_blank" rel="noopener">Why would I need to apply a dropout layer before a convolutional layer? | Quora</a><br>  据说是因为卷积层参数数量远少于全连接层，一般不存在过拟合的问题；<br>  大多数经典框架都只在全连接层上使用dropout，参考《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》<br>  不过，<br>  <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting(2014)</a>、<a href="https://arxiv.org/pdf/1511.07289v3.pdf" target="_blank" rel="noopener">FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)(2016)</a>、<a href="http://torch.ch/blog/2015/07/30/cifar.html" target="_blank" rel="noopener">92.45% on CIFAR-10 in Torch | Torch</a>、<a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/" target="_blank" rel="noopener">Using convolutional neural nets to detect facial keypoints tutorial | danielnouri</a> 等也对卷积层使用了dropout并有一定的提升；<br>  也有一些关于对卷积层使用dropout或者dropout的变体的论文：<br>  <a href="https://arxiv.org/ftp/arxiv/papers/1512/1512.00242.pdf" target="_blank" rel="noopener">Towards Dropout Training for Convolutional Neural Networks(2015)</a>、<br>  <a href="https://arxiv.org/pdf/1411.4280.pdf" target="_blank" rel="noopener">Efficient Object Localization Using Convolutional Networks(2015)</a>、<br>  <a href="http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf" target="_blank" rel="noopener">Analysis on the Dropout Effect in Convolutional Neural Networks(2016)</a>     </li></ul><h3 id="最大范数（Max-Norm）"><a href="#最大范数（Max-Norm）" class="headerlink" title="最大范数（Max-Norm）"></a>最大范数（Max-Norm）</h3><p>每步训练之后，对权重w进行一定约束——<br>$$ w \gets w \frac{r}{||w||_2} $$<br>其中，r为超参数max-norm，$||w||_2$ 表示w的L2范数；        </p><p>减小r将增加惩罚的力度，这将有助于抑制过拟合；<br>同时，如果没有使用BN层，那么max-norm也有抑制梯度消失与爆炸的作用；       </p><p>具体实现：<br>tensorflow没有直接提供max-norm的操作：         </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 借助tf.clip_by_norm构造max_norm操作</span>threshold <span class="token operator">=</span> <span class="token number">1.0</span>clipped_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>clip_by_norm<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clip_norm<span class="token operator">=</span>threshold<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>clip_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clipped_weights<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># [...]</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># [...]</span>        <span class="token keyword">for</span> X_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> zip<span class="token punctuation">(</span>X_batches<span class="token punctuation">,</span> y_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>training_op<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_batch<span class="token punctuation">,</span> y<span class="token punctuation">:</span> y_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 在每一步（训练一个batch）的最后手动调用一次max_norm的操作</span>            clip_weights<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>如果有很多权重需要增加max_norm操作，那代码将变得十分冗余；<br>更加简洁的做法是自己构造一个Regularization——       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">max_norm_regularizer</span><span class="token punctuation">(</span>threshold<span class="token punctuation">,</span> axes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"max_norm"</span><span class="token punctuation">,</span> collection<span class="token operator">=</span><span class="token string">"max_norm"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">max_norm</span><span class="token punctuation">(</span>weights<span class="token punctuation">)</span><span class="token punctuation">:</span>        clipped <span class="token operator">=</span> tf<span class="token punctuation">.</span>clip_by_norm<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clip_norm<span class="token operator">=</span>threshold<span class="token punctuation">,</span> axes<span class="token operator">=</span>axes<span class="token punctuation">)</span>        clip_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> clipped<span class="token punctuation">,</span> name<span class="token operator">=</span>name<span class="token punctuation">)</span>        tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span>collection<span class="token punctuation">,</span> clip_weights<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 注意，这个clip_weights需要在外部使用，所以把它放入collection当中</span>        <span class="token keyword">return</span> None <span class="token comment" spellcheck="true"># 不需要将loss加到整体的全局的loss上，所以只需要返回None</span>    <span class="token keyword">return</span> max_normmax_norm_reg <span class="token operator">=</span> max_norm_regularizer<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">,</span> weights_regularizer<span class="token operator">=</span>max_norm_reg<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 取出collection中的clip_weights操作</span>clip_all_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span><span class="token string">"max_norm"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># [...]</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># [...]</span>        <span class="token keyword">for</span> X_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> zip<span class="token punctuation">(</span>X_batches<span class="token punctuation">,</span> y_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>training_op<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_batch<span class="token punctuation">,</span> y<span class="token punctuation">:</span> y_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>clip_all_weights<span class="token punctuation">)</span></code></pre><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>人为修改训练集数据，达到扩充数据集的目的；<br>但这种修改必须是“可学习的”，比如添加白噪声是没意义的，因为白噪声不可学习；<br>比如做图片分类的时候，可以平移、旋转、缩放图片甚至改变亮度来做到数据增强，使得模型对图片的位置、角度、大小不那么敏感；         </p><p>tensorflow提供了一些图片操纵的函数，可以从api中查到；<br>图像数据的扩充还可以参考 <a href="/2018/04/18/CNN/#数据扩充">卷积神经网络CNN/数据扩充 | Hey~YaHei!</a>      </p><p>此外，还有通过混合训练数据来增强数据的手段，参考《<a href="/2019/03/01/bag-of-tricks3/#数据混合（Mixup）">深度学习小技巧（三）：训练技巧 - 数据混合（Mixup） | Hey~YaHei!</a>》       </p><h3 id="验证集的使用"><a href="#验证集的使用" class="headerlink" title="验证集的使用"></a>验证集的使用</h3><p>验证集用于在训练阶段评测模型预测性能，一般在每轮（epoch）或每个批处理训练（step）后在训练集、验证集上分别做网络前向运算，绘制学习曲线，检验模型泛化能力；<br>比如图像分类准确率：<br>如图a，验证集准确率一直低于训练集准确率，无明显下降趋势，此时模型复杂度欠缺，是为欠拟合；<br>如图b，验证集准确率不仅低于训练集准确率，还有明显下降趋势，此时模型发生过拟合；<br><img src="/Handson-ML/over_fitting.png" alt="over_fitting">    </p><p>其他模型评估方法可参考 <a href="/ML-ZhouZhihua/《机器学习》Ch02.html">机器学习 - 周志华/Ch02 模型评估和选择 | Hey~YaHei!</a><br>以及 <a href="/ML-Andrew/ML-Andrew-notes2.html">机器学习 - 吴恩达/2 过拟合与正则化技术 | Hey~YaHei!</a>     </p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li>重叠池化（Overlapping Pooling）<br>  《<a href="/2018/05/07/漫谈池化层/#重叠池化（Overlapping-Pooling）">漫谈池化层 - 重叠池化| Hey~YaHei!</a>》      </li><li>随机池化（Stochastic Pooling）<br>  《<a href="/2018/05/07/漫谈池化层/#随机池化（Stochastic-Pooling）">漫谈池化层 - 随机池化| Hey~YaHei!</a>》</li><li>平滑标签（Label Smoothing Regularization, LSR）<br>  《<a href="/2019/03/01/bag-of-tricks3/#平滑标签（Label-Smoothing-Regularization-LSR）">深度学习小技巧（三）：训练技巧 - 平滑标签 | Hey~YaHei!</a>》</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>优化器</title>
      <link href="/2018/04/10/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
      <url>/2018/04/10/%E4%BC%98%E5%8C%96%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=466795188&auto=0&height=32"></iframe><br>BGM：<strong>《文豪野犬》ED1</strong><br>唔，就是它，因为这个番才认识太宰治和芥川          </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap11<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><h3 id="常见的加速训练技术"><a href="#常见的加速训练技术" class="headerlink" title="常见的加速训练技术"></a>常见的加速训练技术</h3><ul><li><a href="/2018/04/08/梯度消失与梯度爆炸/#Xavier-Initialization-Glorot-Initialization">恰当的的权重初始化策略</a>    </li><li><a href="/2018/04/08/梯度消失与梯度爆炸/#relu——nonsaturating-activation-function">恰当的激活函数</a>      </li><li><a href="/2018/04/08/梯度消失与梯度爆炸/#批量归一化（Batch-Normalization-BN）">批量归一化</a></li><li><a href="/2018/04/09/复用预训练层">复用部分预训练网络</a></li><li><a href="/2018/04/10/优化器">使用更快的优化器</a></li></ul><p>常见的优化器有：Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam optimization<br>其中<a href="#Adam-optimization">Adam optimization</a>是目前表现最好的优化器；不过它除了学习率之外还有额外两个超参数需要手工调整      </p><h3 id="传统梯度下降"><a href="#传统梯度下降" class="headerlink" title="传统梯度下降"></a>传统梯度下降</h3><p>$$ \theta \gets \theta - \eta \bigtriangledown_\theta J(\theta) $$<br>固定的下降速度（梯度作为速度）        </p><h3 id="动量法（Momentum-optimization）"><a href="#动量法（Momentum-optimization）" class="headerlink" title="动量法（Momentum optimization）"></a>动量法（Momentum optimization）</h3><p>论文：<a href="https://www.researchgate.net/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods" target="_blank" rel="noopener">Some methods of speeding up the convergence of iteration methods(1964)</a>      </p><p>$$ m \gets \beta m + \eta \bigtriangledown_\theta J(\theta), 0 &lt;= \beta &lt;= 1 $$<br>$$ \theta \gets \theta - m $$<br>梯度作为加速度使用，这里m表示动量；<br>引入新的超参数 $\beta$ 与先前的 $m$ 相乘，以继承先前的动量，并防止动量过快增长；<br>当$\beta=0$时为完全摩擦（退化为传统梯度下降），当$\beta=1$时为完全光滑，通常来说取$\beta=0.9$<br>此时下降速度变为：$ \frac{1}{1-\beta} \eta \bigtriangledown_\theta J(\theta) $<br>取$\beta=0.9$时，下降速度理论上变为传统梯度下降的10倍！！     </p><p>具体实现：</p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>MomentumOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></code></pre><h3 id="涅斯捷罗夫加速梯度（Nesterov-Accelerated-Gradient-NAG）"><a href="#涅斯捷罗夫加速梯度（Nesterov-Accelerated-Gradient-NAG）" class="headerlink" title="涅斯捷罗夫加速梯度（Nesterov Accelerated Gradient, NAG）"></a>涅斯捷罗夫加速梯度（Nesterov Accelerated Gradient, NAG）</h3><p>又称<strong>Nesterov Momentum optimization</strong><br>论文：<a href="https://www.researchgate.net/publication/257291640_A_method_of_solving_a_convex_programming_problem_with_convergence_rate_O1k2" target="_blank" rel="noopener">A method of solving a convex programming problem with convergence rate O(1/k^2)(1983)</a>        </p><p>$$ m \gets \beta m + \eta \bigtriangledown_\theta J(\theta + \beta m), 0 &lt;= \beta &lt;= 1 $$<br>$$ \theta \gets \theta - m $$<br>动量法的变体，比动量法更快，而且振荡更小；<br>与动量法相比，区别在于计算的是损失函数在 $\theta + \beta m$ 上而不是 $\theta$ 上的梯度：<br><img src="/Handson-ML/11.3Nesterov.png" alt="Nesterov">            </p><p>具体实现：      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 与动量法的优化器一样，只不过打开了nesterov开关</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>MomentumOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> use_nesterov<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>论文：<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="noopener">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization(2011)</a>          </p><p>$$ s \gets s + \bigtriangledown_\theta J(\theta) \otimes \bigtriangledown_\theta J(\theta) $$<br>$$ \theta \gets \theta - \eta \bigtriangledown_\theta J(\theta) \oslash \sqrt{s+\epsilon} $$      </p><p>其中，<br>$\otimes$ 表示逐项相乘，$\oslash$ 表示逐项相除；<br>$s$ 继承上一次的 $s$，并且累加一个梯度的平方（如果沉浸在某个权重方向上，那么更新速度会越来越快）；<br>$\theta$ 的更新与传统梯度下降类似，不过更新时梯度除以一个 $\sqrt{s+\epsilon}$，$\epsilon$ 通常取 $10^{-10}$ 防止 $s=0$ 的情况；        </p><p>考虑两权重梯度差异显著的情况（如图，$\theta_1$ 平缓，$\theta_2$ 陡峭），<br>传统的梯度下降，会在陡峭的 $\theta_2$ 上很快到达一个比较的位置，而平缓的 $\theta_1$ 方向上梯度下降缓慢；<br>而AdaGrad能够检测到这种特殊情况，从而加速梯度下降<br><img src="/Handson-ML/11.3AdaGrad.png" alt="AdaGrad">            </p><p>简而言之，AdaGrad衰减了学习率，但是会在沉浸的权重方向上加速更新；<br>其学习过程中，学习率是自适应的，这有助于更佳直接地找到全局最优点，而且更容易调整超参数 $\eta$        </p><p>tensorflow中提供了相应的 <code>AdagradOptimizer</code> 优化器<br>AdaGrad在一些简单的任务如线性回归上可能会比较高效         </p><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>该优化器由eoffrey Hinton在其课堂上提出，而没有形成正式的论文，研究者通常用 <strong>slide 29 in lecture 6</strong> 来引用它；<br>幻灯片：<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a><br>视频：<a href="https://www.youtube.com/watch?v=defQQqkXEfE&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=29" target="_blank" rel="noopener">https://www.youtube.com/watch?v=defQQqkXEfE&amp;list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&amp;index=29</a><br>AdaGrad的变体，解决了AdaGrad学习率衰减偏快的问题        </p><p>$$ s \gets \beta s + (1-\beta) \bigtriangledown_\theta J(\theta) \otimes \bigtriangledown_\theta J(\theta) $$<br>$$ \theta \gets \theta - \eta \bigtriangledown_\theta J(\theta) \oslash \sqrt{s+\epsilon} $$      </p><p>通常，$\beta$ 取0.9      </p><p>具体实现：    </p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>RMSPropOptimizer<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> decay<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span></code></pre><h3 id="Adam-optimization"><a href="#Adam-optimization" class="headerlink" title="Adam optimization"></a>Adam optimization</h3><p>论文：<a href="https://arxiv.org/pdf/1412.6980v8.pdf" target="_blank" rel="noopener">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION(2015)</a><br>集成动量法和RMSProp的思想——      </p><p>$$ m \gets \beta_1 m + (1-\beta_1) \bigtriangledown_\theta J(\theta) $$<br>$$ s \gets \beta_2 s + (1-\beta_2) \bigtriangledown_\theta J(\theta) \otimes \bigtriangledown_\theta J(\theta) $$<br>$$ m \gets \frac{m}{1-\beta_1^T} $$<br>$$ s \gets \frac{s}{1-\beta_2^T} $$<br>$$ \theta \gets \theta - \eta m \oslash \sqrt{s + \epsilon} $$</p><p>其中，T表示迭代次数（从1记起）         </p><p>通常，取 $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}, \eta = 0.001$</p><h3 id="Jacobian优化-amp-Hessian优化"><a href="#Jacobian优化-amp-Hessian优化" class="headerlink" title="Jacobian优化 &amp; Hessian优化"></a>Jacobian优化 &amp; Hessian优化</h3><p>以上讨论都是基于一次偏导的Jacobian矩阵，实际上用基于二次偏导的Hessian矩阵可以取得更好的优化效果；<br>但这很难应用到DNN上，因为每层都会输出 $n^2$ 个Hessian矩阵（如果是Jacobian，只需要n个），其中n是参数数量，而DNN参数数量大的惊人，所以基于Hessian的优化反而会因为计算大量的Hessian矩阵而速度下降并且需要大量的空间来进行计算；        </p><h3 id="训练稀疏模型"><a href="#训练稀疏模型" class="headerlink" title="训练稀疏模型"></a>训练稀疏模型</h3><p>有时候需要一个占据空间少、预测时间短的稀疏模型，可以采用以下技术进行稀疏化：       </p><ol><li>用常规的方式训练模型，然后将数值很小的权重置为0       </li><li>用L1惩罚项进行正则化         </li><li>使用Dual Averaging技术（也叫Follow The Regularized Leader, FTRL）<br> 论文：<a href="https://scholar.google.fr/citations?view_op=view_citation&amp;citation_for_view=DJ8Ep8YAAAAJ:Tyk-4Ss8FVUC" target="_blank" rel="noopener">Primal-dual subgradient methods for convex problems(2009)</a><br> tensorflow也提供了相应的优化器 <code>FTRLOptimizer</code>，它是根据<a href="https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf" target="_blank" rel="noopener">Ad Click Prediction: a View from the Trenches(2013)</a>实现的，是FTRL的变体        </li></ol><h3 id="学习计划（learning-schedules）"><a href="#学习计划（learning-schedules）" class="headerlink" title="学习计划（learning schedules）"></a>学习计划（learning schedules）</h3><p>如图，当学习率太低时训练过慢，学习率太高时不容易得到最优解；<br>可以在开始训练时给一系列不同的学习率跑一小段时间，比较它们的loss曲线，来确定一个比较合适的学习率；<br><img src="/Handson-ML/11.3learning_rate.png" alt="">     </p><p>除此之外，还有一些有用的策略，称为学习计划learning schedules——       </p><ul><li>分段常数学习率（Predetermined piecewise constant learning rate）<br>  预设好每过多少epochs就改变学习率；需要比较多的人工调整      </li><li>性能调整（Performance scheduling）<br>  每N步用验证集计算一次loss，当loss不再减小的时候衰减学习率（通常乘以一个因子 $\gamma$）    </li><li>随时间指数衰减（Exponential scheduling）<br>  $$ \eta(t) = \eta_0 10^{-t/r}, t为迭代次数 $$<br>  需要人工调整参数 $\eta_0$ 和r，学习率每r步衰减一次；       </li><li>随时间乘方衰减（Power scheduling）<br>  $$ \eta(t) = \eta_0 (1+t/r)^{-c} $$<br>  通常取 $c=1$，有点类似Exponential scheduling，但衰减速度稍微慢一些</li></ul><p>论文<a href="http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/40808.pdf" target="_blank" rel="noopener">AN EMPIRICAL STUDY OF LEARNING RATES IN DEEP NEURAL NETWORKS FOR SPEECH RECOGNITION(2013)</a> 比较了一些主流的学习计划——<br>在语音识别任务中使用动量法优化的前提下，         </p><ol><li>performance scheduling和exponential scheduling都表现的很好        </li><li>但是exponential scheduling容易实现、容易调整、收敛更快       </li></ol><p>具体实现：</p><pre class=" language-python"><code class="language-python">initial_learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>decay_steps <span class="token operator">=</span> <span class="token number">10000</span>decay_rate <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span><span class="token number">10</span>global_step <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>learning_rate <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>exponential_decay<span class="token punctuation">(</span>initial_learning_rate<span class="token punctuation">,</span> global_step<span class="token punctuation">,</span> decay_steps<span class="token punctuation">,</span> decay_rate<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>MomentumOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> global_step<span class="token operator">=</span>global_step<span class="token punctuation">)</span></code></pre><p>由于AdaGrad, RMSProp和Adam optimization等优化器具有自适应调整学习率的能力，因而不需要额外的学习计划来衰减学习率；</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>复用预训练层</title>
      <link href="/2018/04/09/%E5%A4%8D%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B1%82/"/>
      <url>/2018/04/09/%E5%A4%8D%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E5%B1%82/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=544000838&auto=0&height=32"></iframe><br>BGM：<strong>《Megalo Box》ED</strong><br>NakamuraEmi的歌有种说不出来的特别      </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap11<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><h3 id="迁移学习（transfer-learn）"><a href="#迁移学习（transfer-learn）" class="headerlink" title="迁移学习（transfer learn）"></a>迁移学习（transfer learn）</h3><p>如果已经训练好了一个网络（如可以识别猫、狗等动物），如果需要训练一个新的类似任务的网络（如只识别猫），可以直接使用已有网络的一部分底层，在这些层的基础上加几个层，训练时固定复用层的权重，只训练新加的几个层；     </p><ol><li>可以加速训练过程     </li><li>可以使用较小的训练集    </li><li>但是要求新网络的输入数据大小与复用网络的输入数据大小保持一致</li><li>仅适用于数据的低层次特征相类似的任务      </li><li>任务越接近，可以复用的底层越多；甚至非常接近的任务，可以只替换output层</li><li>如果需要进一步fine-tune，可以在充分训练新加的几个层之后，再整体训练一段时间（复用层的权重也参与训练）     </li></ol><h3 id="复用tensorflow模型"><a href="#复用tensorflow模型" class="headerlink" title="复用tensorflow模型"></a>复用tensorflow模型</h3><p>如果复用整个模型，直接用 <code>tf.Saver</code> 的 <code>restore</code> 方法即可；<br>如果只复用模型的一部分，可以借助 <code>tf.get_collection</code> 函数——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 创建初始化op</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建收集op</span><span class="token comment" spellcheck="true"># ... 第一个参数为key，指明获取哪些对象；key在tf.GraphKeys中有一系列定义（这里表示获取可训练的变量）</span><span class="token comment" spellcheck="true"># ... 第二个参数为scope，接受一个正则表达式，作为一个过滤器来筛选特定scope的对象</span><span class="token comment" spellcheck="true"># ...... 这里表示获取名为hidden1, hidden2, hidden3的三个scope中的所有对象</span><span class="token comment" spellcheck="true"># ... 返回一个对象名称组成的列表</span>reuse_vars <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>TRAINABLE_VARIABLES<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden[123]"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个用于读取原模型的Saver</span><span class="token comment" spellcheck="true"># ... tf.train.Saver的第一个参数为var_list，接受一个字典（一系列键值对）</span><span class="token comment" spellcheck="true"># ...... 在restore时，只从原模型中读出名字与键对应的对象，并且在当前模型中命名为值</span>reuse_vars_dict <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>var<span class="token punctuation">.</span>name<span class="token punctuation">,</span> var<span class="token punctuation">.</span>name<span class="token punctuation">)</span> <span class="token keyword">for</span> var <span class="token keyword">in</span> reuse_vars<span class="token punctuation">]</span><span class="token punctuation">)</span>original_saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span>reuse_vars_dict<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建优化器和训练op</span><span class="token comment" spellcheck="true"># ... 只训练hidden4和输出层，而固定（freezing）hidden[123]的权重</span><span class="token comment" spellcheck="true"># ...... 此时hidden[123]称为frozen layers</span><span class="token comment" spellcheck="true"># ... tf.get_collection获取需要训练的变量列表，并将列表传递给优化器来指定训练的变量</span>train_vars <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_collection<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>GraphKeys<span class="token punctuation">.</span>TRAINABLE_VARIABLES<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden4|outputs"</span><span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>train_vars<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个用于保存新模型的Saver</span>new_saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>init<span class="token punctuation">)</span>    original_saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span><span class="token string">"./my_original_model.ckpt"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># .....训练略sess.run(training_op).....</span>    new_saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"./my_new_model.ckpt"</span><span class="token punctuation">)</span></code></pre><h3 id="复用其他框架的模型"><a href="#复用其他框架的模型" class="headerlink" title="复用其他框架的模型"></a>复用其他框架的模型</h3><ol><li>用其他框架的API获得其模型下的权重     </li><li>创建相应的placeholder占位符    </li><li>创建相应的网络结构      </li><li>创建op操作，将placeholder赋值给网络结构里的权重变量      </li><li>session运行时将已有权重投喂给placeholder      </li></ol><h3 id="加速训练：复用frozen层的输出结果（牺牲空间）"><a href="#加速训练：复用frozen层的输出结果（牺牲空间）" class="headerlink" title="加速训练：复用frozen层的输出结果（牺牲空间）"></a>加速训练：复用frozen层的输出结果（牺牲空间）</h3><p>由于frozen层权重固定，对一个训练集来说，每一组训练数据在frozen层的最终输出是固定的；<br>训练时我们往往需要在重复的数据上训练很多趟，所以如果记录下frozen层的最终输出，<br>可以避免数据在frozen层进行重复计算，从而达到加速训练的目的，<br><em>但相应地会占据大量的内存空间（取决于数据集大小和frozen最后一层输出的大小）</em>          </p><p>具体实现——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ... 假设hidden[123]固定，训练hidden4和outputs ...</span>n_epochs <span class="token operator">=</span> <span class="token number">100</span>n_batches <span class="token operator">=</span> <span class="token number">500</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 先投喂整个数据集来获得frozen层的最终输出</span>    hidden3_outputs <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>hidden3<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_train<span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 分趟训练</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 获取打乱的训练数据索引集（permutation相当于arange和shuffle的混合）</span>        shuffled_idx <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>permutation<span class="token punctuation">(</span>len<span class="token punctuation">(</span>hidden3_outputs<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 构造本趟训练的batch，这里不再直接使用训练集作为输入，而是从前边计算好的hidden3的输出直接输入给下一层</span>        <span class="token comment" spellcheck="true"># ... np.array_split将一个列表等分成若干份</span>        <span class="token comment" spellcheck="true"># ...... np.array可以给定一个索引列表，从中抽取出对应索引的元素组成一个新的列表</span>        <span class="token comment" spellcheck="true"># ...... 如：</span>        <span class="token comment" spellcheck="true"># ......... a = np.arange(10)</span>        <span class="token comment" spellcheck="true"># ......... a[[0,2,4,6]]</span>        <span class="token comment" spellcheck="true"># ......... >> array([0,2,4,6])</span>        hidden3_batches <span class="token operator">=</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>hidden3_outputs<span class="token punctuation">[</span>shuffled_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> n_batches<span class="token punctuation">)</span>        y_batches <span class="token operator">=</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>y_train<span class="token punctuation">[</span>shuffled_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> n_batches<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 对各个batch依次进行训练</span>        <span class="token keyword">for</span> hidden3_batch<span class="token punctuation">,</span> y_batch <span class="token keyword">in</span> zip<span class="token punctuation">(</span>hidden3_batches<span class="token punctuation">,</span> y_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>            sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>training_op<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>hidden3<span class="token punctuation">:</span> hidden3_batch<span class="token punctuation">,</span> y<span class="token punctuation">:</span> y_batch<span class="token punctuation">}</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 直接向hidden3投喂数据</span></code></pre><blockquote><p>np.random.permutation 和 np.random.shuffle的区别：<a href="https://blog.csdn.net/u010099080/article/details/73395601" target="_blank" rel="noopener">https://blog.csdn.net/u010099080/article/details/73395601</a><br>… shuffle只接受list，然后打乱list内的元素并返回<br>… permutation既接受list也接受int<br>…… 如果是list，相当于shuffle( list )<br>…… 如果时int，相当于shuffle( arange(int) )      </p></blockquote><h3 id="Model-Zoos"><a href="#Model-Zoos" class="headerlink" title="Model Zoos"></a>Model Zoos</h3><p>Tensorflow提供了很多现成了主流网络模型，都包含在 <code>tensorflow.contrib.slim.python.slim.nets</code> 包中；    </p><p>tensorflow/models：<br><a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">https://github.com/tensorflow/models</a>          </p><p>caffe提供了更全面的主流网络模型：<br><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">https://github.com/BVLC/caffe/wiki/Model-Zoo</a>       </p><p>Saumitro Dasgupta写了一个转换器，可以方便地将caffe网络转换为tensorflow网络模型：<br><a href="https://github.com/ethereon/caffe-tensorflow" target="_blank" rel="noopener">https://github.com/ethereon/caffe-tensorflow</a>         </p><h3 id="无监督预训练"><a href="#无监督预训练" class="headerlink" title="无监督预训练"></a>无监督预训练</h3><p>如果有一个训练任务，没有类似任务的现成模型可以复用，只有少量标注好的数据和大量未标注的数据，可以——         </p><ol><li>继续标注数据       </li><li>如果标注繁琐或者成本过高，可以使用无监督预训练的方式【如<a href="/2018/04/25/autoencoder/#用自编码器作无监督预训练">用自编码器作无监督预训练</a>】<br> 论文：<a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/35536.pdf" target="_blank" rel="noopener">Why Does Unsupervised Pre-training Help Deep Learning?(2009)</a><br> 使用无监督训练，从底层开始逐层训练各个隐藏层，每次只训练一个层而固定其他层的权重；<br> 无监督训练可以使用Restricted BoltzmannMachines (RBMs）、autoencoders等，目前autoencoders用的更多些；<br> 最后用监督训练的方式，fine-tune较高的layers：<br> <img src="/Handson-ML/11.2unsupervised_pretrain.png" alt="unsupervised pretrain"></li><li>寻找一个训练数据易收集且易标注的类似任务，先训练出一个模型来复用给这个任务     </li><li>在辅助任务上进行预训练，比如：       <ul><li>想训练一个人脸识别的模型，为每一个注册者获取上百张人脸照片是不切实际的。<br>  可以先获取大量随机人脸的照片，训练一个模型来检测两张人脸图片是否对应同一个人；<br>  该任务得到一个比较好的特征检测器，复用该任务的底层权重，进而用少量数据来训练出特定人脸的识别模型      </li><li>想为某个语言处理任务训练一个模型。<br>  可以先获取大量的语句，训练一个分辨语法是否正确的模型。<br>  （比如说把这些语句先都标记为good，然后打乱语序标记为bad进行训练）<br>  该任务得到一个有一定语言能力的模型，复用该任务的底层权重，进而用少量数据来训练目标任务的模型       </li><li><strong>Max Margin Learning</strong>，训练一个打分模型<br>  <em>SVM就是基于Max Margin Learning的一种分类器</em><br>  为预测结果进行打分，用损失函数来训练一个模型，使得预测的好结果比坏结果的分数高于某个阈值</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>梯度消失与梯度爆炸</title>
      <link href="/2018/04/08/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"/>
      <url>/2018/04/08/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=668472&auto=0&height=32"></iframe><br>BGM：<strong>《钢之炼金术师FA》OP1</strong><br>这个吉他混音版真是太棒了~       </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap11<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li><a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>      </li></ol><h3 id="解决梯度爆炸和消失的常用技术"><a href="#解决梯度爆炸和消失的常用技术" class="headerlink" title="解决梯度爆炸和消失的常用技术"></a>解决梯度爆炸和消失的常用技术</h3><ol><li>随机初始化（Xavier Initialization、He Initialization等）</li><li>使用nonsaturating函数（如relu）</li><li>批量归一化（Batch Normalization, BN）</li><li>梯度裁剪（Gradient Clipping）   </li></ol><h3 id="Xavier-Initialization-Glorot-Initialization"><a href="#Xavier-Initialization-Glorot-Initialization" class="headerlink" title="Xavier Initialization(Glorot Initialization)"></a>Xavier Initialization(Glorot Initialization)</h3><p>论文：<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Understanding the difficulty of training deep feedforward neural networks(2010)</a><br>作者Xavier建议：<strong>使每一层的输入输出的方差相等，而且正反向传播的梯度也相等</strong>       </p><p>并针对sigmoid激活函数（logistic激活函数）提出一种初始化方式：     </p><ol><li>各权重用均值为0的正态分布随机数进行初始化，并且标准差根据输入、输出的维度确定——<br> $$ \sigma = \sqrt{ \frac{2}{n_{inputs} + n_{outputs} } } $$       </li><li>用[-r, r]的均匀分布随机数进行初始化——<br> $$ r = \sqrt{ \frac{6}{n_{inputs} + n_{outputs}} } $$        </li></ol><p>此外，另外一篇论文<a href="https://arxiv.org/pdf/1502.01852v1.pdf" target="_blank" rel="noopener">Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification(2015)</a><br>作者He提出类似的关于其他激活函数的初始化建议：           </p><ul><li>tanh<br>  $$ \sigma = 4 \sqrt{ \frac{2}{n_{inputs} + n_{outputs} } } $$<br>  $$ r = 4 \sqrt{ \frac{6}{n_{inputs} + n_{outputs}} } $$        </li><li>relu及其变体（He Initialization）<br>  $$ \sigma = \sqrt{2} \sqrt{ \frac{2}{n_{inputs} + n_{outputs} } } $$<br>  $$ r = \sqrt{2} \sqrt{ \frac{6}{n_{inputs} + n_{outputs}} } $$        </li></ul><p>数据敏感的参数初始化方式：<br>是一种根据自身任务数据集量身定制的参数初始化方式；<br>论文：<a href="https://arxiv.org/pdf/1511.06856.pdf" target="_blank" rel="noopener">Data-dependent Initializations of Convolutional Neural Networks(2016)</a><br>代码：<a href="https://github.com/philkr/magic_init" target="_blank" rel="noopener">philkr/magic_init | github</a>     </p><h3 id="relu——nonsaturating-activation-function"><a href="#relu——nonsaturating-activation-function" class="headerlink" title="relu——nonsaturating activation function"></a>relu——nonsaturating activation function</h3><ul><li>优势      <ul><li>sigmoid和tanh是saturate函数——<a href="https://blog.csdn.net/whu_paprika/article/details/54085670" target="_blank" rel="noopener">【机器学习】saturate的解释 | CSDN</a><br>  sigmoid函数介绍参见<a href="/note_for_MLA/Ch05 logRegres.html">机器学习实战 - 逻辑回归 | Hey~YaHei!</a><br>  sigmoid函数与tanh函数线性相关，具体参见<a href="https://www.zhihu.com/question/50396271?from=profile_question_card" target="_blank" rel="noopener">在神经网络中，激活函数sigmoid和tanh除了阈值取值外有什么不同吗？| 知乎</a><br>  sigmoid、tanh、relu的比较参见<a href="https://blog.csdn.net/zchang81/article/details/70224688" target="_blank" rel="noopener">深度学习——激活函数Sigmoid/Tanh/ReLU | CSDN</a><br>  sigmoid的输出限定在[0, 1]之间；<br>  tanh的输出限定在[-1, 1]之间；<br>  而relu的输出为[0, +∞)，为nonsaturae函数</li><li>计算非常简单、快速     </li></ul></li><li>存在问题（dying relus）<br>  当输入的加权和为负数时，relu将出现“死亡”而开始不停地输出0（因为当输入为负数时relu的梯度一直是0），最终导致网络崩溃；        </li><li><p>变体（解决die问题）<br>  论文 <a href="https://arxiv.org/pdf/1505.00853.pdf" target="_blank" rel="noopener">Empirical Evaluation of Rectified Activations in Convolution Network(2015)</a> 比较了各种不同变体的表现</p><ul><li><strong>leaky relu</strong><br>  $$ LeakyReLU_\alpha(z) = max(\alpha z, z) $$<br>  其中 $\alpha$ 使 $z&lt;0$ 时有一个小梯度，使得relu不会彻底“死亡”，在接下来的训练中有可能被“复活”；<br>  通常 $\alpha$ 取0.01      </li><li><strong>randomized leaky relu（RReLU）</strong><br>  leaky relu的变体，其中 $\alpha$ 在训练中是一个随机数，最终测试时固定为一个平均值        </li><li><strong>parametric leaky relu（PReLU）</strong><br>  leaky relu的变体，其中 $\alpha$ 作为模型的一个参数参与训练<br>  论文指出，PReLU在大型图片数据集上有很好的表现，但在小数据集上很快就过拟合   </li><li><p><strong>exponential linear unit（ELU）</strong><br>  论文 <a href="https://arxiv.org/pdf/1511.07289v5.pdf" target="_blank" rel="noopener">FAST AND ACCURATE DEEP NETWORK LEARNING BY EXPONENTIAL LINEAR UNITS (ELUS)(2016)</a> 指出，ELU可以减少训练时间，并且在测试集上有更好的表现<br>  $$\begin{equation}<br>  ELU_\alpha (z) = \left\{<br>  \begin{array}{rcl}<br>  \alpha (e^z - 1) &amp; &amp; ,z &lt; 0\\<br>  z &amp; &amp; ,z&gt;=0<br>  \end{array} \right.<br>  \end{equation}$$    </p><ul><li>ELU在 $z&lt;0$ 时有负数输出，使得单元的平均输出更接近于0，这有助于缓解梯度消失的问题     </li><li>$\alpha$ 通常取1，但也可以采用其他方式来确定它的值      </li><li>当 $z&lt;0$ 时有非0梯度，有助于避免dying relu问题     </li><li>函数光滑，有助于加速梯度下降（而且当 $\alpha=1$ 时，函数在 $z=0$ 上可导）     </li><li>ELU因为多了指数运算，其计算要比relu慢，但是因为能更快收敛，所以在训练有一定的补偿；不过在测试时，还是会比较慢     </li></ul></li></ul></li><li>选择     <ul><li>一般来说：ELU &gt; leaky ReLU（及其变体） &gt; ReLU &gt; tanh &gt; logistic         </li><li>优先使用<strong>ELU</strong>，但如果需要考虑预测的开销，那么可以使用<strong>leaky ReLU</strong>及其变体       </li><li>如果还要进一步节省时间和计算力，可以使用交叉验证来评估不同的激活函数      </li><li>如果网络出现过拟合而不想花时间去调参和测试，可以使用<strong>RReLU</strong>     </li><li>如果你拥有非常大的数据集，那也可以直接使用<strong>PReLU</strong>    </li></ul></li><li>使用<br>  tensorflow有预置的elu函数     <pre class=" language-python"><code class="language-python">  hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu<span class="token punctuation">)</span></code></pre>  虽然tensorflow没有预置的leaky relus，但是定义起来很容易     <pre class=" language-python"><code class="language-python">  <span class="token keyword">def</span> <span class="token function">leaky_relu</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0.01</span> <span class="token operator">*</span> z<span class="token punctuation">,</span> z<span class="token punctuation">,</span> name<span class="token operator">=</span>name<span class="token punctuation">)</span>  hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>leaky_relu<span class="token punctuation">)</span></code></pre></li></ul><h3 id="批量归一化（Batch-Normalization-BN）"><a href="#批量归一化（Batch-Normalization-BN）" class="headerlink" title="批量归一化（Batch Normalization, BN）"></a>批量归一化（Batch Normalization, BN）</h3><p>随机初始化可以在训练的开始显著减少梯度爆炸和消失的问题，但它不能保证训练过程中不再出现；<br>论文 <a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift(2015)</a> 提出了批量归一化技术来解决这个问题，以及Internal Covariate Shift问题（训练过程中，随着前一层参数的变化，本层的输入分布也随之发生变化）；   </p><p>BN有一种<strong>特征归一化</strong>（Feature Normalization, FN）的变体；<br>作用于网络最后一层的特征表示上（FN随后接的是目标函数层），用于提高习得特征的分辨能力；<br>可用于人脸识别、行人重检测、车辆重检测等任务上；<br>论文：<a href="https://arxiv.org/pdf/1703.08388.pdf" target="_blank" rel="noopener">DeepVisage: Making face recognition simple yet with powerful generalization skills(2017)</a>        </p><p>此外除了BN之外，还有其他各种各样的Normalization，参考《<a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">详解深度学习中的Normalization，BN/LN/WN | 知乎, Juliuszh</a>》。      </p><h4 id="区分：归一化、正则化、标准化"><a href="#区分：归一化、正则化、标准化" class="headerlink" title="区分：归一化、正则化、标准化"></a>区分：归一化、正则化、标准化</h4><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29974820" target="_blank" rel="noopener">机器学习里的黑色艺术：normalization, standardization, regularization</a><br><strong>归一化（Normalization）</strong>：数据预处理，将数据限定在特定范围内，消除量纲对建模的影响；<br><strong>标准化（Standardization）</strong>：数据预处理，使数据符合标准正态分布；<br><strong>正则化（Regularization）</strong>：在损失函数中添加惩罚项，增加建模模糊性，将建模关注点转移到整体趋势上；      </p><h4 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h4><p>归一化的方式有很多种：      </p><ol><li>最大最小值归一化 $x’ = \frac{x - min}{max - min}$</li><li>对数归一化 $ x’ = \frac{lg(x)}{lg(max)} $</li><li>反正切归一化 $ x’= \frac{2arctan(x)}{\pi} $</li><li>零平均归一化 $ x’ = \frac{x - mean}{std} $</li></ol><p>根据论文，该技术在每层的<strong>激活函数之前添加一个BN操作</strong>，使得输入的数据以零点为中心、归一化；并且在每一层中使用两个新的参数来调整输出的范围，使得模型的每一层能够学习到适宜的表示范围和平均值；<br>该算法需要评估每一个输入的mini-batch的平均值和标准差；       </p><p>对于某个mini-batch的输入，归一化的具体过程如下：<br>$$ \mathbb{ x^{(i)} } = \frac{ x^{(i)} - \mu_B }{ \sqrt{\sigma^2_B + \epsilon} } $$<br>$$ z^{(i)} = \gamma \mathbb{ x^{(i)} } + \beta $$<br>其中，<br>$\mu_B$ 和 $\sigma_B$ 分别是该mini-batch的平均数和标准差；<br>$\epsilon$ 是一个很小的数（通常取0.001），用于避免 $\sigma = 0$ 导致分母为0的情况；<br>$\gamma$ 和 $\beta$ 分别是每一层中的两个新的参数，调整后的 $\mathbb{x^{(i)}}$ 通过线性变化后得到归一化的 $z^{(i)}$ 输出      </p><p>在预测阶段因为不再有mini-batch，所以直接使用在整个训练集的平均数、标准差进行计算即可；<br>所以在训练时每层会增加四个参数：$\gamma, \beta, \mu, \sigma$ ，训练时采用移动平均的方式可以高效地获得在整个训练集各层的平均值和标准差；   </p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><ol><li>可以很好的解决梯度爆炸和消失的问题，甚至saturate函数如sigmoid和tanh都可以在深度网络中正常使用； </li><li>网络对权重初始化不再那么敏感    </li><li>可以使用更大的学习率，提高训练速度      </li><li>具有正则化的效果，可以减少dropout等其他正则化技术的使用</li><li>缺点：BN操作增加了模型的复杂度，预测时间将不可避免地增加<br> 但是，在训练完成后，BN层可以合并到前一层的卷积层或全连接层，具体参见《<a href="/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析 - BN层合并 | Hey~YaHei!</a>》        </li></ol><h4 id="在tensorflow中使用BN"><a href="#在tensorflow中使用BN" class="headerlink" title="在tensorflow中使用BN"></a>在tensorflow中使用BN</h4><p>tensorflow提供了现成的BN层操作： <code>tensorflow.contrib.layers.batch_norm</code><br>可以直接作为参数传递给 <code>fully_connected</code> 全连接网络： </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> batch_norm<span class="token comment" spellcheck="true"># ...</span><span class="token comment" spellcheck="true"># 训练标志</span><span class="token comment" spellcheck="true"># ... 如前所述，BN在训练和预测时模型略有不同</span><span class="token comment" spellcheck="true"># ... 训练时，模型需要每次都对mini-batch计算平均值和标准差，并用滑动平均的方式记录整个训练集上的平均值和标准差</span><span class="token comment" spellcheck="true"># ... 预测时则直接使用整个训练集上的平均值和标准差</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>bool<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># BN操作的参数，包括——</span><span class="token comment" spellcheck="true"># ... is_training：标志训练与否状态的占位符</span><span class="token comment" spellcheck="true"># ... decay：使学习率下降的一个因子，v &lt;- v*decay + v*(1-decay)</span><span class="token comment" spellcheck="true"># ... updates_collections：</span>bn_params <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'is_training'</span><span class="token punctuation">:</span> is_training<span class="token punctuation">,</span>    <span class="token string">'decay'</span><span class="token punctuation">:</span> <span class="token number">0.99</span><span class="token punctuation">,</span>    <span class="token string">'updates_collections'</span><span class="token punctuation">:</span> None<span class="token punctuation">}</span><span class="token comment" spellcheck="true"># 指定全连接网络使用BN操作</span>hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope <span class="token operator">=</span> <span class="token string">"hidden1"</span><span class="token punctuation">,</span>                  normalizer_fn <span class="token operator">=</span> batch_norm<span class="token punctuation">,</span> normalizer_params <span class="token operator">=</span> bn_params<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">,</span>                  normalizer_fn <span class="token operator">=</span> batch_norm<span class="token punctuation">,</span> normalizer_params <span class="token operator">=</span> bn_params<span class="token punctuation">)</span>logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"outputs"</span><span class="token punctuation">,</span>                  normalizer_fn <span class="token operator">=</span> batch_norm<span class="token punctuation">,</span> normalizer_params <span class="token operator">=</span> bn_params<span class="token punctuation">)</span></code></pre><p>如果觉得每次都需要指定归一化的函数和参数太麻烦，可以借助 <code>tf.contrib.framework.arg_scope()</code> 来简化代码：     </p><pre class=" language-python"><code class="language-python">bg_params <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token comment" spellcheck="true"># ...}</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>framework<span class="token punctuation">.</span>arg_scope<span class="token punctuation">(</span>        <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token comment" spellcheck="true"># 第一个参数，为with块内统一参数的函数列表（这里只为fully_connected函数统一参数）</span>        normalizer_fn<span class="token operator">=</span>batch_norm<span class="token punctuation">,</span>     <span class="token comment" spellcheck="true"># 对于其他参数，关键字为函数对应的关键字参数名，值为对该参数赋予的值</span>        normalizer_params<span class="token operator">=</span>bn_params<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 在该with块中的fully_connected函数就无需再重复指明归一化的函数和参数</span>    hidden1 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden1"</span><span class="token punctuation">)</span>    hidden2 <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"hidden2"</span><span class="token punctuation">)</span>    logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token string">"outputs"</span><span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p>这里<code>batch_norm</code> 层默认是不带scale的，如果有需要，可以在参数 <code>bn_params</code> 添加一项 <code>&quot;scale&quot;: True</code> 开启；     </p><h3 id="梯度裁剪（Gradient-Clipping）：简单粗暴"><a href="#梯度裁剪（Gradient-Clipping）：简单粗暴" class="headerlink" title="梯度裁剪（Gradient Clipping）：简单粗暴"></a>梯度裁剪（Gradient Clipping）：简单粗暴</h3><p>论文：<a href="http://proceedings.mlr.press/v28/pascanu13.pdf" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks(2012)</a><br>直接限制梯度不能超过一个阈值，在RNN中用的比较多；<br>具体实现：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment" spellcheck="true"># 限制阈值</span>threshold <span class="token operator">=</span> <span class="token number">1.0</span><span class="token comment" spellcheck="true"># 创建优化器</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>GradientDescentOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用optimizer.compute_gradients计算梯度</span>grads_and_vars <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>compute_gradients<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用tf.clip_by_value裁剪梯度grad，如果小于-threshold则取-threshold，如果大于threshold则取threshold</span><span class="token comment" spellcheck="true"># 变量var不变</span>capped_gvs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>clip_by_value<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> <span class="token operator">-</span>threshold<span class="token punctuation">,</span> threshold<span class="token punctuation">)</span><span class="token punctuation">,</span> var<span class="token punctuation">)</span>          <span class="token keyword">for</span> grad<span class="token punctuation">,</span> var <span class="token keyword">in</span> grads_and_vars<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 用optimizer.apply_gradients更新梯度</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>capped_gvs<span class="token punctuation">)</span></code></pre><p>对于优化器，<code>minimize</code> 方法包含 <code>compute_gradients</code> 和 <code>apply_gradients</code>，计算梯度后直接更新；<br>由于需要裁剪梯度，所以需要单独地使用 <code>compute_gradients</code> 和 <code>apply_gradients</code></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Handson-ML</title>
      <link href="/2018/04/08/Handson-ML/"/>
      <url>/2018/04/08/Handson-ML/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=34609001&auto=0&height=32"></iframe><br>BGM：<strong>《血界战线》ED</strong><br>中文名叫《方糖歌曲和苦味舞步》 好像也有叫 《甜蜜情歌和苦涩舞步》 的；<br>原版网易云没版权，这是双声道版，也还行~~这贝斯手很灵性emmm       </p><hr><p>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>》笔记<br>目前结合毕设，主要只看TensorFlow部分，也就是DL的部分——       </p><ul><li><a href="/Handson-ML/9up_and_running_with_tensorflow.html?_blank">09 Up and Running with Tensorflow</a><br>  tensorflow的基本使用     <ul><li><a href="/Handson-ML/9.1Linear_Regression(Normal_Equation).html?_blank">Jupyter - Linear_Regression(Normal_Equation)</a><br>  正规方程实现线性回归      </li><li><a href="/Handson-ML/9.2Linear_Regression(Gradient_Descent).html?_blank">Jupyter - Linear_Regression(Gradient_Descent)</a><br>  梯度下降实现线性回归</li></ul></li><li>10 Introduction to Artificial Neural Networks<br>  简单的神经网络      <ul><li><a href="/Handson-ML/10.1DNN_MNIST(High-level_API).html?_blank">Jupyter - DNN_MNIST(High-level_API)</a><br>  高层API操作实现DNN来完成手写体识别</li><li><a href="/Handson-ML/10.2DNN_MNIST(Plain).html?_blank">Jupyter - DNN_MNIST(Plain)</a><br>  底层操作实现DNN来完成手写体识别，宽度、深度、激活函数的选择      </li></ul></li><li>11 Training Deep Neural Nets<br>  深层神经网络训练中的问题与相关的解决技术<br>  常见的配置为：<br>  初始化（Initialization）：<strong>He Initialization</strong><br>  激活函数（Activation function）：<strong>ELU</strong><br>  归一化（Normalization）：<strong>Batch Normalization, BN</strong><br>  正则化（Regularization）：<strong>Dropout</strong><br>  优化器（Optimizer）：<strong>Adam</strong><br>  学习计划（Learning rate schedule）：<strong>无（Adam具备自适应学习率）</strong>          <ul><li><a href="/2018/04/08/梯度消失与梯度爆炸?_blank">Vanishing/Exploding Gradients Problem</a><br>  梯度爆炸与梯度消失：随机初始化、nonsaturating函数、批量归一化、梯度裁剪      </li><li><a href="/2018/04/09/复用预训练层?_blank">Reusing Pretrained Layers</a><br>  复用与训练层：加速训练过程、解决标注数据少等问题        </li><li><a href="/2018/04/10/优化器?_blank">Faster Optimizers</a><br>  优化器（Momentum, NAG, AdaGrad, RMSProp, Adam）等、训练稀疏模型、学习计划         </li><li><a href="/2018/04/11/正则化技术?_blank">Avoiding Overfitting Through Regularization</a><br>  正则化技术：提前终止、L1和L2范数惩罚、Dropout、最大范数、数据增强等         </li></ul></li><li>12 Distributing TensorFlow Across Devices and Servers<br>  暂略</li><li><a href="/2018/04/18/CNN?_blank">13 Convolutional Neural Networks</a><br>  CNN原理、CNN的tensorflow实现、CNN资源占用计算、几个常见的CNN典型框架      </li><li><a href="/2018/04/22/RNN?_blank">14 Recurrent Neural Networks</a><br>  <em>RNN原理【留坑待填】</em>、RNN的tensorflow实现、应用（词嵌入、机器翻译）         </li><li><a href="/2018/04/25/autoencoder?_blank">15 Autoencoders</a><br>  自编码器原理、自编码器的实现与训练、不同约束下的自编码器       </li><li>16 Reinforcement Learning<br>  暂略</li></ul><p>其他：<a href="/Handson-ML/CS20SI小记.pdf">CS20SI小记</a>         </p><hr><p>2018-04-25<br>呀嘿，真是高产的四月。<br>DL部分基本算是读完了吧，除了分布计算和强化学习的部分，先暂时搁着；<br>还有些地方尤其是原理的部分还需要填坑完善，一边读论文或者其他书籍一边慢慢填上这些坑吧~      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>【搁置】机器学习-吴恩达</title>
      <link href="/2017/09/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
      <url>/2017/09/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
      
        <content type="html"><![CDATA[<p>做毕设，传统ML的学习暂时搁置~<br>唔~跟周志华的《机器学习》同步施工<br>教学视频源：<a href="https://www.bilibili.com/video/av9912938/?_blank" target="_blank" rel="noopener">【中英双语】机器学习（Machine Learning）- 吴恩达（Andrew Ng）</a>      </p><ul><li><a href="/ML-Andrew/ML-Andrew-notes1.html?_blank">1 绪论、线性回归与逻辑回归</a></li><li><a href="/ML-Andrew/ML-Andrew-notes2.html?_blank">2 过拟合与正则化技术</a></li><li><a href="/ML-Andrew/ML-Andrew-notes3.html?_blank">3 非线性分类器——神经网络</a></li><li><a href="/ML-Andrew/ML-Andrew-notes4.html?_blank">4 算法改进</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>【搁置】机器学习-周志华</title>
      <link href="/2017/08/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/"/>
      <url>/2017/08/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/</url>
      
        <content type="html"><![CDATA[<p>做毕设，传统ML的学习暂时搁置~<br>《机器学习》周志华 - 清华大学出版社     </p><ul><li><a href="/ML-ZhouZhihua/《机器学习》Ch01.html?_blank">Ch01 绪论</a></li><li><a href="/ML-ZhouZhihua/《机器学习》Ch02.html?_blank">Ch02 模型估计与选择</a>    </li><li><a href="/ML-ZhouZhihua/《机器学习》Ch03.html?_blank">Ch03 线性模型</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hello World~</title>
      <link href="/2017/08/24/hello%20world/"/>
      <url>/2017/08/24/hello%20world/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=592080&auto=0&height=32"></iframe>     <p>BGM：<strong>《Code Geass反叛的鲁路修》S01E14、S02E03插入曲</strong>，也算是夏丽的角色曲吧。<br><!--S01E14插入是在鲁路修行动时无意中把夏丽父母卷入战争致死，夏丽在喜欢鲁路修、却又得知鲁路修即是zero也是害死自己父母的罪魁祸首，感到十分矛盾与痛苦。为此鲁路修不得已用gease抹除了夏丽这方面的记忆；        S02E03插入是在夏丽死的时候，夏丽因为gease被清除，回想起之前的事情，在矛盾与痛苦中被鲁路修的“弟弟”洛洛（也是个悲惨的角色）杀死，几乎也是这个时候，鲁路修彻底告别自己平凡的一面；         歌曲是夏丽对平凡的鲁鲁修的爱情的内心描述，这个回音般的音效还是很有特色的，跟专辑的名字《Angel Feather Voice》一样，有点像天使的声音。       --></p><hr><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前两年虚拟机崩了<br>博客上的数据全丢，而且没有备份，心疼<br>没有md文件的存根（只有少数几份在win上有备份），只剩下个编译后的网站——<a href="http://zkkzkk368.github.io?_blank" target="_blank" rel="noopener">Hey!YaHei~</a><br>最近没啥学习的动力，重操旧业把博客搭起来玩玩吧<br>这次依旧用<a href="https://hexo.io/?_blank" target="_blank" rel="noopener">hexo框架</a><br><a href="https://github.com/iissnan/hexo-theme-next?_blank" target="_blank" rel="noopener">NexT主题</a>也更换为<a href="https://github.com/viosey/hexo-theme-material?_blank" target="_blank" rel="noopener">Material主题</a>（这个主题看起来还不错）<br>前者其实更加简约，但它的首页感觉有些难受<br>额外搞了个<a href="https://github.com/ele828/hexo-prism-plugin?_blank" target="_blank" rel="noopener">prism</a>语法高亮插件，美滋滋……     </p><p>主要更学习笔记吧，其实自己的笔记别人未必看得懂<br>欢迎订阅RSS（以QQ邮箱为例）：<br><img src="/imgs/RSS_demo.png" alt="RSS_demo">     </p><p>兴许偶尔会写点别的~~<br>估计没人会来看，纯属自娱自乐    </p><h3 id="原博客目录"><a href="#原博客目录" class="headerlink" title="原博客目录"></a>原博客目录</h3><p>（带星号的表示新博客上也有一样的文章）    </p><ul><li>2014-10-29 <a href="http://zkkzkk368.github.io/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93?_blank" target="_blank" rel="noopener">*第一二期作业总结</a><br>  当初大一俱乐部的作业     </li><li>2015-02-05 <a href="http://zkkzkk368.github.io/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93?_blank" target="_blank" rel="noopener">*居然……还我数据？！？！</a><br>  折腾linux的时候躺过的坑    </li><li>2015-03-23 <a href="http://zkkzkk368.github.io/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF?_blank" target="_blank" rel="noopener">*俱乐部春纳网页后端小结</a><br>  大一俱乐部纳新网站制作小结    </li><li>2015-04-05 <a href="http://zkkzkk368.github.io/2015/04/06/fedora%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE?_blank" target="_blank" rel="noopener">fedora安装与配置</a><br>  闲着无聊把ubuntu换成了fedora     </li><li>2015-04-19 <a href="http://zkkzkk368.github.io/2015/04/20/Hexo%E7%9A%84%E4%BD%BF%E7%94%A8?_blank" target="_blank" rel="noopener">Hexo的使用</a><br>  嗯~第一次搭博客哈哈      </li><li>2015-04-22 <a href="http://zkkzkk368.github.io/2015/04/23/python%E7%88%AC%E8%99%AB%E5%88%9D%E6%8E%A2?_blank" target="_blank" rel="noopener">python爬虫初探</a><br>  第一次写爬虫    </li><li>2015-05-07 <a href="http://zkkzkk368.github.io/2015/05/08/C%E8%AF%AD%E8%A8%80%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%BC%96%E7%A8%8B%EF%BC%88WIN%EF%BC%89?_blank" target="_blank" rel="noopener">C语言图形化编程（WIN）</a><br>  C大程写大作业前的学习小结    </li><li>2015-05-10 <a href="http://zkkzkk368.github.io/2015/05/11/javascript%E7%AC%94%E8%AE%B0?_blank" target="_blank" rel="noopener">JS笔记</a><br>  闲来无事小补了一发js，后来写数据结构的bonus居然用上了   </li><li>2015-06-02 <a href="http://zkkzkk368.github.io/2015/06/03/%E7%AE%80%E5%8D%95%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%A7%A3%E6%9E%90%E5%99%A8?_blank" target="_blank" rel="noopener">简单的正则表达式解析器</a><br>  《代码之美》里一段有趣的代码    </li><li>2015-08-11 <a href="http://zkkzkk368.github.io/2015/08/12/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7?_blank" target="_blank" rel="noopener">《vim实用技巧》笔记</a><br>  已烂尾…    </li><li>2015-08-30 <a href="http://zkkzkk368.github.io/2015/08/31/%E3%80%8AC%E4%B8%93%E5%AE%B6%E7%BC%96%E7%A8%8B%E3%80%8B%E5%B0%8F%E8%AE%B0?_blank" target="_blank" rel="noopener">《C专家编程》小记</a><br>  二刷《C专家编程》，随手记了点东西   </li><li>2015-08-30 <a href="http://zkkzkk368.github.io/2015/08/31/%E6%8C%87%E9%92%88&amp;%E6%95%B0%E7%BB%84?_blank" target="_blank" rel="noopener">指针&amp;数组</a>        </li><li>2015-09-20 <a href="http://zkkzkk368.github.io/2015/09/20/Note_for_Python?_blank" target="_blank" rel="noopener">python笔记</a></li><li>2015-09-20 <a href="http://zkkzkk368.github.io/2015/09/20/Note_For_Linux?_blank" target="_blank" rel="noopener">《鸟哥的Linux私房菜》笔记</a></li><li>2015-10-17 <a href="http://zkkzkk368.github.io/2015/10/17/%E3%80%8AJava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E3%80%8B%E7%AC%94%E8%AE%B0?_blank" target="_blank" rel="noopener">《Java核心技术》笔记</a><br>  烂尾…      </li></ul><h3 id="更新日志"><a href="#更新日志" class="headerlink" title="更新日志"></a>更新日志</h3><h4 id="2017-07-24"><a href="#2017-07-24" class="headerlink" title="2017-07-24"></a>2017-07-24</h4><!--> 『情』束缚的不是『头脑』，而是『内心』。> ——《东京暗鸦》11卷5章2节，字野耕平    -->      <p>唉~     </p><h4 id="2017-08-27"><a href="#2017-08-27" class="headerlink" title="2017-08-27"></a>2017-08-27</h4><p>由于markdown与mathjax的语法冲突，mathjax的_时常被markdown错误的渲染<br>故修改<code>node_modules/marked/lib/marked.js</code>文件中的正则表达式来放弃markdown对_的渲染<br>参考：<a href="http://www.cnblogs.com/Ai-heng/p/7282110.html" target="_blank" rel="noopener">hexo博客MathJax公式渲染问题</a>    </p><h4 id="2018-04-25"><a href="#2018-04-25" class="headerlink" title="2018-04-25"></a>2018-04-25</h4><p>修改Material的模板文件<code>themes/material/layout/layout.ejs</code>——<br>在末尾追加添加相关js代码；<br>将mardown语法 <code>[text](url?_blank)</code> 转换成带属性 <code>target=&quot;_blank</code> 的 <code>&lt;a&gt;</code> 标签，<br>弥补markdown不支持“从新标签打开链接”的功能——     </p><pre class=" language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span>    <span class="token keyword">var</span> aTagArr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">.</span>slice<span class="token punctuation">.</span><span class="token function">apply</span><span class="token punctuation">(</span>document<span class="token punctuation">.</span><span class="token function">getElementsByTagName</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    aTagArr<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>e<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> e<span class="token punctuation">.</span>href<span class="token punctuation">.</span><span class="token function">lastIndexOf</span><span class="token punctuation">(</span><span class="token string">"_blank"</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>            e<span class="token punctuation">.</span>target <span class="token operator">=</span> <span class="token string">"_blank"</span><span class="token punctuation">;</span>            e<span class="token punctuation">.</span>href <span class="token operator">=</span> e<span class="token punctuation">.</span>href<span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">"?_blank"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span></code></pre><h4 id="2018-08-24"><a href="#2018-08-24" class="headerlink" title="2018-08-24"></a>2018-08-24</h4><ul><li>更新Material至1.5.6版本<br>  这个版本有点小bug，修复方式参见 <a href="https://github.com/viosey/hexo-theme-material/issues/686" target="_blank" rel="noopener">Issue #686 | github</a> 中michaelcai的回复；       </li><li>引入评论系统<br>  LiveRe评论系统，海外服务，可能加载会稍微慢点，偶尔也不大稳定（加载不出来），坐等LiveRe提供大陆服务吧       </li><li>更改字体源至ustc<br>  原先从google获取，有时候会加载不出来       </li><li>从bootcdn获取jquery和mathjax<br>  提高页面加载速度         </li></ul><h4 id="2019-03-06"><a href="#2019-03-06" class="headerlink" title="2019-03-06"></a>2019-03-06</h4><p>参考：<br>《<a href="https://blog.csdn.net/StaunchKai/article/details/82901437" target="_blank" rel="noopener">hexo 博客开启 https (SSL 证书) | CSDN, staunchKai</a>》<br>《<a href="https://www.jianshu.com/p/8046a12fec4a" target="_blank" rel="noopener">Hexo 博客 之 添加 https | 简书, evenyao</a>》       </p><ul><li>将<code>www.hey-yahei.cn</code>重定向到<code>hey-yahei.cn</code></li><li>新增子域名<code>yahei.zjuarm.club</code>（重定向至<code>hey-yahei.cn</code>）<br>  小彩蛋(?)：<a href="http://zjuarm.club" target="_blank" rel="noopener">http://zjuarm.club</a>           </li><li>增加ssl证书，并强制重定向至https</li></ul>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>【搁置】《机器学习实战》笔记</title>
      <link href="/2017/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/"/>
      <url>/2017/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p><strong>暂时搁着……</strong><br>师兄说先学理论再看实战QAQ，先暂时搁着吧<br>以下页面均由jupyter生成     </p><p>目录：   </p><ul><li>Ch01 机器学习基础   </li><li><a href="/note_for_MLA/Ch02 kNN.html?_blank">Ch02 k-邻近算法</a></li><li><a href="/note_for_MLA/Ch03 trees.html?_blank">Ch03 决策树</a></li><li><a href="/note_for_MLA/Ch04 bayes.html?_blank">Ch04 基于概率论的分类方法：朴素贝叶斯</a></li><li><a href="/note_for_MLA/Ch05 logRegres.html?_blank">Ch05 Logistic回归</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>QT学习之路</title>
      <link href="/2017/01/29/QT%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/"/>
      <url>/2017/01/29/QT%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br>做srtp的时候看的一波资料<br><a href="/others/QT学习之路.pdf">QT学习之路.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《你好放大器》笔记</title>
      <link href="/2016/08/01/%E4%BD%A0%E5%A5%BD%E6%94%BE%E5%A4%A7%E5%99%A8/"/>
      <url>/2016/08/01/%E4%BD%A0%E5%A5%BD%E6%94%BE%E5%A4%A7%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/你好放大器.pdf">你好放大器.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 2 硬件 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《C++编程思想》笔记</title>
      <link href="/2016/07/03/C++%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
      <url>/2016/07/03/C++%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/C++编程思想.pdf">C++编程思想.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>心理学导论</title>
      <link href="/2016/03/09/%E5%BF%83%E7%90%86%E5%AD%A6%E5%AF%BC%E8%AE%BA/"/>
      <url>/2016/03/09/%E5%BF%83%E7%90%86%E5%AD%A6%E5%AF%BC%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/心理学导论.pdf">心理学导论.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>python-sh</title>
      <link href="/2015/09/20/sh%E6%A8%A1%E5%9D%97/"/>
      <url>/2015/09/20/sh%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="sh模块"><a href="#sh模块" class="headerlink" title="sh模块"></a>sh模块</h2><p><a href="http://amoffat.github.io/sh/" target="_blank" rel="noopener">sh官方文档</a>  </p><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="直接使用命令对应的函数"><a href="#直接使用命令对应的函数" class="headerlink" title="直接使用命令对应的函数"></a>直接使用命令对应的函数</h4><p>如：<code>print(sh.ls(&quot;/&quot;))</code>  </p><ol><li>命令参数分别以函数参数的形式给出<br> 如：<code>tar(&quot;cvf&quot;, &quot;/tmp/test.tar&quot;, &quot;/my/home/directory/&quot;)</code><br> 即执行linux命令<code>tar -cvf /tmp/test.tar /my/home/directory/</code>   </li><li>命令名中如果出现横线<code>-</code>则其对应的函数名应改为下划线<code>_</code><br> 如：linux命令<code>google-chrome</code>对应函数<code>google_chrome</code>  </li></ol><h4 id="自定义命令函数"><a href="#自定义命令函数" class="headerlink" title="自定义命令函数"></a>自定义命令函数</h4><p>如：  </p><pre><code>    lscmd = sh.Command(&quot;/bin/ls -l&quot;)      lscmd(&quot;/&quot;)</code></pre><p>即将带参数linux命令<code>/bin/ls -l</code>“封装”成<code>lscmd()</code>  </p><h4 id="提供参数的两种形式"><a href="#提供参数的两种形式" class="headerlink" title="提供参数的两种形式"></a>提供参数的两种形式</h4><p>以linux命令<code>curl http://duckduckgo.com/ -o page.html --silent</code>为例  </p><ol><li>以关键词参数的形式给出<br> <code>sh.curl(&quot;http://duckduckgo.com/&quot;, o=&quot;page.html&quot;, silent=True)</code>  </li><li>以分割的字符串的形式给出<br> <code>sh.curl(&quot;http://duckduckgo.com/&quot;, &quot;-o&quot;, &quot;page.html&quot;, &quot;--silent&quot;)</code>  </li></ol><h4 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h4><p>命令默认前台运行（阻塞式，block），如果将命令转至后台（非阻塞式，doesn’t block），添加关键词参数<code>_bg=True</code>即可<br>如：<code>sh.sleep(3, _bg=True)</code>  </p><h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><p>使用linux中的管道命令，直接用函数的嵌套调用即可<br>如：<code>sh.wc( sh.ls(&quot;/etc&quot;, &quot;-l&quot;), &quot;-l&quot; )</code><br>即linux命令<code>ls /etc -l | wc -l</code>  </p><p>默认情况下，被传递的命令完成后才会传递给管道命令<br>如上例中<code>ls</code>完成后才会传递并执行<code>wc</code>命令   </p><p>给被传递的命令函数加入参数<code>_piped=True</code>后，可以使两个命令同时执行，被传递的命令不断得将新产生的返回信息传递给管道命令<br>如上例中不等<code>ls</code>完成即会传递并执行<code>wc</code>命令，<code>ls</code>不断产生新的信息并不断传递给<code>wc</code>命令   </p><h4 id="数据流重定向"><a href="#数据流重定向" class="headerlink" title="数据流重定向"></a>数据流重定向</h4><p>使用参数<code>_out={filename}</code>和<code>_err={filename}</code>分别将标准输出和错误输出重定向到指定文件中<br>如果文件已存在，默认将清空文件内容后重新写入<br>如：<code>sh.ls(_out=&quot;files.list&quot;)</code>  </p><h4 id="标准输入stdin"><a href="#标准输入stdin" class="headerlink" title="标准输入stdin"></a>标准输入stdin</h4><p>可以给一个命令函数提供标准输入<br>只需要提供参数<code>_in={stdin}</code>即可<br>如：<code>print( sh.cat(_in=&quot;hello&quot;) )</code><br>“stdin”不仅可以是字符串，还可以是文件、队列和任何可迭代对象（列表、集合、字典等）  </p><h4 id="子命令的两种形式"><a href="#子命令的两种形式" class="headerlink" title="子命令的两种形式"></a>子命令的两种形式</h4><p>以linux命令<code>sudo ls /root</code>为例  </p><ol><li>使用命令函数下的对应子函数<br> <code>sh.sudo.ls(&quot;/root&quot;)</code>  </li><li>将子命令作为参数给出<br> <code>sh.sudo( &quot;ls&quot;, &quot;/root&quot; )</code>  </li></ol><p><strong>注意：对于sudo命令，用户必须设置NOPASSWD选项使该用户在命令执行时无需再输入密码才能正常执行</strong>  </p><h4 id="命令回传值的接收与处理"><a href="#命令回传值的接收与处理" class="headerlink" title="命令回传值的接收与处理"></a>命令回传值的接收与处理</h4><p>命令函数的返回内容除了其应有的输出，还包含<code>exit_code</code>属性记录命令的回传值（一般正常执行的回传值为0）<br>如：<br><code>print(sh.ls(&quot;/root&quot;))</code>打印ls命令的输出<br><code>print(sh.ls(&quot;/root&quot;).exit_code)</code>打印ls命令的回传值   </p><ul><li>命令执行失败会引起python出现相应的异常，可以借助python的try/except机制捕获并处理异常<br>  回传值为<code>x</code>的命令错误会触发<code>ErrorReturnCode_x</code>的异常<br>  如：回传值为<code>2</code>的命令错误会触发<code>ErrorReturnCode_2</code>的异常  </li><li>有些命令即使正常执行也会报错，可以在调用相应的函数时给出<code>ok_code={ok_code_list}</code>参数来告知哪些回传值是正常的<br>  如：<code>sh.weird_program(_ok_code=[0,3,5])</code>  </li></ul><h4 id="通配符的使用"><a href="#通配符的使用" class="headerlink" title="通配符的使用"></a>通配符的使用</h4><p>将使用通配符的字符串用<code>sh.glob()</code>函数处理（<strong>注意不是<code>glob.glob()</code>函数</strong>）<br>如：<br><code>sh.ls( sh.glob(&quot;*.py&quot;) )</code><br>即linux命令<code>ls *.py</code>  </p><h3 id="进阶使用"><a href="#进阶使用" class="headerlink" title="进阶使用"></a>进阶使用</h3><p>暂略……</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《vim实用技巧》小记</title>
      <link href="/2015/08/11/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>/2015/08/11/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>=。=最近在看《vim实用技巧》<br>对一些内容作点简单的记录，方便以后查阅  </p><h2 id="Vim解决问题的方式"><a href="#Vim解决问题的方式" class="headerlink" title="Vim解决问题的方式"></a>Vim解决问题的方式</h2><ul><li>多用<code>.</code>命令（一个微型宏）来重复一些简单的操作  </li><li>减少无关的移动，形成合理的撤销块  </li><li>善用复合命令减少操作<br>  <code>A</code>    == <code>$a</code>，插入在行末<br>  <code>C</code> == <code>c$</code>，替换行末字符并持续插入<br>  <code>s</code> == <code>cl</code>，替换当前字符并持续插入<br>  <code>S</code> == <code>^c</code>，替换前一字符并持续插入<br>  <code>I</code> == <code>^i</code>，从行首开始编辑<br>  <code>o</code> == <code>A&lt;CR&gt;</code>，在下方插入新行<br>  <code>O</code> == <code>ko</code>，在上方插入新行<br>  <code>……</code>  </li><li>尽可能使修改、移动变得可重复  </li><li>常用的重复与回退操作  <ul><li>一般的修改<code>{edit}</code>：<br>  <code>.</code>重复，<code>u</code>回退  </li><li>行内的查找<code>[f|F|t|T]{char}</code>：<br>  <code>;</code>重复，<code>,</code>回退  下</li><li>文档内的查找<code>[/|?]{pattern}&lt;CR&gt;</code>：<br>  <code>n</code>重复，<code>N</code>回退  </li><li>执行替换<code>:s/target/replacement</code>：<br>  <code>&amp;</code>重复，<code>u</code>回退  </li><li>执行一系列修改<code>qx{changes}q</code>：<br>  <code>@x</code>重复，<code>u</code>回退  </li></ul></li><li><code>.</code>范式<br>  用一个键移动，用另一个键执行的可重复修改操作  </li></ul><h2 id="普通模式"><a href="#普通模式" class="headerlink" title="普通模式"></a>普通模式</h2><ul><li>停顿思考时切换到普通模式</li><li>合理地切分撤销单元（模式间的切换）  </li><li>如果在插入模式中移动的光标，将会产生一个新的撤销单元  </li><li>删除单词<code>daw</code>，可以解读为”delete a word”，该命令可重复  </li><li>简单的算术运算<code>{num}&lt;C-a&gt;</code>加法，<code>{num}&lt;C-x&gt;</code>减法<br>  不需要把光标移动到数字上<br>  运算为num加减当前光标或光标以后的数字<br>  注意：默认情况下0开头的数字会被当作八进制，0x开头则为十六进制<br>  <em>可以通过<code>set nrformats=</code>关闭进制识别（都作为十进制数）</em>  </li><li>重复&amp;次数<br>  删除多个单词有两种风格——  <ol><li><code>dw</code>+<code>.</code>的重复风格<br> 常用风格，使用灵活，方便回退，无需数单词个数  </li><li><code>d2w</code>或<code>2dw</code>的次数风格<br> 多用于删除一整块词组，如<code>a couple of</code>;可以提高撤销单元的连贯性  </li></ol></li><li>操作 = 操作符 + 动作命令{motion}  <ul><li>操作符  <ul><li><code>c</code>：修改  </li><li><code>d</code>：删除</li><li><code>y</code>：复制到寄存器  </li><li><code>g~</code>：字母大小写反转  </li><li><code>gu</code>：字母转换为小写  </li><li><code>gU</code>：字母转换为大写  </li><li><code>&gt;</code>：增加缩进  </li><li><code>&lt;</code>：减小缩进  </li><li><code>=</code>：自动缩进</li><li><code>!</code>：用外部程序过滤{motion}跨越的行</li></ul></li><li>动作命令  <ul><li><code>[h|j|k|l]</code>：左下上右  </li><li><code>[-|+]</code>：上一行（下一行）的非空白字符</li><li><code>0</code>（数字）：行首（含空白）</li><li><code>^</code>：行首（不含空白），即本行第一个非空白字符  </li><li><code>$</code>：行末  </li><li><code>gg</code>：文首  </li><li><code>G</code>：文末  </li><li><code>[f|F|t|T]{char}</code>：右侧（左侧）的第一个char字符</li><li><code>[;|,]</code>：下一个（上一个）”f|F|t|T”的char字符</li><li><code>:{num}</code>：至第num行  </li><li><code>[w|b]</code>：后一个（前一个）或当前单词的头部（尾部）——含符号  </li><li><code>[W|B]</code>：后一个（前一个）或当前单词的头部（尾部）——跳过符号（仅字母和数字包括负号）</li><li><code>............................</code>  </li><li>动作命令可以待有修饰符前缀<ul><li><code>a</code>：即an，表示一个非空白对象  </li><li><code>i</code>：即inner，表示一个内含对象</li></ul></li></ul></li><li>当一个操作符被连续调用两次，就表示作用于当前行<br>  <em>特例：<code>g~~</code>、<code>guu</code>、<code>gUU</code></em>  </li></ul></li><li>操作符待决模式<br>  键入操作符后vim会进入操作符待决模式，此时vim会等待一个动作命令后执行操作<br>  在该模式下可以按<code>&lt;ESC&gt;</code>来退回普通模式<br>  因为该模式的存在，使得自定义操作符及动作指令能够存在  </li></ul><h2 id="插入模式"><a href="#插入模式" class="headerlink" title="插入模式"></a>插入模式</h2><ul><li>单词输入错误，应删除整个单词再重新输入<br>  <em>可以减少以后输入出错</em>  </li><li>删除  <ul><li><code>&lt;C-h&gt;</code>：删除前一个字符，相当于退格键  </li><li><code>&lt;C-w&gt;</code>：删除前一个单词  </li><li><code>&lt;C-u&gt;</code>：删除至行首  </li><li><strong>注意：只能删除本次插入模式下插入的内容</strong>  </li><li><em>这些命令不是插入模式独有，也不是vim独有，在命令行模式、shell中都能使用它们</em>  </li></ul></li><li>切换到普通模式使用<code>&lt;C-[&gt;</code>更加方便  </li><li>插入-普通模式：<code>&lt;C-o&gt;</code><br>  执行一个普通模式命令后，马上就可以返回到插入命令<br>  在插入模式下，通过<code>&lt;C-o&gt;zz</code>迅速把当前行移动到屏幕正中  </li><li>插入模式下的粘贴几个单词<br>  <code>&lt;C-r&gt;{register}</code>：将register号寄存器的内容粘贴到当前位置<br>  <em>普通模式下复制文本默认保存在0号寄存器中</em><br>  <strong>注意：当文本比较多的时候，应切换到普通模式下操作比较合适</strong>  </li><li>运算<br>  <code>&lt;C-r&gt;={expr}&lt;CR&gt;</code>：执行expr运算并且把结果插入到当前位置  </li><li>用字符编码插入字符  <ul><li>十进制ASCII字符：<code>&lt;C-v&gt;{code}</code><br>  只能插入三位十进制数的ASCII码，注意高位补0  </li><li>十六进制unicode字符：<code>&lt;C-v&gt;u{code}</code><br>  插入四位十六进制数的unicode码，注意高位补0  </li><li>通过二合字母插入字符<br>  <code>&lt;C-k&gt;{char1}{char2}</code><br>  二合字母集可以通过<code>:digraphs</code>查看<br>  <code>:digraph-table</code>可以获得二合字母集更详细的信息  </li></ul></li><li>查看当前字符的编码<br>  <code>ga</code>：屏幕下方会显示当前字符的十进制、十六进制、八进制编码信息  </li><li>虚拟替换模式<br>  <code>R</code>：替换模式，<code>&lt;Tab&gt;</code>会作为一个字符被替换<br>  <code>gR</code>：虚拟替换模式，<code>&lt;Tab&gt;</code>会作为多个字符（一般为八个）被逐个替换（直到末尾才替换掉<code>&lt;Tab&gt;</code>）<br>  <code>r</code>和<code>gr</code>同理～<br>  <strong>应尽量使用虚拟替换模式</strong>  </li></ul><h2 id="可视模式"><a href="#可视模式" class="headerlink" title="可视模式"></a>可视模式</h2><ul><li>三种可视模式  <ol><li><code>v</code>：操作字符文本  </li><li><code>V</code>：操作行文本  </li><li><code>&lt;C-v&gt;</code>：操作块文本  </li></ol></li><li>该模式与其他模式不同，是先选择文本后触发命令  </li><li>选择模式<code>&lt;C-g&gt;</code><br>  类似与windows的选择模式，输入的可见字符会替换掉选中的文本，之后vim进入插入模式<br>  这个模式只是为了迎合windows用户，应尽量少用  </li><li><code>gv</code>：重选上次的高亮选区  </li><li>该模式下的<code>.</code>命令有时候会出现异常<br>  <strong>所以应尽可能使用操作命令而不是可视命令</strong>  </li><li>有时候修改文本的范围很难用动作命令表达出来，这时候才用到可视命令  </li><li>修改多行文本时，只有第一行发生变化，只有当返回普通模式后，其他行才会发生变化  </li><li><code>&lt;C-v&gt;</code>不仅可以选中矩形的区域，还可以选中长短不一的块<br>  当在该模式下选中行末部分，可以实现长短不一的块的选择  </li></ul><h2 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h2><ul><li>三种形式的命令行模式  <ol><li>按下<code>:</code>的Ex命令  </li><li>按下<code>/</code>的查找命令  </li><li>用<code>&lt;C-r&gt;=</code>访问表达式寄存器  </li></ol></li><li>指定命令作用范围<ul><li>指定行<br>  <code>:{number}....</code><br>  只包含数字的Ex命令表示跳转<br>  <strong>可以使用特殊符号<code>$</code>表示最后一行，<code>%</code>表示当前文件的所有行</strong>  </li><li>用地址来指定一个范围<br>  <code>:{start},{end}....</code>，执行命令后光标将跳转到end行  </li><li>用高亮选取指定范围<br>  先用可视模式<code>v</code>,<code>V</code>,<code>&lt;C-v&gt;</code>选取高亮区<br>  再按下<code>:</code>，此时命令行会自动填充成<code>:&#39;&lt;,&#39;&gt;</code>，表示作用在高亮区上  </li><li>用模式指定<br>  <code>:/{pattern1},/{pattern2}/....</code><br>  例如：<code>:/&lt;html&gt;/,/&lt;\/html&gt;/</code>指定了html内的所有内容（包含<code>&lt;html&gt;</code>和<code>&lt;/html&gt;</code>）<br>  <em>注意：斜杠<code>/</code>有特殊含义，需要用反斜杠<code>\</code>转义</em>  </li><li>地址偏移<br>  直接对地址进行<code>+</code>或<code>-</code>运算<br>  如：<code>:/&lt;html&gt;/+1,/&lt;\/html&gt;/-1</code>则排除了<code>&lt;html&gt;</code>和<code>&lt;/html&gt;</code>而只包含了其中间的内容<br>  <strong>如果不加数字，默认偏移量为1</strong>  </li><li>特殊符号总结  <ul><li><code>1</code>：第一行  </li><li><code>0</code>：虚拟行，位于第一行上方，常用于插入到首行等功能</li><li><code>$</code>：最后一行  </li><li><code>.</code>：当前行  </li><li><code>%</code>：整个文件，相当于<code>:1,$</code>  </li><li><code>&#39;m</code>：包含位置标记m的行  </li><li><code>&#39;&lt;</code>和<code>&#39;&gt;</code>：高亮选取的起始和结束行  </li></ul></li></ul></li><li>复制命令：<code>:t</code>或<code>:co</code>或<code>:copy</code><br>  <code>:{range}t {address}</code><br>  与普通模式下的<code>y</code>命令不同，该命令不把文本保存到寄存器  </li><li>移动命令：<code>:move</code>或<code>:m</code><br>  <code>:{range}m {address}</code>  </li><li>在指定范围上执行普通模式命令<br>  <code>:{range}normal {commands}</code>，常用于多行的处理<br>  常用的命令如下：  <ul><li><code>:{range}normal .</code>：对多行重复同一操作   </li><li><code>:{range}normal A;</code>：对多行末尾补上分号  </li><li><code>:{range}normal i//</code>：将多行内容注释掉  </li></ul></li><li>重复上一条命令：<code>@:</code>  </li><li>使用<code>&lt;C-o&gt;</code>进入插入-普通模式时，命令记录会跳转到上一条命令，可以借此进行命令记录的跳转</li><li>自动补全  <ul><li><code>&lt;Tab&gt;</code>自动补全命令<br>  多次按<code>&lt;Tab&gt;</code>会正向遍历补全列表的内容  </li><li><code>&lt;S-Tab&gt;</code>反向遍历补全列表，<em>S表示<code>&lt;shift&gt;</code>按键</em>  </li><li><code>&lt;C-d&gt;</code>显示可用的补全列表  </li><li>自定义补全行为  <ul><li>bash shell形式：<code>set wildmode=longest,list</code>  </li><li>zsh形式：<code>set wildmenu</code>和<code>set wildmode=full</code>  <ul><li><code>wildmenu</code>为补全导航列表  </li></ul></li></ul></li></ul></li><li>将当前单词插入到命令行中<br>  常用于替换命令和查看帮助文档<br>  在命令行模式下，<code>&lt;C-r&gt;&lt;C-w&gt;</code>会复制当前单词到命令行中  </li><li>回溯历史命令  <ul><li>回溯所有命令<br>  在<code>:</code>下保持提示符为空按<code>&lt;up&gt;</code>(<code>&lt;C-p&gt;</code>)和<code>&lt;down&gt;</code>(<code>&lt;C-n&gt;</code>)  <ul><li><code>&lt;C-p&gt;`</code><c-n><code>和</code><up><code></code><down><code>的区别    前者方便，但不能过滤命令    解决方法：    在</code>.vim<code>文件中创建映射项</code>cnoremap <c-p> <up><code></code>cnoremap <c-n> <down>`  </down></c-n></up></c-p></down></up></c-n></li></ul></li><li>过滤回溯的命令<br>  如<code>:help&lt;up&gt;</code>回溯以”help”开头的命令  </li><li>命令历史容量设置<br>  缺省只有20<br>  推荐<code>set history=200</code>  </li><li>除了Ex命令，查找命令也会记录下来并保存在另一个文件中   </li></ul></li><li>一次性执行多条命令<br>  用<code>|</code>隔开命令即可  </li><li>命令行窗口<br>  像一个常规的vim缓冲区，其中每一行是命令历史中的一个条目<br>  可以在该窗口下修改命令历史记录，对之前使用过的命令做调整后方便重复利用<br>  <em>当打开命令行窗口时，它始终拥有焦点，除非关闭它，否则无法切换到其他窗口</em>  <ul><li><code>q:</code>：打开Ex命令的命令行窗口   </li><li><code>q/</code>：打开查找命令的命令行窗口  </li><li><code>&lt;C-f&gt;</code>：从命令行模式切换到命令行窗口（起始命令行模式下输入的内容仍然保留下来）  </li></ul></li><li>运行shell命令<br>  <code>!{command}</code>  <ul><li><code>%</code>代表当前文件名  </li><li><code>:shell</code>可以打开一个交互式的shell会话（通过<code>exit</code>退出）<br>  更好的方法是用<code>&lt;C-z&gt;</code>挂起vim所属进程，用<code>fp</code>唤醒挂起的作业<br>  <em>shell下的<code>jobs</code>命令可以查看当前的作业列表</em>  </li></ul></li><li>大量读取或写入命令输入输出<br>  <code>:read !{cmd}</code>将命令输出读取到当前缓冲区中<br>  <code>:write !{cmd}</code>：将当前缓冲区内容作为命令的标准输入   </li><li>借助shell命令过滤指定范围<br>  <code>:{range}!{filter}</code><br>  如用sort命令，<code>:2,$!sort -t &#39;,&#39; -k 2</code>表示第2行到最后一行之间，以逗号为分隔符的第二字段排序  </li><li>常见的和操作缓冲区文本的Ex命令  <ul><li><code>:{range}d {x}</code>：删除指定内容（并保存到寄存器x中）</li><li><code>:{range}y {x}</code>：复制指定内容到寄存器x中  </li><li><code>:{line}put {x}</code>：在指定行后粘贴寄存器x中的内容   </li><li><code>:{range}t {address}</code>：复制指定内容到address处  </li><li><code>:{range}m {address}</code>：移动指定内容到address处  </li><li><code>:{range}join</code>：连接指定行内容  </li><li><code>:{range}normal {commands}</code>：对指定范围执行普通模式命令  </li><li><code>:{range}s/{pattern}/{string}/{flag}</code>：替换  </li><li><code>:{range}global/{pattern}/{cmd}</code>：对指定范围内匹配pattern的所有行执行cmd命令（Ex）  </li></ul></li></ul><h2 id="管理多个文件"><a href="#管理多个文件" class="headerlink" title="管理多个文件"></a>管理多个文件</h2><h3 id="缓冲区列表"><a href="#缓冲区列表" class="headerlink" title="缓冲区列表"></a>缓冲区列表</h3><p>文件读取后在内存缓冲区中  </p><ul><li><code>:ls</code>命令可以列出所有被载入到内存中的缓冲区列表   <ul><li>每个条目开头的数字为系统自动分配而<strong>不可改变</strong>的缓冲区编号</li><li><code>%</code>：当前窗口中可见的缓冲区   </li><li><code>#</code>：轮换文件<br>  <strong>按<code>&lt;C-^&gt;</code>可以在两个文件之间快速轮换</strong>  </li></ul></li><li>缓冲区切换<ul><li>遍历缓冲区列表：  <ul><li>正向移动：<code>:bn</code>或<code>:bnext</code>  </li><li>反向移动：<code>:bp</code>或<code>:bprevious</code>  </li></ul></li><li><code>:buffer {N}</code>：跳转到指定编号的缓冲区  </li><li><code>:buffer {bufname}</code>：跳转到可以被bufname唯一标识的缓冲区（若多个缓冲区具有同一标识，可以通过<code>&lt;Tab&gt;</code>选择）  </li></ul></li><li><code>:bufdo {commands}</code>：对所有缓冲区执行Ex命令  </li><li><p>创建快速遍历缓冲区列表的键盘映射  </p><pre><code>  nnoremap &lt;silent&gt; [b :bp&lt;CR&gt;    nnoremap &lt;silent&gt; ]b :bn&lt;CR&gt;    nnoremap &lt;silent&gt; [B :bfirst&lt;CR&gt;    nnoremap &lt;silent&gt; ]B :blast&lt;CR&gt;  </code></pre></li><li><p>删除<code>:bd</code>或<code>:bdelect</code><br>  <code>:bd {N1,N2,N3....}</code>：删除列出的缓冲区<br>  <code>:{N,M} bd</code>：删除连续的缓冲区  </p></li></ul><h3 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h3><p>对一批文件进行分组，其<strong>文件顺序可调整*</strong>   </p><ul><li>初始的参数列表为启动时vim的文件列表   </li><li><code>:args</code>：查看参数列表<br>  其中<code>[]</code>表明了当前的活动文件  </li><li><code>:args {arglist}</code>：填充参数列表<br>  arglist可以是文件名、通配符、shell命令的输出结果等   <ul><li>用文件名指定文件<br>  <code>:args {file1,file2,....}</code>  </li><li>用Glob模式指定文件<br>  <code>*</code>：表示0到无穷多个字符，但不包含子目录<br>  <code>**</code>：表示0到无穷多个字符，包含子目录  </li><li>用反引号结构指定文件<br>  将shell命令用反引号括起来，其输出将填充到相应的位置<br>  如：<code>:args \</code>cat .chapters``  </li></ul></li><li>隐藏缓冲区<br>  缓冲区列表中，<code>+</code>代表缓冲区被修改过，此时切换缓冲区会弹出错误信息，可以用感叹号强制切换，此时列表中被标记为<code>a</code>的为活动缓冲区（active），被标记为<code>h</code>的为隐藏活动区（hidden）  <ul><li>处理隐藏缓冲区  <ul><li><code>:w</code>或<code>:write</code>：写入磁盘  </li><li><code>:e</code>或<code>:edit</code>：从磁盘中读入到缓冲区（回滚修改操作）  </li><li><code>:qa</code>或<code>:qall</code>：关闭所有窗口，放弃所有修改  </li><li><code>:wa</code>或<code>:wall</code>：全部写入磁盘   </li></ul></li><li>用<code>:argdo</code>或<code>:bufdo</code>修改一组缓冲区<br>  前提：<strong>打开<code>hidden</code>选项</strong><br>  打开后，对已修改的缓冲区执行<code>:next</code>,<code>:bnext</code>,<code>cnext</code>等命令无需再加感叹号   </li></ul></li></ul><h3 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h3><ul><li>分割  <ul><li><code>&lt;C-w&gt;s</code>：水平切分（同高度），新窗口仍为当前缓冲区   </li><li><code>&lt;C-w&gt;v</code>：垂直切分（同宽度），新窗口仍未当前缓冲区  </li><li><code>:sp {file}</code>或<code>:split {file}</code>：水平切分，新窗口为file内容   </li><li><code>:vsp {file}</code>或<code>:vsplit {file}</code>：垂直切分，新窗口为file内容  </li></ul></li><li>切换  <ul><li><code>&lt;C-w&gt;w</code>：轮换   </li><li><code>&lt;C-w&gt;{h|j|k|l}</code>：切换到左（下上右）侧的窗口  </li><li>若为GVIM，可以直接点击对用窗口进行切换    </li></ul></li><li>关闭  <ul><li><code>:clo</code>或<code>:close</code>或<code>&lt;C-w&gt;c</code>：关闭当前窗口   </li><li><code>:on</code>或<code>:only</code>或<code>&lt;C-w&gt;o</code>：关闭其他重口   </li></ul></li><li>改变窗口大小和重排列  <ul><li><code>&lt;C-w&gt;=</code>：使所有窗口等宽、等高  </li><li><code>&lt;C-w&gt;_</code>：使当前窗口高度最大化  </li><li><code>&lt;C-w&gt;|</code>：使当前窗口宽度最大化   </li><li><code>{N}&lt;C-w&gt;_</code>：使当前窗口高度调整为N行  </li><li><code>{N}&lt;C-w&gt;|</code>：使当前窗口宽度调整为N列  </li><li>若为GVIM，可以直接用鼠标拖动窗口的分界线   </li></ul></li></ul><h3 id="标签页"><a href="#标签页" class="headerlink" title="标签页"></a>标签页</h3><p>对窗口进行分组   </p><ul><li><code>:lcd {path}</code>：设置工作路径<br>  限定标签页的工程范围  <ul><li>只能改变当前窗口，而不是当前标签页  </li><li>如果要改变标签页的所有窗口，应用<code>:windo lcd {path}</code>   </li></ul></li><li>打开和关闭      <ul><li><code>:tabe {file}</code>或<code>tabedit {file}</code>：在新标签页中打开file  </li><li><code>&lt;C-w&gt;T</code>：将当前窗口移动到一个新标签页   </li><li><code>:tabc</code>或<code>:tabclos</code>：关闭当前标签页及其所有窗口   </li><li><code>:tabo</code>或<code>:tabonly</code>：关闭其他标签页及其所有窗口   </li></ul></li><li>切换  <ul><li>标签页从1开始编号   </li><li><code>:tabn {N}</code>或<code>:tabnext {N}</code>或<code>{N}gt</code>：切换到N号标签页   </li><li><code>:tabn</code>或<code>:tabnext</code>或<code>gt</code>：切换到下一标签页  </li><li><code>:tabp</code>或<code>:tabprevious</code>或<code>gT</code>：切换到上一标签页  </li></ul></li><li>重排列  <ul><li><code>:tabmove {N}</code>：将当前标签页移动到N号标签页之后  <ul><li><code>:tabmove 0</code>：表示移动到开头   </li><li><code>:tabmove</code>：表示移动到末尾   </li></ul></li><li>若为GVIM，可以直接用鼠标拖曳</li></ul></li></ul><h2 id="打开及保存文件"><a href="#打开及保存文件" class="headerlink" title="打开及保存文件"></a>打开及保存文件</h2><p>……待施工……</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.3 Linux </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>俱乐部春纳网页后端小结</title>
      <link href="/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF/"/>
      <url>/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="ubuntu下开发环境的搭建"><a href="#ubuntu下开发环境的搭建" class="headerlink" title="ubuntu下开发环境的搭建"></a>ubuntu下开发环境的搭建</h2><h3 id="php-apache-mysql-安装"><a href="#php-apache-mysql-安装" class="headerlink" title="php + apache + mysql 安装"></a>php + apache + mysql 安装</h3><pre><code>sudo apt-get install apache2sudo apt-get install libapache2-mod-php5 php5    sudo apt-get install mysql-server mysql-client</code></pre><p>参考资料：<a href="http://www.cnblogs.com/lynch_world/archive/2012/01/06/2314717.html" target="_blank" rel="noopener">ubuntu下安装Apache+PHP+Mysql</a>   </p><h3 id="apache的使用"><a href="#apache的使用" class="headerlink" title="apache的使用"></a>apache的使用</h3><p>把文件放到<code>/var/www/html</code>目录下，通过<code>localhost</code>访问<br>一般情况下apache是自动打开的，手动开启关闭重启的命令如下：          </p><pre><code>apachectl -k start          apachectl -k stop       apachectl -k restart       </code></pre><h3 id="mysql的使用"><a href="#mysql的使用" class="headerlink" title="mysql的使用"></a>mysql的使用</h3><h4 id="登录前"><a href="#登录前" class="headerlink" title="登录前"></a>登录前</h4><p>登录：<code>mysql [-h 服务器] -u 用户名 -p</code>然后按照提示输入密码<br>修改密码：<code>mysqladmin -u 用户名 password &quot;新密码&quot;</code>然后按照提示输入原密码      </p><h4 id="登录后"><a href="#登录后" class="headerlink" title="登录后"></a>登录后</h4><h5 id="用户相关操作、创建数据库（root）"><a href="#用户相关操作、创建数据库（root）" class="headerlink" title="用户相关操作、创建数据库（root）"></a>用户相关操作、创建数据库（root）</h5><p>创建用户：<code>insert into mysql.user(Host, User, Password) values(&#39;主机&#39;, &#39;用户名&#39;, password(&#39;密码&#39;));</code>          </p><ul><li>本地访问的主机是localhost， 如果要远程通过phpmyadmin管理数据库，那么主机是%*<br>创建数据库：<code>creat database 数据库名;</code><br>用户授权：<code>grant [select, update, insert, delect, all] on 数据库.表 to &#39;用户名&#39;@‘主机名’ identified by &quot;密码&quot;;</code>       </li><li>如果是所有数据库或所有表，用*代替*         </li><li>授权后刷新系统权限表：<code>flush privileges;</code>*<br>删除用户：<code>delect from user where User=&#39;用户名&#39; and Host=&#39;主机名&#39;;</code>;<br>删除后刷新系统权限表：<code>flush privileges;</code><br>删除数据库：<code>drop database 数据库名;</code><br>修改用户密码：<code>update mysql.user set password=password(&#39;新密码&#39;) where User=&#39;用户名&#39; and Host=&#39;主机名&#39;;</code><br>修改后刷新系统权限表：<code>flush privileges;</code>       </li></ul><h5 id="切换数据库："><a href="#切换数据库：" class="headerlink" title="切换数据库："></a>切换数据库：</h5><pre><code>use 数据库名;</code></pre><h5 id="显示数据："><a href="#显示数据：" class="headerlink" title="显示数据："></a>显示数据：</h5><pre><code>select 列 from 表名;</code></pre><p>若显示全部内容，列使用*      </p><h5 id="查看数据库："><a href="#查看数据库：" class="headerlink" title="查看数据库："></a>查看数据库：</h5><pre><code>show databases;</code></pre><p><strong>数据库中的charset尽量使用utf8,而不是gbk，防止暴露服务器信息</strong><br>参考资料：<a href="http://www.2cto.com/Article/201310/247718.html" target="_blank" rel="noopener">GBK字符编码（字符集）缺陷导致web安全漏洞</a>【虽然看不太懂】           </p><h3 id="phpmyadmin的使用"><a href="#phpmyadmin的使用" class="headerlink" title="phpmyadmin的使用"></a>phpmyadmin的使用</h3><h4 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h4><p>从官网下载后解压到<code>/var/www/html</code>，通过<code>localhost</code>访问      </p><h4 id="配置："><a href="#配置：" class="headerlink" title="配置："></a>配置：</h4><p>如果本地访问，直接使用就好<br>如果要连接到远程服务器的mysql，那么打开目录下的<code>libraries/config.defaut.php</code>文件，找到<code>$cfg[&#39;Servers&#39;][$i][&#39;host&#39;]</code>将值改为相应的服务器</p><h2 id="CI框架"><a href="#CI框架" class="headerlink" title="CI框架"></a>CI框架</h2><h3 id="安装：-1"><a href="#安装：-1" class="headerlink" title="安装："></a>安装：</h3><p>从官网下载后解压到<code>/var/www/html</code>目录下即可<br>通过<code>localhost/...../index.php/..控制器../..方法../..参数..</code>访问<br>相关子目录：            </p><blockquote><p><code>application</code>：用户主要目录（主要是views, controllers, models, config）<br><code>system</code>：框架的系统目录，一般不用管<br><code>user_guide</code>：用户手册，英文版，不需要可以删除         </p></blockquote><h3 id="设计模式：MVC（Model-View-Controller）"><a href="#设计模式：MVC（Model-View-Controller）" class="headerlink" title="设计模式：MVC（Model - View - Controller）"></a>设计模式：MVC（Model - View - Controller）</h3><h4 id="模型（Model）：处理页面和数据库的交互"><a href="#模型（Model）：处理页面和数据库的交互" class="headerlink" title="模型（Model）：处理页面和数据库的交互"></a>模型（Model）：处理页面和数据库的交互</h4><h5 id="数据库配置：-application-config-database-php文件"><a href="#数据库配置：-application-config-database-php文件" class="headerlink" title="数据库配置：.../application/config/database.php文件"></a>数据库配置：<code>.../application/config/database.php</code>文件</h5><p><code>$db[&#39;default&#39;][&#39;hostname&#39;]</code>：主机名<br><code>$db[&#39;default&#39;][&#39;username&#39;]</code>：mysql用户名<br><code>$db[&#39;default&#39;][&#39;password&#39;]</code>：mysql密码<br><code>$db[&#39;default&#39;][&#39;database&#39;]</code>：所用的数据库     </p><h5 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h5><pre class=" language-php"><code class="language-php"><span class="token delimiter">&lt;?</span> php              <span class="token keyword">Class</span> 模型名（首字母大写，与文件名相同，单文件名全小写） <span class="token keyword">extends</span> <span class="token class-name">CI_model</span>        <span class="token punctuation">{</span>                <span class="token keyword">public</span> <span class="token keyword">function</span> 方法名<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token punctuation">{</span>                        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>                <span class="token punctuation">}</span>        <span class="token punctuation">}</span></code></pre><p>特殊方法：__construct()，构建函数，当模型被调用时，首先自动调用这个函数<br>通常要在构建函数中载入数据库<code>$this-&gt;load-&gt;database();</code>      </p><h5 id="获取数据库内容"><a href="#获取数据库内容" class="headerlink" title="获取数据库内容"></a>获取数据库内容</h5><h6 id="获取整张表"><a href="#获取整张表" class="headerlink" title="获取整张表"></a>获取整张表</h6><p><code>$query=$this-&gt;db-&gt;get(&#39;表名&#39;);</code>返回Active Record类并且赋给变量$query<br><code>$query-&gt;result_array();</code>调用类的result_array()方法，返回一个包含各行内容的字典，可以在视图上通过foreach分别取出每一行<br><em>对于每一行，是一个以字段名为索引的字典</em></p><h6 id="获取某一行的内容"><a href="#获取某一行的内容" class="headerlink" title="获取某一行的内容"></a>获取某一行的内容</h6><p><code>$query=$this-&gt;db-&gt;get_where(&#39;表名&#39;, 筛选条件);</code>筛选条件是一个{‘字段名’=&gt;值}的数组<br><code>$query-&gt;row_array();</code>调用类的row_array()方法，返回一个包含匹配行的内容的字典        </p><h5 id="向数据库写入内容"><a href="#向数据库写入内容" class="headerlink" title="向数据库写入内容"></a>向数据库写入内容</h5><p><code>$this-&gt;input-&amp;gt;post(&#39;提交内容的name&#39;);</code>返回表当内对应name的value<br><code>$this-&gt;db-&gt;insert(&#39;表名&#39;, 内容)</code>写入数据库，内容是一个字典       </p><h4 id="视图（View）：要展示的静态页面"><a href="#视图（View）：要展示的静态页面" class="headerlink" title="视图（View）：要展示的静态页面"></a>视图（View）：要展示的静态页面</h4><p>可以把一个页面写成多个文件，显示时再拼接起来<br><em>比如可以创建一个templates文件夹，里面放一些各个网页通用头和尾</em><br>必须是php文件      </p><h5 id="html-gt-php改写"><a href="#html-gt-php改写" class="headerlink" title="html -&gt; php改写"></a>html -&gt; php改写</h5><p>连接的js和css等文件，还有图片等资源不能放在application目录下<br>需要在根目录下创建一个文件夹，把这些资源放进去<br>主要是修改连接的js和csss等文件、图片等资源的文件的地址          </p><ul><li>可以借助url的辅助函数<br>  需要在方法中加入<code>$this-&gt;load-&gt;helper(&#39;url&#39;);</code>载入辅助函数       <pre><code>  把根目录或者资源的目录写入`..../application/config/config.php`的`$config[&#39;base_url&#39;]`         </code></pre>  利用<code>base_url()</code>函数          <pre><code>  *如config中的base_url设置为localhost/join/resources*            *那么，base_url(‘image/background.jpg’)就会返回字符串loacalhost/join/resources/image/background.jpg*          </code></pre></li></ul><p>js，css文件内不能写php语句，如果需要使用这个函数，可以把内容直接复制到页面上，而不通过连接<br><strong>注意：改写的时候要注意不要不小心把原文件上的括号、分号给删掉了</strong>       </p><h4 id="控制器（Controller）：包含各种方法"><a href="#控制器（Controller）：包含各种方法" class="headerlink" title="控制器（Controller）：包含各种方法"></a>控制器（Controller）：包含各种方法</h4><h5 id="形式-1"><a href="#形式-1" class="headerlink" title="形式"></a>形式</h5><pre class=" language-php"><code class="language-php"><span class="token delimiter">&lt;?</span> php            <span class="token keyword">class</span> 控制器名（首字母大写，与文件名相同，但文件名全小写） <span class="token keyword">extends</span> <span class="token class-name">CI_controller</span>        <span class="token punctuation">{</span>                <span class="token keyword">public</span> <span class="token keyword">function</span> 方法名<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token punctuation">{</span>                        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>                <span class="token punctuation">}</span>        <span class="token punctuation">}</span></code></pre><p>特殊方法：<strong>construct()，构建函数<br>通常需要加入`parents::</strong>construct();<code>，……并不知道有什么卵用，照搬=。=</code>$this-&gt;load-&gt;model(‘模型名’);<code>载入模型</code>$this-&gt;load-&gt;view(‘视图文件名’, $data);`：不是只显示这个视图的内容，而是把这个文件的 内容写到当前方法的页面上，并且把变量$data内的数据传递到这个视图文件上           </p><h5 id="传递的数据在视图文件上的访问"><a href="#传递的数据在视图文件上的访问" class="headerlink" title="传递的数据在视图文件上的访问"></a>传递的数据在视图文件上的访问</h5><p>如<code>$data[&#39;news&#39;][&#39;title&#39;]</code><br>在视图上通过<code>$news[&#39;title]</code>来访问这个变量           </p><h5 id="调用模型上的方法："><a href="#调用模型上的方法：" class="headerlink" title="调用模型上的方法："></a>调用模型上的方法：</h5><p><code>$this-&gt;模型名-&gt;方法名；</code>        </p><h5 id="表单数据的传递"><a href="#表单数据的传递" class="headerlink" title="表单数据的传递"></a>表单数据的传递</h5><p>controllers要有一个方法来接受表单的数据<br>form的action应指向这个方法<br>这个方法用来处理表单传递的数据，并且调用模型的方法与数据库进行交互           </p><h2 id="提交到服务器"><a href="#提交到服务器" class="headerlink" title="提交到服务器"></a>提交到服务器</h2><h3 id="登录："><a href="#登录：" class="headerlink" title="登录："></a>登录：</h3><p><code>ssh 用户名@服务器</code>，然后输入密码        </p><h3 id="服务器与本地文件的传输"><a href="#服务器与本地文件的传输" class="headerlink" title="服务器与本地文件的传输:"></a>服务器与本地文件的传输:</h3><p>scp命令，用法与cp命令相似<br>比如上传本地<code>~/home.php</code>到服务器的主目录下:<br><code>scp ~/home.php 用户名@服务器:home.php</code></p><h3 id="网站的目录："><a href="#网站的目录：" class="headerlink" title="网站的目录："></a>网站的目录：</h3><p>一般来说，<br>对于nginx，放在<code>/usr/share/nginx/html/</code>目录下<br>对于apache，放在<code>/var/www/html/</code>目录下         </p><p>nginx的配置挺复杂的，还有待以后进一步的学习</p><h3 id="关闭ci框架的debug模式"><a href="#关闭ci框架的debug模式" class="headerlink" title="关闭ci框架的debug模式"></a>关闭ci框架的debug模式</h3><p><strong>网站挂上去后最好关闭debug模式（防止程序信息暴露在前台）</strong><br><code>..../application/config/database.php</code>中的<code>$db[&#39;default&#39;][&#39;db_debug&#39;]</code>修改为FALSE即可</p><h2 id="github"><a href="#github" class="headerlink" title="github"></a>github</h2><p>其实一开始写后端的时候并没有git= =时间紧啊，没时间给我慢慢玩<br>不过网站挂上去，在写查重和改写雨停的优化版的时候就玩了玩git          </p><h3 id="github在Windows上有客户端，直接使用就好"><a href="#github在Windows上有客户端，直接使用就好" class="headerlink" title="github在Windows上有客户端，直接使用就好"></a>github在Windows上有客户端，直接使用就好</h3><h3 id="在linux上需要用git"><a href="#在linux上需要用git" class="headerlink" title="在linux上需要用git"></a>在linux上需要用git</h3><h4 id="安装git："><a href="#安装git：" class="headerlink" title="安装git："></a>安装git：</h4><p><code>sudo apt-get install git</code></p><h4 id="配置git："><a href="#配置git：" class="headerlink" title="配置git："></a>配置git：</h4><h5 id="本地创建ssh-key："><a href="#本地创建ssh-key：" class="headerlink" title="本地创建ssh key："></a>本地创建ssh key：</h5><p><code>ssh-keygen -t rsa -C &quot;邮箱&quot;</code>，会提示确认路径和密码，一路回车默认即可          </p><h5 id="在github中添加ssh-key："><a href="#在github中添加ssh-key：" class="headerlink" title="在github中添加ssh key："></a>在github中添加ssh key：</h5><p>进入Account Settings，选择SSH Key，Add SSH Key，title随便填，key复制粘贴文件<code>~/.ssh/id_rsa.pub</code>的全部内容</p><h5 id="设置username和email"><a href="#设置username和email" class="headerlink" title="设置username和email"></a>设置username和email</h5><p>每次commit都会记录这两个信息<br><code>git config --global user.name=&quot;用户名&quot;</code><br><code>git config --global user.email=&quot;邮箱&quot;</code>           </p><h4 id="设置要git的目录"><a href="#设置要git的目录" class="headerlink" title="设置要git的目录"></a>设置要git的目录</h4><p>进入要git的目录<br>初始化：<code>git init</code><br>添加远程地址：<code>git remote add origin git@github.com:github用户名/远程仓库名.git</code>            </p><h5 id="使用（简单的git命令）"><a href="#使用（简单的git命令）" class="headerlink" title="使用（简单的git命令）"></a>使用（简单的git命令）</h5><p><code>git clone &lt;ADDRESS&gt;</code>：克隆代码库，可以是本地代码库，也可以是远程代码库（USERNAME@HOST:PATH）<br><code>git add &lt;-i&gt; &lt;FILE&gt; ....</code>：【-i，交互式】添加文件到代码库中<br><code>git rm &lt;FILE&gt;</code>：将文件移出代码库<br><code>git commit -m &lt;MESSAGE&gt;</code>：提交更改，将修改的文件提交到缓冲区<br><code>git push &lt;ORIGIN&gt; &lt;BRANCH&gt;</code>：推送代码库到远程代码库<br><code>git pull</code>：从远程同步代码库到本地并合并<br><code>git branch</code>：查看当前分支<br><code>git branch &lt;BRANCH&gt;</code>：创建分支（主分支为Master）<br><code>git branch -d &lt;BRANCH&gt;</code>：删除分支<br><code>git checkout &lt;BRANCH&gt;</code>：切换到指定分支<br><code>git checkout -- &lt;FILE&gt;</code>：从缓冲区替换本地改动<br><code>git log</code>：查看提交的历史记录<br><code>git status</code>：显示当前的修改状态<br><code>git reset &lt;LOG&gt;</code>：恢复到历史版本<br>参考资料：<br><a href="http://www.bootcss.com/p/git-guide/" target="_blank" rel="noopener">git - 简易指南</a><br><a href="http://www.eoeandroid.com/thread-274556-1-1.html" target="_blank" rel="noopener">【Github教程】史上最全github使用方法：github入门到精通</a></p><h2 id="附：ubuntu下的sublime-text"><a href="#附：ubuntu下的sublime-text" class="headerlink" title="附：ubuntu下的sublime text"></a>附：ubuntu下的sublime text</h2><h3 id="安装：sudo-apt-get-install-sublime-text"><a href="#安装：sudo-apt-get-install-sublime-text" class="headerlink" title="安装：sudo apt-get install sublime-text"></a>安装：<code>sudo apt-get install sublime-text</code></h3><h3 id="中文支持"><a href="#中文支持" class="headerlink" title="中文支持"></a>中文支持</h3><p>参考资料：<a href="http://jingyan.baidu.com/article/6fb756eca7af6c241858fbf2.html" target="_blank" rel="noopener">解决Ubuntu下Sublime Text 2无法输入中文的方法</a></p><h3 id="Markdown支持"><a href="#Markdown支持" class="headerlink" title="Markdown支持"></a>Markdown支持</h3><p>参考资料：<a href="http://www.jianshu.com/p/378338f10263" target="_blank" rel="noopener">sublime text 2 下的Markdown写作</a><br>但是语法高亮的设置我一直找不到,最后是在<code>Package Control: install Pakage</code>中直接安装MarkdownEditing，但是语法高亮效果不太满意,不知道有没有更好的解决方法</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>雨停的前端做的好赞！！！诶，不会美工的小码农真是不能比啊╮(╯▽╰)╭</p><p>=  =学习CI，用服务器的时候遇到好多逗比的问题去请教蛋蛋和六神       </p><p>感谢蛋蛋和六神的指导嘿嘿～～～                   </p><p>CI官网的教程写的有点晦涩，不太好懂，这里只是个人在使用中自己的理解，可能会有一些错误，欢迎指出～        </p><p>不过网站还不会挂咯，找个时间得学学怎么挂网站～～</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>居然……还我数据？！？！</title>
      <link href="/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93/"/>
      <url>/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>在学校的时候把windows还原了一遍，导致linux进不去了，orz<br>回来之后重装linux，结果不小心操作不当，把硬盘格式化了！！<br>后来发现硬盘又出了点问题，哎烦，查了好多东西，试了好多方法，而且恰巧在看鸟哥的第三章（硬盘分区方面的内容），折腾了三天半终于把这娃整好，真是感动<br>查的东西好杂，让我写个总结冷静冷静  </p><p>##还原windows导致linux无法进入的原因<br>windows是相当霸道的，在安装时，<strong>win会主动覆盖掉MBR，也就直接把linux的引导加载程序给覆盖掉</strong>，linux就自然无法进入啦</p><p>##那么，我们把linux引导加载程序grub修复了就好啦？<br>咳咳……蛋蛋让我去百度查查怎么修复grub，不过太弱了，看了三种修复的教程都没能看懂，最后似乎还把linux整坏了！！！只好重装linux，可又忘记重新分区，选了个“<strong>清除所有程序和数据</strong>”…………呵呵，linux比windows还霸道，清除的不只是linux的系统分区，而是全硬盘啊！！把我全硬盘都格式化成ext4，真是吓哭我了……      </p><p>##找回丢失的数据</p><ol><li><strong>硬盘被格式化并不会直接把上面的数据抹去，不被新数据覆盖的区段的数据其实是可以复原的；</strong>  </li><li><strong>而linux安装的位置在硬盘的头部，我的主要数据是放在DEF盘，并不会被覆盖，修复数据还是有戏的</strong><br><em>重要的数据放在C盘是极不安全的，一旦系统崩溃，还原、重装之后就被新的系统文件覆盖，找都找不回来……所以平常要把桌面、我的文档、收藏夹等数据设置在DEF盘，至少应该把文档所在的文件夹放在DEF盘，再创建快捷方式到桌面上……非常非常重要的数据要做好备份，备份到网络上（并不可靠）或者备份到移动硬盘等其他设备上</em>  </li></ol><p>所以呢，网上找找PE，通过UltraISO等软件把iso镜像烧进U盘里，插进计算机，开机启动项设置为该U盘，在PE下利用diskgenius等分区工具进行“<strong>重建分区表</strong>”就可以啦。它会自动搜索电脑上丢失的分区，把还存在数据恢复出来   </p><p><em>但是，恢复的数据会有少数被损坏，也就是说不可能完好无损的找回来，所以平时要保护好自己的数据啊？！？！？！！！</em><br><em>而且，找回来的分区必须设置为主分区= =好麻烦！改回逻辑分区的话需要把数据移到其他地方，把这些分区删除后新建扩展分区，在扩展分区下新建逻辑分区，再把数据移回来！</em><br><strong>因为一个硬盘只能存在四个主分区或扩展分区（其中扩展分区至多只能有一个）</strong><br>嗯……我比较懒，所以把原本win上的四个磁盘缩减为三个磁盘，都作为主分区，留下一个作为扩展分区给linux<br>另外呢，<strong>win系统分区必须激活设置为活动分区！！</strong>   </p><p>##安装windows<br>找回数据时，把dell出厂的镜像分区也给找回来了，但是还原程序被格掉，重装后也识别不了出厂镜像= =它那鬼镜像（wim）还分成了两组，每组两个，着实看不懂要怎么用……只好放弃正版win8，上网找了原版的win8.1企业版，在PE下通过winNT安装，再用激活工具破解，从此又过上了盗版的日子<br><em>不过镜像我还留着，据说有方法能够调用，但是实在复杂，等我有空好好研究研究</em>  </p><p>安装windows其实有个小插曲，一开始NT一直识别不了我下载的win8.1的安装程序，折腾半天后无奈的检验了一下SHA1，简直哭，SHA1值并不匹配——也就是下载的时候文件被损坏了！<br><strong>所以啊，以后安装重要的、安装过程复杂的程序、软件，一定一定要注意先校验一下MD5值或者SHA1值</strong>（在官网下载的话一般会提供）<br><em>否则如果在安装过程中才发现损坏，会很麻烦的。比如安装系统，装一半的时候才告诉你文件损坏，可是你原来的win早被格式化了，整起来可就麻烦多了（所幸现在系统安装程序在安装前大都保守地检验了一下安装文件的完整性）</em></p><p>###internal hard disk drive not found？<br>这条错误信息着实折腾了我好久，到底是无法读取硬盘呢还是无法调用驱动程序？<br>大概是后者吧，毕竟我在BIOS中能找到硬盘的信息，在PE下也能正常访问硬盘的数据<br>硬盘驱动？这个一般操作系统都会提供——一开始我以为只要把系统装上，这个问题就能够解决，然而并没有这么简单！<br>在PE下安装系统后，需要重启进行最后的设置，但是每次都不能成功进入系统，都卡在这句错误信息上！<br>怎么办？我尝试了百度上的各种办法，最后终于成功了？！——<br><em>在BIOS的boot选项卡下把Secure Boot设置为Disabled，把Boot List Option改为Legacy</em><br>竟然这样就把问题解决了？！百思不得其解，只好求教度娘这两个选项的含义</p><p>####Secure Boot  <a href="http://bbs.taobao.com/catalog/thread/154503-260136140.htm" target="_blank" rel="noopener">##参考资料##</a><br>若干年前，各大主板厂商推出UEFI取代历史悠久的BIOS，UEFI全称为“统一的可扩展固定接口”。<br>而Secure Boot是UEFI的一部分，它采用密钥，防止恶意软件侵入操作系统和硬件驱动程序</p><p>但是UEFI并不能得到广泛的推广，原因是微软的态度并不积极，他们要求安装Windows8时要关闭Secure Boot，而对预装的Windows8需要打开Secure Boot，所以我在安装windows时就必须把Secure Boot关闭啦！</p><p>####Boot List Option<br>硬盘启动模式，包括Legacy和UEFI两种  </p><p>UEFI如上文介绍，是一种新的BIOS；<br>Legacy则是传统的BIOS模式  </p><p>在UEFI下安装的系统以后只能由UEFI模式进入系统；<br>在Legacy下安装的系统以后只能由Legacy模式进入系统  </p><p>甚是复杂！！</p><p>##Linux安装<br>引导方式有两种——   </p><ol><li>win引导linux：<br> 把引导分区设置为挂载/boot的分区<br> 通过软件easybcd等软件进行引导  </li><li>linux引导win：（默认）引导程序将被安装到MBR上  </li></ol><p>##最后<br>电脑修的累死我咯，但是能修好，重新见到windows桌面，真是非常的愉悦，感觉三天半没白忙）<br>说不定以后找不到工作还能给人修修电脑啊哈哈哈哈哈哈！！！  </p><p>唯一不满意的是没能把出厂的原版系统找回来，等有空接着研究研究吧！</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>【百度俱乐部】第一二期作业总结</title>
      <link href="/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/"/>
      <url>/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>呃，要用markdown写文章呢差点忘了……<br>这期作业写了我八九个小时啊，翻阅了好多资料，干脆写份总结算了。下面就按照我做作业的思路、途中遇到的问题及解决方法展开来写吧——  </p><h2 id="大致设想"><a href="#大致设想" class="headerlink" title="大致设想"></a>大致设想</h2><p>首先，模仿百度主页嘛，找张图片，做个输入框，搞个submit的按钮，简单安上导航栏，右上角意思意思搞个“登陆”、“注册”的鬼玩意。   </p><h3 id="input-text的尺寸调整"><a href="#input-text的尺寸调整" class="headerlink" title="input text的尺寸调整"></a>input text的尺寸调整</h3><ul><li>text是没有width、height属性的</li><li>宽度其实可以通过字段size调整</li><li>高度可以通过style属性中的字体高度font-size来调整</li><li>另外还有style中的padding，可以调整输入框的内边距，不然输入的时候字压着边框太丑<h3 id="text和submit之间总存在缝隙"><a href="#text和submit之间总存在缝隙" class="headerlink" title="text和submit之间总存在缝隙"></a>text和submit之间总存在缝隙</h3>取消缝隙，首先要设置两个元素的margin为0，另外submit默认是有边框的，所以还要设置submit的border为0。但如果将两元素的代码分成两行，则间隙仍不能取消<br><input type="text" style="margin:0px"><input type="submit" style="margin:0px;border:0;background-color:blue">     </li></ul><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>submit<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px<span class="token punctuation">;</span><span class="token property">border</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token property">background-color</span><span class="token punctuation">:</span>blue</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span></code></pre><ul><li>其实回车也会留下空格的，所以只要把回车去掉，将两个元素挤在一行即可<br><input type="text" style="margin:0px"><input type="submit" style="margin:0px;border:0;background-color:blue"></li></ul><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>text<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>input</span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>submit<span class="token punctuation">"</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">="</span><span class="token attr-value"><span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span>px<span class="token punctuation">;</span><span class="token property">border</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token property">background-color</span><span class="token punctuation">:</span>blue</span><span class="token punctuation">"</span></span><span class="token punctuation">></span></span></code></pre><h3 id="text和submit高低错位"><a href="#text和submit高低错位" class="headerlink" title="text和submit高低错位"></a>text和submit高低错位</h3><p>首先考虑用top，bottom属性设置上下留空白解决，但是无论怎么改变值都没有变化<br>查过资料后知道，设置left,right,top,bottom前需要设置position    </p><blockquote><p>position有五种值：<br><code>static</code>（默认）：不定位<br><code>relative</code>：相对于块<br><code>absolute</code>：相对于页面<br><code>fixed</code>：相对于视窗<br><code>inherit</code>：继承父元素的值   </p></blockquote><h2 id="栅格系统"><a href="#栅格系统" class="headerlink" title="栅格系统"></a>栅格系统</h2><p>我直接在<code>&lt;body&gt;&lt;/body&gt;</code>中加入  </p><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>container<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>row<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>col-md-1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>...<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>col-md-offset-1<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>...<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span></code></pre><p>结果什么都没发生！！！<br>后来才知道，是忘了<code>&lt;link href=&quot;http://cdn.bootcss.com/bootstrap/3.2.0/css/bootstrap.css&quot; rel=&quot;stylesheet&quot;&gt;</code>……呵……呵……简直逗比~</p><blockquote><p>说道栅格系统，顺便截一段资料过来吧：<br>【<a href="http://v3.bootcss.com/css/#grid" target="_blank" rel="noopener">栅格系统  bootstrap</a>】</p><ul><li>行列必须包含在.container里</li><li>列（.col-xx-xx）必须包含在行（.row）里</li></ul></blockquote><p><table border="1"><br>        <tr><br>            <td> </td><br>            <td>超小屏幕 手机（&lt;768px) </td><br>            <td>小屏幕 平板 （&gt;=768px) </td><br>            <td>中等屏幕 桌面显示器 (&gt;=992px) </td><br>            <td>大屏幕 大桌面显示器 (&gt;=1200px) </td><br>        </tr><br>        <tr><br>            <td>栅格系统行为</td><br>            <td>总是水平</td><br>            <td colspan="3">开始是堆叠在一起的，当大于这些阈值时将变为水平排列C</td><br>        </tr><br>        <tr><br>            <td><font color="red">.container</font>最大宽度</td><br>            <td>None（自动）</td><br>            <td>750px</td><br>            <td>970px</td><br>            <td>1170px</td><br>        </tr><br>        <tr><br>            <td>类前缀</td><br>            <td><font color="red">.col-xs-</font></td><br>            <td><font color="red">.col-sm-</font></td><br>            <td><font color="red">.col-md-</font></td><br>            <td><font color="red">.col-lg-</font></td><br>        </tr><br>        <tr><br>            <td>列（column）数</td><br>            <td colspan="4">12</td><br>        </tr><br>        <tr><br>            <td>最大列宽</td><br>            <td>自动</td><br>            <td>~62px</td><br>            <td>~81px</td><br>            <td>~97px</td><br>        </tr><br>        <tr><br>            <td>槽（gutter）宽</td><br>            <td colspan="4">30px（每列左右均有15px）</td><br>        </tr><br>        <tr><br>            <td>可嵌套</td><br>            <td colspan="4">是</td><br>        </tr><br>        <tr><br>            <td>偏移（offsets）</td><br>            <td colspan="4">是</td><br>        </tr><br>        <tr><br>            <td>列排序</td><br>            <td colspan="4">是</td><br>        </tr><br></table><br>做作业的时候忘记练习表格了……所以手码下来……<br>哇靠！手码表格真是累死了……<br><br><br>列合并：colspan属性</p><h2 id="逗比功能：鼠标悬浮事件"><a href="#逗比功能：鼠标悬浮事件" class="headerlink" title="逗比功能：鼠标悬浮事件"></a>逗比功能：鼠标悬浮事件</h2><p>效果：鼠标移到submit上，submit会自动跳到输入框的另外一侧，就是不给你点，还要在输入框上出现（don’t touch me！），但是不要影响搜索！要搜索的话只能点击图片，为了别太坑还得做一个小小的提示框，但不能太大——简直逗比的我……  </p><h3 id="submit跳动"><a href="#submit跳动" class="headerlink" title="submit跳动"></a>submit跳动</h3><ul><li>鼠标悬浮事件Mouseover</li><li>在两个分别设置两个submit，让他们不能同时出现，display属性，toggle方法<br>  起始状态下，左边的dispaly为none<br>  触发事件后，对两个submit都采用toggle方法  <h3 id="在输入框中出现don’t-touch-me"><a href="#在输入框中出现don’t-touch-me" class="headerlink" title="在输入框中出现don’t touch me"></a>在输入框中出现don’t touch me</h3></li><li>text输入框value属性的获取和修改  <ul><li>获取：$(“…”).val()方法即可返回表单的字段   </li><li>修改：$(“…”).attr(“…”,”…”)方法即可将第一个参数匹配的属性修改为第二个参数的内容</li></ul></li><li>但是这个用起来似乎有点麻烦……在百度上看到另外一种解决办法<ul><li>document.#form.#text.value = document.#text.#text.value + “(Don’t touch me!)”;</li><li><strong>要注意的是：这里的#form和#text是form标签和input标签的<font color="red">name</font>属性而不是<del><font color="red"> id </font></del>属性！而且这里不需要打井号，如：document.form.text.value</strong><h3 id="点击图片自动跳转到百度搜索"><a href="#点击图片自动跳转到百度搜索" class="headerlink" title="点击图片自动跳转到百度搜索"></a>点击图片自动跳转到百度搜索</h3></li></ul></li><li>跳转<ul><li>在百度搜索试着搜索aaa，搜索页得到的网址是<code>http://www.baidu.com/baidu?wd=aaa</code></li><li>那么猜测，直接修改网址末尾的aaa为搜索的内容是否可行？结果是可以的……</li><li>那么，我们只需要把搜索内容连接到<code>http://www.baidu.com/baidu?wd=</code>末尾作为图片的超链接即可实现</li></ul></li><li>搜索内容的获取<ul><li>键盘keyup事件<br>  <em>与其相对的还有一个keydown事件，down是指按下键盘（包括输入法还未把输出到对话框）触发的时间，up是指松开键盘（包括输入法把内容输出到对话框上）触发的时间，</em><br>  <strong>显然这里要用的是应该是keyup事件</strong><br>  <em>另外，使用keyup事件不会在不通过键盘输入来修改内容的情况下触发，也就是说，前边用JQ修改输入框内容并不会触发事件，不影响内容的获取</em></li><li>将搜索内容与<code>http://www.baidu.com/baidu?wd=</code>组成的连接作为图片连接的href属性，使用<code>attr(&quot;href&quot;,&quot;...&quot;)</code>方法即可  <h3 id="警告框"><a href="#警告框" class="headerlink" title="警告框"></a>警告框</h3></li></ul></li><li>使用bootstrap，自己再对字体、尺寸稍作修改即可</li><li>但是千万别忘了<br>  <code>&lt;link href=&quot;http://cdn.bootcss.com/bootstrap/3.2.0/css/bootstrap.css&quot; rel=&quot;stylesheet&quot;&gt;</code></li><li>display:none</li><li><p>show()方法</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>使用JQ的格式——<br>注意：fuction紧跟后面只有一个右括号<br>ready的右括号在最后边，中间的代码即花括号的内容是fuction的内容   </p><pre class=" language-html"><code class="language-html">$(document).ready**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>font</span> <span class="token attr-name">color</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>red<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>(<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>font</span><span class="token punctuation">></span></span>**function**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>font</span> <span class="token attr-name">color</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>green<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>()<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>font</span><span class="token punctuation">></span></span>**{       <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>br</span><span class="token punctuation">></span></span>}**<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>font</span> <span class="token attr-name">color</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>red<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>)<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>font</span><span class="token punctuation">></span></span>**;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>br</span><span class="token punctuation">></span></span>    $(document).ready(function(){  $("...").click(function(){      $("...").attr("","");  });});</code></pre></li><li><p>选择器在圆括号内，除了document和this之外都要加双引号，类要加’.’，id要加’#’</p></li><li>JQ的内容一定要嵌套在<code>$(document).ready(function(){.....});</code>内</li><li>语句后边别忘了分号</li></ul><h2 id="登陆、注册弹框"><a href="#登陆、注册弹框" class="headerlink" title="登陆、注册弹框"></a>登陆、注册弹框</h2><h3 id="在百度搜到一种遮罩式弹框的模板"><a href="#在百度搜到一种遮罩式弹框的模板" class="headerlink" title="在百度搜到一种遮罩式弹框的模板"></a>在百度搜到一种遮罩式弹框的模板</h3><p><a href="http://www.jb51.net/article/34951.htm" target="_blank" rel="noopener">jQuery+css+html实现页面遮罩弹出框</a><br>具体代码如下：</p><h4 id="CSS部分"><a href="#CSS部分" class="headerlink" title="CSS部分:"></a>CSS部分:</h4><pre class=" language-css"><code class="language-css"><span class="token selector">body </span><span class="token punctuation">{</span>     <span class="token property">font-family</span><span class="token punctuation">:</span>Arial, Helvetica, sans-serif<span class="token punctuation">;</span>     <span class="token property">font-size</span><span class="token punctuation">:</span><span class="token number">12</span>px<span class="token punctuation">;</span>     <span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#main</span> </span><span class="token punctuation">{</span>     <span class="token property">height</span><span class="token punctuation">:</span><span class="token number">1800</span>px<span class="token punctuation">;</span>     <span class="token property">padding-top</span><span class="token punctuation">:</span><span class="token number">90</span>px<span class="token punctuation">;</span>     //打开弹窗的按钮    <span class="token property">text-align</span><span class="token punctuation">:</span>center<span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#fullbg</span> </span><span class="token punctuation">{</span>     <span class="token property">background-color</span><span class="token punctuation">:</span>gray<span class="token punctuation">;</span>     <span class="token property">left</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span>     <span class="token property">opacity</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">;</span>        //背景设置为灰色    <span class="token property">position</span><span class="token punctuation">:</span>absolute<span class="token punctuation">;</span>     <span class="token property">top</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">;</span>     <span class="token property">z-index</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">;</span>     <span class="token property">filter</span><span class="token punctuation">:</span><span class="token function">alpha</span><span class="token punctuation">(</span>opacity=<span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token property">-moz-opacity</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">;</span>     <span class="token property">-khtml-opacity</span><span class="token punctuation">:</span><span class="token number">0.5</span><span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#dialog</span> </span><span class="token punctuation">{</span>     <span class="token property">background-color</span><span class="token punctuation">:</span><span class="token hexcode">#fff</span><span class="token punctuation">;</span>     <span class="token property">border</span><span class="token punctuation">:</span><span class="token number">5</span>px solid <span class="token function">rgba</span><span class="token punctuation">(</span><span class="token number">0</span>,<span class="token number">0</span>,<span class="token number">0</span>, <span class="token number">0.4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token property">height</span><span class="token punctuation">:</span><span class="token number">400</span>px<span class="token punctuation">;</span>     <span class="token property">left</span><span class="token punctuation">:</span><span class="token number">50%</span><span class="token punctuation">;</span>     <span class="token property">margin</span><span class="token punctuation">:</span>-<span class="token number">200</span>px <span class="token number">0</span> <span class="token number">0</span> -<span class="token number">200</span>px<span class="token punctuation">;</span>     <span class="token property">padding</span><span class="token punctuation">:</span><span class="token number">1</span>px<span class="token punctuation">;</span>     <span class="token property">position</span><span class="token punctuation">:</span>fixed <span class="token important">!important</span><span class="token punctuation">;</span> //弹出的浮动对话框     <span class="token property">position</span><span class="token punctuation">:</span>absolute<span class="token punctuation">;</span>     <span class="token property">top</span><span class="token punctuation">:</span><span class="token number">50%</span><span class="token punctuation">;</span>     <span class="token property">width</span><span class="token punctuation">:</span><span class="token number">400</span>px<span class="token punctuation">;</span>     <span class="token property">z-index</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">;</span>     <span class="token property">border-radius</span><span class="token punctuation">:</span><span class="token number">5</span>px<span class="token punctuation">;</span>     <span class="token property">display</span><span class="token punctuation">:</span>none<span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector">dialog p </span><span class="token punctuation">{</span>     <span class="token property">margin</span><span class="token punctuation">:</span><span class="token number">0</span> <span class="token number">0</span> <span class="token number">12</span>px<span class="token punctuation">;</span>     <span class="token property">height</span><span class="token punctuation">:</span><span class="token number">24</span>px<span class="token punctuation">;</span>     <span class="token property">line-height</span><span class="token punctuation">:</span><span class="token number">24</span>px<span class="token punctuation">;</span>     <span class="token property">background</span><span class="token punctuation">:</span><span class="token hexcode">#CCCCCC</span><span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#dialog</span> p<span class="token class">.close</span> </span><span class="token punctuation">{</span>     <span class="token property">text-align</span><span class="token punctuation">:</span>right<span class="token punctuation">;</span>     <span class="token property">padding-right</span><span class="token punctuation">:</span><span class="token number">10</span>px<span class="token punctuation">;</span> <span class="token punctuation">}</span> <span class="token selector"><span class="token id">#dialog</span> p<span class="token class">.close</span> a </span><span class="token punctuation">{</span>     <span class="token property">color</span><span class="token punctuation">:</span><span class="token hexcode">#fff</span><span class="token punctuation">;</span>     <span class="token property">text-decoration</span><span class="token punctuation">:</span>none<span class="token punctuation">;</span> <span class="token punctuation">}</span> </code></pre><h4 id="HTML部分"><a href="#HTML部分" class="headerlink" title="HTML部分:"></a>HTML部分:</h4><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>main<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>javascript:showBg();<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>点击这里查看效果<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>fullbg<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>dialog<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>p</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>close<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>a</span> <span class="token attr-name">href</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>#<span class="token punctuation">"</span></span> <span class="token attr-name">onclick</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>closeBg();<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>关闭<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>a</span><span class="token punctuation">></span></span>          <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span><span class="token punctuation">></span></span>正在加载，请稍后....<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span></code></pre><p>####JQ部分:</p><pre class=" language-js"><code class="language-js"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span>    <span class="token comment" spellcheck="true">//显示灰色 jQuery 遮罩层</span>    <span class="token keyword">function</span> <span class="token function">showBg</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">var</span> bh <span class="token operator">=</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"body"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">height</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">var</span> bw <span class="token operator">=</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"body"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">width</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"#fullbg"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">css</span><span class="token punctuation">(</span><span class="token punctuation">{</span>            height<span class="token punctuation">:</span>bh<span class="token punctuation">,</span>            width<span class="token punctuation">:</span>bw<span class="token punctuation">,</span>            display<span class="token punctuation">:</span><span class="token string">"block"</span>        <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"#dialog"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">//关闭灰色 jQuery 遮罩</span>    <span class="token keyword">function</span> <span class="token function">closeBg</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">"#fullbg,#dialog"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">hide</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span></code></pre><p>只要稍微修改一些数值就可以直接拿来用啦!!</p><h3 id="第二个弹框"><a href="#第二个弹框" class="headerlink" title="第二个弹框"></a>第二个弹框</h3><p>但是……直接套用模板，把html再复制一份企图创建第二个弹框就出现问题了，两个按钮弹出的是同一个框，咋办咧？<br>观察一下代码，发现JQ部分的<code>$(&quot;#dialog&quot;).show();</code>语句是对id为dialog的操作，如果在html里直接复制了第一个弹框，id都为dialog，show只为作用于第一个匹配项………<br>如果想要第二个弹框，就得让两个弹框的id不同，改为dialog1和dialog2单击后调用两个不一样的函数，也就是简单的复制一下showBg()函数，并且改为showBg1()和showBg2，并在css部分做出相应的修改就可以啦~     </p><h3 id="模态框"><a href="#模态框" class="headerlink" title="模态框"></a>模态框</h3><p>上面这个弹框太low了，其实bootstrap已经给我们提供了一个模态框的组件，方便又好看<br>但是偶尔会出现一些奇怪的bug…………</p><h2 id="给鼠标悬浮在submit上的时候加个音频"><a href="#给鼠标悬浮在submit上的时候加个音频" class="headerlink" title="给鼠标悬浮在submit上的时候加个音频"></a>给鼠标悬浮在submit上的时候加个音频</h2><p>嘿嘿，逗比功能就是拿来搞怪的，既然如此干嘛不个它加个笑声的音频呢？  </p><h3 id="找到一个简短的小黄人笑声的音频"><a href="#找到一个简短的小黄人笑声的音频" class="headerlink" title="找到一个简短的小黄人笑声的音频"></a>找到一个简短的小黄人笑声的音频</h3><h3 id="添加一个-lt-audio-gt-标签"><a href="#添加一个-lt-audio-gt-标签" class="headerlink" title="添加一个&lt;audio&gt;标签"></a>添加一个<code>&lt;audio&gt;</code>标签</h3><p>controls属性：如果不设置的话，默认不显示播放器；如果设置为controls=”controls”则显示播放器<br>play()方法：让指定的播放器播放音频<br>具体代码如下：  </p><h4 id="html部分："><a href="#html部分：" class="headerlink" title="html部分："></a>html部分：</h4><pre class=" language-html"><code class="language-html"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>audio</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>laugh<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>...<span class="token punctuation">"</span></span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>audio/mpeg<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>audio</span><span class="token punctuation">></span></span></code></pre><h4 id="JQ部分：（插入在submit的mouseover事件下）"><a href="#JQ部分：（插入在submit的mouseover事件下）" class="headerlink" title="JQ部分：（插入在submit的mouseover事件下）"></a>JQ部分：（插入在submit的mouseover事件下）</h4><pre class=" language-js"><code class="language-js">laugh<span class="token punctuation">.</span><span class="token function">play</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong> 奇怪的是！这里要直接对这个对象使用方法，id不加井号；</strong><br><strong> 而<code>$(&quot;#laugh&quot;).play();</code>却行不通？！</strong>    </p><h2 id="咳咳，总结——"><a href="#咳咳，总结——" class="headerlink" title="咳咳，总结——"></a>咳咳，总结——</h2><p>感觉像是玩上瘾了，居然两天花了八九个小时做这个网页，而且这两天还是很多课不是没课的……<br>又在第三天花了四五个小时写下这个玩意儿……我也是够拼哒= =<br>虽然很好玩，做出来也很有成就感……但是………我的眼睛啊！！！！！不知道近视又是加深的几度？！</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
