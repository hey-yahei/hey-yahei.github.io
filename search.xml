<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Winograd卷积原理</title>
      <link href="/2019/08/21/winograd_convolution/"/>
      <url>/2019/08/21/winograd_convolution/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=4956877&auto=0&height=32"></iframe>     <p>Winograd算法最早于1980年由Shmuel Winograd在《<a href="https://share.weiyun.com/5yu1Eku" target="_blank" rel="noopener">Arithmetic complexity of computations(1980)</a>》中提出，主要用来减少FIR滤波器的计算量。<br>该算法类似FFT，将数据映射到另一个空间上，用加减运算代替部分乘法运算，在“加减运算速度远高于乘法运算”的前提下达到明显的加速效果（与FFT不同的是，Winograd将数据映射到一个实数空间而非复数空间）。<br>比如，<br>直接实现一个 $m$ 输出、$r$ 参数的FIR滤波器 $F(m,r)$，一共需要 $m \times r$ 次乘法运算；<br>但使用Winograd算法，忽略变换过程的话，仅仅需要 $m + r - 1$ 次乘法运算。      </p><h3 id="F-2-3"><a href="#F-2-3" class="headerlink" title="$F(2,3)$"></a>$F(2,3)$</h3><p><strong>如果直接计算 $F(2,3)$</strong>：<br>$$<br>F(2,3)=\left[\begin{array}{lll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} \\ {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]\left[\begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[\begin{array}{l}{d_0g_0+d_1g_1+d_2g_2} \\ {d_1g_0+d_2g_1+d_3g_2}\end{array}\right]<br>$$<br>其中，<br>$d_0, d_1, d_2$和$d_1, d_2, d_3$为连续的两个输入序列；<br>$g_0, g_1, g_2$为FIR的三个参数；<br>这个过程一共需要6次乘法，和4次加法       </p><p><strong>而Winograd算法指出，$F(2,3)$ 可以这样计算</strong>：<br>$$<br>F(2,3)=\left[\begin{array}{lll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} \\ {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]\left[\begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[\begin{array}{l}{m_{1}+m_{2}+m_{3}} \\ {m_{2}-m_{3}-m_{4}}\end{array}\right]<br>$$<br>其中，<br>$$<br>\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right) g_{0}} &amp; {m_{2}=\left(d_{1}+d_{2}\right) \frac{g_{0}+g_{1}+g_{2}}{2}} \\ {m_{4}=\left(d_{1}-d_{3}\right) g_{2}} &amp; {m_{3}=\left(d_{2}-d_{1}\right) \frac{g_{0}-g_{1}+g_{2}}{2}}\end{array}<br>$$      </p><p>该用矩阵运算可以表示成：<br>$$<br>Y=A^{T}\left[(G g) \odot\left(B^{T} d\right)\right]<br>$$<br>其中，$\odot$表示点乘，而<br>$$<br>B^{T}=\left[\begin{array}{rrrr}{1} &amp; {0} &amp; {-1} &amp; {0} \\ {0} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {-1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {0} &amp; {-1}\end{array}\right],<br>G=\left[\begin{array}{rrr}{1} &amp; {0} &amp; {0} \\ {\frac{1}{2}} &amp; {\frac{1}{2}} &amp; {\frac{1}{2}} \\ {\frac{1}{2}} &amp; {-\frac{1}{2}} &amp; {\frac{1}{2}} \\ {0} &amp; {0} &amp; {1}\end{array}\right],<br>A^{T}=\left[\begin{array}{rrrr}{1} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {-1} &amp; {-1}\end{array}\right]<br>$$<br>$$<br>g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>d=\left[\begin{array}{llll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]^{T}<br>$$      </p><p>这……似乎反而把问题变得十分复杂，但实际上它的计算量却真真切切地减少了——      </p><ol><li>由于$g$是固定的FIR滤波器参数，那么$Gg$可以提前计算并得到一个 $4\times1$ 的列向量    </li><li>由于$d$是变化的输入序列，所以每次计算FIR的时候都需要对输入$d$做一个变换$B^Td$，得到一个 $4 \times 1$ 的列向量，这个过程需要4次加法（注意$B^T$矩阵的元素值）      </li><li>然后$Gg$和$B^Td$进行点乘，共计4次乘法      </li><li>最后$A^T$与$[(Gg)\odot(B^Td)]$做乘法，共计4次加法</li></ol><p>过程1可以提前完成，变换过程2和计算过程3、4共计<strong>4次乘法和8次加法</strong>，相比于直接FIR的<strong>6次乘法、4次加法</strong>，乘法次数下降为原来的$\frac{2}{3}$（推广到一般情况，直接FIR跟Winograd的乘法次数分别是$m \times r$和$m+r-1$）。<br>但天下没有免费的午餐，既然速度得到提升，那么肯定需要付出代价——算法的加速往往是需要以额外的空间为代价的：原先FIR只需要存储3个参数$g$，但现在需要存储4个参数$Gg$（推广到一般情况，分别是$r$和$m+r-1$）；      </p><h3 id="F-2-times2-3-times3"><a href="#F-2-times2-3-times3" class="headerlink" title="$F(2\times2, 3\times3)$"></a>$F(2\times2, 3\times3)$</h3><p>参考arm的一份幻灯片：<a href="https://www.slideshare.net/embeddedvision/even-faster-cnns-exploring-the-new-class-of-winograd-algorithms-a-presentation-from-arm?from_action=save" target="_blank" rel="noopener">Even Faster CNNs: Exploring the New Class of Winograd Algorithms</a>      </p><p>接下来我们将一维的$F(2,3)$扩展到二维的$F(2\times2, 3\times3)$，有两种扩展方式，一是通过堆叠$F(2,3)$来实现$F(2\times2, 3\times3)$，二是通过嵌套$F(2,3)$来实现$F(2\times2, 3\times3)$。后者计算量减小幅度更大，但前者占用内存更少。以k3s1的卷积为例——<br><em>为了跟幻灯片的符号统一，在这一部分中用$k$来表示输入，$w$表示权重，$r$表示输出</em><br><img src="/imgs/winograd/conv_k3s1.jpg" alt="conv_k3s1">    </p><p>$$<br>W = \left[\begin{array}{lll}{w_{0}} &amp; {w_{1}} &amp; {w_{2}} \\ {w_{3}} &amp; {w_{4}} &amp; {w_{5}} \\ {w_{6}} &amp; {w_{7}} &amp; {w_{8}}\end{array}\right]<br>$$<br>对直接卷积来说，该过程一共需要36次乘法和32次加法。         </p><p>参考<a href="https://www.slideshare.net/embeddedvision/even-faster-cnns-exploring-the-new-class-of-winograd-algorithms-a-presentation-from-arm?from_action=save" target="_blank" rel="noopener">Even Faster CNNs: Exploring the New Class of Winograd Algorithms</a>，将输入按滑窗分块后展开成向量并堆叠成矩阵，将权重展开成向量——<br><img src="/imgs/winograd/F_2x2_3x3_1.jpg" alt="F_2x2_3x3_1">     </p><p>对矩阵和向量进行分块——<br><img src="/imgs/winograd/F_2x2_3x3_2.jpg" alt="F_2x2_3x3_2">    </p><h4 id="堆叠实现"><a href="#堆叠实现" class="headerlink" title="堆叠实现"></a>堆叠实现</h4><p>$$<br>\begin{aligned}<br>\left[\begin{array}{lll}{K_0} &amp; {K_1} &amp; {K_2} \\ {K_1} &amp; {K_2} &amp; {K_3} \end{array}\right]<br>\left[\begin{array}{l}{W_0} \\ {W_1} \\ {W_2} \end{array}\right] &amp;= \left[\begin{array}{l}{R_0} \\ {R_1} \end{array}\right] = \left[\begin{array}{l}{K_0W_0+K_1W_1+K_2W_2} \\ {K_1W_0+K_2W_1+K_3W_2} \end{array}\right] \\<br>\\<br>&amp;= \left[\begin{array}{l}{F_{(2,3)}(D_0,W_0)+F_{(2,3)}(D_1,W_1)+F_{(2,3)}(D_2,W_2)} \\ {F_{(2,3)}(D_1,W_0)+F_{(2,3)}(D_2,W_1)+F_{(2,3)}(D_3,W_2)} \end{array}\right]<br>\end{aligned}<br>$$<br>其中，$D_i$是$K_i$对应的输入序列，也即卷积输入的第$i$行<br>$$<br>D = \left[\begin{array}{llll}<br>{k_0} &amp; {k_4} &amp; {k_8} &amp; {k_{12}} \\<br>{k_1} &amp; {k_5} &amp; {k_9} &amp; {k_{13}} \\<br>{k_2} &amp; {k_6} &amp; {k_{10}} &amp; {k_{14}} \\<br>{k_3} &amp; {k_7} &amp; {k_{11}} &amp; {k_{15}}<br>\end{array}\right] = \left[\begin{array}{l} D_0 &amp; D_1 &amp; D_2 &amp; D_3 \end{array}\right]<br>$$</p><p>也就是说，$F(2\times2, 3\times3)$在这里分成了6次$F(2,3)$以及4次额外的加法，总计<strong>24次乘法和44次加法</strong>（注意：虽然这里做了6次$F(2,3)$但是输入序列的变换只需要做4次，所以加法次数是44次而非52次），相比于直接卷积的<strong>36次乘法和32次加法</strong>，乘法次数跟一维的$F(2,3)$一样，也下降为原来的$\frac{2}{3}$，同理也需要付出3倍于$F(2,3)$额外的空间代价（三个$W$）：   </p><h4 id="嵌套实现"><a href="#嵌套实现" class="headerlink" title="嵌套实现"></a>嵌套实现</h4><p>参考《<a href="https://www.cnblogs.com/shine-lee/p/10906535.html#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84winograd" target="_blank" rel="noopener">卷积神经网络中的Winograd快速卷积算法 | shinelee, 博客园</a>》<br>$$<br>\begin{aligned}<br>\left[\begin{array}{lll}{K_0} &amp; {K_1} &amp; {K_2} \\ {K_1} &amp; {K_2} &amp; {K_3} \end{array}\right] &amp;=<br>\left[ \begin{array}{c}{R_0} \\ {R_1}\end{array}\right] =<br>\left[ \begin{array}{c}{K_0 W_0 + K_1 W_1 + K_2 W_2} \\ {K_1 W_0 + K_2 W_1 + K_3 W_2} \end{array} \right] \\<br>\\<br>&amp;= \left[\begin{array}{l}{F_{(2,3)}(D_0,W_0)+F_{(2,3)}(D_1,W_1)+F_{(2,3)}(D_2,W_2)} \\ {F_{(2,3)}(D_1,W_0)+F_{(2,3)}(D_2,W_1)+F_{(2,3)}(D_3,W_2)} \end{array}\right]<br>\\<br>&amp;= \left[ \begin{array}{c} {A^{T}\left[(G W_0) \odot\left(B^{T} D_0 \right)\right] + A^{T}\left[(G W_1) \odot\left(B^{T} D_1 \right)\right] + A^{T}\left[(G W_2) \odot\left(B^{T} D_2 \right)\right]} \\ {A^{T}\left[(G W_0) \odot\left(B^{T} D_1 \right)\right] + A^{T}\left[(G W_1) \odot\left(B^{T} D_2 \right)\right] + A^{T}\left[(G W_2) \odot\left(B^{T} D_3 \right)\right]} \end{array} \right] \\<br>\\<br>&amp;=A^{T}\left[\left[G [W_0 \ W_1 \ W_2 ] G^{T}\right] \odot\left[B^{T} [D_0 \ D_1 \ D_2 \ D_3] B\right]\right]A \\<br>\\<br>&amp;=A^{T}\left[\left[G w G^{T}\right] \odot\left[B^{T} d B\right]\right] A \\<br>\\<br>&amp;\textit{(…w =&gt; g…)} \\<br>\\<br>&amp;=A^{T}\left[\left[G g G^{T}\right] \odot\left[B^{T} d B\right]\right] A<br>\end{aligned}<br>$$<br>也即，<br>$$F(2\times2, 3\times3) = A^{T} \left[ U \odot V \right] A$$<br>其中，<br>$$U = G g G^{T}$$<br>$$V = B^{T} d B$$      </p><p>与$F(2,3)$同理，可以推导出，$F(2\times2, 3\times3)$需要<strong>16次乘法和56次加法</strong>（$V=B^{T} d B$过程32次加法、$M=U \odot V$过程16次乘法、$Y=A^TMA$过程24次加法），相比于直接卷积的<strong>36次乘法和32次加法</strong>，乘法次数下降为原来的$\frac{16}{36}$。计算量的减少比堆叠实现要明显，但也需要更多的额外空间代价：直接计算只需要存储9个参数的$g$，$F(2\times2,3\times3)$则需要存储16个参数的$GgG^T$（推广到一般情况，分别为$r^2$和$(r+m-1)^2$）</p><h3 id="G-、-B-T-、-A-T"><a href="#G-、-B-T-、-A-T" class="headerlink" title="$G$、$B^T$、$A^T$"></a>$G$、$B^T$、$A^T$</h3><p>Winograd算法需要推导出相应的变换矩阵$G$、$B^T$和$A^T$，但具体的推导过程似乎有些复杂，我现在还没弄懂。所幸 <a href="https://github.com/andravin/wincnn" target="_blank" rel="noopener">wincnn | github</a>提供了一个解算$G$、$B^T$、$A^T$的工具，除了前述的$F(2,3)$，常用的还有$F(4,3)$和$F(6,3)$，它们对应的变换矩阵如下：    </p><ul><li>$F(4,3)$<br>  $$<br>  B^{T}=\left[\begin{array}{rrrrrr}<br>  {4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-4} &amp; {-4} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {4} &amp; {-4} &amp; {-1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-2} &amp; {-1} &amp; {2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {2} &amp; {-1} &amp; {-2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  G=\left[\begin{array}{rrr}<br>  {1/4} &amp; {0} &amp; {0} \\<br>  {-1/6} &amp; {-1/6} &amp; {-1/6} \\<br>  {-1/6} &amp; {1/6} &amp; {-1/6} \\<br>  {1/24} &amp; {1/12} &amp; {1/6} \\<br>  {1/24} &amp; {-1/12} &amp; {1/6} \\<br>  {0} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  \\<br>  A^{T}=\left[\begin{array}{rrrrrr}<br>  {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {2} &amp; {-2} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {4} &amp; {4} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {8} &amp; {-8} &amp; {1}<br>  \end{array}\right]<br>  $$<br>  $$<br>  g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>  d=\left[\begin{array}{llllll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}} &amp; {d_4} &amp; {d_5}\end{array}\right]^{T}<br>  $$  </li><li>$F(6,3)$<br>  $$<br>  B^{T}=\left[\begin{array}{rrrrrrrr}<br>  {1} &amp; {0} &amp; {-21/4} &amp; {0} &amp; {21/4} &amp; {0} &amp; {-1} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {-17/4} &amp; {-17/4} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-1} &amp; {1} &amp; {17/4} &amp; {-17/4} &amp; {-1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {1/2} &amp; {1/4} &amp; {-5/2} &amp; {-5/4} &amp; {2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-1/2} &amp; {1/4} &amp; {5/2} &amp; {-5/4} &amp; {-2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {2} &amp; {4} &amp; {-5/2} &amp; {-5} &amp; {1/2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-2} &amp; {4} &amp; {5/2} &amp; {-5} &amp; {-1/2} &amp; {1} &amp; {0} \\<br>  {0} &amp; {-1} &amp; {0} &amp; {21/4} &amp; {0} &amp; {-21/4} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  \\<br>  G=\left[\begin{array}{rrr}<br>  {1} &amp; {0} &amp; {0} \\<br>  {-2/9} &amp; {-2/9} &amp; {-2/9} \\<br>  {-2/9} &amp; {2/9} &amp; {-2/9} \\<br>  {1/90} &amp; {1/45} &amp; {2/45} \\<br>  {1/90} &amp; {-1/45} &amp; {2/45} \\<br>  {32/45} &amp; {16/45} &amp; {8/45} \\<br>  {32/45} &amp; {-16/45} &amp; {8/45} \\<br>  {0} &amp; {0} &amp; {1}<br>  \end{array}\right],<br>  \\<br>  A^{T}=\left[\begin{array}{rrrrrrrr}<br>  {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {2} &amp; {-2} &amp; {1/2} &amp; {-1/2} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {4} &amp; {4} &amp; {1/4} &amp; {1/4} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {8} &amp; {-8} &amp; {1/8} &amp; {-1/8} &amp; {0} \\<br>  {0} &amp; {1} &amp; {1} &amp; {16} &amp; {16} &amp; {1/16} &amp; {1/16} &amp; {0} \\<br>  {0} &amp; {1} &amp; {-1} &amp; {32} &amp; {-32} &amp; {1/32} &amp; {-1/32} &amp; {1} \\<br>  \end{array}\right]<br>  $$<br>  $$<br>  g=\left[\begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},<br>  d=\left[\begin{array}{llllll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}} &amp; {d_4} &amp; {d_5} &amp; {d_6} &amp; {d_7}\end{array}\right]^{T}<br>  $$  </li></ul><p>注意：与$F(2,3)$不同，由于$F(4,3)$和$F(6,3)$的$A^T$、$B^T$出现了非$[0,1,-1]$的元素，所以除了点乘过程，还会引入额外的乘法运算。      </p><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p>接下来我们对$F(2,3)$、$F(4,3)$、$F(6,3)$、$F(2\times2,3\times3)$、$F(4\times4,3\times3)$、$F(6\times6,3\times3)$的效果做一简单比较。      </p><table><thead><tr><th style="text-align:center">Winograd</th><th style="text-align:center">原始乘法次数</th><th style="text-align:center">Win乘法次数</th><th style="text-align:center">理论加速比</th><th style="text-align:center">乘法加速比（含变换）</th><th style="text-align:center">原始内存</th><th style="text-align:center">Win内存</th><th style="text-align:center">内存增长</th></tr></thead><tbody><tr><td style="text-align:center">$F(2,3)$</td><td style="text-align:center">6</td><td style="text-align:center">4(4)</td><td style="text-align:center">1.50</td><td style="text-align:center">1.50</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">1.33</td></tr><tr><td style="text-align:center">$F(4,3)$</td><td style="text-align:center">12</td><td style="text-align:center">12(6)</td><td style="text-align:center">2.00</td><td style="text-align:center"><font color="red">1.00</font></td><td style="text-align:center">3</td><td style="text-align:center">6</td><td style="text-align:center">2.00</td></tr><tr><td style="text-align:center">$F(6,3)$</td><td style="text-align:center">18</td><td style="text-align:center">28(8)</td><td style="text-align:center">2.25</td><td style="text-align:center"><font color="red">0.64</font></td><td style="text-align:center">3</td><td style="text-align:center">8</td><td style="text-align:center">2.67</td></tr><tr><td style="text-align:center">堆$F(2\times2,3\times3)$</td><td style="text-align:center">36</td><td style="text-align:center">24(24)</td><td style="text-align:center">1.50</td><td style="text-align:center">1.50</td><td style="text-align:center">9</td><td style="text-align:center">12</td><td style="text-align:center">1.33</td></tr><tr><td style="text-align:center">堆$F(4\times4,3\times3)$</td><td style="text-align:center">144</td><td style="text-align:center">126(72)</td><td style="text-align:center">2.00</td><td style="text-align:center">1.14</td><td style="text-align:center">9</td><td style="text-align:center">18</td><td style="text-align:center">2.00</td></tr><tr><td style="text-align:center">堆$F(6\times6,3\times3)$</td><td style="text-align:center">324</td><td style="text-align:center">396(144)</td><td style="text-align:center">2.25</td><td style="text-align:center"><font color="red">0.82</font></td><td style="text-align:center">9</td><td style="text-align:center">24</td><td style="text-align:center">2.67</td></tr><tr><td style="text-align:center">嵌$F(2\times2,3\times3)$</td><td style="text-align:center">36</td><td style="text-align:center">16(16)</td><td style="text-align:center">2.25</td><td style="text-align:center">2.25</td><td style="text-align:center">9</td><td style="text-align:center">16</td><td style="text-align:center">1.78</td></tr><tr><td style="text-align:center">嵌$F(4\times4,3\times3)$</td><td style="text-align:center">144</td><td style="text-align:center">48(36)</td><td style="text-align:center">4.00</td><td style="text-align:center">3.00</td><td style="text-align:center">9</td><td style="text-align:center">36</td><td style="text-align:center">4.00</td></tr><tr><td style="text-align:center">嵌$F(6\times6,3\times3)$</td><td style="text-align:center">324</td><td style="text-align:center">102(64)</td><td style="text-align:center">5.06</td><td style="text-align:center">3.18</td><td style="text-align:center">9</td><td style="text-align:center">64</td><td style="text-align:center">7.11</td></tr></tbody></table><p><em>以上数据是我自己手推的，可能有不正确的地方，欢迎指正~</em>      </p><ul><li><code>Win乘法次数</code>列括号内表示不考虑变换产生的乘法运算（即仅考虑点乘的乘法）     </li><li>注意，在$V = B^{T} d B$和$Y = A^T M A$的计算过程中，$A^T$或$B^T$矩阵内元素绝对值相同的乘法运算其实是可以合并的</li><li>从理论的乘法加速比来看（只考虑点乘的乘法），Winograd都有相当理想的加速效果，嵌套实现的$F(6\times6,3\times3)$甚至有5.06倍的加速</li><li>但考虑到$V = B^{T} d B$和$Y = A^T M A$的计算过程中也有大量的乘法，实际乘法加速效果并没有那么高，嵌套实现的$F(6\times6,3\times3)$其实也只有3.12倍的加速效果；$F(6,3)$和堆叠实现的$F(6\times6,3\times3)$反而出现明显的减速；     </li><li>$F(2,3)$和$F(2\times2, 3\times3)$非常有意思，其$A^T$矩阵和$B^T$矩阵的元素均为$[0,1,-1]$——也就是说在变换过程中不会产生额外的乘法开销</li><li>从额外的内存开销上来看（仅考虑参数占用的内存），二维大于一维、嵌套实现大于堆叠实现</li><li>尽管$V = B^{T} d B$和$Y = A^T M A$的计算过程中也有大量的乘法，但观察可以发现$F(4,3)$和$F(6,3)$的$A^T$矩阵和$B^T$中有相当多的元素恰好是$2^n$，也就是说，用Winograd计算量化的卷积应该会有神奇的加成（<em>对浮点运算来说，能否直接通过对指数部分做加法实现？？</em>）    </li></ul><h3 id="Winograd卷积"><a href="#Winograd卷积" class="headerlink" title="Winograd卷积"></a>Winograd卷积</h3><p>以上介绍了一维和二维的Winograd算法，但实际在神经网络的特征图却通常都是三维的，没法直接往上套。不过前文在介绍二维Winograd的时候，我们除了嵌套之外还用了堆叠一维Winograd来达到二维Winograd的结果，同样的也可以用堆叠的二维Winograd来将其应用到三维的卷积当中。    </p><p>前述只讨论了一些比较简单的情况，事实上在CNN中，由于输入的特征图只需要变换一次，而却会被多个滤波器复用，所以输入变换过程的额外开销会被平摊——卷积的滤波器（也即输出通道）越多，那么输入变换产生的额外开销的影响就越小。    </p><p>至于特征图的额外空间代价，嵌套实现的$F(m \times m,r \times r)$会产生$(\frac{m+r-1}{m})^2$倍的内存占用（而<a href="/2019/03/28/Conv-Family/#im2col">im2col</a>为$r^2$倍）。</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNet全家桶</title>
      <link href="/2019/07/24/MobileNet-Family/"/>
      <url>/2019/07/24/MobileNet-Family/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1329129037&auto=0&height=32"></iframe><p>MobileNet自2017年发布v1以来就被广泛应用在移动端，随后又分别在2018年和2019年发布了v2和v3。去年《<a href="/2018/08/05/MobileNets_v1/">MobileNets v1模型解析 | Hey~YaHei！</a>》一文中已经讨论过v1的主要贡献，趁着前阵子<em>（已经是两个月前了其实）</em>刚刚发布v3，不妨把整个MobileNet家族放在一起稍作讨论。     </p><h3 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h3><p>论文：《<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(2017)</a>》     </p><h4 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>用深度可分离卷积（DW卷积提取特征+点卷积组合特征）取代传统的卷积，大幅提升特征提取的效率</li><li>进而利用深度可分离卷积设计出高效的直筒式网络MobileNet       </li></ol><h4 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h4><h5 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h5><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center>   <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用M个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>可以看到，传统的卷积中用 $N$ 个不同的卷积核不厌其烦地对原图进行特征提取来得到 $N$ 通道的输出，这其中必定从原图中提取到了大量的重复特征。有没有可能<strong>只用单个卷积核来做特征提取，最后依旧能输出多通道的特征图</strong>呢？——这就是深度向卷积分解的核心思路（或者换个角度，DW卷积就是分组卷积的一种极端情况）。<br>观察①阶段中的第三个子阶段，该阶段将多张单通道特征图按通道堆叠起来之后“拍扁”，如果去掉这个“拍扁”的过程，其实就可以提取得到一张 $M$ 通道的特征图啦，再经过一个 $M$ 维空间到 $N$ 维空间的线性映射，就能够和普通的卷积操作一样得到一张 $N$ 通道的特征图 $O$。完整的卷积过程如下图所示——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>①、②阶段与传统卷积①阶段的前两个阶段完全相同，③阶段比传统卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上</li></ol><h5 id="效率比较"><a href="#效率比较" class="headerlink" title="效率比较"></a>效率比较</h5><p><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><p>对于普通的卷积操作，        </p><ul><li>输出 $N$ 通道特征图需要 $N$ 个卷积核，故参数数量为 $M \times N \times D_K \times D_K$；     </li><li>一个 $D_K \times D_K$ 的卷积核在原图的某个位置的某个通道上需要进行 $D_K \times D_K$ 次乘加操作，输出特征图大小为 $D_O \times D_O$，原图通道数量为 $M$，共有 $N$ 个不同的卷积核，故乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M \times N$           </li></ul><p>对于深度向分解后的卷积操作，       </p><ul><li>特征提取只使用了一个 $D_K \times D_K$ 的卷积核，组合过程为了作线性映射用了 $N$ 个 $1 \times 1$ 的卷积核，故参数数量为 $M \times N + M \times D_K \times D_K$；       </li><li>特征提取过程只有一个卷积核，所以该过程乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M$。同样的，组合过程容易算得需要 $D_O \times D_O \times M \times N$ 次乘加操作。故乘加操作的总数量为 $D_O \times D_O \times M \times ( D_K \times D_K + N)$             </li></ul><p>总的来说，参数数量和乘加操作的运算量均下降为原来的 $\frac{1}{D_K^2} + \frac{1}{N}$，通常使用 $3 \times 3$ 的卷积核，也就是下降为原来的九分之一到八分之一左右。而从论文的实验部分来看，准确率也只有极小的下降。    </p><center><img src="/imgs/MobileNet/dw_vs_full.png" alt="dw_vs_full"></center>    <h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><center><img src="/imgs/MobileNet/v1_body_architecture.png" alt="v1_body_architecture"></center>    </p><ul><li>第一层使用普通的卷积层，最前端的特征提取非常重要，宁可存在重复的特征信息，也不该放掉；     </li><li>随后是一系列的深度向分解卷积层，用于逐层次提取特征，用步长不为1的卷积替代池化做降采样，同时整体也满足通道加深，特征图分辨率降低的CNN一般特点；        </li><li>最后也是常规的全局平均池化、全连接、Softmax；      </li><li>每次卷积操作之后都紧跟一个BN层（在预测阶段可以被合并）和一个RELU层；      <center><img src="/imgs/MobileNet/dw_vs_full_train.png" alt="dw_vs_full_train"></center>    </li><li>而且，深度向分解的卷积中绝大多数参数和运算都集中在 $1 \times 1$ 的pointwise卷积运算当中，这种运算恰恰是能够被 <code>GEneral Matrix Multiply(GEMM)</code> 函数直接实现而不需要经过im2col或im2row的；    <center><img src="/imgs/MobileNet/resource.png" alt="resource"></center>    </li><li>论文中还提到两个压缩模型的因子，分别用于输入图片分辨率收缩和网络宽度收缩来进一步精简模型；    </li><li>由于MobileNet本身为小模型，不容易过拟合，故不需要过多的正则化策略和数据增强策略；且Depthwise Convolution参数数量比较少，只需要加入一个很小的权重衰减甚至可以不需要权重衰减     </li></ul><h3 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h3><p>论文：《<a href="http://arxiv.org/pdf/1801.04381v3.pdf" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks(2018)</a>》         </p><h4 id="主要贡献-1"><a href="#主要贡献-1" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>取消通道收缩时的激活层：通道收缩时使用非线性激活会带来信息丢失      </li><li>将relu改为relu6以限制激活的输出范围       </li><li>引入反残差结构：MobileNet本身通道数量较少，引入“通道扩增-特征提取-通道收缩”的反残差结构有助于提高特征提取的能力    </li></ol><h4 id="通道收缩时使用非线性激活带来信息丢失"><a href="#通道收缩时使用非线性激活带来信息丢失" class="headerlink" title="通道收缩时使用非线性激活带来信息丢失"></a>通道收缩时使用非线性激活带来信息丢失</h4><p>用一个随机矩阵$T$，将一个初始的二维螺旋映射到一个$n$维空间后，经过一个非激活单元ReLU，再由$T^{-1}$反映射回二维空间。可以观察到，当n比较小（如n=2,3）时，反映射后的图形已经完全不像一个螺旋，出现明显的信息丢失：而当n比较大时，由于足够的冗余信息的存在，可以有效抵抗非线性单元带来的信息丢失：<br><img src="/imgs/MobileNet/relu_information_loss.jpg" alt="relu_information_loss">      </p><h4 id="反残差结构"><a href="#反残差结构" class="headerlink" title="反残差结构"></a>反残差结构</h4><p>v2参考了resnet，引入了shortcut设计，但不同的是：    </p><ul><li>resnet采用“通道收缩-特征提取-通道扩增”的残差结构，对输入特征进行压缩，也减少了特征提取的参数量和计算量；     </li><li>而MobileNet本身模型就不大，采用了相反的“通道扩增-特征提取-通道收缩”的反残差结构，以提高特征提取的能力      </li></ul><p><img src="/imgs/MobileNet/inverted_residual.jpg" alt="inverted_residual"><br><img src="/imgs/MobileNet/inverted_residual2.jpg" alt="inverted_residual2">     </p><p>同时可以看到，与残差结构不同，反残差结构在elt+的输入都是通道扩增前的小通道特征图，可以说对带宽是比较友好的。<br>对于通道扩增的倍数，论文里建议是取5-10，如MobileNet v2里取$t=6$。       </p><h4 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="/imgs/MobileNet/MobieleNet_v2.jpg" alt="MobieleNet_v2">        </p><h3 id="MobileNet-v3"><a href="#MobileNet-v3" class="headerlink" title="MobileNet v3"></a>MobileNet v3</h3><p>论文：《<a href="https://arxiv.org/pdf/1905.02244" target="_blank" rel="noopener">Searching for MobileNetV3(2019)</a>》      </p><h4 id="主要贡献-2"><a href="#主要贡献-2" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>用NAS搜索整体的网络架构    </li><li>用NetAdapt搜索合适的网络宽度    </li><li>引入注意力机制的SE模块    </li><li>引入h-swish激活函数</li></ol><h4 id="网络搜索"><a href="#网络搜索" class="headerlink" title="网络搜索"></a>网络搜索</h4><p>正如论文标题所示，MobileNet v3的结构主要是通过Network Architecture Search(NAS)和NetAdapt搜索出来的。不过也有手工调整的部分，比如最后一个stage，作者将全局平均池化提前，并取消其中繁冗的特征提取、通道收缩过程，从而进一步提高的模型的效率。<br><img src="/imgs/MobileNet/v3_last_stage.jpg" alt="v3_last_stage">         </p><h4 id="注意力机制：SE模块"><a href="#注意力机制：SE模块" class="headerlink" title="注意力机制：SE模块"></a>注意力机制：SE模块</h4><p>v3引入了《<a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">Squeeze-and-Excitation Networks(2017)</a>》的SE模块，通过全连接层对全局平均池化后的特征图进行通道重要性的评估，并将权重应用到特征图的通道上。<br><img src="/imgs/MobileNet/v3_block.jpg" alt="v3_block">        </p><h4 id="h-swish激活函数"><a href="#h-swish激活函数" class="headerlink" title="h-swish激活函数"></a>h-swish激活函数</h4><p>《<a href="http://arxiv.org/pdf/1702.03118" target="_blank" rel="noopener">Sigmoid-weighted linear units for neural network function approximation in reinforcement learning(2017)</a>》提出了新型激活函数swish并取得不错的效果——<br>$$swish(x) = x \cdot \sigma(x)$$<br>其中$\sigma(\cdot)$为sigmoid函数，<br>$$\sigma(x)=\frac{1}{1+e^{-x}}$$     </p><p>不过sigmoid函数的指数运算终究太麻烦了，于是v3提出了hard-swish，通过分段函数ReLU6和相应的线性变换来近似sigmoid函数：<br>$$\text{h-swish}(x) = x \cdot \frac{ReLU6(x+3)}{6}$$      </p><p><strong>sigmoid和h-sigmoid、swish和h-swish的比较如下</strong>：<br><img src="/imgs/MobileNet/h-swish.jpg" alt="h-swish">      </p><p>作者发现，    </p><ol><li>近似版本的h-swish能起到跟swish十分接近的作用       </li><li>h-swish无论在软件还是硬件层面都非常容易实现，而且解决了量化网络时sigmoid的近似实现带来的数值精度损失问题    </li></ol><h4 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h4><p>左图为large版本，右图为small版本——<br><img src="/imgs/MobileNet/MobileNet_v3.jpg" alt="MobileNet_v3">      </p><ol><li>是否使用SE是platform-aware NAS权衡速度和精度之后做出的选择     </li><li>由于h-swish的速度比relu慢，而浅层的特征图比较大，所以网络的浅层更倾向于更快的relu，只有在深层使用h-swish；不过第一层卷积的通道数比较少，再加上这里h-swish替代relu有比较好的效果，所以v3还是倾向于第一层卷积依旧使用h-swish</li><li>尽管3x3卷积已经成为手工设计网络主流，但似乎5x5卷积也有用武之地</li></ol><h3 id="FD-MobileNet"><a href="#FD-MobileNet" class="headerlink" title="FD-MobileNet"></a>FD-MobileNet</h3><p>论文：《<a href="http://arxiv.org/pdf/1802.03750" target="_blank" rel="noopener">FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy(2018)</a>》     </p><p>Fast-Downsample MobileNet是MobileNet-v1的一个变种，采用快速下采样策略（提前下采样）来减小特征图尺寸，并增大网络宽度提高模型的特征提取能力，<strong>FD-MobileNet1.0和MobileNet0.5的比较</strong>——<br><img src="/imgs/MobileNet/FD-MobileNet.jpg" alt="FD-MobileNet">     </p><h3 id="ImageNet上的实验结果"><a href="#ImageNet上的实验结果" class="headerlink" title="ImageNet上的实验结果"></a>ImageNet上的实验结果</h3><h4 id="MobileNet-v1-1"><a href="#MobileNet-v1-1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h4><p><img src="/imgs/MobileNet/v1_experiment.jpg" alt="v1_experiment"></p><h4 id="MobileNet-v2-1"><a href="#MobileNet-v2-1" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h4><p><img src="/imgs/MobileNet/v2_experiment.jpg" alt="v2_experiment"></p><h4 id="MobileNet-v3-1"><a href="#MobileNet-v3-1" class="headerlink" title="MobileNet v3"></a>MobileNet v3</h4><p><img src="/imgs/MobileNet/v3_experiment.jpg" alt="v3_experiment"></p><h4 id="FD-MobileNet-1"><a href="#FD-MobileNet-1" class="headerlink" title="FD-MobileNet"></a>FD-MobileNet</h4><p><img src="/imgs/MobileNet/FD_experiment.jpg" alt="FD_experiment"></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>线性量化</title>
      <link href="/2019/07/23/Quantization/"/>
      <url>/2019/07/23/Quantization/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=638385&auto=0&height=32"></iframe>     <p>简单来说，量化就是将浮点存储（运算）转换为整型存储（运算）的一种模型压缩技术。如果按《<a href="https://www.jiqizhixin.com/articles/2018-05-22-9" target="_blank" rel="noopener">当前深度神经网络模型压缩和加速都有哪些方法？-机器之心</a>》一文的分类方式，量化属于参数共享的一种——将原始数据聚为若干类（比如int8量化为$2^8=256$类），量化后的整型值就相当于类的索引号。    </p><p>按照聚类中心是否均匀分布，可以把量化分为线性量化和非线性量化。       </p><ul><li>如《<a href="https://arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding(2016)</a>》就是典型的<strong>非线性量化</strong>，作者采用不规则的聚类中心，通过KMEANS对浮点权重进行量化，并在训练过程中逐步修正聚类中心——不规则的聚类中心往往能在相同的比特数下取得更小的量化误差，但由于必须先查表取得原始数据再进行计算，所以对计算的加速没有任何帮助，这种方法更多是用来压缩模型文件大小。   </li><li>与非线性量化不同，<strong>线性量化</strong>采用均匀分布的聚类中心，原始数据跟量化后的数据存在一个简单的线性变换关系。而卷积、全连接本身也只是简单的线性计算，因此在线性量化中可以直接用量化后的数据进行直接计算，不仅可以压缩模型文件的大小，还能带来明显的速度提升。        </li></ul><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><ol><li><strong>更少的存储开销和带宽需求</strong><br> 使用更少的比特数存储数据，有效减少应用对存储资源的依赖（但现代系统往往拥有相对丰富的存储资源，这一点已经不算是采用量化的主要动机）     </li><li><strong>更快的计算速度</strong><br> 对多数处理器而言，整型运算的速度一般（但不总是）比浮点运算更快     </li><li><strong>更低的能耗与占用面积</strong><br> <img src="/imgs/Quantization/energy_and_area.jpg" alt="energy_and_area"><br> <em>数据来源：《<a href="https://media.nips.cc/Conferences/2015/tutorialslides/Dally-NIPS-Tutorial-2015.pdf" target="_blank" rel="noopener">High-Performance Hardware for Machine Learning(NIPS2015)</a>》</em><br> 从上图可以看到，FP32乘法的能耗是INT8乘法能耗的18.5倍，芯片占用面积则是27.3倍——对芯片设计和FPGA设计而言，更少的资源占用意味着相同数量的单元下可以设计出更多的计算单元；而更少的能耗意味着更少的发热，和更长久的续航。     </li><li><strong>尚可接受的精度损失</strong><br> 量化相当于对模型权重引入噪声，所幸CNN本身对噪声不敏感（甚至在训练过程中，模拟量化所引入的权重加噪还有利于防止过拟合），在合适的比特数下量化后的模型并不会带来很严重的精度损失。按照<a href="https://gluon-cv.mxnet.io/model_zoo/classification.html" target="_blank" rel="noopener">gluoncv</a>提供的报告，经过int8量化之后，ResNet50_v1和MobileNet1.0_v1在ILSVRC2012数据集上的准确率仅分别从77.36%、73.28%下降为76.86%、72.85%。</li><li><strong>趋势</strong><br> 无论是移动端还是服务器端，都可以看到新的计算设备正不断迎合量化技术。比如NPU/APU/AIPU等基本都是支持int8（甚至更低精度的int4）计算的，并且有相当可观的TOPs，而Mali GPU开始引入int8 dot支持，Nvidia也不例外——<br> <img src="/imgs/Quantization/GTX-vs-RTX-and-NPU.jpg" alt="GTX-vs-RTX-and-NPU"></li></ol><h3 id="线性量化"><a href="#线性量化" class="headerlink" title="线性量化"></a>线性量化</h3><p>常见的线性量化过程可以用以下数学表达式来表示：<br>$$r = Round(S(q - Z))$$<br>其中，<br>$q$ 是float32的原始值；<br>$Z$ 是float32的偏移量，也可以量化为int32；<br>$S$ 是float32的缩放因子；<br>$Round(\cdot)$ 是四舍五入近似取整的数学函数，除了四舍五入，向上、向下取整也是可以的；<br>$r$ 是量化后的一个整数值。       </p><p>而我们需要做的，就是确定合适的 $S$ 和 $Z$        </p><h4 id="对称和非对称"><a href="#对称和非对称" class="headerlink" title="对称和非对称"></a>对称和非对称</h4><p>参考：<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Algorithms - Quantization | Distiller</a><br>根据参数 $Z$ 是否为零可以将线性量化分为两类——对称和非对称。      </p><ul><li><p><strong>对称量化</strong><br>  <center><img src="/imgs/Quantization/symmetric-mode.png" alt="symmetric-mode"></center><br>  在对称量化中，$r$ 是用有符号的整型数值来表示的，此时 $Z=0$，且 $q=0$ 时恰好有 $r=0$。<br>  比如简单地取，<br>  $$S = \frac{2^{n-1} - 1}{max(|x|)}$$<br>  $$Z = 0$$<br>  其中，<br>  $n$ 是用来表示该数值的位宽，<br>  $x$ 是数据集的总体样本。       </p><p>  对称量化比较简单，不仅实现简单，而且由于$Z=0$运算也变得非常简单。      </p></li><li><p><strong>非对称量化</strong><br>  <center><img src="/imgs/Quantization/asymmetric-mode.png" alt="asymmetric-mode"></center><br>  比如简单地取，<br>  $$S = \frac{2^{n-1} - 1}{max(x)-min(x)}$$<br>  $$Z = min(x)$$<br>  非对称量化比较灵活，通常 $r$ 是用无符号的整型数值来表示，此时 $Z \neq 0$。        </p></li></ul><h4 id="逐层、逐组和逐通道"><a href="#逐层、逐组和逐通道" class="headerlink" title="逐层、逐组和逐通道"></a>逐层、逐组和逐通道</h4><p>按照量化的粒度（共享量化参数的范围）可以分为逐层、逐组和逐通道——     </p><ul><li><strong>逐层</strong>量化以一个层为单位，整个layer的权重共用一组缩放因子$S$和偏移量$Z$；      </li><li><strong>逐组</strong>量化以组为单位，每个group使用一组$S$和$Z$；</li><li><strong>逐通道</strong>量化则以通道为单位，每个channel单独使用一组$S$和$Z$。</li></ul><p>当 $group=1$ 时，逐组量化与逐层量化等价；<br>当 $group=num\_filters$ （即dw卷积）时，逐组量化逐通道量化等价      </p><h4 id="在线和离线"><a href="#在线和离线" class="headerlink" title="在线和离线"></a>在线和离线</h4><p>按照激活值的量化方式，可以分为在线（online）量化和离线（offline）量化。       </p><ul><li>在线量化指激活值的 $S$ 和 $Z$ 在实际推断过程中根据实际的激活值动态计算；    </li><li>离线量化指提前确定好激活值的 $S$ 和 $Z$      </li></ul><p>由于不需要动态计算量化参数，通常离线量化的推断速度更快些，通常有三种方法来确定相关的量化参数——      </p><ol><li><strong>指数平滑平均</strong><br> 将校准数据集投喂给模型，收集每个量化的层的输出特征图，计算每个batch的 $S$ 和 $Z$，并通过指数平滑平均更新 $S$ 和 $Z$     </li><li><strong>直方图截断</strong><br> 以$S = \frac{2^n - 1}{max(|x|)}$为例，由于有的特征图会出现偏离较远的奇异值，导致max非常大，所以可以通过直方图截取的形式，比如抛弃最大的前1%数据，以前1%分界点的数值作为max计算量化参数      </li><li><strong>KL散度校准</strong><br> 参考：《<a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf" target="_blank" rel="noopener">8-bit Inference with TensorRT</a>》<br> TensorRT的校准方案，通过KL散度（也称为相对熵，用以描述两个分布之间的差异）来评估量化前后分布的差异，搜索并选取KL散度最小的量化参数     </li></ol><h3 id="模拟量化实验"><a href="#模拟量化实验" class="headerlink" title="模拟量化实验"></a>模拟量化实验</h3><p>我在Gluon上用mobilenet和resnet50做了简单的模拟量化实验（量化卷积和全连接）：<a href="https://github.com/hey-yahei/Quantization.MXNet#simulate-quantization" target="_blank" rel="noopener">Quantization.MXNet | github</a>          </p><table><thead><tr><th style="text-align:center">IN dtype</th><th style="text-align:center">IN offline</th><th style="text-align:center">WT dtype</th><th style="text-align:center">WT qtype</th><th style="text-align:center">Merge BN</th><th style="text-align:center">w/o 1st conv</th><th style="text-align:center">M-Top1 Acc</th><th style="text-align:center">R-Top1 Acc</th></tr></thead><tbody><tr><td style="text-align:center">float32</td><td style="text-align:center">/</td><td style="text-align:center">float32</td><td style="text-align:center">/</td><td style="text-align:center"></td><td style="text-align:center">/</td><td style="text-align:center">73.28%</td><td style="text-align:center">77.36%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">44.57%</td><td style="text-align:center">55.97%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.84%</td><td style="text-align:center">76.92%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.92%</td><td style="text-align:center">76.90%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.72%</td><td style="text-align:center">77.00%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.58%</td><td style="text-align:center">76.81%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">70.66%</td><td style="text-align:center">76.71%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">15.21%</td><td style="text-align:center">76.62%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">32.70%</td><td style="text-align:center">76.61%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">14.70%</td><td style="text-align:center">76.60%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">47.80%</td><td style="text-align:center">56.21%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.93%</td><td style="text-align:center">77.33%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.85%</td><td style="text-align:center">77.31%</td></tr><tr><td style="text-align:center">uint8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.68%</td><td style="text-align:center">77.35%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.63%</td><td style="text-align:center">77.22%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">72.68%</td><td style="text-align:center">77.08%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">x</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">72.75%</td><td style="text-align:center">77.11%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">naive</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">72.04%</td><td style="text-align:center">76.69%</td></tr><tr><td style="text-align:center">int8</td><td style="text-align:center">KL</td><td style="text-align:center">int8</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">72.67%</td><td style="text-align:center">77.07%</td></tr></tbody></table><ol><li>由于大部分网络使用relu作为激活函数，所以往往使用uint能获得更小的量化误差，不过也有的平台只支持int8的量化（比如ncnn）；     </li><li>离线量化的校准数据集从训练集抽取（每个类别抽取5张图片，共计5000张），naive校准采用指数平滑平均的方式收集最大值并计算量化参数，KL校准搜索KL散度最小的量化参数；</li><li>由于合并BN层后，不同通道的权重的分布差异变得更加显著，这对冗余网络（如resnet）跟逐通道量化的影响较小，但逐层量化紧凑网络（如mobilent）就直接崩溃（对dw卷积影响显著）；</li><li>大部分情况下naive校准和KL校准效果都挺接近，不过在合并BN后的逐通道量化下，KL校准则明显由于naive校准；</li></ol><p>为了进一步比较naive校准和KL校准，在cifar_resnet56_v1模型上又补充了几个简单实验：     </p><table><thead><tr><th style="text-align:center">IN dtype</th><th style="text-align:center">WT dtype</th><th style="text-align:center">WT qtype</th><th style="text-align:center">Merge BN</th><th style="text-align:center">w/o 1st conv</th><th style="text-align:center">Top1 Acc@naive</th><th style="text-align:center">Top1 Acc@KL</th></tr></thead><tbody><tr><td style="text-align:center">float32</td><td style="text-align:center">float32</td><td style="text-align:center">/</td><td style="text-align:center"></td><td style="text-align:center">/</td><td style="text-align:center">93.60%</td><td style="text-align:center">93.60%</td></tr><tr><td style="text-align:center">uint6</td><td style="text-align:center">int6</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">93.09%</td><td style="text-align:center">93.83%</td></tr><tr><td style="text-align:center">uint5</td><td style="text-align:center">int5</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">92.71%</td><td style="text-align:center">93.29%</td></tr><tr><td style="text-align:center">uint4</td><td style="text-align:center">int4</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">91.62%</td><td style="text-align:center">89.27%</td></tr><tr><td style="text-align:center">uint3</td><td style="text-align:center">int3</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">81.75%</td><td style="text-align:center">55.98%</td></tr></tbody></table><p>可以看到，    </p><ul><li>在比特数还比较高（量化误差比较小）的情况下，KL校准明显优于naive校准；     </li><li>但当比特数比较低的时候（量化误差变大）的情况下，KL校准的优势逐渐缩小，甚至最后崩溃            </li></ul><h3 id="重训练量化实验"><a href="#重训练量化实验" class="headerlink" title="重训练量化实验"></a>重训练量化实验</h3><p>当量化的比特数比较大的情况下（如8bits），逐通道量化、KL校准已经能取得很不错的效果；但当比特数非常小的时候，量化带来的精度损失还是比较明显的，此时我们可以按照《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2017)</a>》做简单的恢复训练：<a href="https://github.com/hey-yahei/Quantization.MXNet#quantization-aware-training" target="_blank" rel="noopener">Quantization.MXNet | github</a>          </p><p><strong>BN层的处理</strong>：<br>通常我们会把训练好的模型的BN层融合到卷积层当中来减少推断时间，量化卷积权重之前也需要先把BN层融合进去，但是在训练过程中我们又需要BN层来抑制过拟合，那么可以在训练过程中引入伪BN——<br>正常的BN是对卷积的输出特征图进行操作，而按照合并BN的思路，伪BN则是直接对权重进行操作。先用BN的参数与原始权重进行计算取得“融合”后的权重，再用新的权重进行卷积，在训练过程中正常更新原始的卷积权重和BN的参数。         </p><center><img src="/imgs/Quantization/fake_bn.jpg" alt="fake_bn"></center>      <p>使用伪BN后，每次前向传播需要两次卷积运算——     </p><ol><li>用原始权重卷积，卷积输出用于更新伪BatchNorm的平均值、标准差，这次卷积运算不需要反向传播；</li><li>另外一次用量化过的权重卷积，卷积输出的结果作为下一层的输入，这一次的卷积运算需要反向传播</li></ol><p>这样一来，训练的速度会明显降低，但却保留的BN本身的效果。      </p><p><strong>4bits量化的恢复训练效果如下</strong>：    </p><table><thead><tr><th style="text-align:center">DataType</th><th style="text-align:center">QuantType</th><th style="text-align:center">Offline</th><th style="text-align:center">Retrain</th><th style="text-align:center">FakeBN</th><th style="text-align:center">Top-1 Acc</th></tr></thead><tbody><tr><td style="text-align:center">fp32/fp32</td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">93.60%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">layer</td><td style="text-align:center">naive</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">84.95%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">layer</td><td style="text-align:center">KL</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">73.36%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">layer</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">90.77%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">channel</td><td style="text-align:center">naive</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">91.62%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">channel</td><td style="text-align:center">KL</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">89.27%</td></tr><tr><td style="text-align:center">uint4/int4</td><td style="text-align:center">channel</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">93.19%</td></tr></tbody></table><ol><li>由于比特数较低，KL校准的效果变得非常差；    </li><li>相比于naive校准，恢复训练总能取得更低一些的精度损失    </li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>漫谈卷积层</title>
      <link href="/2019/03/28/Conv-Family/"/>
      <url>/2019/03/28/Conv-Family/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=760533&auto=0&height=32"></iframe>         <p>去年刚入手深度学习的时候写过一篇《<a href="/2018/04/18/CNN/">卷积神经网络CNN | Hey~YaHei!</a>》，简单介绍过卷积层——不过经过一年的学习和实践，对卷积又有了新的认识，原本讲道理应该直接更新修改去年的那篇博文的。但是那篇博文涉及面较广，不单单讲卷积，而且之后还写过几篇引用了那篇博文的内容，拾起来魔改似乎会打乱博客的结构，想想还是新开一篇，希望各位看官别嫌弃我炒冷饭。       </p><p>本文要点：    </p><ol><li>提出卷积层的背景；</li><li>标准卷积的过程和原理；</li><li>标准卷积的计算原理（im2）；</li><li>高效卷积：分组卷积、深度向分解卷积、点向卷积；</li><li>其他卷积的变种：空洞卷积和变形卷积</li></ol><hr><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>参考：《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>》Chap13       </p><p>卷积的起源有点仿生的味道，这基于生理心理学上的研究，人在感知视觉的时候具有     </p><ol><li><strong>层次性</strong>，底层的视神经往往能感知简单的图像结构，随着层次的提高，高层的视神经可以感知到更加抽象的图形结构最后乃至图形的概念。举个栗子，最底层的视神经能够感知简单的点，随着层次的提高，逐渐能感受线、简单的线段组合、乃至有概念的图像如鱼、房子等；       <center><img width="600" src="/imgs/Conv-Family/bg-hierachy.png"></center>     </li><li><strong>局部性</strong>，视神经具备有限的感受野，能够将注意力集中的视觉上的局部区域，感受野随着神经网络的深入逐步扩大；     <center><img width="400" src="/imgs/Conv-Family/bg-local.png"></center>     </li></ol><h2 id="标准卷积（Standard-Convolution）"><a href="#标准卷积（Standard-Convolution）" class="headerlink" title="标准卷积（Standard Convolution）"></a>标准卷积（Standard Convolution）</h2><p>参考：    </p><ul><li>《<a href="/2018/04/18/CNN/#卷积层（Conv）">卷积神经网络CNN - 卷积层（Conv）| Hey~YaHei!</a>》     </li><li>《<a href="/2019/01/07/MXNet-OpSummary/#卷积">模型参数与计算量 - 卷积 | Hey~YaHei!</a>》</li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li></ul><h3 id="过程和原理"><a href="#过程和原理" class="headerlink" title="过程和原理"></a>过程和原理</h3><p>卷积层并不是使用严格数学意义的卷积运算，而是使用保留卷积性质但抛弃可交换性的互相关函数；<br><em>卷积运算具有可交换性，这在数学证明上是很有用的，但在神经网络的应用中却是一个比较鸡肋的性质</em><br>卷积操作选用一定大小的卷积核（下图黄色区域）在原始数据上移动，与重合部分数据做乘和运算；<br>依次类推，最终输出一张特征图（Feature Map）     </p><center><img width="400" src="/Handson-ML/12Convolutional_Kernel.gif"></center>       <p>卷积核的作用相当一个滤波器，其参数是经过学习得到的，可以用于提取图片中的特征；<br>由于核参数是随机初始化的，所以它们很可能会提取出不同的特征；<br>由低层的卷积层提取简单特征，然后逐层堆叠卷积层，将简单特征逐渐抽象为更高层次的语义概念；      </p><h3 id="参数和计算量"><a href="#参数和计算量" class="headerlink" title="参数和计算量"></a>参数和计算量</h3><p>首先考虑一个单通道输入输出，输出图大小为 $m_o \times n_o$，核大小为 $k_m \times k_n$，带偏置，步长1，不补零的卷积，       </p><p><center><img height="300" src="/imgs/MXNet-OpSummary/convolution.gif"></center><br>$$<br>\begin{aligned} O _ { 11 } = &amp; w _ { 11 } I _ { 11 } + w _ { 12 } I _ { 12 } + w _ { 13 } I _ { 13 } + \\ &amp; w _ { 21 } I _ { 21 } + w _ { 22 } I _ { 22 } + w _ { 23 } I _ { 23 } + \\ &amp; w _ { 31 } I _ { 31 } + w _ { 32 } I _ { 32 } + w _ { 33 } I _ { 33 } + b \end{aligned}<br>$$<br>其参数数量为 $k_m \times k_n + 1$，乘加次数为 $m_o \times n_o \times k_m \times k_n$。       </p><p>推广到$c_i$通道输入，$c_o$通道输出的情况，<br>其参数数量为 $(k_m \times k_n + 1) \times c_i \times c_o$，乘加次数为 $m_o \times n_o \times k_m \times k_n \times c_i \times c_o$。       </p><h3 id="卷积核大小"><a href="#卷积核大小" class="headerlink" title="卷积核大小"></a>卷积核大小</h3><p>通常来说，大核卷积可以被多层小核卷积所替代；<br>比如用三层3x3卷积核的卷积层可以提取到一层7x7卷积核的卷积层一样的特征——<br><img src="/Handson-ML/12Multi_Conv_Layers.png" alt="12Multi_Conv_Layers"><br>而且，使用多层小核卷积层由以下优势：    </p><ol><li>减少参数<br> 7x7卷积核有 $7 \times 7 = 49$ 个参数，而三层3x3卷积核只有 $3 \times 3 \times 3 = 27$ 个参数      </li><li>增加网络深度<br> 增加网络容量和复杂度    </li></ol><p>博文《<a href="/2019/02/28/bag-of-tricks2/#改进2：拆解大核卷积">深度学习小技巧（二）：模型微调 - 改进2：拆解大核卷积 | Hey~YaHei!</a>》也曾介绍过，改进版的Inception和ResNet目前都已经将7x7卷积层替换成了三层3x3卷积层的堆叠，这里就不再赘述。        </p><h2 id="卷积的计算原理"><a href="#卷积的计算原理" class="headerlink" title="卷积的计算原理"></a>卷积的计算原理</h2><p>参考：《<a href="/2019/02/28/bag-of-tricks2/#高效的1x1卷积">深度学习小技巧（二）：模型微调 - 高效的1x1卷积 | Hey~YaHei!</a>》      </p><p>卷积的计算方式可以分为两种——一是<strong>直接按照原理计算</strong>，二是采用<strong>im2col技术进行矩阵乘法</strong>；     </p><h3 id="直接卷积的潜在问题"><a href="#直接卷积的潜在问题" class="headerlink" title="直接卷积的潜在问题"></a>直接卷积的潜在问题</h3><p>直接卷积非常直观易懂，但它对目前流行的计算设备来说极其不友好。     </p><p>首先考虑3x3的单通道特征图，以及k2s1的卷积核——      </p><div><center><img src="/imgs/bag-of-tricks/conv_raw.jpg" alt="conv_raw"></center></div>       <p>按照卷积计算，<br>$$y_{11} = w_{11}x_{11} + w_{12}x_{12} + w_{21}x_{21} + w_{22}x_{22}$$<br>$$y_{12} = w_{11}x_{12} + w_{12}x_{13} + w_{21}x_{22} + w_{22}x_{23}$$<br>$$y_{21} = w_{11}x_{21} + w_{12}x_{22} + w_{21}x_{31} + w_{22}x_{32}$$<br>$$y_{22} = w_{11}x_{22} + w_{12}x_{23} + w_{21}x_{32} + w_{22}x_{33}$$</p><p>按照“行先序”，特征图和卷积核在内存中是这样排列的——       </p><div><center><img src="/imgs/bag-of-tricks/conv_store.jpg" alt="conv_store"></center></div>       <p>我们用不同的颜色标注出卷积计算中的访存过程（相同颜色的数据相乘）——       </p><div><center><img src="/imgs/bag-of-tricks/conv_access_ram.jpg" alt="conv_access_ram"></center></div>      <p>众所周知，由于程序的<strong>局部性原理</strong>（通常相邻代码段会访问相邻的内存块），现代处理器通常会按块从内存中读取数据到高速缓存中以缓解访存速度和计算速度的巨大差异导致的“<strong>内存墙</strong>”问题。换句话说，如果计算需要从内存中读取<code>x12</code>的数据，那么往往相邻的<code>x11</code>、<code>x13</code>等数据也会被一起读取到高速缓存上，当下次计算需要用到<code>x11</code>或<code>x13</code>时处理器就可以快速地从高速缓存中取出数据而不需要从内存中调取，大大提高了程序的速度。<br><em>注：L1缓存的读取速度是RAM的50-100倍！（数据来源：《<a href="https://book.douban.com/subject/7006537/" target="_blank" rel="noopener">计算机体系结构：量化研究方法</a>》）</em>       </p><p>而从上边展示出来的访存过程中可以看到，直接对于特征图数据的访问过程十分散乱，直接用行先序存储的特征图参与计算是非常愚蠢的选择。      </p><h3 id="im2col"><a href="#im2col" class="headerlink" title="im2col"></a>im2col</h3><p><center><img src="/imgs/bag-of-tricks/conv_im2col.png" alt="conv_im2col"></center><br>其实思路非常简单：把每一次循环所需要的数据都排列成列向量，然后逐一堆叠起来形成矩阵（按通道顺序在列方向上拼接矩阵）。<br>比如$C_i \times W_i \times H_i$大小的输入特征图，$K \times K$大小的卷积核，输出大小为$C_o \times W_o \times H_o$，<br>输入特征图将按需求被转换成$(K*K)\times(C_i*W_o*H_o)$的矩阵，卷积核将被转换成$C_o\times(K*K)$的矩阵，调用GEMM库两矩阵相乘也就完成了所谓的卷积计算。由于按照计算需求排布了数据顺序，每次计算过程中总是能够依次访问特征图数据，迎合了局部性原理，极大地提高了计算卷积的速度！     </p><p>我用NDArry（<em>MXNet提供的一个类似numpy的数学计算库</em>）实现了一个基于im2col卷积层，除了支持基本的标准卷积外，还支持分组卷积以及量化卷积的模拟运算（int8权重、uint8输入、int32的累积）——<br>实现代码：<a href="https://github.com/hey-yahei/Quantization.MXNet/blob/master/nn/quantized_conv.py#L13" target="_blank" rel="noopener">Quantization.MXNet/nn/quantized_conv.py#L13 | github, hey-yahei</a><br>测试代码：<a href="https://github.com/hey-yahei/Quantization.MXNet/blob/master/tests/test_quantized_conv.py" target="_blank" rel="noopener">Quantization.MXNet/tests/test_quantized_conv.py | github, hey-yahei</a>        </p><p>不过要注意的是，<strong>im2col也不一定就比直接卷积好</strong>！！     </p><ol><li>首先，im2col需要额外的数据重组过程，当一个卷积层非常小（主要是通道比较窄）的时候，直接卷积反而可能会比im2col快（尽管现代网络一般都会用比较宽的通道）；    </li><li>im2col在重组数据的时候需要对数据产生大量的副本（约 $K^2$ 倍的内存占用）；</li><li>当采用1x1卷积的时候，im2col和直接卷积的过程其实是等价的；      </li><li>im2col的目的是迎合程序局部性原理，如果使用了定制化的硬件（比如用FPGA直接实现3x3的卷积运算单元），那么im2col就失去了意义而且反而增加了开销        </li></ol><p><em>卷积实现的方式还有很多，暂且只介绍im2col，之后有空会独立开一篇文章细谈。</em><br>《<a href="/2019/08/21/winograd_convolution/">Winograd卷积原理 | Hey~YaHei!</a>》      </p><h2 id="高效卷积"><a href="#高效卷积" class="headerlink" title="高效卷积"></a>高效卷积</h2><p>早在去年的《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》一文中，我们就分解了标准卷积的工作方式——      </p><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><div><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center></div>     <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用M个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>其中 Ⅲ 阶段野蛮地将 $M$ 张特征图拍扁成一张特征图，似乎显得不那么优雅且存在大量的冗余，于是开始有人研究如何让卷积过程变得高效，就出现了<strong>分组卷积</strong>和<strong>深度向分解卷积</strong>。       </p><h3 id="分组卷积（Groupwise-Convolution）"><a href="#分组卷积（Groupwise-Convolution）" class="headerlink" title="分组卷积（Groupwise Convolution）"></a>分组卷积（Groupwise Convolution）</h3><p>分组卷积最初是《<a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks(2012)</a>》提出，作者最初是出于并行目的（作者有三张仅3G显存的GTX580）把网络拆解到多张显卡上进行训练。顾名思义，<strong><em>分组卷积是对输入通道进行分组，分别施加卷积，最后将卷积结果堆叠起来——也可以理解为将原来的一层卷积等分成若干并行的支路，最后再堆叠汇合</em></strong>。<br><img src="/imgs/Conv-Family/groupwise convolution.jpg" alt="groupwise convolution"><br>如图所示，传统卷积的 Ⅲ 被拆解成两部分（分组卷积示意图中的Ⅲ和Ⅳ），不再简单的拍扁成一张特征图，而是分成G组分别拍扁，最后堆叠得到G张特征图。这样做有两个好处：    </p><ol><li>传统卷积中Ⅲ阶段的输入包含Ⅱ阶段的所有输出，而分组卷积Ⅲ阶段被分割为G组，每组的输入都是独立的——简而言之，Ⅲ阶段可以分散到多个不同的计算设备上完成而不需要频繁的通信（最终Ⅳ阶段再汇总到主设备上，甚至可以暂存在不同设备上，数据汇总推迟到Ⅴ阶段）<br><em>不过目前主流框架似乎没有针对分组卷积做充分的优化，而是简单地对输入、卷积核分组，然后逐一使用标准卷积来实现；一般使用分组卷积的时候也就分个两三组，问题倒是不大，不过到了把分组做到极致的深度向卷积，这个问题就凸显出来，在GPU上专门优化过的深度向卷积要比用分组方式实现的卷积快10倍左右，而CPU并行能力有限，这一问题并不明显，这一点早前已经在《<a href="/2018/08/24/mssd-try2/#Depthwise-Convolution和Standard-Convolution-Group-的比较">基于MobileNet-SSD的目标检测Demo（一） | Hey~YaHei!</a>》一文中有所提及。</em>      </li><li>参数数量和乘加次数均下降为标准卷积的1/G         </li></ol><p>简单的实现方法可以参考我的 <a href="https://github.com/hey-yahei/Quantization.MXNet/blob/master/nn/quantized_conv.py#L57" target="_blank" rel="noopener">Quantization.MXNet/nn/quantized_conv.py#L57 | github, hey-yahei</a>。     </p><h3 id="深度向卷积分解（Depthwise-Separable-Convolution）"><a href="#深度向卷积分解（Depthwise-Separable-Convolution）" class="headerlink" title="深度向卷积分解（Depthwise Separable Convolution）"></a>深度向卷积分解（Depthwise Separable Convolution）</h3><p>标准卷积同时聚合了所有通道信息和所有空间信息，分组卷积则同时聚合了所有通道信息和部分空间信息，深度向卷积分解直接割裂了通道信息和空间信息的聚合过程，从形式上看则将标准卷积分解为一次深度向卷积和一次点向卷积——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>这一过程又称为深度向卷积（Depthwise Convoluon），是分组卷积的一种特例（每个通道作为一组）；<br>①、②阶段与标准卷积①阶段的前两个阶段完全相同，③阶段比标准卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>这一过程称又称为点向卷积（Pointwise Convolution）；<br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上         </li></ol><p>此时参数数量和乘加次数均下降为标准卷积的 $\frac{1}{D_K^2} + \frac{1}{N}$，不仅如此，由于深度向卷积分解中有 $\frac{N}{D_K^2 +N}$ 的乘加计算集中在1x1卷积上，而通常 $N&gt;&gt;D_K^2$ 也即绝大多数乘加计算由1x1卷积贡献（比如Mobilenet中有94.86%的乘加计算集中在1x1卷积上），所以其计算效率是非常高的。         </p><h3 id="异构卷积（HetConv）"><a href="#异构卷积（HetConv）" class="headerlink" title="*异构卷积（HetConv）"></a>*异构卷积（HetConv）</h3><p>论文：《<a href="https://arxiv.org/pdf/1903.04120.pdf" target="_blank" rel="noopener">HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs(2019)</a>》<br>印度三哥提出来的一种卷积形式，论文里的实验结果非常可观，但似乎没啥影响力（有人说三哥论文数据爱造假？），作者没有开源也没见人复现过，姑且再放着看看吧。      </p><p><center><img src="/imgs/Conv-Family/HetConv.jpg" alt="HetConv"></center><br>思路倒也简单，有点不定长数组的味道。     </p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>高效卷积具有较强的特征提取效率、较高的并行能力，基于这些特点，高效卷积通常被应用于两种场合：     </p><ol><li>注重<strong>速度</strong>，搭建参数数量、乘加次数均比较少的网络，如MobileNet等；    </li><li>注重<strong>性能</strong>，用高效卷积替代标准卷积的同时扩张卷积的数量（直到接近标准卷积的规模），如Xception、ResNeXt等       </li></ol><h2 id="其他卷积变种"><a href="#其他卷积变种" class="headerlink" title="其他卷积变种"></a>其他卷积变种</h2><h3 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h3><p>论文：《<a href="https://arxiv.org/pdf/1511.07122.pdf" target="_blank" rel="noopener">Multi-scale Context Aggregation by Dilated Convolutions(2015)</a>》    </p><p>为卷积核增加空洞来扩大感受野，比如      </p><ul><li>3x3的卷积，增加1的空洞，可以在不增加参数数量和运算量的情况下，取得等同5x5卷积的感受野；    </li><li>同理，3x3的卷积，增加2的空洞，可以取得等同7x7卷积的感受野       </li></ul><div><center><img src="/imgs/Conv-Family/Dilated Convolution.jpg" alt="Dilated Convolution"></center></div>       <p>简而言之，就是通过牺牲卷积的“分辨率”，来获取更大的“视野”；<br>原理很简单，却很有效，目前广泛应用在语义分割领域。       </p><h3 id="变形卷积"><a href="#变形卷积" class="headerlink" title="变形卷积"></a>变形卷积</h3><p>论文：《<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable Convolutional Networks(2017)</a>》     </p><p>这两年特别火的一种卷积，利用一个辅助的标准卷积根据input拟合出offset，然后根据offset在input上进行带偏移的采样，最后施加卷积运算——与名字不同，并不是对卷积核进行变形，而是对采样点进行偏移。       </p><p><div><center><img src="/imgs/Conv-Family/Deformable Convolution.jpg" alt="Deformable Convolution"></center></div><br>这种卷积广泛存在于各种目标检测、语义分割比赛的冠军方案中。     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>快如闪电的人脸检测——Tengine+libfacedetection</title>
      <link href="/2019/03/21/Tengine-libfacedetection/"/>
      <url>/2019/03/21/Tengine-libfacedetection/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1337935927&auto=0&height=32"></iframe>           <p>前几天，深圳大学于仕琪老师突然开源了libfacedetection——号称最快的人脸检测项目（<a href="https://mp.weixin.qq.com/s/yd0pwTMq6epaCfAz6lSRTw" target="_blank" rel="noopener">超越OpenCV，史上最快人脸检测系统开源 | 新智元</a>）吸引了一大波人脸检测应用开发者围观，紧接着Tengine也立马对libfacedetection模型增加了支持，让我们一起来看看，在Tengine加持下的libfacedetection能快到什么程度吧！       </p><h3 id="libfacedetection"><a href="#libfacedetection" class="headerlink" title="libfacedetection"></a>libfacedetection</h3><p>github项目：<a href="https://github.com/ShiqiYu/libfacedetection" target="_blank" rel="noopener">https://github.com/ShiqiYu/libfacedetection</a>          </p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>本次开源的库是基于卷积神经网络实现的，320x240模型如下图所示：<br><img src="/imgs/Tengine-libfacedetection/model.jpg" alt="model"></p><p>其整体框架参照<a href="/2018/08/06/SSD/">SSD</a>，并由若干组类似<a href="/2018/05/02/经典的CNN分类架构/#VGG-Nets">VGG</a>的卷积层组合堆叠而成——     </p><ul><li>和VGG一样，大部分降采样都由2x2最大池化完成，不过libfacedetction的入口部分则与<a href="/2018/05/02/经典的CNN分类架构/#GoogLeNet">Inception</a>、<a href="/2018/05/02/经典的CNN分类架构/#ResNet">ResNet</a>类似——由卷积层完成降采样（Inception、ResNet这种大网络喜欢一上来先来一层7x7/s2的卷积下采样，<em>改进版则用三层3x3卷积替换7x7</em>；像<a href="/2018/08/05/MobileNets_v1/">Mobilenet</a>这些小网络就直接一层3x3/s2的下采样，这里的libfacedetection也是如此）；       </li><li>与VGG类似的卷积层组合，两到三个带激活的卷积层一组，不同的是，VGG用若干3x3卷积堆叠作为一组，而libfacedetection则是采用【3x3 + 1x1】或【3x3 + 1x1 + 3x3】的组合，将其中一个3x3卷积层替换为1x1不仅降低了MAC而且避免了<a href="/2019/02/28/bag-of-tricks2/#im2col">im2col</a>；</li><li>比较特别的是，每一组卷积之后送去回归和分类之前都经过一个Normilization层对特征图进行归一化操作；</li><li>libfacedetection设置了四组锚框，总计$3600+600+140+45=4385$；</li><li>参数数量约660K，乘加次数约80M（作为参照：MobileNet-v1-1.00约4.25M参数和570M次乘加运算）<br>  <em>没细算，参数数量用caffemodel的文件大小简单除了个4；乘加次数则是用gluon随手搭了个主干（实际应该比这个数字更大一些），然后调<a href="/2019/01/07/MXNet-OpSummary/">mxop</a>测算了一下MAC，粗糙数据，仅供参考。</em>      </li></ul><h4 id="检测效果"><a href="#检测效果" class="headerlink" title="检测效果"></a>检测效果</h4><p>原图：<br><img src="/imgs/Tengine-libfacedetection/test_org.jpg" alt="test">       </p><p>FP32精度输出：<br><img src="/imgs/Tengine-libfacedetection/test_fp32.jpg" alt="test">       </p><p>INT8精度输出：<br><img src="/imgs/Tengine-libfacedetection/test_int8.jpg" alt="test">       </p><h4 id="性能表现"><a href="#性能表现" class="headerlink" title="性能表现"></a>性能表现</h4><!--曾有人在[FDDB](http://vis-www.cs.umass.edu/fddb/)上进行测试过libfacedetection的表现（召回率）——      | 误检数 | libfacedetection | haarcascade-alt2 | MTCNN | | :---: | :---:  | :---: |    :---:    || 100   | 0.8236 | 0.6723 | 0.9083     || 200   | 0.8334 | 0.7045 | 0.9225      || 500   | 0.8387 | 0.7358 | 0.9354      || 1000  | 0.8429 | 0.7405 | 0.9435      || 2000  | 0.8513 | 0.7605 | 0.9505      |***数据来源：《[人脸检测背景介绍和发展现状 | 知乎, YaqiLYU](https://zhuanlan.zhihu.com/p/32702868)》***     **注意：此处libfacedetection指的是以前的非深度学习版本，开源的CNN版本还没看谁做过测试=。=最近比较懒，过阵子要是闲着没事自己测一个玩玩。**        --><p>至于速度，于老师的github上已经有了与OpenCV Haar+AdaBoost的比较数据，引用至此而不再赘述：<br><img src="/imgs/Tengine-libfacedetection/experiment_github.png" alt="experiment">        </p><h3 id="加持Tengine的libfacedetection"><a href="#加持Tengine的libfacedetection" class="headerlink" title="加持Tengine的libfacedetection"></a>加持Tengine的libfacedetection</h3><p>用开发者版的<strong>Tengine</strong>分别在EAIDK-610（<code>RK3399,4A53@1.4GHz+2A72@1.8GHz</code>）和树莓派3B（<code>BCM2837,4A53@1.2GHz</code>）、树莓派3B+（<code>BCM2837,4A53@1.4Hz</code>）平台上测试<strong>libfacedetection</strong>——<br><em>使用于老师github上的caffe模型，输入大小为320x240。</em>       </p><h4 id="EAIDK-610"><a href="#EAIDK-610" class="headerlink" title="EAIDK-610"></a>EAIDK-610</h4><table><thead><tr><th style="text-align:center">CPU-Core</th><th style="text-align:center">Time(fp32)</th><th style="text-align:center">FPS(fp32)</th><th style="text-align:center">Time(int8)</th><th style="text-align:center">FPS(int8)</th><th style="text-align:center">FPS(int8)/FPS(fp32)</th></tr></thead><tbody><tr><td style="text-align:center">1A53</td><td style="text-align:center">68.26ms</td><td style="text-align:center">14.65</td><td style="text-align:center">59.25ms</td><td style="text-align:center">16.88</td><td style="text-align:center">1.1520</td></tr><tr><td style="text-align:center">2A53</td><td style="text-align:center">50.66ms</td><td style="text-align:center">19.74</td><td style="text-align:center">49.61ms</td><td style="text-align:center">20.16</td><td style="text-align:center">1.0213</td></tr><tr><td style="text-align:center">3A53</td><td style="text-align:center">45.97ms</td><td style="text-align:center">21.75</td><td style="text-align:center">47.90ms</td><td style="text-align:center">20.88</td><td style="text-align:center">0.9598</td></tr><tr><td style="text-align:center">4A53</td><td style="text-align:center">42.97ms</td><td style="text-align:center">23.27</td><td style="text-align:center">46.03ms</td><td style="text-align:center">21.73</td><td style="text-align:center">0.9335</td></tr><tr><td style="text-align:center">1A72</td><td style="text-align:center">34.70ms</td><td style="text-align:center">28.82</td><td style="text-align:center">24.56ms</td><td style="text-align:center">40.71</td><td style="text-align:center">1.4126</td></tr><tr><td style="text-align:center">2A72</td><td style="text-align:center">26.41ms</td><td style="text-align:center">37.86</td><td style="text-align:center">20.59ms</td><td style="text-align:center">48.57</td><td style="text-align:center">1.2828</td></tr></tbody></table><p><strong><em>由于大小核共同运算时速度比单个大核还慢，所以此处不列出大小核测试的数据。</em></strong>       </p><ul><li>最快可以达到<strong>48.57FPS</strong>！      </li><li>2A53相对于1A53有相对可观的提升，3A53、4A53就只有少量的提升了；     </li><li>相比于A53，大核的A72量化后能有比较明显的提升          </li></ul><h4 id="树莓派3B"><a href="#树莓派3B" class="headerlink" title="树莓派3B"></a>树莓派3B</h4><table><thead><tr><th style="text-align:center">CPU-Core</th><th style="text-align:center">Time(fp32)</th><th style="text-align:center">FPS(fp32)</th><th style="text-align:center">Time(int8)</th><th style="text-align:center">FPS(int8)</th><th style="text-align:center">FPS(int8)/FPS(fp32)</th></tr></thead><tbody><tr><td style="text-align:center">1A53</td><td style="text-align:center">93.96ms</td><td style="text-align:center">10.64</td><td style="text-align:center">104.11ms</td><td style="text-align:center">9.61</td><td style="text-align:center">0.9025</td></tr><tr><td style="text-align:center">2A53</td><td style="text-align:center">66.74ms</td><td style="text-align:center">14.98</td><td style="text-align:center">62.90ms</td><td style="text-align:center">15.90</td><td style="text-align:center">1.0611</td></tr><tr><td style="text-align:center">3A53</td><td style="text-align:center">58.03ms</td><td style="text-align:center">17.23</td><td style="text-align:center">50.73ms</td><td style="text-align:center">19.71</td><td style="text-align:center">1.1438</td></tr><tr><td style="text-align:center">4A53</td><td style="text-align:center">56.57ms</td><td style="text-align:center">17.68</td><td style="text-align:center">43.69ms</td><td style="text-align:center">22.89</td><td style="text-align:center">1.2947</td></tr></tbody></table><h4 id="树莓派3B-1"><a href="#树莓派3B-1" class="headerlink" title="树莓派3B+"></a>树莓派3B+</h4><table><thead><tr><th style="text-align:center">CPU-Core</th><th style="text-align:center">Time(fp32)</th><th style="text-align:center">FPS(fp32)</th><th style="text-align:center">Time(int8)</th><th style="text-align:center">FPS(int8)</th><th style="text-align:center">FPS(int8)/FPS(fp32)</th></tr></thead><tbody><tr><td style="text-align:center">1A53</td><td style="text-align:center">89.90ms</td><td style="text-align:center">11.12</td><td style="text-align:center">94.61ms</td><td style="text-align:center">10.57</td><td style="text-align:center">0.9502</td></tr><tr><td style="text-align:center">2A53</td><td style="text-align:center">65.87ms</td><td style="text-align:center">15.18</td><td style="text-align:center">58.34ms</td><td style="text-align:center">17.14</td><td style="text-align:center">1.1290</td></tr><tr><td style="text-align:center">3A53</td><td style="text-align:center">59.76ms</td><td style="text-align:center">16.73</td><td style="text-align:center">47.99ms</td><td style="text-align:center">20.84</td><td style="text-align:center">1.2451</td></tr><tr><td style="text-align:center">4A53</td><td style="text-align:center">58.22ms</td><td style="text-align:center">17.18</td><td style="text-align:center">41.26ms</td><td style="text-align:center">24.24</td><td style="text-align:center">1.4111</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tengine </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习小技巧（三）：训练技巧</title>
      <link href="/2019/03/01/bag-of-tricks3/"/>
      <url>/2019/03/01/bag-of-tricks3/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=1320572967&auto=0&height=32"></iframe>       </p><blockquote><p>傻乎乎~傻乎乎~傻乎乎~~~~~~~                          </p></blockquote><p>呼~终于到了解读论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》的最后一部分。         </p><hr><h2 id="余弦学习率衰减（Cosine-Learning-Rate-Decay）"><a href="#余弦学习率衰减（Cosine-Learning-Rate-Decay）" class="headerlink" title="余弦学习率衰减（Cosine Learning Rate Decay）"></a>余弦学习率衰减（Cosine Learning Rate Decay）</h2><p>论文：《<a href="https://arxiv.org/pdf/1608.03983.pdf" target="_blank" rel="noopener">SGDR: Stochastic Gradient Descent with Warm Restarts(2016)</a>》           </p><p>先前《<a href="/2019/02/23/bag-of-tricks1/">深度学习小技巧（一）：高效训练 - 学习率衰减 | Hey~YaHei!</a>》已经讨论过学习率衰减的问题啦，它本质上是一种模拟退火策略，通过逐步降低振荡幅度来更好的逼近最优点。而早在《<a href="/2018/04/10/优化器/#学习计划（learning-schedules）">优化器 - 学习计划（learning schedules） | Hey~YaHei!</a>》我们也讨论过各种各样的衰减策略，其中指数衰减在很长一段时间中都是非常受欢迎的。      </p><p>不过近年来大家突发奇想——光是逼近局部最优点哪里够用，不如想办法让它时不时来一次大震动，把它甩到另外一个位置开荒去？于是热重启技术就诞生了！<br><em>关于学习率的事情近两年其实有很多有意思的进展，比如学习率测试、周期性学习率、热重启技术，以及传统Adam的各种改进（比如最近北大和浙大本科生提出的AdamRound），这部分本文先按下不细讲，之后有时间应该会专门开一篇文章讨论学习率的问题。</em>           </p><p><center><img src="/imgs/bag-of-tricks/cos_lr_decay.png" alt="cos_lr_decay"></center><br>在每个周期的开始和末尾学习率变化缓慢，分别让模型有充分的时间跳出局部最优点、更逼急局部最优点；而中间部分则呈现出接近线性的下降趋势，这与周期性学习率的思想是类似的；同时每个周期开始学习率都会反弹到较高的水平，这是一种典型的模拟退火策略。        </p><h2 id="平滑标签（Label-Smoothing-Regularization-LSR）"><a href="#平滑标签（Label-Smoothing-Regularization-LSR）" class="headerlink" title="平滑标签（Label Smoothing Regularization, LSR）"></a>平滑标签（Label Smoothing Regularization, LSR）</h2><p>该正则化技巧最初由《<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision(2015)</a>》提出（Chapter 7），并应用在inception-v2的训练上。      </p><p>首先考虑传统的softmax和交叉熵——<br>$$q _ { i } = \frac { \exp \left( z _ { i } \right) } { \sum _ { j = 1 } ^ { K } \exp \left( z _ { j } \right) }$$<br>$$\mathrm { H } ( \mathrm { p } , \mathrm { q } ) = - \sum _ { k=1 } ^ { K } p_k \cdot \log q_k $$        </p><p>通常来说，对模型最后一层全连接层的输出 $z_i$ 按第一个公式进行归一化作为各个分类的预测概率 $q_i$；训练时则应用第二个公式，对实际概率 $p_k$ 和预测概率 $q_k$ 的对数相乘求和得到损失 $H(p,q)$，训练的目标即是最小化损失 $H(p,q)$。<br>一般我们已知了实际分类，所以自然地用独热码的方式选取——<br>$$p = \delta _ { k , y } = \left\{ \begin{array} { l } { 1 , i = y } \\ { 0 , i \neq y } \end{array} \right.$$<br>此时实际的概率分布是一个冲激函数，论文《<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision(2015)</a>》指出，用冲激函数的实际概率分布进行训练是有问题的——<strong><em>模型会趋向去用极高的概率来输出一个分类结果（或者说，模型会倾向于“武断”），这将不利于泛化（容易发生过拟合）</em></strong>。      </p><p>为了解决这一问题，文中提出了平滑标签LSR，将实际概率函数改造为<br>$$p ^ { \prime } (k) = ( 1 - \epsilon ) \delta _ { k , y } + \epsilon u ( k )$$<br>其中，$u(k)$ 是人为引入的一个固定分布（可以看作是为概率分布引入固定分布的噪声），并且由参数 $\epsilon$ 控制相对权重。那么对应的，交叉熵公式就可以展开为<br>$$H \left( p ^ { \prime } , q \right) = - \sum _ { k = 1 } ^ { K } p ^ { \prime } ( k ) \log q ( k )  = ( 1 - \epsilon ) H ( p , q ) + \epsilon H ( u , q )$$<br>从损失函数的角度上看，LSR相当于为损失函数增加了人为引入的先验概率 $u(k)$ 和预测概率 $q(k)$ 之间的惩罚项，并且赋予了相对权重 $\epsilon$。         </p><p>inception-v2选用了均匀分布的 $u(k)$ 作为先验概率，即 $u(k) = 1 / 1000$ 并且设置相对权重 $\epsilon = 0.1$，使得top-1和top-5错误率都下降了约0.2%。           </p><h2 id="知识蒸馏（Knowledge-Distillation）"><a href="#知识蒸馏（Knowledge-Distillation）" class="headerlink" title="知识蒸馏（Knowledge Distillation）"></a>知识蒸馏（Knowledge Distillation）</h2><p>论文：《<a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network(2015)</a>》     </p><p>知识蒸馏也是模型压缩技术的一部分，以后应该会花时间细致地学习它，这里暂时不展开讲。知识蒸馏的总体思想，就是用一个训练好的大容量模型去指导一个小容量模型训练，从而得到更好的表现。      </p><p>假设已经训练好一个大容量模型（比如ResNet-152），然后用它去指导另一个小容量模型（比如ResNet-50）训练，他们最后一层全连接层的输出（也就是各个分类的得分）分别为 $r$ 和 $z$，设实际标签的概率分布为 $p$，那么可以用如下损失函数进行训练，<br>$$H(p,z,r) = \ell ( p , \operatorname { softmax } ( z ) ) + T ^ { 2 } \ell ( \operatorname { softmax } ( r / T ) , \operatorname { softmax } ( z / T ) )$$<br>其中，$\ell(\cdot)$ 是基本的损失函数（如交叉熵），<br>$T$ 是一个超参数（Temperature），可以看作是用来设定“向老师学习的热情程度”。         </p><h2 id="数据混合（Mixup）"><a href="#数据混合（Mixup）" class="headerlink" title="数据混合（Mixup）"></a>数据混合（Mixup）</h2><p>论文：《<a href="https://arxiv.org/pdf/1710.09412.pdf" target="_blank" rel="noopener">mixup: Beyond Empirical Risk Minimization(2017)</a>》       </p><h3 id="经验风险最小化（Empirical-Risk-Minimization-ERM）"><a href="#经验风险最小化（Empirical-Risk-Minimization-ERM）" class="headerlink" title="经验风险最小化（Empirical Risk Minimization, ERM）"></a>经验风险最小化（Empirical Risk Minimization, ERM）</h3><p>这是当前深度学习训练中最常见的思路，假设有一个学习任务，其特征为 $X$，标签为 $Y$，其实际的联合分布为 $P(X,Y)$，为任务建模 $f(x)$，其损失为 $\ell (f(x), y)$，那么可以定义期望风险<br>$$R ( f ) = \int \ell ( f ( x ) , y ) \mathrm { d } P ( x , y )$$<br>注意到分布 $P$ 在实际应用中是未知的，所以我们往往采用数据驱动的方式，投喂训练集 $\mathcal { D } = \left\{ \left( x _ { i } , y _ { i } \right) \right\} _ { i = 1 } ^ { n }$，从而估计出 $P$ 的经验分布<br>$$P _ { \delta } ( x , y ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \delta \left( x = x _ { i } , y = y _ { i } \right)$$<br>其中，$\delta \left( x = x _ { i } , y = y _ { i } \right)$ 是一个冲激函数（也叫狄拉克函数）<br>$$\delta \left( x = x _ { i } , y = y _ { i } \right) = \left\{ \begin{array} { l } { 1 , (x,y) = (x_i,y_i) } \\ { 0 , others } \end{array} \right.$$<br>用经验分布计算出来的风险即为经验风险<br>$$R _ { \delta } ( f ) = \int \ell ( f ( x ) , y ) \mathrm { d } P _ { \delta } ( x , y ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \ell \left( f \left( x _ { i } \right) , y _ { i } \right)$$<br>相应地，ERM就是通过最小化经验风险的一种训练方式。       </p><h3 id="邻域风险最小化（Vicinal-Risk-Minimization-VRM）"><a href="#邻域风险最小化（Vicinal-Risk-Minimization-VRM）" class="headerlink" title="邻域风险最小化（Vicinal Risk Minimization, VRM）"></a>邻域风险最小化（Vicinal Risk Minimization, VRM）</h3><p>ERM用有限的数据来训练模型，直观地看，模型只要拥有足够的“记忆力”，把整个训练集的分布都原原本本地记下就可以得到一个看似“非常棒”的模型，这就是深度学习中典型的过拟合问题。<br>针对这一问题，论文《<a href="http://papers.nips.cc/paper/1876-vicinal-risk-minimization.pdf" target="_blank" rel="noopener">Vicinal risk minimization(2000)</a>》提出了邻域风险最小化，其实思路非常简单——不再直接用训练集上的经验分布来估计实际分布，而是在经验分布的基础上引入高斯噪声，使得数据量变为“无限”（简单的数据生成），以此来缓解过拟合问题。<br>$$P _ { \nu } ( \tilde { x } , \tilde { y } ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \nu ( \tilde { x } , \tilde { y } | x _ { i } , y _ { i } )$$<br>$$\nu ( \tilde { x } , \tilde { y } | x _ { i } , y _ { i } ) = \mathcal { N } \left( \tilde { x } - x _ { i } , \sigma ^ { 2 } \right) \delta \left( \tilde { y } = y _ { i } \right)$$<br>也即<br>$\tilde { x }$ 在正态分布 $\mathcal { N } \left( \tilde { x } - x _ { i } , \sigma ^ { 2 } \right)$ 上随机抽样；<br>$\tilde { y } = y_i$ 与ERM相同。      </p><h3 id="mixup"><a href="#mixup" class="headerlink" title="mixup"></a>mixup</h3><p>VRM只是给特征 $X$ 引入高斯噪声，取得了不错的成效，但好像还不够合理。mixup则提出了另一种邻域分布——<br>$$\tilde { x } = \lambda x _ { i } + ( 1 - \lambda ) x _ { j }$$<br>$$\tilde { y } = \lambda y _ { i } + ( 1 - \lambda ) y _ { j }$$<br>其中，$\lambda \sim Beta(\alpha, \alpha)$ 且 $\alpha \in (0, \infty)$。<br>简而言之，<strong>mixup将多个数据用加权的方式混合在一起，以此来估计实际分布</strong>；而当 $\alpha \rightarrow 0$时，mixup退化为ERM。     </p><ul><li>用三个及三个以上的数据做mixup并不会有额外的提升，反而会在训练过程中增加计算复杂度</li><li>在一个batch内做mixup和不同batch间做mixup带来的效果很接近，而前者对IO更友好</li><li>只在特征上做mixup的效果不如同时在特征和标签上都做mixup</li><li>mixup提高了对坏样本（比如标签错误）的容忍性、对对抗样例的鲁棒性、同时也能使GAN训练过程变得稳定        </li><li>若$\alpha \rightarrow \infty$，CIFAR-10上可以取得很低的错误率，而ImageNet上却反而会提高错误率</li><li>模型容量越大，训练效果对参数 $\alpha$ 就越不敏感，从而mixup能取得越好的效果</li></ul><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h4><p>基本的数据增强：特定比例尺缩放、随机裁剪、水平翻转（无色域扰动）         </p><p><center><img src="/imgs/bag-of-tricks/mixup_classification1.jpg" alt="mixup_classification1"></center>        </p><p><center><img src="/imgs/bag-of-tricks/mixup_classification2.jpg" alt="mixup_classification2"></center>         </p><ul><li>$\alpha$ 越大，正则化效果越明显，甚至会出现欠拟合       </li><li>容量越大的网络mixup的效果越好，而且mixup的效果往往要在训练后期才体现出来</li></ul><h4 id="对坏样本的容忍性测试"><a href="#对坏样本的容忍性测试" class="headerlink" title="对坏样本的容忍性测试"></a>对坏样本的容忍性测试</h4><p>参考 <a href="https://github.com/pluskid/fitting-random-labels" target="_blank" rel="noopener">fitting-random-labels | github, pluskid</a> 的实验，随机将一定比例的训练数据的标签替换为随机噪声生成带坏样本的数据集进行训练。       </p><p><center><img src="/imgs/bag-of-tricks/mixup_corruption.jpg" alt="mixup_corruption"></center><br><em>其中，Best和Last分别指最佳的测试结果、最后一个epoch的测试结果。</em>          </p><ul><li>mixup的效果比dropout的效果更好    </li><li>mixup和dropout是互为补充的，可以同时使用</li></ul><h4 id="对对抗样例的鲁棒性测试"><a href="#对对抗样例的鲁棒性测试" class="headerlink" title="对对抗样例的鲁棒性测试"></a>对对抗样例的鲁棒性测试</h4><p><strong><em>第一次看到这种测试实验，具体还不是很理解，先放着等之后啃啃相关论文</em></strong>          </p><p>参照论文《<a href="https://arxiv.org/pdf/1312.6199.pdf" target="_blank" rel="noopener">Intriguing properties of neural networks(2013)</a>》生成对抗样例。      </p><blockquote><p>Adversarial examples are obtained by adding tiny (visually imperceptible) perturbations to legitimate examples in order to deteriorate the performance of the model. The adversarial noise is generated by ascending the gradient of the loss surface with respect to the legitimate example.        </p></blockquote><p>参照论文《<a href="https://arxiv.org/pdf/1412.6572.pdf" target="_blank" rel="noopener">Explaining and Harnessing Adversarial Examples(2014)</a>》设计测试，并且得出结论：mixup有助于提高模型对对抗样例的鲁棒性。<br><em>一些指标反正现在也看不懂，就不列在这了。</em>             </p><h4 id="切除研究（Ablation-Study）"><a href="#切除研究（Ablation-Study）" class="headerlink" title="切除研究（Ablation Study）"></a>切除研究（Ablation Study）</h4><p>参考《<a href="https://www.quora.com/In-the-context-of-deep-learning-what-is-an-ablation-study" target="_blank" rel="noopener">In the context of deep learning, what is an ablation study? | Quora</a>》和《<a href="https://www.zhihu.com/question/60170398/answer/207709956" target="_blank" rel="noopener">什么是 ablation study？ | 知乎</a>》。<br>说白了切除研究就是对照实验，控制单一变量来对比有无某一结构的结果。     </p><p><center><img src="/imgs/bag-of-tricks/mixup_ablation_study.jpg" alt="mixup_ablation_study"></center><br>其中，<br>AC表示在所有分类之间做mixup，SC表示只在同一分类内做mixup；<br>RP表示在随机数据对之间做mixup，KNN表示在k近邻（k=200）之间做mixup。       </p><ul><li>随机数据对之间做mixup比k近邻效果好</li><li>对浅层表示做mixup似乎有点作用？但不明显，而对深层表示做mixup效果较差</li><li>mixup的效果比标签平滑和加高斯噪声的效果好</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>深度学习小技巧（二）：模型微调</title>
      <link href="/2019/02/28/bag-of-tricks2/"/>
      <url>/2019/02/28/bag-of-tricks2/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=4898499&auto=0&height=32"></iframe>       <p>接着上一篇文章《<a href="/2019/02/23/bag-of-tricks1/">深度学习小技巧（一）：高效训练 | Hey~YaHei!</a>》继续解读论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》，论文中以ResNet为例提出了一些简单的微调技巧，并且取得了一定的成果。且不说准确率如何，论文中除了分析准确率有着怎样怎样的提升之外，还关注了产生了额外开销，并且通过分析、实验量化了这些开销，这是值得肯定的（比那些不考虑开销，盲目微调，通过牺牲很多速度来提高那一点点准确率的论文，不知道要高到哪里去！）      </p><hr><h2 id="以ResNet为例"><a href="#以ResNet为例" class="headerlink" title="以ResNet为例"></a>以ResNet为例</h2><p>原始的ResNet模型可以参考《<a href="/2018/05/02/经典的CNN分类架构/#ResNet">经典的CNN分类架构 - ResNet | Hey~YaHei!</a>》，其核心在于应用了shortcut（原文称为skip connection）技术使得深层网络也能够被有效训练，具体细节这里就不再赘述。      </p><center><img src="/imgs/bag-of-tricks/resnet_a.jpg" alt="resnet_a"></center>           <h3 id="改进1：推迟下采样"><a href="#改进1：推迟下采样" class="headerlink" title="改进1：推迟下采样"></a>改进1：推迟下采样</h3><p>该改进方法最初是在Torch上提出的，目前这一改进也已经被广泛地应用。<br>首先观察原始模型的下采样模块——       </p><center><img src="/imgs/bag-of-tricks/resnet_downsample.jpg" alt="resnet_downsample"></center><br>其PathA依次经过<br>1. 1x1的卷积，完成通道的收缩，并且步长为2以实现下采样<br>2. 3x3的卷积，通道数量不变，主要用于提取特征<br>3. 1x1的卷积，完成通道的扩张<br><br>其中第一个卷积用来作为下采样，所以步长设为了1——但你仔细想想会发现，核大小1x1、步长2的卷积会造成3/4的信息丢失！以6x6的特征图为例，如下图所示，只有红色部分的信息能够传递到下一层去，非红色部分均不参与卷积计算。<br><center><img src="/imgs/bag-of-tricks/conv_k1s2.jpg" alt="conv_k1s2"></center>           <p>由此可见，在1x1的卷积层作下采样是不明智的，更好的做法是把下采样过程挪到3x3的卷积上，如下图所示，由于卷积核宽度大于步长，卷积核在移动过程中能够遍历输入特征图上的所有信息（甚至还能有重叠）：      </p><center><img src="/imgs/bag-of-tricks/conv_k3s2.jpg" alt="conv_k3s2"></center>           <p>下采样模块就变为——     </p><center><img src="/imgs/bag-of-tricks/resnet_b_downsample.jpg" alt="resnet_b_downsample"></center>           <h3 id="改进2：拆解大核卷积"><a href="#改进2：拆解大核卷积" class="headerlink" title="改进2：拆解大核卷积"></a>改进2：拆解大核卷积</h3><p>如《<a href="/2018/04/18/CNN/#卷积层（Conv）">卷积神经网络CNN - 卷积层（Conv） | Hey~YaHei!</a>》所述，大核卷积层可以由多层小核卷积替代实现，这不仅可以减少参数，还能加深网络深度以增加网络容量和复杂度。<br>Inception也早在《<a href="https://arxiv.org/pdf/1512.00567.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision(2015)</a>》一文中对Inceptionv1做出改进，分别用三个和两个3x3卷积的级联去替代7x7和5x5的卷积。<br>这一技巧同样适用于ResNet——      </p><center><img src="/imgs/bag-of-tricks/resnet_c_input.jpg" alt="resnet_c_input"></center>           <h3 id="改进3：用平均池化替代1x1卷积做下采样"><a href="#改进3：用平均池化替代1x1卷积做下采样" class="headerlink" title="改进3：用平均池化替代1x1卷积做下采样"></a>改进3：用平均池化替代1x1卷积做下采样</h3><p>下采样模型的PathA和PathB都需要做下采样才能正确地加和，改进1只针对PathA做了改进，其实PathB也用了1x1的卷积做下采样。为此，论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》用平均池化接替了PathB中的降采样工作：       </p><center><img src="/imgs/bag-of-tricks/resnet_d_downsample.jpg" alt="resnet_d_downsample"></center>           <h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><center><img src="/imgs/bag-of-tricks/model_tweak_results.jpg" alt="model_tweak_results"></center><br>其中A、B、C、D分别代表原始、改进1、改进2、改进3的模型。       </p><p>经过改进之后，最终的ResNet-50-D准确率提高了0.95%。但也不得不承认，以上的改进都增加了模型的运算复杂度，FLOPs增加了约13%，但实测速度只下降了3%。       </p><h2 id="关于FLOPs和实测速度"><a href="#关于FLOPs和实测速度" class="headerlink" title="关于FLOPs和实测速度"></a>关于FLOPs和实测速度</h2><p>你可能会意外，为什么运算量明明增加了13%，可实测速度却只下降了3%呢？？      </p><h3 id="多分支网络"><a href="#多分支网络" class="headerlink" title="多分支网络"></a>多分支网络</h3><p>首先要注意到，ResNet由于应用了shortcut技术，相比于传统的直筒式网络增加了分支，不同分支是可以并行计算的，而计算FLOPs时却是把不同分支的运算量依次累加起来。        </p><h3 id="高效的1x1卷积"><a href="#高效的1x1卷积" class="headerlink" title="高效的1x1卷积"></a>高效的1x1卷积</h3><p>早在《<a href="/2018/08/05/MobileNets_v1/">MobileNets v1模型解析 | Hey~YaHei!</a>》一文中就提及过——      </p><blockquote><p>深度向分解的卷积中绝大多数参数和运算都集中在 1×1 的pointwise卷积运算当中，这种运算恰恰是能够被 GEneral Matrix Multiply(GEMM) 函数高度优化的。</p></blockquote><p>为什么1x1卷积能够被高度优化？首先要先从卷积计算的实现讲起——      </p><h4 id="卷积的原理"><a href="#卷积的原理" class="headerlink" title="卷积的原理"></a>卷积的原理</h4><p>详细原理可以参阅《<a href="/2018/04/18/CNN/#卷积层（Conv）">卷积神经网络CNN - 卷积层（Conv） | Hey~YaHei!</a>》和《<a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/conv-layer.html" target="_blank" rel="noopener">5.1二维卷积层 | 动手学深度学习</a>》。<br><img src="/imgs/bag-of-tricks/convolutional_kernel.gif" alt="convolutional_kernel"></p><p>首先考虑3x3的单通道特征图，以及k2s1的卷积核——<br><img src="/imgs/bag-of-tricks/conv_raw.jpg" alt="conv_raw">       </p><p>按照卷积计算，<br>$$y_{11} = w_{11}x_{11} + w_{12}x_{12} + w_{21}x_{21} + w_{22}x_{22}$$<br>$$y_{12} = w_{11}x_{12} + w_{12}x_{13} + w_{21}x_{22} + w_{22}x_{23}$$<br>$$y_{21} = w_{11}x_{21} + w_{12}x_{22} + w_{21}x_{31} + w_{22}x_{32}$$<br>$$y_{22} = w_{11}x_{22} + w_{12}x_{23} + w_{21}x_{32} + w_{22}x_{33}$$</p><p>按照“行先序”，特征图和卷积核在内存中是这样排列的——<br><img src="/imgs/bag-of-tricks/conv_store.jpg" alt="conv_store">       </p><p>我们用不同的颜色标注出卷积计算中的访存过程（相同颜色的数据相乘）——<br><img src="/imgs/bag-of-tricks/conv_access_ram.jpg" alt="conv_access_ram">      </p><p>众所周知，由于程序的<strong>局部性原理</strong>（通常相邻代码段会访问相邻的内存块），现代处理器通常会按块从内存中读取数据到高速缓存中以缓解访存速度和计算速度的巨大差异导致的“<strong>内存墙</strong>”问题。换句话说，如果计算需要从内存中读取<code>x12</code>的数据，那么往往相邻的<code>x11</code>、<code>x13</code>等数据也会被一起读取到高速缓存上，当下次计算需要用到<code>x11</code>或<code>x13</code>时处理器就可以快速地从高速缓存中取出数据而不需要从内存中调取，大大提高了程序的速度。<br><em>注：L1缓存的读取速度是RAM的50-100倍！（数据来源：《<a href="https://book.douban.com/subject/7006537/" target="_blank" rel="noopener">计算机体系结构：量化研究方法</a>》）</em>       </p><p>而从上边展示出来的访存过程中可以看到，直接对于特征图数据的访问过程十分散乱，直接用行先序存储的特征图参与计算是非常愚蠢的选择。<br>因此深度学习框架往往通过牺牲空间的手段（约扩增$K \times K$倍），将特征图转换成庞大的矩阵来进行卷积计算，这就是常说的im2col操作。       </p><h4 id="im2col"><a href="#im2col" class="headerlink" title="im2col"></a>im2col</h4><p>参考：<br>《<a href="https://blog.csdn.net/dwyane12138/article/details/78449898" target="_blank" rel="noopener">im2col的原理和实现 | CSDN, dwyane12138</a>》<br>《<a href="https://www.zhihu.com/question/28385679?sort=created" target="_blank" rel="noopener">在Caffe中如何计算卷积？ | 知乎, 贾扬清</a>》        </p><p><center><img src="/imgs/bag-of-tricks/conv_im2col.png" alt="conv_im2col"></center><br>其实思路非常简单：把每一次循环所需要的数据都排列成列向量，然后逐一堆叠起来形成矩阵（按通道顺序在列方向上拼接矩阵）。<br>比如$C_i \times W_i \times H_i$大小的输入特征图，$K \times K$大小的卷积核，输出大小为$C_o \times W_o \times H_o$，<br>输入特征图将按需求被转换成$(K*K)\times(C_i*W_o*H_o)$的矩阵，卷积核将被转换成$C_o\times(K*K)$的矩阵，调用GEMM库两矩阵相乘也就完成了所谓的卷积计算。由于按照计算需求排布了数据顺序，每次计算过程中总是能够依次访问特征图数据，迎合了局部性原理，极大地提高了计算卷积的速度！     </p><h4 id="特别的1x1"><a href="#特别的1x1" class="headerlink" title="特别的1x1"></a>特别的1x1</h4><p>回到1x1的卷积，它的im2col非常特殊——其原始存储结构跟im2col的重排列矩阵是完全相同的！！也就是说，1x1卷积甚至不需要im2col的过程，拿起来就能直接算，节省了数据重排列的时间和空间，所以哪怕是在相同FLOPs的前提下，1x1卷积也要比3x3卷积快速、高效得多。<br><em>当然，这是建立在局部性原理和冯诺依曼结构的基础之上，对于非冯结构的计算体系可能就不适用了。</em>      </p><p>这也是为什么MobileNet在论文最后要大肆鼓吹说他94.86%的运算量都集中1x1的卷积运算上，它的快速可不仅仅体现在“少参数，少运算量”上！     </p><p>同理，前文中改进1和改进3看似增加了很多运算量，但这些运算量都是负担在1x1卷积上的，这就使得实测速度的下降远没有运算量增加那么明显！     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>深度学习小技巧（一）：高效训练</title>
      <link href="/2019/02/23/bag-of-tricks1/"/>
      <url>/2019/02/23/bag-of-tricks1/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=638400&auto=0&height=32"></iframe>        <p>最近无意在MXNet论坛上翻出一篇不错的综述性（实验性）论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》，作者主要从<strong>高效训练</strong>、<strong>模型微调</strong>、<strong>训练技巧</strong>三个方面列举了一些常见的深度学习小技巧并附上了丰富的比较实验。<br>之后我将抽空按照这三个方面，以该论文为起点，配以相关的资料和个人理解，连更三篇相关的博文(*^__^*)             </p><hr><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><ul><li>训练      <ul><li>随机抽样并将uint8转换为float32</li><li>随机裁剪出比例尺在$[\frac{3}{4}, \frac{4}{3}]$之间、尺寸大小在$[8\%, 100\%]$之间的图像块，并resize为224x224的图像     </li><li>以0.5的概率水平翻转       </li><li>随机使用$[0.6, 1.4]$的系数对亮度、饱和度、对比度进行扰动       </li><li>用正态分布$\mathcal { N } ( 0,0.1 )$的随机系数为图像添加PCA噪声</li><li>图像数值去均值，除以方差</li></ul></li><li>预测      <ul><li>保持比例尺，将短边缩放到256</li><li>从中央裁剪出224x224的图像</li><li>与训练减去相同的均值，除以相同的方差</li></ul></li></ul><h2 id="高效训练"><a href="#高效训练" class="headerlink" title="高效训练"></a>高效训练</h2><p>为了提高训练效率（偶尔训练结果甚至还能有一些微小的提升），目前主流的技巧包括<strong>增大训练批次</strong>、<strong>降低训练精度</strong>两个方向，其主要目的在于<strong>减少复杂度</strong>和<strong>提高并行度</strong>。           </p><h3 id="大批次训练"><a href="#大批次训练" class="headerlink" title="大批次训练"></a>大批次训练</h3><p>参考：《<a href="https://arxiv.org/pdf/1711.00489.pdf" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size(2017)</a>》         </p><p>我们往往直观地认为，加大批次能够让模型在同一次迭代中见识更多的样本，能使得学习更加稳定并取得更好的效果（比如最极端的批梯度下降法），但近年来的各种实验研究表明事实并非如此——<strong>在相同学习率、相同epoch下，过大的批大小训练效果反而会比较小的批大小训练结果更差</strong>。一般研究认为，这是因为随着批次的增加，样本整体噪声均值不变但方差却被减小，而多数研究认为样本的噪声有助于SGD规避局部最优点（避开尖锐的最优点，提高整体的泛化能力）。论文《<a href="https://arxiv.org/pdf/1609.04836.pdf" target="_blank" rel="noopener">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima(2016)</a>》将这一现象称为“<strong>泛化差距（generalization gap）</strong>”并展开了详细的讨论。同时论文《<a href="https://arxiv.org/pdf/1710.06451.pdf" target="_blank" rel="noopener">A bayesian perspective on generalization and stochastic gradient descent(2017)</a>》经过一系列实验还指出一个经验关系——$B _ { \text {opt} } \propto \epsilon N$，也即最佳的批大小 $B_{opt}$ 随着学习率 $\epsilon$ 和数据集样本总数 $N$ 的乘积等比例增大（缩小）。     </p><h4 id="随机梯度下降（SGD）"><a href="#随机梯度下降（SGD）" class="headerlink" title="随机梯度下降（SGD）"></a>随机梯度下降（SGD）</h4><p>《<a href="/ML-Andrew/ML-Andrew-notes1.html#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88gradient-descent%EF%BC%89%E7%AE%97%E6%B3%95">吴恩达《机器学习》笔记 - 1绪论、线性回归与逻辑回归 - 梯度下降（gradient descent）算法 | Hey~YaHei</a>》已经讨论过梯度下降法，这是机器学习中常用的基本优化方法。传统的梯度下降法的更新方式分为两种，分别是批梯度下降（即每次循环先计算整个数据集上的损失，然后统一更新权重）和增量梯度下降（又称随机梯度下降，每次循环只计算一个数据样本的损失，频繁地更新权重）。前者收敛缓慢，后者受噪声影响大难以收敛到一个很好的最优点。<br>如今大家常说的“<strong>随机梯度下降（SGD）</strong>”其实指的是小批次随机梯度下降，该方法介于批梯度下降和增量梯度下降之间，每次循环用若干个样本（数量往往远小于数据集总样本数）的损失来更新权重。      </p><h4 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h4><p>一般认为，要达到强凸函数的最小点需要在搜索过程中不断衰减学习率，比如论文《<a href="http://xueshu.baidu.com/usercenter/paper/show?paperid=266744235943375e790f06d44a334efa&amp;site=xueshu_se" target="_blank" rel="noopener">A stochastic approximation method(1951)</a>》提出学习率应当满足的收敛条件：<br>（<em>前提：批大小batch_size固定</em>）<br>$$\sum _ { i = 1 } ^ { \infty } \epsilon _ { i } = \infty$$<br>$$\sum _ { i = 1 } ^ { \infty } \epsilon _ { i } ^ { 2 } &lt; \infty$$<br>其中，$\epsilon_i$ 为第i次更新权重所使用的学习率。       </p><ul><li>前者保证了经过无穷次更新，无论权重如何初始化，最终都能更新到任意点；      </li><li>后者要求有足够的学习率衰减速率，使得模型能够收敛到最优点而不是在最优点附近反复震荡       </li></ul><p>注意上述两式是以批大小固定为前提进行讨论的，而《<a href="https://arxiv.org/pdf/1710.06451.pdf" target="_blank" rel="noopener">A Bayesian Perspective on Generalization and Stochastic Gradient Descent(2017)</a>》引入变化的批大小这一因素，重新思考了SGD的收敛条件：<br>对于梯度下降的更新公式 $\omega_t = \omega_{t-1} - \epsilon \frac{dC}{d\omega_{t-1}}$，从连续时间的角度上看可以得到<br>$$\frac { d \omega } { d t } = - \epsilon \frac { d C } { d \omega } + \eta ( t )$$<br>其中，C为无噪声的损失，$\eta(t)$是一个高斯分布的随机噪声，同时论文中将其建模为 $mean(\eta(t))=0$、$variance(\eta(t))=gF(\omega)\delta(t-t’)$ 的高斯随机噪声，其中$F(\omega)$是权重间梯度波动的协方差，$g=\epsilon(\frac{N}{B}-1)$描述了噪声的波动范围，N、B分别是数据集大小和批大小。<br>通常来说$B &lt;&lt; N$，所以有$g \approx \epsilon \frac{N}{B}$；由此可见，在满足$B&lt;&lt;N$的前提下等比例放大（缩小）$\epsilon$和$B$效果是相当的。      </p><p><strong>学习率衰减实际是一种模拟退火策略。</strong>       </p><blockquote><p>模拟退火来源于物理上的降温退火问题——一个处于很高温度的物体，现在要给它降温，使物体内能降到最低。我们常规的思维是，越快越好，让它的温度迅速地降低。然而，实际上，过快地降温使得物体来不及有序地收缩，难以形成结晶。而结晶态，才是物体真正内能降到最低的形态。正确的做法，是徐徐降温，也就是退火，才能使得物体的每一个粒子都有足够的时间找到自己的最佳位置并紧密有序地排列。开始温度高的时候，粒子活跃地运动并逐渐找到一个合适的状态。在这过程中温度也会越降越低，温度低下来了，那么粒子也渐渐稳定下来，相较于以前不那么活跃了。这时候就可以慢慢形成最终稳定的结晶态了。     </p></blockquote><p>这一过程演化到优化问题上就成为一种模拟退火策略（实际上深度学习中的<strong>退火</strong>可能更加广泛，大概一切有助于跳脱局部最优点，寻找更佳的全局最优点的策略都能称为退火了吧）。训练一开始使用较大的学习率，在比较大的空间上搜索较优的区域，待权重相对稳定后衰减学习率缩小搜索空间，从而取得更好的训练结果并且提高了模型的鲁棒性。       </p><h4 id="大批次训练技巧"><a href="#大批次训练技巧" class="headerlink" title="大批次训练技巧"></a>大批次训练技巧</h4><p>前文已经提到，盲目增大批大小其实无益于提升训练效果，但却有各种小技巧——      </p><h5 id="1-等比例增大学习率"><a href="#1-等比例增大学习率" class="headerlink" title="1.等比例增大学习率"></a>1.等比例增大学习率</h5><p>前边提到在满足$B&lt;&lt;N$的前提下等比例放大（缩小）$\epsilon$和$B$效果是相当的，既然如此，我们在增大批大小的同时等比例增大学习率不仅可以避开泛化差距问题，还有助于加速收敛速度。<br>而对于带动量的优化器，等比例增大 $\epsilon / ( 1 - m )$ 也能取得相同的结果（其中m为动量系数）。      </p><h5 id="2-学习率预热"><a href="#2-学习率预热" class="headerlink" title="2.学习率预热"></a>2.学习率预热</h5><p>训练之初由于参数随机初始化（尤其是非预训练模型），数值上与目标参数组相去甚远，如果采用太大的学习率会出现参数数值的不稳定，不利于模型收敛。因此，从较低的学习率开始训练会比较好，比如：     </p><ul><li>《<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition(2015)</a>》先用较低的学习率（0.01）预热，直到训练错误率低于80%之后再恢复到正常的学习率（0.1）。              <blockquote><p>In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.       </p></blockquote></li><li>《<a href="https://arxiv.org/pdf/1706.02677.pdf" target="_blank" rel="noopener">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour(2017)</a>》2.2节指出常数学习率预热会在学习率突变阶段对模型收敛产生不良印象，并提出了线性学习率预热，用斜坡来代替学习率的突变。           <center><img src="/imgs/bag-of-tricks/lr_warmup.jpg" alt="lr_warmup"></center>        </li></ul><h5 id="3-零gamma初始化"><a href="#3-零gamma初始化" class="headerlink" title="3.零gamma初始化"></a>3.零gamma初始化</h5><p>众所周知，<a href="http://zh.gluon.ai/chapter_convolutional-neural-networks/batch-norm.html" target="_blank" rel="noopener">批归一化BN</a>通常将参数$\gamma$和$\beta$分别初始化为1和0，也就是初始状态下BN只做归一化而不做拉伸和偏移。<br>但对于残差单元来说，将$\gamma$初始化为1其实不一定是最佳选择，残差单元的结构如下图所示，每个卷积层之后都会接一个BN层。      </p><center><img src="/imgs/bag-of-tricks/residual_unit.jpg" alt="residual_unit"></center>       <p>论文《<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks(2018)</a>》指出<strong>将第二个BN层的$\gamma$初始化为0能取得更好的结果</strong>！此时初始状态下相当于屏蔽掉了卷积层的输出，使得初始模型更加简单，有利于初始阶段模型的训练。      </p><center><img src="/imgs/bag-of-tricks/zero_gamma.jpg" alt="zero_gamma"></center>     <p>该技巧同样适用于其他类似的shortcut结构。       </p><h5 id="4-无偏置衰减"><a href="#4-无偏置衰减" class="headerlink" title="4.无偏置衰减"></a>4.无偏置衰减</h5><p>作为正则化手段的一种，<a href="http://zh.gluon.ai/chapter_deep-learning-basics/weight-decay.html" target="_blank" rel="noopener">权重衰减</a>经常被用来约束权重过度增长，缓解过拟合现象的发生。         </p><p>除了衰减权重之外，其他参数（如偏置）能否也应用衰减技术呢？《<a href="https://arxiv.org/pdf/1807.11205.pdf" target="_blank" rel="noopener">Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes(2018)</a>》作者在ResNet-50和AlexNet上做了实验，结果表明，<strong>只有全连接层和卷积层的权重适合做衰减，偏置以及BN层的$\gamma$和$\beta$参数都不适合做衰减！</strong>           </p><center><img src="/imgs/bag-of-tricks/bn_decay_resnet50.jpg" alt="bn_decay_resnet50"></center><br><br><br><center><img src="/imgs/bag-of-tricks/bn_bias_decay_alexnet.jpg" alt="bn_bias_decay_alexnet"></center>       <p>另外，《<a href="https://openreview.net/pdf?id=rJ4uaX2aW" target="_blank" rel="noopener">Large batch training of convolutional networks with layer-wise adaptive rate scaling(2018)</a>》研究发现，在训练初期，不同层的权重的原始更新项 $\nabla L \left( w _ { t } \right)$ 和 惩罚项 $||\omega||$ 相差悬殊，比如迭代一次后的AlexNet——      </p><center><img src="/imgs/bag-of-tricks/lars_alexnet_ratio.jpg" alt="lars_alexnet_ratio"></center>        <p>这使得训练初期模型的收敛方向对初始权重和初始学习率十分敏感，而导致模型发散、不容易收敛。于是论文提出了<strong>层级自适应比例缩放（Layer-wise Adaptive Rate Scaling, LARS）</strong>，其每层的在每次更新时的学习率都是根据梯度和权重的比例计算出来的：<br>将原始的SGD更新项<br>$$\Delta w _ { t } = \lambda * \nabla L \left( w _ { t } \right)$$     </p><p>改造为<br>$$\triangle w _ { t } ^ { l } = \gamma * \lambda ^ { l } * \nabla L ( w _ { t } ^ { l } )$$       </p><p>其中，$w_t^l$第$l$层的权重，$\gamma$ 用于学习率变化的全局策略（比如整体衰减等），$\lambda^l$ 表示第$l$层的学习率，且<br>$$\lambda ^ { l } = \eta \times \frac { \left| w ^ { l } \right| } { \left| \nabla L \left( w ^ { l } \right) \right| }$$<br>其中，$\eta &lt; 1$ 参数反映了我们对“随机梯度$\nabla L \left( w _ { t } ^ { l } \right)$与真实梯度很接近”的信任程度，越接近（比如批大小越大时，随机梯度就跟真实梯度越接近），那么$\eta$参数就可以设置越大。       </p><p>应用LARS之后，模型收敛速度变得更快——      </p><center><img src="/imgs/bag-of-tricks/alexnet_lars.jpg" alt="alexnet_lars"></center>        <p>而且对于分类任务，batch_size在16K以内都不会有明显的负面影响——      </p><center><img src="/imgs/bag-of-tricks/googlenet_lars.jpg" alt="googlenet_lars"></center>         <h3 id="低精度训练"><a href="#低精度训练" class="headerlink" title="低精度训练"></a>低精度训练</h3><p>现在新出的GPU陆续开始支持低精度FP16计算，并且计算速率远高于FP32，比如V100就提供了100TFLOPS的FP16计算，而其FP32只有14TFLOPS。<br>论文《<a href="https://arxiv.org/pdf/1710.03740.pdf" target="_blank" rel="noopener">Mix Precision Training(2017)</a>》提出一种混合精度的训练过程——    </p><ul><li>用FP16存储所有的参数和激活      </li><li>用FP16计算梯度     </li><li>每份参数都保留一份FP32副本用于更新（更新后再转为FP16）</li><li>为损失乘以一个缩放因子使得FP32的梯度可以更好得跟FP16数据对齐（论文3.2节）<br>  由于FP32比FP16拥有更多的数据位保存指数部分，使得相对于FP16，FP32可以保存小得多或大得多的数值，那么当数值超出FP16范围时就会被截断而导致训练过程中梯度消失或扩散，最终模型发散无法收敛。论文经过实验表明，给FP16的梯度乘以8（即增加三位的指数部分）就足以使训练结果与纯FP32的结果相当。      </li></ul><p>论文经过实验证明，即使使用混合精度也可以训练出跟纯FP32接近的效果——      </p><center><img src="/imgs/bag-of-tricks/mixed_precision_result.jpg" alt="mixed_precision_result"></center>         <p>而且在检测任务上，如果不对损失进行缩放对齐，有可能会导致训练发散——     </p><center><img src="/imgs/bag-of-tricks/mixed_precision_detection.jpg" alt="mixed_precision_detection"></center>         <h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>baseline：批大小256，FP32<br>efficient：批大小1024，FP16    </p><center><img src="/imgs/bag-of-tricks/efficient_results.jpg" alt="efficient_results"></center>         ]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MXNet上的重训练量化</title>
      <link href="/2019/01/23/MXNet-RT_Quantization/"/>
      <url>/2019/01/23/MXNet-RT_Quantization/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=820284&auto=0&height=32"></iframe> <p>本月初随手写了个简单的pip包，用来计算mxnet-gluon模型的参数数量和运算开销，从《<a href="/2019/01/07/MXNet-OpSummary/#结果">模型参数与运算量 - 结果 | Hey~YaHei!</a>》可以看到我们常用的CNN网络的大小和运算开销都是参差不齐的——比如常用的MobileNetv1虽然比ResNet50v1少了约6%的精度，但参数数量和运算量上看，ResNet50v1竟然是MobileNetv1的七倍左右！    </p><p>而从《<a href="/2018/08/05/MobileNets_v1/">MobieleNets v1模型解析 | Hey~YaHei!</a>》可以看到，与ResNet不同的是，MobileNets采用的是一种非常紧凑、高效的卷积计算。除了这种方式，还有许多模型压缩的技巧，比如按《<a href="https://www.jiqizhixin.com/articles/2018-05-22-9" target="_blank" rel="noopener">当前深度神经网络模型压缩和加速都有哪些方法？ - 机器之心</a>》，可以把模型压缩分为<strong>参数修剪和共享</strong>、<strong>低秩因子分解</strong>、<strong>转移/紧凑卷积滤波器</strong>、<strong>知识蒸馏</strong>四个大类。      </p><p>目前应用的比较多的，应该是属于参数修剪和共享类的<strong>裁剪</strong>和<strong>量化</strong>技术。模型压缩的水还很深，我只是这一两个月才开始入的门，不敢瞎扯。本文接下来只简单讨论一下<strong><em>量化</em></strong>技术。     </p><h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p>参考：《<a href="https://www.jiqizhixin.com/articles/2018-06-01-11" target="_blank" rel="noopener">超全总结：神经网络加速之量化模型 | PaperWeekly</a>》<br>简单的说，量化就是降低模型运算的精度，比如把32位的浮点运算变为8位的定点运算（甚至在二值网络或三值网络中乘法运算还变成了简单的加减运算），从而达到大幅度的压缩和加速模型。<br>常见的线性量化过程可以用以下数学表达式来表示：<br>$$r = Round(S(q - Z))$$<br>其中，<br>$q$ 是float32的原始值，<br>$Z$ 是float32的偏移量，<br>$S$ 是float32的缩放因子，<br>$Round(\cdot)$ 是四舍五入近似取整的数学函数，<br>$r$ 是量化后的一个整数值       </p><p>$S$ 和 $Z$ 是量化的两个参数，如何找到合适的 $S$ 和 $Z$ 正是大家研究量化技术的最终目标。<br>或者可以换一个角度看，量化研究的是<strong>表示范围与精确度的权衡</strong>。        </p><h3 id="零点位置：对称量化和非对称量化"><a href="#零点位置：对称量化和非对称量化" class="headerlink" title="零点位置：对称量化和非对称量化"></a>零点位置：对称量化和非对称量化</h3><p>参考：<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Algorithms - Quantization | Distiller</a><br>$Z$ 参数的选择可以分为两类——对称和非对称。      </p><h4 id="对称量化"><a href="#对称量化" class="headerlink" title="对称量化"></a>对称量化</h4><p><center><img src="/imgs/MXNet-RT_Quantization/symmetric-mode.png" alt="symmetric-mode"></center><br>在对称量化中，$r$ 是用有符号的整型数值来表示的，此时 $Z=0$，且$r=0$ 正好是 $\frac{max(q)-min(q)}{2}$ 的量化结果。<br>比如简单地取，<br>$$S = \frac{2^{n-1} - 1}{max(|x|)}$$<br>$$Z = 0$$<br>其中，<br>$n$ 是用来表示该数值的位宽，<br>$x$ 是数据集的总体样本。       </p><p>对称量化比较简单，不仅实现简单，而且由于$Z=0$运算也变得非常简单。      </p><h4 id="非对称量化"><a href="#非对称量化" class="headerlink" title="非对称量化"></a>非对称量化</h4><p><center><img src="/imgs/MXNet-RT_Quantization/asymmetric-mode.png" alt="asymmetric-mode"></center><br>比如简单地取，<br>$$S = \frac{2^{n-1} - 1}{max(x)-min(x)}$$<br>$$Z = min(x)$$<br>非对称量化比较灵活，通常 $r$ 是用无符号的整型数值来表示，此时 $Z \neq 0$。          </p><h3 id="线性量化和截断量化【占坑】"><a href="#线性量化和截断量化【占坑】" class="headerlink" title="线性量化和截断量化【占坑】"></a>线性量化和截断量化【占坑】</h3><p>还没看DoReFa和PACT的论文……        </p><h3 id="逐层量化和逐通道量化"><a href="#逐层量化和逐通道量化" class="headerlink" title="逐层量化和逐通道量化"></a>逐层量化和逐通道量化</h3><p><strong>逐层量化</strong>在整一层上使用同一对量化参数；<br><strong>逐通道量化</strong>则是对每一个通道使用单独的量化参数；<br><em>后文中的重训练量化实现采用的是逐层量化</em>            </p><h3 id="训练后量化和重训练量化"><a href="#训练后量化和重训练量化" class="headerlink" title="训练后量化和重训练量化"></a>训练后量化和重训练量化</h3><h4 id="训练后量化"><a href="#训练后量化" class="headerlink" title="训练后量化"></a>训练后量化</h4><ul><li>最简单的方式是直接利用<strong>最大最小值</strong>，比如量化权重时直接计算每一层里的权重的最大最小值然后代入上述的式子中计算S和Z，计算比较简单，即使在实际应用当中在加载模型时计算量化参数也不会有很大开销；      </li><li>更为复杂的则是采用<strong>聚类</strong>算法，而不是简单的确定S和Z，此时运算会变得比较复杂——相邻两个数值间的差距不再是固定的，需要通过查表的方式来实现；     </li><li>由于输入和激活是动态的，不像静态的权重可以直接事先计算好S和Z，所以在训练后量化中经常会将为输入动态计算S和Z；     </li><li>为了让模型计算更快，也有一些技巧可以静态量化输入和激活，比如<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">Quantization - MXNet | github</a> 允许模型通过平滑平均的方式为每一个输入和激活确定静态的S和Z；          </li></ul><p>除此之外，一些论文以及《<a href="https://nervanasystems.github.io/distiller/algo_quantization/index.html" target="_blank" rel="noopener">Algorithms - Quantization | Distiller</a>》指出——      </p><ol><li>量化位宽小于8时，模型精度会出现比较严重的下降；      </li><li>对于ResNet等冗余网络，训练后量化已经可以取得不错的效果；但对于MobileNet等紧凑的网络，训练后量化会对精度造成比较大的伤害；    </li><li>对第一层卷积、最后一层全连接层进行量化会对精度造成比较大的伤害……      </li></ol><h4 id="重训练量化"><a href="#重训练量化" class="headerlink" title="重训练量化"></a>重训练量化</h4><p>参考：《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2017)</a>》      </p><p><center><img src="/imgs/MXNet-RT_Quantization/retrain-quantize.jpg" alt="retrain-quantize"></center><br>由于训练后量化会对MobileNet等紧凑网络的精度造成比较大的伤害，所以有人开始提出要进行重训练量化……<br>思路非常简单，<strong>把量化参数塞入网络当中，并在训练过程中模拟量化过程（缩放、偏移、截断、复原），用32位浮点数进行训练，最终将训练得到的量化参数固化到网络当中</strong>。       </p><p>根据<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize#quantized-accuracy-results" target="_blank" rel="noopener">Tensorflow - Quantize</a>模块的描述，Mobilenet-v1-224-1经过重训练后在ImageNet上的准确率从70.9%下降到69.7%（降幅1.2%）。<br>关于量化对准确率的影响，还有两件事情让人费解：        </p><ol><li>按照<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">MXNet - Quantization</a>给出的数据，仅仅使用校准而非重训练实现的静态量化，MobieleNet-1.0的准确率居然只下降了<strong>0.15%</strong>？！？！？！着实让人吃惊，不过要注意到它的单精度推断准确率也只有69.76%、而且测试数据集是他自己提供的一个<code>val_256_q90.rec</code>的1.4G数据集——对，我试着探索了一番始终没查到这个数据集从哪来的，难道是MXNet自己做的？<br> <img src="/imgs/MXNet-RT_Quantization/mxnet_quantize.jpg" alt="mxnet_quantize">     </li><li><p>按照<a href="http://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html" target="_blank" rel="noopener">Gluon Model zoo</a>的数据，MobileNetv1_224_1.0和MobileNetv2_224_1.0的准确率分别为<strong>71.05%</strong>和<strong>71.92%</strong>（文档没说测试集是哪，但一般应该用的是ILSVRC2012的验证集吧）；另外按照<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize#quantized-accuracy-results" target="_blank" rel="noopener">Tensorflow - Quantize</a>的数据，MobileNetv1_224_1.0和MobileNetv2_224_1.0的准确率分别为<strong>70.9%</strong>和<strong>71.8%</strong>（依旧没有指明测试集，应该也是ILSVRC2012验证集吧，不过这不是重点）——但到了<a href="https://gluon-cv.mxnet.io/model_zoo/classification.html" target="_blank" rel="noopener">Gluon CV</a>的数据，MobileNetv1_224_1.0和MobileNetv2_224_1.0的准确率分别为<strong>73.28%</strong>和<strong>71.92%</strong>（其实他没指明输入图片尺寸，不过inception指明是299了，那其他网络应该也是默认的224吧）——v1竟然反而比v2多了一个百分点还多？？（黑人问号……）       </p><p> <strong>Gluon Model Zoo：</strong><br> <img src="/imgs/MXNet-RT_Quantization/gluon_model_zoo_mobilenet.jpg" alt="gluon_model_zoo_mobilenet">      </p><p> <strong>Tensorflow - Quantize：</strong><br> <img src="/imgs/MXNet-RT_Quantization/tensorflow_quantize.jpg" alt="tensorflow_quantize">     </p><p> <strong>Gluon CV：</strong><br> <img src="/imgs/MXNet-RT_Quantization/gluon_cv_mobilenet.jpg" alt="gluon_cv_mobilenet">      </p></li></ol><h2 id="重训练量化的MXNet实现"><a href="#重训练量化的MXNet实现" class="headerlink" title="重训练量化的MXNet实现"></a>重训练量化的MXNet实现</h2><p>MXNet提供了简单的量化——<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">Quantization - MXNet | github</a>，不过可惜只支持简单的训练后量化。所以我决定参考Intel基于Pytorch实现的<a href="https://github.com/NervanaSystems/distiller" target="_blank" rel="noopener">Distiller</a>和谷歌的论文，实现一个简单的MXNet网络的重训练量化。       </p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>项目已开源至<a href="https://github.com/hey-yahei/Quantization.MXNet" target="_blank" rel="noopener">github</a>，这里只写一下大体的思路：     </p><ul><li>总体思路就是进行模拟量化，然后把量化参数作为可训练参数进行训练；      </li><li>将relu都替换为relu6，因为我发现量化后如果还用relu，冗余网络影响不大，但对紧凑网络来说精度会骤减5%-6%，伤害很大；    </li><li>改造gluon网络的思路跟我先前写的 <a href="https://github.com/hey-yahei/OpSummary.MXNet" target="_blank" rel="noopener">OpSummary.MXNet | github</a> 类似，往相关的Block塞Parameter、重写hybrid_forward对象函数、借助钩子（forward_hook和forward_pre_hook）来收集更新某些平滑参数；     </li><li>通过解析参数名来匹配对应的参数（主要是在合并BN阶段）；      </li><li>利用MXNet提供的底层库函数（MXQuantizeSymbol）固化量化模型的结构，然后将对应的参数解算或映射到最终的参数名和数据类型（官方的底层库不支持静态量化，所幸这个改动不算麻烦，自己写了个脚本去把动态量化的max、min改为静态的数值就行）；    </li><li>训练前期输入和激活的量化用动态实现，同时用指数平滑平均（EMA）累积训练集上的最大最小值，当累积结果相对稳定（比如迭代一万次后）再改为静态量化对其他参数进行微调；        </li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><h4 id="量化前后"><a href="#量化前后" class="headerlink" title="量化前后"></a>量化前后</h4><p>调试过程比较繁琐，所以为简单起见还没用ImageNet进行实验而使用了CIFAR100数据集（20分类，50000训练集，10000验证集），统一不使用数据增强来进行训练。分别训练出准确率分别为<strong>83.45%</strong>、<strong>84.20%</strong>、<strong>89.35%</strong>的<strong>MobileNet_1_0</strong>、<strong>MobileNet_1_0_ReLU6</strong>和<strong>ResNet50_v1</strong>作为baseline——     </p><table><thead><tr><th style="text-align:center">Quantization</th><th style="text-align:center">MobileNet_1_0_ReLU</th><th style="text-align:center">MobileNet_1_0_ReLU6</th><th style="text-align:center">ResNet50_v1</th></tr></thead><tbody><tr><td style="text-align:center">FP32</td><td style="text-align:center">83.45%</td><td style="text-align:center">84.20%</td><td style="text-align:center">89.35%</td></tr><tr><td style="text-align:center">UINT8_ONLINE</td><td style="text-align:center">76.61%</td><td style="text-align:center">77.66%</td><td style="text-align:center">89.11%</td></tr><tr><td style="text-align:center">UINT8_OFFLINE_CALIB</td><td style="text-align:center">72.10%</td><td style="text-align:center">77.44%</td><td style="text-align:center">88.96%</td></tr><tr><td style="text-align:center">UINT8_OFFLINE_RETRAIN</td><td style="text-align:center">80.72%</td><td style="text-align:center">83.03%</td><td style="text-align:center">/</td></tr><tr><td style="text-align:center">UINT8_OFFLINE_FAKEBN</td><td style="text-align:center">80.52%</td><td style="text-align:center">83.00%</td><td style="text-align:center">/</td></tr></tbody></table><p>其中，<br><strong>FP32</strong> 为单精度浮点下的模型；<br><strong>UINT8_ONLINE</strong> 为8位非对称量化、动态量化激活的模型；<br><strong>UINT8_OFFLINE_CALIB</strong> 为8位非对称量化、静态量化激活（在整个训练集上前向传播一次后用EMA校准最大最小值）的模型；<br><strong>UINT8_OFFLINE_RETRAIN</strong> 为8位非对称量化、重训练（不合并BN层）的模型；<br><strong>UINT8_OFFLINE_FAKEBN</strong> 为8位非对称量化、重训练（伪BN操作）的模型；<br><em>由于即使不重训练，ResNet精度也没有很明显的下降，所以调试过程中没有尝试对ResNet做重训练。</em>        </p><p>可以看到，即使没有重训练，量化后的Resnet50_v1也只有少量的精度下降；<br>但MobileNet_1_0精度下降非常明显，静态量化激活甚至会导致精度下降11%左右，不过经过重训练后静态量化激活的模型精度只下降了3.5%左右（其实依旧很多）……后来发现这是激活函数的缘故，把ReLU都替换成ReLU6之后，重训练静态量化的模型精度仅仅下降1.2%！<br>关于RELU6可以参考：<a href="https://stackoverflow.com/questions/47220595/why-the-6-in-relu6/47220765" target="_blank" rel="noopener">Why the 6 in relu6? | stackoverflow</a>，简单来说，ReLU6可以将定点数的整数部分限制在6以内，防止量化误差在传播过程中过分扩大。      </p><h4 id="伪·批归一化"><a href="#伪·批归一化" class="headerlink" title="伪·批归一化"></a>伪·批归一化</h4><p>不过吧，细心的你可能会想起来为了加快推断速度，往往会将BN层融到卷积、全连接等线性层中去（参考《<a href="/2018/08/08/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析 - BN层合并 | Hey~YaHei!</a>》）。现在BN层和卷积层之间隔着一个非线性的量化（因为有截断、取整、取最值的过程），这可怎么办？既然量化后不方便合并BN层，那在量化前（重训练前），提前把BN层融合掉好了，这就是论文《<a href="http://arxiv.org/pdf/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference(2017)</a>》提到的伪BatchNorm操作，把BN层的参数都丢给卷积层，训练过程中卷积层既要做伪归一化，又要做伪量化！       </p><p><center><img src="/imgs/MXNet-RT_Quantization/fake_bn.jpg" alt="fake_bn"></center><br>当然，这是有额外代价的，训练的前向传播过程中，需要做两次卷积运算——      </p><ol><li>用原始权重卷积，卷积输出用于更新伪BatchNorm的平均值、标准差，这次卷积运算不需要反向传播；       </li><li>另外一次用量化过的权重卷积，卷积输出的结果作为下一层的输入，这一次的卷积运算需要反向传播         </li></ol><p>这样一来，训练速度大概会下降30%-40%左右（实测）；<br>另外，不引入伪·批归一化的情况下，重训练收敛非常快！几乎算是迭代几百次（哦，我用的是Adam）就接近收敛，引入后精度下降特别厉害（起初甚至以为我程序有问题，然后反复验证我伪BN的输出结果），需要训一段时间才能得到最后的结果：    </p><p><center><img src="/imgs/MXNet-RT_Quantization/mobilenet_quant_relu6.jpg" alt="mobilenet_quant_relu6"></center>      </p><p><center><img src="/imgs/MXNet-RT_Quantization/mobilenet_quant_relu6_fakebn.jpg" alt="mobilenet_quant_relu6_fakebn"></center><br><em>图(上)：不合并BN的学习曲线（前一万次迭代采用动态量化(lr=1e-5)，之后转为静态量化做微调(lr=1e-5)）</em><br><em>图(下)：合并BN后的学习曲线（前一万次迭代采用动态量化(lr=1e-5)，之后转为静态量化做微调(lr=1e-6)）</em>     </p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ol><li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize" target="_blank" rel="noopener">Tensorflow - Quantize</a>：Tensorflow的重训练量化模块      </li><li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization" target="_blank" rel="noopener">Tensorflow - Quantization</a>：Tensorflow的量化OP       </li><li><a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">MXNet - Quantization</a>：MXNet的训练后量化模块     </li><li><a href="https://nervanasystems.github.io/distiller/" target="_blank" rel="noopener">Neural Network Distiller</a>：Intel的开源模型压缩库（基于Pytorch）      </li><li>《<a href="https://arxiv.org/pdf/1806.08342.pdf" target="_blank" rel="noopener">Quantizing deep convolutional networks for efficient inference: A whitepaper</a>》<br> 谷歌Tensorflow官方发布的量化白皮书，译文可参考 <a href="https://blog.csdn.net/guvcolie/article/details/81286349" target="_blank" rel="noopener">CSDN上Colie-Li的翻译</a>      </li><li>《<a href="https://www.jiqizhixin.com/articles/2018-05-22-9" target="_blank" rel="noopener">当前深度神经网络模型压缩和加速都有哪些方法？ | PaperWeekly, 小一一</a>    </li><li>《<a href="https://www.jiqizhixin.com/articles/2018-05-18-4" target="_blank" rel="noopener">让机器“删繁就简”：深度神经网络加速与压缩 | 深度学习大讲堂, 程健</a>》    </li><li>《<a href="https://www.jiqizhixin.com/articles/2018-06-01-11" target="_blank" rel="noopener">超全总结：神经网络加速之量化模型 | PaperWeekly, 郝泽宇</a>》    </li><li>《<a href="https://blog.csdn.net/daniaokuye/article/details/82746661" target="_blank" rel="noopener">模型压缩开源库整理 | CSDN, 库页</a>》      </li></ol><h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><ol><li>编了freeze的代码，输出来的静态图结构看起来也符合预期，但目前还没实际加载运行过 <del>（因为不巧，MXNet对量化的支持还不够好，官方的<a href="https://github.com/apache/incubator-mxnet/tree/master/example/quantization" target="_blank" rel="noopener">Quantization - MXNet</a>分为普通CPU推断和Xeon系列的CPU推断，前者竟然不支持带group的卷积，实验室的电脑又正好都是i系列的，十分头疼= =等开了学我再想办法找台Intel Xeon处理器的电脑测试一下）</del> 好像不是处理器的缘故，mkldnn应该是支持所有的intel处理器的吧，现在关键是<code>Symbol</code>居然没有<code>get_backend_symbol</code>方法，我装的应该是最新版本才对啊；    </li><li>除了验证freeze结果之外，还需要测一下量化后的模型推断速度有多大提升；      </li><li>尝试逐通道量化；      </li><li>训完的模型目前只能在台式机跑跑，想再看看有没有可能在Tengine或ncnn上跑跑我的量化模型；      </li><li>看起来量化的效果还不错，但我比较好奇量化后的模型跨数据集时会不会出现严重的精度下降，可以做个实验比较一下~     </li><li>目前只对分类网络进行量化，想试一下对检测网络MobileNet-SSD和人脸识别网络MobileFaceNet网络的量化结果；      </li><li>继续看看DoReFa和PACT的量化方式；       </li></ol><p>代码还比较粗糙，等着开了学再进一步完善吧！    </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>模型参数与计算量</title>
      <link href="/2019/01/07/MXNet-OpSummary/"/>
      <url>/2019/01/07/MXNet-OpSummary/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=449578848&auto=0&height=32"></iframe>     <p>这一两个月比较忙，没什么时间空下来写写博文，加上最近处于摸索阶段，各种思路还没有理清，不敢瞎写。     </p><p>这两天看到Lyken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>，萌生了一个写一个MXNet的计数器的想法，项目已经开源到<a href="https://github.com/hey-yahei/OpSummary.MXNet" target="_blank" rel="noopener">github</a>上，并且做个pip的包，嘻嘻……第一次做包，虽然只是一个简单的工具，还是截图留个念——<br><img src="/imgs/MXNet-OpSummary/mxop-pip.png" alt="mxop-pip">         </p><h3 id="参数量与计算量"><a href="#参数量与计算量" class="headerlink" title="参数量与计算量"></a>参数量与计算量</h3><ul><li><strong>参数量</strong> 是指模型含有多少参数，直接决定模型文件的大小，也影响模型推断时对内存的占用量     </li><li><strong>计算量</strong> 是指模型推断时需要多少计算次数，通常是以<strong><em>MAC(Multiply ACcumulate，乘积累加)</em></strong>次数来表示    </li></ul><p>这两者其实是评估模型时非常重要的参数，<strong>一个实际要应用的模型不应当仅仅考虑它在准确率上有多出色的表现，还应该要考虑它的鲁棒性、扩展性以及对资源的依赖程度</strong>，但事实上很多论文都不讨论他们模型需要多少计算力，一种可能是他们的定位还是纯学术研究——提出一种新的思路，即使这种思路不便于应用，但未来说不定计算力上来了，或者有什么飞跃性的改进方法来改进这一问题，或者提出自己的思路来启发其他研究者的研究（抛砖引玉）；另一种可能就是……他们在有意识地回避这一问题，我总觉得很多人是在回避这一问题，无论是论文还是各种AI比赛的解决方案（比赛本身只关注准确率指标本身也不够合理）。      </p><p>接下来，我们试着用不同的视角重新审视以前那些常用的CNN OP——       </p><h4 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h4><p>首先考虑一个3输入、3输出、有偏置的全连接层（Layer2），       </p><p><center><img height="300" src="/imgs/MXNet-OpSummary/fc.png"></center><br>$$<br>\begin{array} { l } { a _ { 1 } ^ { ( 2 ) } = x _ { 0 } + w _ { 11 } x _ { 1 } + w _ { 12 } x _ { 2 } + w _ { 13 } x _ { 3 } } \\ { a _ { 2 } ^ { ( 2 ) } = x _ { 0 } + w _ { 21 } x _ { 1 } + w _ { 22 } x _ { 2 } + w _ { 23 } x _ { 3 } } \\ { a _ { 3 } ^ { ( 2 ) } = x _ { 0 } + w _ { 31 } x _ { 1 } + w _ { 32 } x _ { 2 } + w _ { 33 } x _ { 3 } } \end{array}<br>$$<br>其参数数量为 $3\times3$，乘加次数为 $3\times3$。<br>这是一个典型的矩阵和向量之间的乘法运算，<br>推广到n输入、m输出、有偏置的全连接层，其参数数量为 $m \times n$，乘加次数为 $m \times n$。      </p><h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><p>首先考虑一个单通道输入输出，输出图大小为 $m_o \times n_o$，核大小为 $k_m \times k_n$，带偏置，步长1，不补零的卷积，       </p><p><center><img height="300" src="/imgs/MXNet-OpSummary/convolution.gif"></center><br>$$<br>\begin{aligned} O _ { 11 } = &amp; w _ { 11 } I _ { 11 } + w _ { 12 } I _ { 12 } + w _ { 13 } I _ { 13 } + \\ &amp; w _ { 21 } I _ { 21 } + w _ { 22 } I _ { 22 } + w _ { 23 } I _ { 23 } + \\ &amp; w _ { 31 } I _ { 31 } + w _ { 32 } I _ { 32 } + w _ { 33 } I _ { 33 } + b \end{aligned}<br>$$<br>其参数数量为 $k_m \times k_n + 1$，乘加次数为 $m_o \times n_o \times k_m \times k_n$。<br>推广到$c_i$通道输入，$c_o$通道输出的情况，其参数数量为 $(k_m \times k_n + 1) \times c_i \times c_o$，乘加次数为 $m_o \times n_o \times k_m \times k_n \times c_i \times c_o$。       </p><p>这是标准卷积的情况，如果是深度向分解的卷积，参考博文《<a href="/2018/08/05/MobileNets_v1/#效率比较">MobileNets v1模型解析/深度向卷积分解/效率比较 | Hey~YaHei!</a>》可以知道，其参数数量为 $(k_m \times k_n + c_o) \times c_i$，乘加次数为 $m_o \times n_o \times (k_m \times k_n + c_o) \times c_i$，这里为简化运算忽略了偏置。      </p><h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><p>池化跟卷积的操作比较相近，最大池化仅仅是比较操作，其计算量往往可以忽略不计；平均池化则会涉及到 $m_o \times m_i \times (k_m \times k_n - 1) \times c$ 次加法和 $m_o \times n_o \times c$ 次除法（输入输出通道均为 $c$），两者都没有参数。      </p><h4 id="批归一化BN"><a href="#批归一化BN" class="headerlink" title="批归一化BN"></a>批归一化BN</h4><p>假设输入数量为N，<br>$$bn[j] = \frac{\gamma (conv[j] - mean)}{\sqrt{variance}} + \beta$$<br>可以很容易看到，其参数数量为 $2N$，运算包含 $2N$ 次加法（包括减法）和 $N$ 次乘法。<br>这里要注意，在推断过程中，variance和mean都是已知的，所以 $\frac{\gamma}{\sqrt{variance}}$ 可以直接合并为一个值，<br>甚至，博文《<a href="/2018/08/08/MobileNets-SSD/#BN层合并">MobileNet-SSD网络解析/BN层合并 | Hey~YaHei!</a>》提到过BN层可以直接融入前边的线性层（如卷积和全连接），此时BN层不会造成任何开销。     </p><h3 id="OpSummary"><a href="#OpSummary" class="headerlink" title="OpSummary"></a>OpSummary</h3><p><a href="https://github.com/hey-yahei/OpSummary.MXNet" target="_blank" rel="noopener">hey-yahei/OpSummary.MXNet | github</a><br>知道了如何计算各层的参数数量和运算次数，我们就可以编写一个小工具来为MXNet模型统计参数数量和计算量。     </p><h4 id="钩子"><a href="#钩子" class="headerlink" title="钩子"></a>钩子</h4><p>为了计算运算量，必须能够取得每一层的参数、输入和输出大小，当然可以根据各层的参数一层层的推算，但这似乎太麻烦了。受Lyken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>启发，我们可以为每个Block注册一个hook，每次Block经过前向传播后都会调用这个hook（准确的说，有两种hook，pre_hook在前向传播前调用，hook在前向传播后调用）。      </p><h4 id="超参数获取"><a href="#超参数获取" class="headerlink" title="超参数获取"></a>超参数获取</h4><p>MXNet中想读取一个Block的超参数实在有些麻烦，因为它把超参数全都存在私有属性里了！——<a href="https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/nn/conv_layers.py#L105" target="_blank" rel="noopener">mxnet/gluon/nn/conv_layers.py#L105 | github</a>，不像Pytorch的Module是直接把超参数放在公共属性上——<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L20" target="_blank" rel="noopener">torch/nn/modules/conv.py#L20 | github</a>。<br>有两种思路来获取超参数——     </p><ol><li>从输入输出、公共变量（如Conv的weight和bias）的shape来推断    </li><li>解析字符串<br> MXNet的Block都重载了 <code>__repr__</code> 方法，比如<a href="https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/nn/conv_layers.py#L143" target="_blank" rel="noopener">mxnet/gluon/nn/conv_layers.py#L143 | github</a>，用于打印Block的超参数。那……我们其实可以用 <code>str(nn.Block)</code> 的方式来取得这个字符串，然后进行解析= =好麻烦啊      </li></ol><h4 id="统计"><a href="#统计" class="headerlink" title="统计"></a>统计</h4><p>yken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>在统计各个模块的参数数量和运算次数时，是注册了一个公共的缓冲区来进行累加（参考<a href="https://github.com/Lyken17/pytorch-OpCounter/blob/master/thop/utils.py#L28" target="_blank" rel="noopener">pytorch-OpCounter/thop/utils | github</a>），而MXNet并没有提供这样的缓冲区（<em>或许只是我不知道？</em>），我的解决办法是——<br>写一个拥有静态变量的函数作为累加函数，调用Block的apply方法，让每个Children把自己的统计结果依次累加给这个静态变量，最后从静态变量取出统计结果。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>用OpSummary把MXNet提供的model_zoo里所有的模型都测试了一遍，结果如下表所示：<br>Top1 Acc和Top5 Acc数据来源于 <a href="http://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html" target="_blank" rel="noopener">MXNet文档</a>    </p><table><thead><tr><th style="text-align:right">Model</th><th style="text-align:right">Params(M)</th><th style="text-align:right">Muls(G)</th><th style="text-align:right">*Params(M)</th><th style="text-align:right">*Muls(G)</th><th style="text-align:right">Top1 Acc</th><th style="text-align:right">Top5 Acc</th></tr></thead><tbody><tr><td style="text-align:right">AlexNet</td><td style="text-align:right">61.10</td><td style="text-align:right">0.71</td><td style="text-align:right">2.47</td><td style="text-align:right">0.66</td><td style="text-align:right">0.5492</td><td style="text-align:right">0.7803</td></tr><tr><td style="text-align:right">VGG11</td><td style="text-align:right">132.86</td><td style="text-align:right">7.61</td><td style="text-align:right">9.22</td><td style="text-align:right">7.49</td><td style="text-align:right">0.6662</td><td style="text-align:right">0.8734</td></tr><tr><td style="text-align:right">VGG13</td><td style="text-align:right">133.04</td><td style="text-align:right">11.30</td><td style="text-align:right">9.40</td><td style="text-align:right">11.18</td><td style="text-align:right">0.6774</td><td style="text-align:right">0.8811</td></tr><tr><td style="text-align:right">VGG16</td><td style="text-align:right">138.63</td><td style="text-align:right">15.47</td><td style="text-align:right">14.71</td><td style="text-align:right">15.35</td><td style="text-align:right">0.7323</td><td style="text-align:right">0.9132</td></tr><tr><td style="text-align:right">VGG19</td><td style="text-align:right">143.67</td><td style="text-align:right">19.63</td><td style="text-align:right">20.02</td><td style="text-align:right">19.51</td><td style="text-align:right">0.7411</td><td style="text-align:right">0.9135</td></tr><tr><td style="text-align:right">VGG11_bn</td><td style="text-align:right">132.87</td><td style="text-align:right">7.62</td><td style="text-align:right">9.23</td><td style="text-align:right">7.49</td><td style="text-align:right">0.6859</td><td style="text-align:right">0.8872</td></tr><tr><td style="text-align:right">VGG13_bn</td><td style="text-align:right">133.06</td><td style="text-align:right">11.32</td><td style="text-align:right">9.42</td><td style="text-align:right">11.20</td><td style="text-align:right">0.6884</td><td style="text-align:right">0.8882</td></tr><tr><td style="text-align:right">VGG16_bn</td><td style="text-align:right">138.37</td><td style="text-align:right">15.48</td><td style="text-align:right">14.73</td><td style="text-align:right">15.36</td><td style="text-align:right">0.7310</td><td style="text-align:right">0.9176</td></tr><tr><td style="text-align:right">VGG19_bn</td><td style="text-align:right">143.69</td><td style="text-align:right">19.65</td><td style="text-align:right">20.05</td><td style="text-align:right">19.52</td><td style="text-align:right">0.7433</td><td style="text-align:right">0.9185</td></tr><tr><td style="text-align:right">Inception_v3</td><td style="text-align:right">23.87</td><td style="text-align:right">5.72</td><td style="text-align:right">21.82</td><td style="text-align:right">5.72</td><td style="text-align:right">0.7755</td><td style="text-align:right">0.9364</td></tr><tr><td style="text-align:right">ResNet18_v1</td><td style="text-align:right">11.70</td><td style="text-align:right">1.82</td><td style="text-align:right">11.19</td><td style="text-align:right">1.82</td><td style="text-align:right">0.7093</td><td style="text-align:right">0.8992</td></tr><tr><td style="text-align:right">ResNet34_v1</td><td style="text-align:right">21.81</td><td style="text-align:right">3.67</td><td style="text-align:right">21.3</td><td style="text-align:right">3.67</td><td style="text-align:right">0.7437</td><td style="text-align:right">0.9187</td></tr><tr><td style="text-align:right">ResNet50_v1</td><td style="text-align:right">25.63</td><td style="text-align:right">3.87</td><td style="text-align:right">23.58</td><td style="text-align:right">3.87</td><td style="text-align:right">0.7647</td><td style="text-align:right">0.9313</td></tr><tr><td style="text-align:right">ResNet101_v1</td><td style="text-align:right">44.70</td><td style="text-align:right">7.59</td><td style="text-align:right">42.65</td><td style="text-align:right">7.58</td><td style="text-align:right">0.7834</td><td style="text-align:right">0.9401</td></tr><tr><td style="text-align:right">ResNet152_v1</td><td style="text-align:right">60.40</td><td style="text-align:right">11.30</td><td style="text-align:right">58.36</td><td style="text-align:right">11.30</td><td style="text-align:right">0.7900</td><td style="text-align:right">0.9438</td></tr><tr><td style="text-align:right">ResNet18_v2</td><td style="text-align:right">11.70</td><td style="text-align:right">1.82</td><td style="text-align:right">11.18</td><td style="text-align:right">1.82</td><td style="text-align:right">0.7100</td><td style="text-align:right">0.8992</td></tr><tr><td style="text-align:right">ResNet34_v2</td><td style="text-align:right">21.81</td><td style="text-align:right">3.67</td><td style="text-align:right">21.30</td><td style="text-align:right">3.67</td><td style="text-align:right">0.7440</td><td style="text-align:right">0.9208</td></tr><tr><td style="text-align:right">ResNet50_v2</td><td style="text-align:right">25.60</td><td style="text-align:right">4.10</td><td style="text-align:right">23.55</td><td style="text-align:right">4.10</td><td style="text-align:right">0.7711</td><td style="text-align:right">0.9343</td></tr><tr><td style="text-align:right">ResNet101_v2</td><td style="text-align:right">44.64</td><td style="text-align:right">7.82</td><td style="text-align:right">42.59</td><td style="text-align:right">7.81</td><td style="text-align:right">0.7853</td><td style="text-align:right">0.9417</td></tr><tr><td style="text-align:right">ResNet152_v2</td><td style="text-align:right">60.33</td><td style="text-align:right">11.54</td><td style="text-align:right">58.28</td><td style="text-align:right">11.53</td><td style="text-align:right">0.7921</td><td style="text-align:right">0.9431</td></tr><tr><td style="text-align:right">DenseNet121</td><td style="text-align:right">8.06</td><td style="text-align:right">2.85</td><td style="text-align:right">7.04</td><td style="text-align:right">2.85</td><td style="text-align:right">0.7497</td><td style="text-align:right">0.9225</td></tr><tr><td style="text-align:right">DenseNet161</td><td style="text-align:right">28.90</td><td style="text-align:right">7.76</td><td style="text-align:right">26.69</td><td style="text-align:right">7.76</td><td style="text-align:right">0.7770</td><td style="text-align:right">0.9380</td></tr><tr><td style="text-align:right">DenseNet169</td><td style="text-align:right">14.31</td><td style="text-align:right">3.38</td><td style="text-align:right">12.64</td><td style="text-align:right">3.38</td><td style="text-align:right">0.7617</td><td style="text-align:right">0.9317</td></tr><tr><td style="text-align:right">DenseNet201</td><td style="text-align:right">20.24</td><td style="text-align:right">4.32</td><td style="text-align:right">18.32</td><td style="text-align:right">4.31</td><td style="text-align:right">0.7732</td><td style="text-align:right">0.9362</td></tr><tr><td style="text-align:right">MobileNet_v1_1.00</td><td style="text-align:right">4.25</td><td style="text-align:right">0.57</td><td style="text-align:right">3.23</td><td style="text-align:right">0.57</td><td style="text-align:right">0.7105</td><td style="text-align:right">0.9006</td></tr><tr><td style="text-align:right">MobileNet_v1_0.75</td><td style="text-align:right">2.60</td><td style="text-align:right">0.33</td><td style="text-align:right">1.83</td><td style="text-align:right">0.33</td><td style="text-align:right">0.6738</td><td style="text-align:right">0.8782</td></tr><tr><td style="text-align:right">MobileNet_v1_0.50</td><td style="text-align:right">1.34</td><td style="text-align:right">0.15</td><td style="text-align:right">0.83</td><td style="text-align:right">0.15</td><td style="text-align:right">0.6307</td><td style="text-align:right">0.8475</td></tr><tr><td style="text-align:right">MobileNet_v1_0.25</td><td style="text-align:right">0.48</td><td style="text-align:right">0.04</td><td style="text-align:right">0.22</td><td style="text-align:right">0.04</td><td style="text-align:right">0.5185</td><td style="text-align:right">0.7608</td></tr><tr><td style="text-align:right">MobileNet_v2_1.00</td><td style="text-align:right">3.54</td><td style="text-align:right">0.32</td><td style="text-align:right">2.26</td><td style="text-align:right">0.32</td><td style="text-align:right">0.7192</td><td style="text-align:right">0.9056</td></tr><tr><td style="text-align:right">MobileNet_v2_0.75</td><td style="text-align:right">2.65</td><td style="text-align:right">0.19</td><td style="text-align:right">1.37</td><td style="text-align:right">0.19</td><td style="text-align:right">0.6961</td><td style="text-align:right">0.8895</td></tr><tr><td style="text-align:right">MobileNet_v2_0.50</td><td style="text-align:right">1.98</td><td style="text-align:right">0.10</td><td style="text-align:right">0.70</td><td style="text-align:right">0.09</td><td style="text-align:right">0.6449</td><td style="text-align:right">0.8547</td></tr><tr><td style="text-align:right">MobileNet_v2_0.25</td><td style="text-align:right">1.53</td><td style="text-align:right">0.03</td><td style="text-align:right">0.25</td><td style="text-align:right">0.03</td><td style="text-align:right">0.5074</td><td style="text-align:right">0.7456</td></tr><tr><td style="text-align:right">SqueezeNet1_0</td><td style="text-align:right">1.25</td><td style="text-align:right">0.82</td><td style="text-align:right">0.74</td><td style="text-align:right">0.73</td><td style="text-align:right">0.5611</td><td style="text-align:right">0.7909</td></tr><tr><td style="text-align:right">SqueezeNet1_1</td><td style="text-align:right">1.24</td><td style="text-align:right">0.35</td><td style="text-align:right">0.72</td><td style="text-align:right">0.26</td><td style="text-align:right">0.5496</td><td style="text-align:right">0.7817</td></tr></tbody></table><p>由于分类网络经常用作其他框架（如目标检测的SSD）的backbone，所以这里增加了<strong>*Params</strong>列和<strong>*Muls</strong>列用于表示除去最后几个分类的Layer之后的结果。具体丢弃的层参见 <a href="https://github.com/hey-yahei/OpSummary.MXNet/blob/master/tests/test_gluon_utils.py" target="_blank" rel="noopener">mxop/tests/test_gluon_utils.py | github</a> 文件的 <code>dropped_layers</code> 变量。    </p><p><img src="/imgs/MXNet-OpSummary/Parameters.jpg" alt="Parameters"></p><p><img src="/imgs/MXNet-OpSummary/Multiplication.jpg" alt="Multiplication">   </p><p>emmm还是比较直观的嘛！希望上述的图表对大家挑选backbone的时候能有所帮助~    </p><h4 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h4><p>目前我只模仿Lyken17的<a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">pytorch-OpCounter</a>实现了简单的参数与计算量计数，并且制作了pip包（你可以按照我github页面上的说明用pip安装 <strong>mxop</strong> 包）；<br>等之后有时间，我想继续      </p><ol><li>依次输出各个层的参数与计算量而不是整个模型，分析各个层的比例     </li><li>支持MXNet的静态图模型（根据json文件解析参数并推算参数量和计算量，而不是用动态图的hook）</li><li>支持MXNet的量化模型</li><li>……      </li></ol><!-----------------------------------------------------     **【几句闲话】**       从去年大四开始学习深度学习就一直在想，深度学习虽然表现很好，但计算量也非常大，这种东西真的能在实际中应用么？比如说我本科毕设用了谷歌的im2txt，一个为静态图片生成一句话描述的模型，在我的PC上用CPU跑，大概是2.5秒钟能完成一张图片的描述——如果作为一个[PC端的图片管理软件](https://github.com/hey-yahei/image-management)（为图库生成描述方便索引）勉强还是能够接受，但如果说想把这种东西应用到嵌入式上（比如我在论文中提到的，辅助盲人导航或者了解周遭世界），实时性会是一个很大的考验——说到底，从学术研究到实际应用终究还有太多路要走（我觉得关键在于方案的**鲁棒性、扩展性和小型化**，而不是那1%的准确率提升，当然，这也看领域）。【[如何看待张潼老师离职腾讯？放浪者的回答 | 知乎](https://www.zhihu.com/question/307359849/answer/565900443?from=timeline&utm_medium=social&utm_oi=75499502043136&utm_source=wechat_session)】       > 然而这跟能否应用到具体业务上具有不亚于天壤之别的鸿沟-----这个鸿沟不仅仅是国内很多企业领袖意识不到，就算是大部分研究人员自己也意识不到。在国外由于有大批量PHD水平的高级工程师甘作码农，还能够尽力去弥补research -> product/service的这个鸿沟（与其说是去弥补这个鸿沟，不如说是尽己所能去做出取舍和折中）。而在国内学术研究和工程开发严重脱节的情况下，这个鸿沟比国外要大很多很多。这是行业现状和人员储备的问题。没错，我们需要绘制蓝图创造未来的研究者、科学家，看起来光鲜亮丽、高大上的科学家，但我们还需要数十数百倍于科学家的工程师来弥补学术和业务的鸿沟，时常在想——***我们是不是真的需要这么多“画饼子”的人？***（个人的浅薄之见，说不定只是我嵌入式学多了太过实用主义，欢迎交流）    去年暑期带着疑问去了Open AI Lab实习（hhh还有幸促成了学院和Open AI Lab、Arm China在教学上的合作），第一次了解到MobileNet、ShuffleNet这些轻量化的模型以及Tengine这种可以充分有效“榨干”系统资源的前向推断引擎，并且看到他们优异的性能表现，顿时觉得深度学习在嵌入式上的应用还是很有希望的。实习回来后，又经过几个月的研究思考，了解裁剪、量化、知识蒸馏等模型压缩技术，以及Tengine、NNAPI、ncnn等优化加速的前向推理引擎和PocketFlow等自动压缩工具、TVM等优化策略搜索工具……发现其实就这两年，大家都纷纷开始重视端侧的部署这一块上。     总而言之，算是大致确定了自己的研究方向——**模型压缩**，    不如先从量化开始——定个小目标，试着在MXNet上实现重训练量化，并且能一步步解决问题，成功把模型部署到Tengine上！      -->]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>树莓派也能玩转深度学习——Tengine推断引擎</title>
      <link href="/2018/10/13/RasPi-Tengine/"/>
      <url>/2018/10/13/RasPi-Tengine/</url>
      
        <content type="html"><![CDATA[<p>一直以来，树莓派以其良好的社区生态，广受嵌入式爱好者、创客欢迎。在一些相关的社区上（比如<a href="http://shumeipai.nxez.com/what-raspi-used-for" target="_blank" rel="noopener">树莓派实验室</a>），我们可以看到非常丰富的应用示例及其教程。但在树莓派上的深度学习应用并不常见，这主要是受到树莓派计算力的限制，比如之前看到过有人把yolov2原原本本生硬地部署到树莓派上，结果每一帧检测耗时高达<strong>6分钟</strong>！！作一帧目标检测花费6分钟这实在是无法忍受的！<br><em>如果是用yolov2-tiny的话会快很多，但耗时依旧接近<strong>40秒</strong>，参考<a href="https://blog.csdn.net/wjbwjbwjbwjb/article/details/77688625" target="_blank" rel="noopener">树莓派3B上测试YOLO效果 | CSDN</a></em>     </p><p>那树莓派只能跟深度学习无缘了么？那可未必！    </p><h3 id="Tengine"><a href="#Tengine" class="headerlink" title="Tengine"></a>Tengine</h3><p><a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">OADI/Tengine | github</a>       </p><blockquote><p>Tengine 是OPEN AI LAB为嵌入式设备开发的一个轻量级、高性能并且模块化的引擎。<br>Tengine在嵌入式设备上支持CPU，GPU，DLA/NPU，DSP异构计算的计算框架，实现异构计算的调度器，基于ARM平台的高效的计算库实现，针对特定硬件平台的性能优化，动态规划计算图的内存使用，提供对于网络远端AI计算能力的访问支持，支持多级别并行，整个系统模块可拆卸，基于事件驱动的计算模型，吸取已有AI计算框架的优点，设计全新的计算图表示。    </p></blockquote><h3 id="编译安装开源版Tengine"><a href="#编译安装开源版Tengine" class="headerlink" title="编译安装开源版Tengine"></a>编译安装开源版Tengine</h3><h4 id="安装相关工具"><a href="#安装相关工具" class="headerlink" title="安装相关工具"></a>安装相关工具</h4><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span> cmake</code></pre><ul><li><strong>git</strong> 是一个版本控制系统，稍后将用来从 <a href="http://github.com" target="_blank" rel="noopener">github</a> 网站上下载Tengine的源码     </li><li><strong>cmake</strong> 是一个编译工具，用来产生make过程中所需要的Makefile文件      </li></ul><h4 id="安装支持库"><a href="#安装支持库" class="headerlink" title="安装支持库"></a>安装支持库</h4><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libprotobuf-dev protobuf-compiler libboost-all-dev libgoogle-glog-dev libopencv-dev libopenblas-dev</code></pre><ul><li><strong>protobuf</strong> 是一种轻便高效的数据存储格式，这是caffe各种配置文件所使用的数据格式</li><li><strong>boost</strong> 是一个c++的扩展程序库，稍后Tengine的编译依赖于该库</li><li><strong>google-glog</strong> 是一个google提供的日志系统的程序库</li><li><strong>opencv</strong> 是一个开源的计算机视觉库</li><li><strong>openblas</strong> 是一个开源的基础线性代数子程序库</li></ul><h4 id="下载-amp-编译"><a href="#下载-amp-编译" class="headerlink" title="下载&amp;编译"></a>下载&amp;编译</h4><ol><li>从github上下载最新的开源版Tengine源码     <pre class=" language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/OAID/Tengine.git</code></pre></li><li>切换工作目录到Tengine      <pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> Tengine</code></pre></li><li>准备好配置文件<br>Tengine目录下提供了配置模板 <code>makefile.config.example</code> 文件        <pre class=" language-bash"><code class="language-bash"><span class="token function">cp</span> makefile.config.example makefile.config</code></pre></li><li>修改配置文件 <code>makefile.config</code><br>由于开源版的Tengine不支持针对armv7的优化，所以需要用openblas替代实现；<br>将 <code>CONFIG_ARCH_ARM64=y</code> 这一行注释掉（行首加井号 <code>#</code>）以关闭ARM64架构的优化实现；<br>解除 <code>CONFIG_ARCH_ARM32=y</code> 和<code>CONFIG_ARCH_BLAS=y</code> 这一行解除注释（删除行首的井号 <code>#</code>）以开启BLAS计算库的实现方式      </li><li>编译并安装      <pre class=" language-bash"><code class="language-bash"><span class="token function">make</span> -j4<span class="token function">make</span> <span class="token function">install</span></code></pre>这里的 <code>-j4</code> 表示开启四个线程进行编译       </li></ol><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><ol><li>下载mobilenet-ssd模型并放置在 <code>Tengine/models</code> 目录下<br>下载链接（提取码为57vb）：<a href="https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g" target="_blank" rel="noopener">https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g</a>     </li><li>将工作目录切换到mobilenet-ssd示例程序的目录下      <pre class=" language-bash"><code class="language-bash"><span class="token function">cd</span> ~/Tengine/examples/mobilenet_ssd</code></pre></li><li>编译示例程序    <pre class=" language-bash"><code class="language-bash">cmake -DTENGINE_DIR<span class="token operator">=</span>/home/pi/Tengine <span class="token keyword">.</span><span class="token function">make</span></code></pre>这里 <code>-DTENGINE_DIR</code>用于为cmake指定环境变量TENGINE_DIR，该变量可以在CMakeLists.txt文件中找到       </li><li>运行示例程序     <pre class=" language-bash"><code class="language-bash">./MSSD</code></pre>可以看到对一张照片进行目标检测，总共耗时1148.32ms<br><img src="/imgs/RasPi-Tengine/mssd-opensource.png" alt="mssd-opensource">     </li></ol><h3 id="树莓派开发者版Tengine"><a href="#树莓派开发者版Tengine" class="headerlink" title="树莓派开发者版Tengine"></a>树莓派开发者版Tengine</h3><p>最近 <a href="http://www.openailab.com/" target="_blank" rel="noopener">Open AI Lab公司</a> 和 <a href="http://www.cbeis.zju.edu.cn/" target="_blank" rel="noopener">浙江大学生物医学工程与仪器科学学院</a> 在嵌入式人工智能领域上开展了教学合作，公司为学院提供了速度更快的针对armv7优化的Tengine版本用于教学用途，接下来让我们看看这个树莓派开发者版的Tengine到底有多快吧！     </p><font color="red"><strong><em><br>预编译的教育版的版本已经比较老，不再适用当前的开源版版本，建议使用开发者版——填写《<a href="https://v.eqxiu.cn/s/YJ2EYiLt?eqrcode=1&amp;share_level=1&amp;from_user=2c36f266-b758-49c8-93e4-f007ff48356f&amp;from_id=201903224d37f2e3&amp;share_time=1553223057579&amp;from=singlemessage" target="_blank" rel="noopener">Tengine AIoT应用征集大赛邀请函</a>》OpenAILab会将开发者版发送至你的邮箱。</em></strong></font>            <ol><li>用树莓派开发者版Tengine的动态链接库覆盖掉原先的开源版<br>动态链接库路径为：<code>Tengine/install/lib/libtengine.so</code><br><em>编译时，make会在build目录下产生libtengine.so动态链接库，而make instll将动态链接库、头文件等拷贝到install目录下</em><br><img src="/imgs/RasPi-Tengine/replace.png" alt="replace">      </li><li>重新运行mobilenet-ssd的示例程序<br>可以看到，单帧耗时从1148.32ms下降为<strong>286.136ms</strong>，速度有了非常明显的提升！<br><img src="/imgs/RasPi-Tengine/mssd-education.png" alt="mssd-education"></li></ol><h3 id="小试牛刀"><a href="#小试牛刀" class="headerlink" title="小试牛刀"></a>小试牛刀</h3><p>用上高性能的树莓派开发者版Tengine，看看mobilenet-ssd在树莓派上能表现如何——     </p><p>为了方便，视频流直接从mp4文件读取，原始视频如下：     </p><iframe height="500" width="700" src="http://player.youku.com/embed/XMzkyNjYzNTIyNA==" frameborder="0" 'allowfullscreen'=""></iframe> <ol><li>从 <a href="https://github.com/hey-yahei/my_blog/tree/master/RasPi-Tengine/mobilenet_ssd" target="_blank" rel="noopener">hey-yahei/my_blog/RasPi-Tengine/mobilenet-ssd | github</a> 上下载源码，并放置在 <code>Tengine/example</code> 目录下    </li><li>检查 <code>CMakeLists.txt</code> 文件中TENGINE_DIR变量是否正确指向Tengine路径</li><li>执行 <code>cmake .</code> 生成Makefile</li><li>执行 <code>make</code> 编译程序</li><li>执行 <code>./MSSD</code> 运行程序         </li></ol><p>实际效果如下：       </p><p><iframe height="500" width="700" src="http://player.youku.com/embed/XMzkyNjYzNTc4OA==" frameborder="0" 'allowfullscreen'=""></iframe><br>由于一部分cpu资源被用于视频的解码工作（对于支持硬解码的平台来说不存在这个问题），可以看到单帧耗时有所下降（400ms-700ms），但对于多数应用场景来说这个帧率是绰绰有余的。      </p><hr><p>本文开头我们说道，<br>直接在树莓派上配置darknet部署的yolo网络，yolov2单帧耗时接近<strong>6分钟</strong>，yolov2-tiny单帧耗时接近<strong>40秒</strong>；<br>而在树莓派上配置Tengine部署的yolov2网络，在blas实现下单帧耗时不到<strong>8秒</strong>（参考<a href="https://songrbb.github.io/2018/08/17/%E5%88%A9%E7%94%A8Tengine%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E8%B7%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">利用Tengine在树莓派上跑深度学习网络 | songrbb</a>），在针对armv7优化实现的教育版下单帧耗时甚至不到<strong>2秒</strong>！     </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RK3399 </tag>
            
            <tag> RasPi </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于MobileNet-SSD的目标检测Demo（二）</title>
      <link href="/2018/09/10/mssd-try3/"/>
      <url>/2018/09/10/mssd-try3/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=31352588&auto=0&height=32"></iframe>      <p>上一篇文章《<a href="/2018/08/24/mssd-try2/">基于MobileNet-SSD的目标检测Demo（一）</a>》介绍了如何在VOC数据集的基础上削减分类训练出自己的分类器，并且尝试着进一步把SSD改为SSDLite。但作为一个Demo，在RK3399上MobileNet-SSD每秒钟只能检测6-7帧，如果每次检测后再把视频内容展现出来，那么展示的视频也只有6-7帧，这样的展示效果似乎不太好。在本篇文章中，我们将尝试把视频的获取与展示和检测任务分离开来，分别放在两个不同的线程上工作，同时将不同的线程绑定到不同的cpu核上，使得两者的工作不会冲突。         </p><hr><h3 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h3><p>进程和线程是操作系统中的两个重要概念。</p><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>考虑在51单片机或是STM32上开发程序，通常这些程序都是串行结构。打个比方，      </p><ol><li>写个数码管的动态驱动，让四个个数码管持续显示数值<code>1217</code>       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token number">1217</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li>用一个超声波模块进行测距，并且用数码管显示结果       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token function">ultrasonicGetDatum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre></li><li>超声波数量少还没关系，数码管还是能正常驱动，如果多来几个呢？（简化一下，数码管只显示求和的结果）       <pre class=" language-c"><code class="language-c"><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">for</span><span class="token punctuation">(</span>i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span><span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span><span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">ultrasonicgetDatum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>CPU大部分时间都用去等超声波信号了呀，数码管根本就不能驱动起来，那换种方式——      <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>sum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        sum<span class="token operator">+</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        segment<span class="token operator">=</span>sum<span class="token punctuation">;</span>        i<span class="token operator">=</span>sum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>这样一来，如果超声波数据无效就继续驱动数码管，不会让CPU空等。    </li><li>那如果不是简单的求和运算呢？        <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ultras<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        ultras<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        segment<span class="token operator">=</span><span class="token function">process</span><span class="token punctuation">(</span>ultras<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 变成了其他复杂的运算</span>        i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>万一<code>process</code>函数运算过程很复杂，占用了很多CPU的时间，导致数码管不能及时刷新，那数码管依旧驱动不起来。当然，你可以去推算<code>process</code>运算的复杂程度，人为地去拆解运算，变成<code>process[0]</code>、<code>process[1]</code>、……、<code>process[n]</code>最后再由<code>combine</code>把中间结果整合起来（注意这里要保证每个操作都足够小，不会占用太多运算时间）。     <pre class=" language-c"><code class="language-c">i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i_process<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>segment<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>ultras<span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">]</span><span class="token punctuation">;</span>tmp<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">ultrasonicDataValid</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        ultras<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token function">ultrasonicGetData</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">ultrasonicTrig</span><span class="token punctuation">(</span><span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>i<span class="token operator">>=</span><span class="token number">99</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        tmp<span class="token punctuation">[</span>i_process<span class="token punctuation">]</span><span class="token operator">=</span>process<span class="token punctuation">[</span>i_process<span class="token punctuation">]</span><span class="token punctuation">(</span>ultras<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>i_process<span class="token operator">==</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            segment<span class="token operator">=</span><span class="token function">combine</span><span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">;</span>            i_process<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span><span class="token keyword">else</span><span class="token punctuation">{</span>            i_process<span class="token operator">++</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">segmentShow</span><span class="token punctuation">(</span>segment<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre>看起来确实可行，但是手工拆解运算，这也太恶心了吧，而且这一点都不优雅，万一我程序开发着开发着，这个运算过程发生改变了怎么办？重新拆解运算？那不得炸毛？！     </li><li>那换个思路，我们让两个任务分时进行吧，每个任务轮流运算10ms，超时就带上你的中间结果滚蛋            <pre class=" language-c"><code class="language-c"><span class="token comment" spellcheck="true">// 设置定时器，每10ms中产生一次中断</span><span class="token function">timer_handler</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token comment" spellcheck="true">// 中断处理</span>    <span class="token function">save_metadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 触发中断后保存数据</span>    <span class="token function">change_task</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 切换到另一个任务</span>    <span class="token function">load_metadata</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 取出新换入任务的中间数据</span>    <span class="token function">task_run</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token comment" spellcheck="true">// 继续任务</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// ……省略任务定义</span></code></pre>这样虽然增加了额外的开销（任务的调度），但形成了一个通用化的任务调度功能，无论具体任务怎么改变都能够适用，减少CPU闲置的机会，可以更好的压榨CPU     </li></ol><p>实际应用当中，我们经常都能碰见这种多任务的情况，人为地分配任务给处理器需要大量的推算和分解，费时费力还不易调整。在这里，我们形成了一个简单的通用的任务调度功能，其实这也是现代操作系统的基本功能之一，对操作系统而言，这些任务就是一个个的“<strong>进程</strong>”，中间数据被称为“<strong>进程上下文</strong>”，而操作系统有一个专门的模块负责“<strong>进程调度</strong>”的工作，这里举例的固定时间片是一种最简单的调度方式。         </p><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p><center><img src="/imgs/mssd-try/try3/process.png" alt="process"></center><br>这是一张进程状态转换图，         </p><ul><li>新建一个进程时处于“<strong>新建态</strong>”，至于加载到“就绪态”还是“就绪/挂起态”就取决于操作系统翻不翻你牌子；      </li><li>“<strong>就绪态</strong>”指的是进程已经就绪的状态，比如进程执行所需要的资源（比如键盘、鼠标、显卡等外设）可用，并且已经被操作系统翻牌，在等待执行的队伍里排着队；   </li><li>“<strong>阻塞态</strong>”指的是进程未就绪，比如执行的所需资源还未到位，进程本身处于等待的状态；      </li><li>“就绪态”和“阻塞态”还分别有对应的“<strong>就绪/挂起态</strong>”和“<strong>阻塞/挂起态</strong>”，这是进程本身处于就绪或未就绪的状态，但操作系统还没有翻他们牌子；     </li><li>“<strong>运行态</strong>”和“<strong>退出态</strong>”不难理解，就是进程运行中的状态，以及进程完全运行结束而将退出的状态         </li></ul><p>进程上下文保存在一个特殊的称为<strong>进程控制块（PCB）</strong>的结构里，其中包含        </p><ol><li>进程标识信息（各种标识符）</li><li>进程状态信息（寄存器、栈指针等）</li><li>进程控制信息（调度和状态的相关信息，比如进程状态、优先级、事件等）     </li></ol><p>常见的用户进程创建有两种，       </p><ol><li>用户运行一个程序，这个程序会放到一个进程上<br>比如直接运行一个编译好的c程序，或是一个python程序；       </li><li>由现有进程派生<br>比如在c程序中调用fork函数来派生出一个新的进程——        <pre class=" language-c"><code class="language-c"> <span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span> <span class="token keyword">int</span> <span class="token function">main</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>     <span class="token keyword">int</span> pid<span class="token punctuation">,</span> ppid<span class="token punctuation">;</span>     pid <span class="token operator">=</span> <span class="token function">fork</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// fork函数将派生出一个相同的进程，返回新进程的id（对于原始进程返回0）</span>     <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d first output from both processes\n"</span><span class="token punctuation">,</span> pid<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token function">sleep</span> <span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"This is the child's pid,  output by the parent process\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"is printed inside the child process if the fork succeeds \n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         pid <span class="token operator">=</span> <span class="token function">getpid</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// getpid函数可以获取当前进程的id</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"%d %s"</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token string">"is the child pid printed by the child, obtained by getpid()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token punctuation">}</span><span class="token keyword">else</span>         <span class="token function">printf</span> <span class="token punctuation">(</span><span class="token string">"fork failed\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">return</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span></code></pre></li></ol><p>进程间的通信通常由信号量、中断和共享空间实现，简单地说，      </p><ul><li><strong>信号量</strong>，事实上就是一个整型变量。操作系统负责维护一个信号量池，进程可以在该池注册带有名称的信号量，多个进程间可以对同一个信号量进行加法或减法操作（通常称为PV操作，该操作由操作系统管理，不会读写冲突），但减法的结果不能小于0，否则进程就会阻塞挂起，等待有足够的信号量可以减去。信号量有点像资源的指示标志，进程占用资源的时候作减法，释放资源的时候做加法。          </li><li><strong>中断</strong>，是Linux提供的一套机制，事实上<code>kill</code>指令就是对目标进程发送一个特定的中断信号，进程接收到中断之后会跳转到指定的中断处理函数进行处理（当然进程也可以设置忽略某一些信号）。在Linux中可以通过指令<code>kill -l</code>查看可用信号，如下：       <pre class=" language-bash"><code class="language-bash">  <span class="token comment" spellcheck="true"># kill -l</span>   1<span class="token punctuation">)</span> SIGHUP       2<span class="token punctuation">)</span> SIGINT       3<span class="token punctuation">)</span> SIGQUIT      4<span class="token punctuation">)</span> SIGILL       5<span class="token punctuation">)</span> SIGTRAP   6<span class="token punctuation">)</span> SIGABRT      7<span class="token punctuation">)</span> SIGBUS       8<span class="token punctuation">)</span> SIGFPE       9<span class="token punctuation">)</span> SIGKILL     10<span class="token punctuation">)</span> SIGUSR1  11<span class="token punctuation">)</span> SIGSEGV     12<span class="token punctuation">)</span> SIGUSR2     13<span class="token punctuation">)</span> SIGPIPE     14<span class="token punctuation">)</span> SIGALRM     15<span class="token punctuation">)</span> SIGTERM  16<span class="token punctuation">)</span> SIGSTKFLT   17<span class="token punctuation">)</span> SIGCHLD     18<span class="token punctuation">)</span> SIGCONT     19<span class="token punctuation">)</span> SIGSTOP     20<span class="token punctuation">)</span> SIGTSTP  21<span class="token punctuation">)</span> SIGTTIN     22<span class="token punctuation">)</span> SIGTTOU     23<span class="token punctuation">)</span> SIGURG      24<span class="token punctuation">)</span> SIGXCPU     25<span class="token punctuation">)</span> SIGXFSZ  26<span class="token punctuation">)</span> SIGVTALRM   27<span class="token punctuation">)</span> SIGPROF     28<span class="token punctuation">)</span> SIGWINCH    29<span class="token punctuation">)</span> SIGIO       30<span class="token punctuation">)</span> SIGPWR  31<span class="token punctuation">)</span> SIGSYS      34<span class="token punctuation">)</span> SIGRTMIN    35<span class="token punctuation">)</span> SIGRTMIN+1  36<span class="token punctuation">)</span> SIGRTMIN+2  37<span class="token punctuation">)</span> SIGRTMIN+3  38<span class="token punctuation">)</span> SIGRTMIN+4  39<span class="token punctuation">)</span> SIGRTMIN+5  40<span class="token punctuation">)</span> SIGRTMIN+6  41<span class="token punctuation">)</span> SIGRTMIN+7  42<span class="token punctuation">)</span> SIGRTMIN+8  43<span class="token punctuation">)</span> SIGRTMIN+9  44<span class="token punctuation">)</span> SIGRTMIN+10 45<span class="token punctuation">)</span> SIGRTMIN+11 46<span class="token punctuation">)</span> SIGRTMIN+12 47<span class="token punctuation">)</span> SIGRTMIN+13  48<span class="token punctuation">)</span> SIGRTMIN+14 49<span class="token punctuation">)</span> SIGRTMIN+15 50<span class="token punctuation">)</span> SIGRTMAX-14 51<span class="token punctuation">)</span> SIGRTMAX-13 52<span class="token punctuation">)</span> SIGRTMAX-12  53<span class="token punctuation">)</span> SIGRTMAX-11 54<span class="token punctuation">)</span> SIGRTMAX-10 55<span class="token punctuation">)</span> SIGRTMAX-9  56<span class="token punctuation">)</span> SIGRTMAX-8  57<span class="token punctuation">)</span> SIGRTMAX-7  58<span class="token punctuation">)</span> SIGRTMAX-6  59<span class="token punctuation">)</span> SIGRTMAX-5  60<span class="token punctuation">)</span> SIGRTMAX-4  61<span class="token punctuation">)</span> SIGRTMAX-3  62<span class="token punctuation">)</span> SIGRTMAX-2  63<span class="token punctuation">)</span> SIGRTMAX-1  64<span class="token punctuation">)</span> SIGRTMAX</code></pre>  不同的信号有不同的含义，其中10号的<code>SIGUSR1</code>和12号的<code>SIGUSR2</code>是两个可以用户自定义的中断信号。       </li><li><strong>共享空间</strong>，类似进程内部的全局变量，由操作系统负责维护，跟信号量一样各个共享空间拥有自己的标识符，不同进程都可以访问相同的共享空间，但是要注意防止访问冲突（比如一个进程对空间写操作，同时又有另一个进程对空间进程读或写操作），这通常是通过信号量来实现的。并且在共享的过程要注意避免进程死锁（各个进程各自占有一部分但并不充足资源，导致进程同时陷入无休止的阻塞状态），有一些专门用于检测死锁和防止死锁的算法，此处不展开讨论。       </li></ul><p>具体的实现不展开讲，因为我们接下来要用到的是线程而不是进程。<br>如果你对进程间通信感兴趣，也可以参考我本科期间的一个课程作业，一个 <a href="/others/chat.zip">简单的本地聊天程序</a> （具体使用方法参见<code>chat.c</code>里的注释）。      </p><h4 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h4><p>在单一程序中，进程的粒度似乎还是太大了，如果把一个程序的大任务细分多个小任务，明明大家都是同一个目标，却要使用独立的上下文，上下文频繁地保存和加载，这样似乎不太方便。于是就产生了粒度更小的<strong>线程</strong>，一个进程可以拥有多个线程，这些线程共享一个上下文环境。<br>相比于进程，       </p><ol><li>创建一个线程的速度要快得多；       </li><li>终止一个线程的速度也要快得多；     </li><li>线程间的切换也比进程间切换快，因为不需要交换上下文；     </li><li>线程的通信效率更高，因为线程可以直接通过共享全局变量来实现通信。            </li></ol><p>线程的通信方式比进程简单，这跟进程的“信号量+共享空间”的组合有些类似，      </p><ul><li><strong>线程锁</strong>，类似进程通信中的信号量，但这个变量只有两种状态——“上锁”和“解锁”，用于保护共享的内存空间不会出现读写冲突；     </li><li><strong>全局变量</strong>，类似进程通信中的共享空间。         </li></ul><p>简单的实现方式——      </p><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span></span>pthread_mutex_t mutex<span class="token punctuation">;</span>          <span class="token comment" spellcheck="true">// 线程锁（用于保护shared_variable变量）</span><span class="token keyword">int</span> shared_variable <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">// 线程间共享的全局变量</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">thread_write</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 写入前上锁（如果mutex锁住，则阻塞等待）</span>    shared_variable<span class="token operator">++</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 写入后解锁</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">thread_read</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 读取前上锁（如果mutex锁住，则阻塞等待）</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d\n"</span><span class="token punctuation">,</span> shared_variable<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 读取后解锁</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    pthread_t id1<span class="token punctuation">,</span> id2<span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 初始化线程锁</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> thread_write<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 创建写线程</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> thread_read<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 创建读线程</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 阻塞主线程，等待线程id1执行完毕</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 阻塞主线程，等待线程id2执行完毕</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 销毁线程锁</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p><code>pthread_create</code>、<code>pthread_join</code>、<code>pthread_mutex_init</code>还可以传递其他参数，其他复杂的用法可以自行查阅资料。     </p><h4 id="处理器调度"><a href="#处理器调度" class="headerlink" title="处理器调度"></a>处理器调度</h4><p>在背景中我们提到了一种规定时间的调度方法，接下来我们也简单介绍其他一些处理器的调度策略。<br>假设有A-E五个进程集合，他们的启动时间和CPU占用总时间如下表所示——       </p><table><thead><tr><th style="text-align:center">进程</th><th style="text-align:center">启动时间</th><th style="text-align:center">CPU占用总时间</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">t=0</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">t=2</td><td style="text-align:center">6</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">t=4</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">t=6</td><td style="text-align:center">5</td></tr><tr><td style="text-align:center">E</td><td style="text-align:center">t=8</td><td style="text-align:center">2</td></tr></tbody></table><p><strong>先来先服务策略（First Come First Served, FCFS）</strong><br>非抢占策略，进程依次进入等待队列，先进入队列的先使用CPU，直到进程结束再从队列取出下一个进程。<br><img src="/imgs/mssd-try/try3/process-FCFS.png" alt="process">       </p><p><strong>轮转策略（Round Robin, RR）</strong><br>一种基于时钟的抢占策略，又称为时间片策略，设定一个时间片q，每次从进程等待队列中取出一个进程执行一个时间片，如果没执行完就放回等待队列的队尾，然后从队首取出下一个进程出来执行一个时间片。假设q=1，则<br><img src="/imgs/mssd-try/try3/process-RR.png" alt="process"><br><em>注意：这里假设如果同时发生“中断”和“新服务入队”，则先将新服务入队，再交换进程。</em>             </p><p><strong>最短进程优先（Shortest Process Next, SPN）</strong><br>一种基于预计处理时间的非抢占策略，每次从等待队列中取出预计处理时间最短的进程出来执行，直到进程结束再从进程取出下一个预计处理时间最短的进程。<br><img src="/imgs/mssd-try/try3/process-SPN.png" alt="process">       </p><p><strong>最短剩余时间（Shortest Remaining Time, SRT）</strong><br>一种基于预计剩余处理时间的抢占策略，在SPN的基础上，当有新进程加入队列时总会估算各个进程的剩余时间，然后选择预计剩余处理时间最短的进程出来执行。<br><img src="/imgs/mssd-try/try3/process-SRT.png" alt="process">       </p><p><strong>最高响应比优先（Highest Response Ratio Next, HRRN）</strong><br>非抢占策略，定义一个响应比参数，$响应比=\frac{等待时间+预计处理时间}{预计处理时间}$，每次从等待队列中挑选响应比最高的进程出来执行，直到进程执行结束再从队列中取出下一个响应比最高的进程。<br><img src="/imgs/mssd-try/try3/process-HRRN.png" alt="process">       </p><p><strong>反馈调度</strong><br>可以注意到FCFS、RR策略相对简单，不太能很好的利用CPU，而SPN、SRT、HRRN策略虽然不错，但依赖于处理时间和剩余处理时间的估计，而现实应用中这种时间估计往往是难以实现的。因此产生了反馈调度策略，对进程进行分级（而不是简单的一个等待队列），进程运行时间长调度的优先级越低，每被抢占一次就下降一级。<br><img src="/imgs/mssd-try/try3/process-feedback.png" alt="process"><br>反馈调度往往会和前述的简单调度（比如FCFS或RR）结合使用，同时长进程周转时间会出现惊人的增加的现象（长进程多次被抢占，优先级不断下降，而长期得不到调度），所以会有一些相应的补偿措施（比如设置允许被抢占的次数，每当超过这个次数才会对进程进行降级操作）。           </p><p><strong>实时调度</strong><br>嵌入式开发中还常见一些实时调度策略，与前述策略不同的时间，这些任务有最后期限的限制，这通常分为两种——一种是“硬实时”，要求必须满足最后期限的限制，否则将给系统带来不可接受的破坏或致命的错误（任务超时完成是无意义的）；另一种是“软实时”，希望能够满足最后期限的限制，但并非强制（即使超时完成任务也有意义）。    </p><h3 id="拆解目标检测demo"><a href="#拆解目标检测demo" class="headerlink" title="拆解目标检测demo"></a>拆解目标检测demo</h3><p>前边介绍了进程、线程以及处理器调度的概念和简单的使用，接下来我们考虑如何将我们的目标检测demo拆解成两个线程以提高展示的流畅性。       </p><p>我们的目标是将demo拆解成 <strong>目标检测</strong> 和 <strong>视频流获取和展示</strong> 两个线程，其中需要共享的数据包括 <strong>图像数据</strong> 和 <strong>检测结果</strong> 两部分（为了响应退出按钮，后续的示例程序还会额外增加一个作为退出标志的共享数据），每部分数据需要配备一把线程锁进行读写保护。       </p><p>最终程序如下——<br><strong><em>注意：这里不仅划分了线程，还针对不同线程的任务分配了cpu，比如RK3399上CPU0-3是四个小核，我们用来做视频流的获取、检测结果的标注和视频流的展示；CPU4-5是大核，我们用来做核心的目标检测任务。两个不同的线程使用不同的CPU核，互不冲突，合理地分配CPU对应用程序也会有一定的提升。</em></strong>     </p><pre class=" language-cpp"><code class="language-cpp"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/imgproc/imgproc.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/highgui/highgui.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"tengine_c_api.h"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"common.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;pthread.h></span>    </span><span class="token comment" spellcheck="true">// 包含线程控制相关的库</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_PROTO "models/MobileNetSSD_deploy.prototxt"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_MODEL "models/MobileNetSSD_deploy.caffemodel"</span><span class="token keyword">struct</span> Box<span class="token punctuation">{</span>    <span class="token keyword">float</span> x0<span class="token punctuation">;</span>    <span class="token keyword">float</span> y0<span class="token punctuation">;</span>    <span class="token keyword">float</span> x1<span class="token punctuation">;</span>    <span class="token keyword">float</span> y1<span class="token punctuation">;</span>    <span class="token keyword">int</span> class_idx<span class="token punctuation">;</span>    <span class="token keyword">float</span> score<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> input_data<span class="token punctuation">,</span> <span class="token keyword">int</span> img_h<span class="token punctuation">,</span>  <span class="token keyword">int</span> img_w<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        std<span class="token operator">::</span>cerr <span class="token operator">&lt;&lt;</span> <span class="token string">"Failed to read image from camera.\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span><span class="token function">resize</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    img<span class="token punctuation">.</span><span class="token function">convertTo</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> CV_32FC3<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>img_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>img<span class="token punctuation">.</span>data<span class="token punctuation">;</span>    <span class="token keyword">int</span> hw <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w<span class="token punctuation">;</span>    <span class="token keyword">float</span> mean<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> img_h<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> w <span class="token operator">&lt;</span> img_w<span class="token punctuation">;</span> w<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                input_data<span class="token punctuation">[</span>c <span class="token operator">*</span> hw <span class="token operator">+</span> h <span class="token operator">*</span> img_w <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.007843</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">*</span>img_data <span class="token operator">-</span> mean<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                img_data<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span> threshold<span class="token punctuation">,</span><span class="token keyword">float</span><span class="token operator">*</span> outdata<span class="token punctuation">,</span><span class="token keyword">int</span> num<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> class_names<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"background"</span><span class="token punctuation">,</span>                            <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span>                            <span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span>                            <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span>                            <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span>                            <span class="token string">"sheep"</span><span class="token punctuation">,</span> <span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_h <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>height<span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_w <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>width<span class="token punctuation">;</span>    std<span class="token operator">::</span>vector<span class="token operator">&lt;</span>Box<span class="token operator">></span> boxes<span class="token punctuation">;</span>    <span class="token keyword">int</span> line_width<span class="token operator">=</span>raw_w<span class="token operator">*</span><span class="token number">0.002</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// printf("detect ruesult num: %d \n",num);</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>=</span>threshold<span class="token punctuation">)</span><span class="token punctuation">{</span>            Box box<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>class_idx<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>score<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            boxes<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>box<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// printf("%s\t:%.0f%%\n", class_names[box.class_idx], box.score * 100);</span>            <span class="token comment" spellcheck="true">// printf("BOX:( %g , %g ),( %g , %g )\n",box.x0,box.y0,box.x1,box.y1);</span>        <span class="token punctuation">}</span>        outdata<span class="token operator">+</span><span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span>boxes<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        Box box<span class="token operator">=</span>boxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x1<span class="token operator">-</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>y1<span class="token operator">-</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>line_width<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>ostringstream score_str<span class="token punctuation">;</span>        score_str<span class="token operator">&lt;&lt;</span>box<span class="token punctuation">.</span>score<span class="token punctuation">;</span>        std<span class="token operator">::</span>string label <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">string</span><span class="token punctuation">(</span>class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> score_str<span class="token punctuation">.</span><span class="token function">str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> baseLine <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span>Size label_size <span class="token operator">=</span> cv<span class="token operator">::</span><span class="token function">getTextSize</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>baseLine<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token operator">-</span> label_size<span class="token punctuation">.</span>height<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>label_size<span class="token punctuation">.</span>width<span class="token punctuation">,</span> label_size<span class="token punctuation">.</span>height <span class="token operator">+</span> baseLine<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CV_FILLED<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">putText</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> label<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">,</span>                    cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">float</span> outdata<span class="token punctuation">[</span><span class="token number">15</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 线程间共享变量——检测结果</span>cv<span class="token operator">::</span>Mat frame<span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 线程间共享变量——图像数据</span><span class="token keyword">int</span> detect_num<span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 线程间共享变量——检测结果</span><span class="token keyword">bool</span> quit_flag <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// 程序间共享变量——退出标志</span>graph_t graph<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 与共享变量对应的线程锁</span>pthread_mutex_t m_frame<span class="token punctuation">,</span> m_outdata<span class="token punctuation">,</span> m_quit<span class="token punctuation">;</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">th_vedio</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 将线程绑定到cpu0-3上</span>    cpu_set_t mask<span class="token punctuation">;</span>    <span class="token function">CPU_ZERO</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">sched_setaffinity</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cpu_set_t<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error: setaffinity()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span>VideoCapture <span class="token function">capture</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_WIDTH<span class="token punctuation">,</span> <span class="token number">960</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_HEIGHT<span class="token punctuation">,</span> <span class="token number">540</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span><span class="token function">namedWindow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> CV_WINDOW_NORMAL<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">cvResizeWindow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> <span class="token number">1280</span><span class="token punctuation">,</span> <span class="token number">720</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        capture <span class="token operator">>></span> frame<span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">float</span> show_threshold<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment" spellcheck="true">// 上锁</span>        <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> show_threshold<span class="token punctuation">,</span> outdata<span class="token punctuation">,</span> detect_num<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 解锁</span>        cv<span class="token operator">::</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 解锁</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> cv<span class="token operator">::</span><span class="token function">waitKey</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'q'</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 上锁</span>            quit_flag <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>            <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 解锁</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token function">usleep</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 注意必须sleep（不然太过频繁地取帧会影响检测线程的调度）</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token function">th_detect</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// 将该线程绑定到cpu4-5上</span>    cpu_set_t mask<span class="token punctuation">;</span>    <span class="token function">CPU_ZERO</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">CPU_SET</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">sched_setaffinity</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cpu_set_t<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mask<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Error: setaffinity()\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// input</span>    <span class="token keyword">int</span> img_h <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_w <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_size <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>input_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> img_size<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> node_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> tensor_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    tensor_t input_tensor <span class="token operator">=</span> <span class="token function">get_graph_input_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> node_idx<span class="token punctuation">,</span> tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_tensor_valid</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Get input node failed : node_idx: %d, tensor_idx: %d\n"</span><span class="token punctuation">,</span>node_idx<span class="token punctuation">,</span>tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> dims<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">set_tensor_shape</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dims<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">prerun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> repeat_count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>repeat <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">"REPEAT_COUNT"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>repeat<span class="token punctuation">)</span>        repeat_count <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">strtoul</span><span class="token punctuation">(</span>repeat<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> out_dim<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    tensor_t out_tensor<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 上锁</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>quit_flag<span class="token punctuation">)</span>  <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">// 解锁</span>        <span class="token keyword">struct</span> timeval t0<span class="token punctuation">,</span> t1<span class="token punctuation">;</span>        <span class="token keyword">float</span> total_time <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>f<span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> repeat_count<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token comment" spellcheck="true">// 上锁</span>            <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span>  img_w<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 解锁</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t0<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">set_tensor_buffer</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">run_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">float</span> mytime <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t1<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span>t0<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t0<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span><span class="token punctuation">;</span>            total_time <span class="token operator">+</span><span class="token operator">=</span> mytime<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"--------------------------------------\n"</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"repeat "</span> <span class="token operator">&lt;&lt;</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" times, avg time per run is "</span> <span class="token operator">&lt;&lt;</span> total_time <span class="token operator">/</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" ms\n"</span><span class="token punctuation">;</span>        out_tensor <span class="token operator">=</span> <span class="token function">get_graph_output_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">get_tensor_shape</span><span class="token punctuation">(</span> out_tensor<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment" spellcheck="true">// 上锁</span>        detect_num <span class="token operator">=</span> out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">15</span> <span class="token operator">?</span> out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">:</span> <span class="token number">15</span><span class="token punctuation">;</span>        <span class="token function">memcpy</span><span class="token punctuation">(</span>outdata<span class="token punctuation">,</span> <span class="token function">get_tensor_buffer</span><span class="token punctuation">(</span>out_tensor<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>detect_num<span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">pthread_mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment" spellcheck="true">// 解锁</span>    <span class="token punctuation">}</span>    <span class="token function">free</span><span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> std<span class="token operator">::</span>string root_path <span class="token operator">=</span> <span class="token function">get_root_path</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>string proto_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string model_file<span class="token punctuation">;</span>    <span class="token keyword">int</span> res<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token punctuation">(</span> res<span class="token operator">=</span><span class="token function">getopt</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">,</span><span class="token string">"p:m:h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">switch</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token string">'p'</span><span class="token operator">:</span>                proto_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'m'</span><span class="token operator">:</span>                model_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'h'</span><span class="token operator">:</span>                std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"[Usage]: "</span> <span class="token operator">&lt;&lt;</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" [-h]\n"</span>                          <span class="token operator">&lt;&lt;</span> <span class="token string">"   [-p proto_file] [-m model_file]\n"</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token keyword">default</span><span class="token operator">:</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>model_name <span class="token operator">=</span> <span class="token string">"mssd_300"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>proto_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        proto_file <span class="token operator">=</span> DEF_PROTO<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"proto file not specified,using "</span><span class="token operator">&lt;&lt;</span> proto_file <span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>model_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        model_file <span class="token operator">=</span> DEF_MODEL<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"model file not specified,using "</span><span class="token operator">&lt;&lt;</span> model_file <span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// init tengine</span>    <span class="token function">init_tengine_library</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">request_tengine_version</span><span class="token punctuation">(</span><span class="token string">"0.1"</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">load_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> <span class="token string">"caffe"</span><span class="token punctuation">,</span> proto_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"load model done!\n"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// create graph</span>    graph <span class="token operator">=</span> <span class="token function">create_runtime_graph</span><span class="token punctuation">(</span><span class="token string">"graph"</span><span class="token punctuation">,</span> model_name<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_graph_valid</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"create graph0 failed\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// 初始化线程锁</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 创建线程</span>    pthread_t id1<span class="token punctuation">,</span> id2<span class="token punctuation">;</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> th_vedio<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_create</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> th_detect<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 等待线程</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_join</span><span class="token punctuation">(</span>id2<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// 销毁线程锁</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_frame<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_outdata<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">pthread_mutex_destroy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>m_quit<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">postrun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">destroy_runtime_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">remove_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><hr><p>在本文中，我们将尝试把视频的获取与展示和检测任务分离开来，分别放在两个不同的线程上工作，同时将不同的线程绑定到不同的cpu核上，使得两者的工作不会冲突。但在实际使用中你会发现，模型对于小目标的检测能力还是有所欠缺，下一篇文章我们将探究如何改善检测模型的小目标检测能力。</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>基于MobileNet-SSD的目标检测Demo（一）</title>
      <link href="/2018/08/24/mssd-try2/"/>
      <url>/2018/08/24/mssd-try2/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=536622304&auto=0&height=32"></iframe>    <p>上一篇文章《<a href="/2018/08/21/mssd-try1/">训练MobileNet-SSD | Hey~YaHei!</a>》介绍了如何训练自己的MobileNet-SSD模型并部署在Tengine平台上。<br>本文将继续尝试根据实际情况删减多余类别进行训练，并用Depthwise Convolution进一步替换Standard Convolution。         </p><hr><h3 id="削减类别"><a href="#削减类别" class="headerlink" title="削减类别"></a>削减类别</h3><p>VOC数据集包含二十个类别的物体，分别是——aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, foa, train, tvmonitor，有时候我们想用VOC数据集训练，但并不需要这么多类别，而caffe-ssd提供的数据处理工具<code>create_list.sh</code>和<code>create_data.sh</code>默认是处理所有的20个分类的。如果我们不想重写这些数据处理工具，可以从根源入手，也就是直接修改数据集里的标注信息，把多余分类的信息删去。          </p><h4 id="处理数据集"><a href="#处理数据集" class="headerlink" title="处理数据集"></a>处理数据集</h4><p>首先观察一下VOC数据集的结构——<br><img src="/imgs/mssd-try/try1/dataset_tree.png" width="350">       </p><ul><li>Annotations：存放图片的标注信息，每张图片对应一个xml文件      </li><li>ImageSets：存放图片的分类列表，包含三个子目录：       <ul><li>Layout：存放与人体部位有关的图片列表文件</li><li>Main：存放物体分类中每一个分类的图片列表文件</li><li>Segmentation：存放与图像分别有关的图片列表文件</li></ul></li><li>JPEGImages：存放所有的图片</li><li><del><em>NewAnnotations：忽略吧……是我自己生成的目录</em></del></li><li>SegmentationClass：存放类别分割任务的蒙版文件</li><li>SegmentationObject：存放实体分割任务的蒙版文件</li></ul><p>JPEGImages目录下每张图片都包含一到多个物体，这些物体的位置、类别信息都记录再Annotations目录下的同名xml文件中，文件内容类似：     </p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>folder</span><span class="token punctuation">></span></span>VOC2007<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>folder</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filename</span><span class="token punctuation">></span></span>008973.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filename</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>database</span><span class="token punctuation">></span></span>The VOC2007 Database<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>database</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>PASCAL VOC2007<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>image</span><span class="token punctuation">></span></span>flickr<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>image</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flickrid</span><span class="token punctuation">></span></span>335707085<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flickrid</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>owner</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>flickrid</span><span class="token punctuation">></span></span>kjmurray<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>flickrid</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>Katherine Murray<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>owner</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>size</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>width</span><span class="token punctuation">></span></span>500<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>width</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>height</span><span class="token punctuation">></span></span>333<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>height</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>depth</span><span class="token punctuation">></span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>depth</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>size</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>segmented</span><span class="token punctuation">></span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>segmented</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>cow<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">></span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">></span></span>271<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">></span></span>43<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">></span></span>444<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">></span></span>                        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">></span></span>279<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span></code></pre><p>而caffe-ssd的数据处理工具正是根据这些xml文件提供的标记进行处理的，所以说，<strong>我们可以通过遍历xml文件，判断object的类别，如果是我们不需要的，则把对应的object标签删去来达到削减类别的目的</strong>，除此之外还要处理对应的图片路径列表和图片大小列表，删除多余的项。     </p><p>根据这一思路，可以写一个简单的py脚本来实现：       </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true">#-*- coidng: utf-8 -*-</span><span class="token keyword">import</span> xml<span class="token punctuation">.</span>etree<span class="token punctuation">.</span>ElementTree <span class="token keyword">as</span> ET<span class="token keyword">import</span> osVOC_ROOT <span class="token operator">=</span> <span class="token string">"/home/zhengkai/data/VOCdevkit/"</span>CLASS2KEEP <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'background'</span><span class="token punctuation">,</span>            <span class="token string">'aeroplane'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span>            <span class="token string">'bottle'</span><span class="token punctuation">,</span> <span class="token string">'bus'</span><span class="token punctuation">,</span> <span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'chair'</span><span class="token punctuation">,</span>            <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'bicycle'</span><span class="token punctuation">,</span> <span class="token string">'motorbike'</span><span class="token punctuation">,</span>            <span class="token string">'boat'</span><span class="token punctuation">,</span> <span class="token string">'sofa'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">process_xml</span><span class="token punctuation">(</span>src_path<span class="token punctuation">,</span> dst_path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    解析并处理xml文件——    解析源文件，删除无关object标签，    如果存在有效object，则写入到目标文件，并返回True；    否则，直接返回False。    """</span>    tree <span class="token operator">=</span> ET<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>src_path<span class="token punctuation">)</span>    root <span class="token operator">=</span> tree<span class="token punctuation">.</span>getroot<span class="token punctuation">(</span><span class="token punctuation">)</span>    no_objs <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">for</span> obj <span class="token keyword">in</span> root<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">"object"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        cls <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">"name"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        <span class="token keyword">if</span> cls <span class="token operator">not</span> <span class="token keyword">in</span> CLASS2KEEP<span class="token punctuation">:</span>            root<span class="token punctuation">.</span>remove<span class="token punctuation">(</span>obj<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            no_objs <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token keyword">if</span> <span class="token operator">not</span> no_objs<span class="token punctuation">:</span>        tree<span class="token punctuation">.</span>write<span class="token punctuation">(</span>dst_path<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 记录有效的xml文件名</span>    valid_lst <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 处理xml文件，新的xml文件写入到NewAnnotations目录下</span>    <span class="token keyword">for</span> dataset <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"VOC2007/"</span><span class="token punctuation">,</span> <span class="token string">"VOC2012/"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        raw_anno_dir <span class="token operator">=</span> VOC_ROOT <span class="token operator">+</span> dataset <span class="token operator">+</span> <span class="token string">"Annotations/"</span>        dst_anno_dir <span class="token operator">=</span> VOC_ROOT <span class="token operator">+</span> dataset <span class="token operator">+</span> <span class="token string">"NewAnnotations/"</span>        <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>dst_anno_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Create a new dir: "</span> <span class="token operator">+</span> dst_anno_dir<span class="token punctuation">)</span>            os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>dst_anno_dir<span class="token punctuation">)</span>        <span class="token keyword">for</span> xml_filename <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>raw_anno_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> process_xml<span class="token punctuation">(</span>raw_anno_dir <span class="token operator">+</span> xml_filename<span class="token punctuation">,</span> dst_anno_dir <span class="token operator">+</span> xml_filename<span class="token punctuation">)</span><span class="token punctuation">:</span>                valid_lst<span class="token punctuation">.</span>append<span class="token punctuation">(</span>xml_filename<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 处理图片路径列表txt文件，根据valid_lst筛选有效的图片路径</span>    <span class="token keyword">for</span> filename <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"test.txt"</span><span class="token punctuation">,</span> <span class="token string">"trainval.txt"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"new_"</span> <span class="token operator">+</span> filename<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> nf<span class="token punctuation">:</span>            <span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> of<span class="token punctuation">:</span>                <span class="token keyword">for</span> line <span class="token keyword">in</span> of<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> valid_lst<span class="token punctuation">:</span>                        nf<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"Annotations"</span><span class="token punctuation">,</span> <span class="token string">"NewAnnotations"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 处理图片大小列表txt文件，根据valid_lst筛选有效的图片大小</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"new_test_name_size.txt"</span><span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> nf<span class="token punctuation">:</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"test_name_size.txt"</span><span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> of<span class="token punctuation">:</span>            <span class="token keyword">for</span> line <span class="token keyword">in</span> of<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> valid_lst<span class="token punctuation">:</span>                    nf<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"Annotations"</span><span class="token punctuation">,</span> <span class="token string">"NewAnnotations"</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="生成lmdb文件"><a href="#生成lmdb文件" class="headerlink" title="生成lmdb文件"></a>生成lmdb文件</h4><p>修改caffe-ssd数据处理工具中的标签映射文件<code>labelmap_voc.prototxt</code>，该文件由若干个类似下边的<code>item</code>组成：      </p><pre><code>item {    name: &quot;none_of_the_above&quot;    label: 0    display_name: &quot;background&quot;}</code></pre><ul><li>name：物体类别在xml文件中出现的名称</li><li>label：标签对应的数值（为方便处理，建议序号从0递增）</li><li>display_name：该类别最后要展示出来的名称</li></ul><p>删除映射文件中多余类别对应的<code>item</code>，然后按顺序重新为各个类别编号（修改label项）；     </p><p>修改<code>create_list.sh</code>脚本，将<strong>第29行</strong>的<code>Annotations</code>改为<code>NewAnnotations</code>——       </p><pre class=" language-bash"><code class="language-bash"><span class="token punctuation">[</span>27<span class="token punctuation">]</span> label_file<span class="token operator">=</span><span class="token variable">$bash_dir</span>/<span class="token variable">$dataset</span><span class="token string">"_label.txt"</span><span class="token punctuation">[</span>28<span class="token punctuation">]</span> <span class="token function">cp</span> <span class="token variable">$dataset_file</span> <span class="token variable">$label_file</span><span class="token punctuation">[</span>29<span class="token punctuation">]</span> <span class="token function">sed</span> -i <span class="token string">"s/^/<span class="token variable">$name</span>\/NewAnnotations\//g"</span> <span class="token variable">$label_file</span><span class="token punctuation">[</span>30<span class="token punctuation">]</span> <span class="token function">sed</span> -i <span class="token string">"s/$/.xml/g"</span> <span class="token variable">$label_file</span></code></pre><p>然后跟上一篇文章一样，依次执行脚本<code>create_list.sh</code>和<code>create_data.sh</code>即可。     </p><h4 id="训练和部署"><a href="#训练和部署" class="headerlink" title="训练和部署"></a>训练和部署</h4><p>训练和部署过程与 《<a href="/2018/08/21/mssd-try1/#训练">训练MobileNet-SSD/开始训练MobileNet-SSD/训练(部署) | Hey~YaHei!</a>》 基本相同；<br>微小的区别在于，     </p><ol><li>生成模型文件时<br> <code>./gen_model.sh 21</code>中<code>21</code>要换成实际的类别数量（含背景background）；      </li><li>要使用新的标签映射文件<code>labelmap.prototxt</code>；</li><li>应用程序中标签要对应修改<br> 如《<a href="/2018/08/04/RK3399-Tengine/#目标检测网络MobileNet-SSD">RK3399上Tengine平台搭建/目标检测网络MobileNet-SSD | Hey~YaHei!</a>》最后列出的代码中，<code>post_process_ssd</code>函数里的<code>class_names</code>数组常量要对应修改（索引号与<code>labelmap.prototxt</code>文件里的<code>label</code>标签一一对应）。         </li></ol><h3 id="Depthwise-Convolution和Standard-Convolution-Group-的比较"><a href="#Depthwise-Convolution和Standard-Convolution-Group-的比较" class="headerlink" title="Depthwise Convolution和Standard Convolution(Group)的比较"></a>Depthwise Convolution和Standard Convolution(Group)的比较</h3><p>观察chuanqi305的 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/deploy.prototxt" target="_blank" rel="noopener">MobileNet-SSD模型文件deploy.prototxt</a> 可以发现，其中的Depthwise Convolution都是使用特殊的caffe原生卷积层（group参数与num_output参数相等）来实现的。     </p><p>查阅caffe官方文档，</p><blockquote><p>group (g) [default 1]: If g &gt; 1, we restrict the connectivity of each filter to a subset of the input. Specifically, the input and output channels are separated into g groups, and the ith output group channels will be only connected to the ith input group channels.</p></blockquote><p>可以知道，group参数强制输入通道和输出通道分为若干组，一组输入通道卷积运算得到组序相同的输出通道，所以使 <code>group == num_output</code> 确实可以得到与Depthwise Convolution相同的结果。但是，“分组”？从字面的意思上看，不免让人怀疑，这个group是不是通过简单的循环实现的，如果真是如此，不同的group还会并行的计算吗？我们不妨做个简单的实验——<br>我在github上找到第三方实现的 <a href="https://github.com/yonghenglh6/DepthwiseConvolution" target="_blank" rel="noopener">caffe - Depthwise Convolution层</a>，根据其README的说明将其放到caffe源码下重新编译caffe-ssd得到专门实现的<code>DepthwiseConvolution</code>。<br>在单卡GTX1080Ti、Intel E5-2683环境下测试结果如下（分别重复运行100次，单位：秒/百次）：     </p><table><thead><tr><th style="text-align:center">caffe-mode</th><th style="text-align:center">Standard Convolution(Group)</th><th style="text-align:center">Depthwise Convolution</th></tr></thead><tbody><tr><td style="text-align:center">cpu-only</td><td style="text-align:center">26.13568</td><td style="text-align:center">23.40233</td></tr><tr><td style="text-align:center">gpu</td><td style="text-align:center">6.93938</td><td style="text-align:center">0.53499</td></tr><tr><td style="text-align:center">gpu-cudnn</td><td style="text-align:center">6.86799</td><td style="text-align:center">0.53779</td></tr></tbody></table><p>很显然，在cpu-only模式下，两者没有太大区别；在gpu模式下（无论是caffe自身的加速库还是cudnn加速库），专门实现的DepthwiseConvolution都要<strong>快10倍左右</strong>！<br>除此之外，<a href="https://github.com/yonghenglh6/DepthwiseConvolution" target="_blank" rel="noopener">Depthwise Convolution Layer | github</a>的README也给出了一些测试数据。      </p><p>那Tengine有专门实现的DepthwiseConvolution层吗？<br>从<a href="https://github.com/OAID/Tengine/blob/master/doc/operator_ir.md" target="_blank" rel="noopener">官方的文档</a>上看，确实没有专门的DepthwiseConvolution层，如果试着在模型文件里使用<code>DepthwiseConvolution</code>也会看到报错。不过！在源码 <a href="https://github.com/OAID/Tengine/blob/master/executor/operator/arm64/conv/conv_2d_dw.cpp#L222" target="_blank" rel="noopener">executor/operator/arm64/conv/conv_2d_dw.cpp | github, Tengine</a>可以看到有一个 <code>isDepthwiseSupported</code> 函数——      </p><pre class=" language-cpp"><code class="language-cpp"><span class="token keyword">static</span> <span class="token keyword">bool</span> <span class="token function">isDepthwiseSupported</span><span class="token punctuation">(</span><span class="token keyword">const</span> ConvParam <span class="token operator">*</span> param<span class="token punctuation">,</span> <span class="token keyword">const</span> TShape<span class="token operator">&amp;</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> input_c<span class="token operator">=</span>input_shape<span class="token punctuation">.</span><span class="token function">GetC</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> group<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>group<span class="token punctuation">;</span>    <span class="token keyword">int</span> kernel_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>kernel_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> kernel_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>kernel_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> stride_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>stride_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> stride_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>stride_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> dilation_h<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>dilation_h<span class="token punctuation">;</span>    <span class="token keyword">int</span> dilation_w<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>dilation_w<span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_h0<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_w0<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_h1<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> pad_w1<span class="token operator">=</span>param<span class="token operator">-</span><span class="token operator">></span>pads<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>group <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">||</span> input_c <span class="token operator">!=</span> group <span class="token operator">||</span> kernel_h <span class="token operator">!=</span> <span class="token number">3</span> <span class="token operator">||</span> kernel_w <span class="token operator">!=</span> <span class="token number">3</span> <span class="token operator">||</span>       pad_h0 <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> pad_w0 <span class="token operator">!=</span><span class="token number">1</span> <span class="token operator">||</span> pad_h0 <span class="token operator">!=</span> pad_h1 <span class="token operator">||</span> pad_w0 <span class="token operator">!=</span> pad_w1 <span class="token operator">||</span>       dilation_h <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> dilation_w <span class="token operator">!=</span> <span class="token number">1</span> <span class="token operator">||</span> stride_w <span class="token operator">!=</span> stride_h<span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token boolean">true</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>也就是说，<strong>Tengine会自行判断<code>Convolution</code>层是否属于<code>DepthwiseConvolution</code>并相应作出优化</strong>，对应的汇编实现为 <a href="https://github.com/OAID/Tengine/blob/master/executor/operator/arm64/conv/dw_k3s1p1.S" target="_blank" rel="noopener">executor/operator/arm64/conv/dw_k3s1p1.S | github, Tengine</a>。     </p><h3 id="进一步替换Depthwise-Convolution"><a href="#进一步替换Depthwise-Convolution" class="headerlink" title="进一步替换Depthwise Convolution"></a>进一步替换Depthwise Convolution</h3><p>从《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》一文中可以看到，chuanqi305在设计MobileNet-SSD时还是保守地在<code>Conv14_1</code>到<code>Conv17_2</code>使用Standard Convolution，我们不妨进一步把这一部分也替换为深度向分解的卷积，替换的方式也很简单，举个例子：<br>对于某个传统的Convolution层        </p><pre><code>layer {  name: &quot;conv14_2&quot;  type: &quot;Convolution&quot;  bottom: &quot;conv14_1&quot;  top: &quot;conv14_2&quot;  param {    lr_mult: 1.0    decay_mult: 1.0  }  param {    lr_mult: 2.0    decay_mult: 0.0  }  convolution_param {    num_output: 512    pad: 1    kernel_size: 3    stride: 2    weight_filler {      type: &quot;msra&quot;    }    bias_filler {      type: &quot;constant&quot;      value: 0.0    }  }}layer {  name: &quot;conv14_2/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2&quot;  top: &quot;conv14_2&quot;}</code></pre><p>修改为</p><pre><code>layer {  name: &quot;conv14_2_new/dw&quot;  type: &quot;DepthwiseConvolution&quot;  bottom: &quot;conv14_1_new&quot;  top: &quot;conv14_2_new/dw&quot;  param {    lr_mult: 0.1    decay_mult: 0.1  }  convolution_param {    num_output: 256    bias_term: false    pad: 1    kernel_size: 3    stride: 2    group: 256    engine: CAFFE    weight_filler {      type: &quot;msra&quot;    }  }}layer {  name: &quot;conv14_2_new/dw/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2_new/dw&quot;  top: &quot;conv14_2_new/dw&quot;}layer {  name: &quot;conv14_2_new&quot;  type: &quot;Convolution&quot;  bottom: &quot;conv14_2_new/dw&quot;  top: &quot;conv14_2_new&quot;  param {    lr_mult: 0.1    decay_mult: 0.1  }  convolution_param {    num_output: 512    bias_term: false    kernel_size: 1    weight_filler {      type: &quot;msra&quot;    }  }}layer {  name: &quot;conv14_2_new/relu&quot;  type: &quot;ReLU&quot;  bottom: &quot;conv14_2_new&quot;  top: &quot;conv14_2_new&quot;}</code></pre><p>要注意几点，    </p><ol><li>修改后的层要给个新的名字，避免初始化权重的时候从预训练好的模型误导入权重；      </li><li>训练模型<code>train.prototxt</code>和测试模型<code>test.prototxt</code>可别忘了加上BN层；      </li><li>部署在Tengine上的时候要记得把type从<code>DepthwiseConvolution</code>替换为<code>Convolution</code>。            </li></ol><p>……其他层也做类似的修改即可。<br>替换前后比较——      </p><table><thead><tr><th style="text-align:center">VOC2007-test</th><th style="text-align:center">MobileNet-SSD</th><th style="text-align:center">MobileNet-SSDLite</th></tr></thead><tbody><tr><td style="text-align:center">mAP</td><td style="text-align:center">0.727</td><td style="text-align:center">0.718</td></tr><tr><td style="text-align:center">FPS(1080Ti)</td><td style="text-align:center">258</td><td style="text-align:center">278</td></tr><tr><td style="text-align:center">caffemodel</td><td style="text-align:center">23MB</td><td style="text-align:center">16MB</td></tr></tbody></table><hr><p>本文介绍了如何削减VOC数据集上多余类别进行训练，并且尝试用深度向分解的卷积层进一步替换传统的卷积层，同时比较了专门优化加速的DepthwiseConvolution和Convolution(Group)在效率上的差别。<br>下一篇文章《<a href="/2018/09/10/mssd-try3">基于MobileNet-SSD的目标检测Demo（二）</a>》将介绍如何把<strong>目标检测</strong>和<strong>视频解码与显示</strong>分别放到两个线程上，来提高目标检测demo的流畅性。        </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>训练MobileNet-SSD</title>
      <link href="/2018/08/21/mssd-try1/"/>
      <url>/2018/08/21/mssd-try1/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=26142483&auto=0&height=32"></iframe>     <p>《<a href="/2018/08/04/RK3399-Tengine">RK3399上Tengine平台搭建 | Hey~YaHei!</a>》一文介绍了RK3399和Tengine并且尝试跑通了MobilNet-SSD网络，而随后又分别用《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》、《<a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>》《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》三篇文章分别介绍了MobileNet v1、SSD和MobileNet-SSD。<br>接下来，本文将尝试训练自己的MobileNet-SSD并且部署在Tengine平台上。     </p><hr><h3 id="安装配置cuda、caffe"><a href="#安装配置cuda、caffe" class="headerlink" title="安装配置cuda、caffe"></a>安装配置cuda、caffe</h3><p>cuda的安装网上有非常多的教程，比如《<a href="https://www.cnblogs.com/iloveblog/p/7683349.html" target="_blank" rel="noopener">Ubuntu16.04+cuda9.0安装教程 | 贝多芬的悲桑, cnblogs</a>》和《<a href="https://blog.csdn.net/huang826336127/article/details/78754767" target="_blank" rel="noopener">安装cuda-8.0 | 代码小哥, csdn</a>》，过程也很简单，在官网下载你需要的版本对应的<code>.run</code>文件，直接运行按提示安装即可。          </p><p>caffe由于要使用SSD框架，所以要编译安装caffe的ssd分支——       </p><h4 id="下载caffe-ssd源码"><a href="#下载caffe-ssd源码" class="headerlink" title="下载caffe-ssd源码"></a>下载caffe-ssd源码</h4><ul><li>直接用git把仓库克隆到本地并切换到ssd分支       <pre class=" language-bash"><code class="language-bash">  <span class="token function">git</span> clone https://github.com/weiliu89/caffe.git  <span class="token function">cd</span> caffe  <span class="token function">git</span> checkout ssd</code></pre></li><li>github服务器在海外，网络不是很稳定，你可以试着挂vpn下载源码。<br>  如果你的linux没有配置vpn，但windows或mac有，那也可以直接在 <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">weiliu89/caffe at ssd | github</a> 上下载zip压缩包，再拷贝到linux用<code>unzip</code>指令解压；       </li><li>如果你没有vpn，也可以到我的网盘上下载：<a href="https://pan.baidu.com/s/1wR0iJcvTgT7c4vwJF1pVUQ#list/path=%2Fblog-share%2Fmobilenet_ssd" target="_blank" rel="noopener">blog-share/mobilenet_ssd/caffe-ssd.zip | 百度网盘</a></li></ul><h4 id="编译caffe-ssd"><a href="#编译caffe-ssd" class="headerlink" title="编译caffe-ssd"></a>编译caffe-ssd</h4><p>编译过程可以参照 <a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener">Caffe | Installation</a> 来进行；     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> CAFFE_ROOT<span class="token operator">=</span>/your/caffe/root/path<span class="token comment" spellcheck="true"># 进入caffe源码的根目录</span><span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span><span class="token comment" spellcheck="true"># 从模板拷贝一份编译的配置文件</span><span class="token function">cp</span> Makefile.config.example Makefile.config<span class="token comment" spellcheck="true"># 按需要修改编译配置</span><span class="token comment" spellcheck="true"># vim Makefile.config</span><span class="token comment" spellcheck="true"># 开始编译（参数j表示编译使用的线程数量，一般数值越大越快，取决于你cpu支持的线程数）</span><span class="token function">make</span> -j8<span class="token comment" spellcheck="true"># 修改环境变量</span><span class="token keyword">echo</span> <span class="token string">"export PYTHONPATH=<span class="token variable">$CAFFE_ROOT</span>/python:<span class="token variable">$PYTHONPATH</span>"</span> <span class="token operator">>></span> ~/.bashrc<span class="token function">source</span> ~/.bashrc<span class="token comment" spellcheck="true"># 编译python包</span><span class="token function">make</span> py<span class="token comment" spellcheck="true"># 编译测试程序</span><span class="token function">make</span> <span class="token function">test</span> -j8<span class="token comment" spellcheck="true"># 测试</span><span class="token function">make</span> runtest -j8</code></pre><p>关于配置文件，一般直接用默认配置就行，       </p><ul><li>如果你想用cudnn加速而不是caffe自己提供的加速库（caffe不建议用cudnn），给<strong>第5行</strong>解除注释。      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>4<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># cuDNN acceleration switch (uncomment to build with cuDNN).</span>  <span class="token punctuation">[</span>5<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># USE_CUDNN := 1</span></code></pre></li><li>如果你不想用gpu，给<strong>第8行</strong>解除注释，      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>7<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CPU-only switch (uncomment to build without GPU support).</span>  <span class="token punctuation">[</span>8<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CPU_ONLY := 1</span></code></pre></li><li>如果你的cuda没有安装在默认位置，你可能需要在<strong>第28行</strong>修改变量<code>CUDA_DIR</code>       <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>27<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CUDA directory contains bin/ and lib/ directories that we need.</span>  <span class="token punctuation">[</span>28<span class="token punctuation">]</span> CUDA_DIR :<span class="token operator">=</span> /usr/local/cuda  <span class="token punctuation">[</span>29<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># On Ubuntu 14.04, if cuda tools are installed via</span>  <span class="token punctuation">[</span>30<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span>  <span class="token punctuation">[</span>31<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># CUDA_DIR := /usr</span></code></pre></li><li>如果你用的是anaconda，或者你要用python3接口，你可能需要修改      <pre class=" language-bash"><code class="language-bash">  <span class="token punctuation">[</span>64<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># NOTE: this is required only if you will compile the python interface.</span>  <span class="token punctuation">[</span>65<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># We need to be able to find Python.h and numpy/arrayobject.h.</span>  <span class="token punctuation">[</span>66<span class="token punctuation">]</span> PYTHON_INCLUDE :<span class="token operator">=</span> /usr/include/python2.7 \  <span class="token punctuation">[</span>67<span class="token punctuation">]</span>         /usr/lib/python2.7/dist-packages/numpy/core/include  <span class="token punctuation">[</span>68<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Anaconda Python distribution is quite popular. Include path:</span>  <span class="token punctuation">[</span>69<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Verify anaconda location, sometimes it's in root.</span>  <span class="token punctuation">[</span>70<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># ANACONDA_HOME := $(HOME)/anaconda2</span>  <span class="token punctuation">[</span>71<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span>  <span class="token punctuation">[</span>72<span class="token punctuation">]</span>         <span class="token variable"><span class="token variable">$(</span>ANACONDA_HOME<span class="token variable">)</span></span>/include/python2.7 \  <span class="token punctuation">[</span>73<span class="token punctuation">]</span>         <span class="token variable"><span class="token variable">$(</span>ANACONDA_HOME<span class="token variable">)</span></span>/lib/python2.7/site-packages/numpy/core/include \  <span class="token punctuation">[</span>74<span class="token punctuation">]</span>   <span class="token punctuation">[</span>75<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># Uncomment to use Python 3 (default is Python 2)</span>  <span class="token punctuation">[</span>76<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_LIBRARIES := boost_python3 python3.5m</span>  <span class="token punctuation">[</span>77<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># PYTHON_INCLUDE := /usr/include/python3.5m \</span>  <span class="token punctuation">[</span>78<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></code></pre></li></ul><p><strong><em>编译过程中如果出现 <code>google::protobuf</code> 或者 <code>google::protoc</code> 相关的报错，你可能需要到 <a href="https://github.com/google/protobuf/releases" target="_blank" rel="noopener">google/protobuf | github</a> 下载合适版本的protobuf到本地编译并且配置环境变量（可以用<code>protoc --version</code>指令查看当前使用的protobuf版本）</em></strong>    </p><h3 id="开始训练MobileNet-SSD"><a href="#开始训练MobileNet-SSD" class="headerlink" title="开始训练MobileNet-SSD"></a>开始训练MobileNet-SSD</h3><p>首先，先跑通默认的 <a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">MobileNet-SSD</a>——      </p><h4 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h4><p>MobileNet-SSD默认使用<a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="noopener">Pascal VOC</a>的2007和2012数据集，<br>下载以下数据集，并解压到同一个目录下：     </p><ul><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar" target="_blank" rel="noopener">VOC2007 - training/validation data</a></li><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar" target="_blank" rel="noopener">VOC2007 - test data</a></li><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar" target="_blank" rel="noopener">VOC2012 - training/validation data</a></li><li><a href="http://host.robots.ox.ac.uk:8080/" target="_blank" rel="noopener">VOC2012 - test data</a>    </li></ul><p>解压后目录如下图所示：<br><img src="/imgs/mssd-try/try1/dataset_tree.png" width="350">       </p><p>然后用caffe-ssd提供的 <a href="https://github.com/weiliu89/caffe/tree/ssd/data/VOC0712" target="_blank" rel="noopener">VOC数据集处理工具</a> 对数据集进行处理——    </p><ol><li>按实际情况修改并执行脚本<code>$CAFFE_ROOT/data/VOC0712/create_list.sh</code><br> 将<strong>第3行</strong>的<code>root_dir</code>变量修改为你的VOC数据集目录，比如按照我的目录树，则设置为<code>$HOME/data/VOCdevkit</code>       <pre class=" language-bash"><code class="language-bash"> <span class="token punctuation">[</span>1<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">#!/bin/bash</span> <span class="token punctuation">[</span>2<span class="token punctuation">]</span>  <span class="token punctuation">[</span>3<span class="token punctuation">]</span> root_dir<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/ <span class="token punctuation">[</span>4<span class="token punctuation">]</span> sub_dir<span class="token operator">=</span>ImageSets/Main <span class="token punctuation">[</span>5<span class="token punctuation">]</span> bash_dir<span class="token operator">=</span><span class="token string">"$(cd "</span><span class="token punctuation">$(</span>dirname <span class="token string">"<span class="token variable">${BASH_SOURCE[0]}</span>"</span><span class="token punctuation">)</span><span class="token string">" &amp;&amp; pwd)"</span></code></pre></li><li>按实际情况修改并执行脚本<code>$CAFFE_ROOT/data/VOC0712/create_data.sh</code><br> 将<strong>第7行</strong>的<code>data_root_dir</code>变量修改为你的VOC数据集目录，比如按照我的目录树，则设置为<code>$HOME/data/VOCdevkit</code>       <pre class=" language-bash"><code class="language-bash"> <span class="token punctuation">[</span>1<span class="token punctuation">]</span> cur_dir<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">cd</span> <span class="token punctuation">$(</span> <span class="token function">dirname</span> $<span class="token punctuation">{</span>BASH_SOURCE<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">}</span> <span class="token variable">)</span></span> <span class="token operator">&amp;&amp;</span> <span class="token function">pwd</span> <span class="token punctuation">)</span> <span class="token punctuation">[</span>2<span class="token punctuation">]</span> root_dir<span class="token operator">=</span><span class="token variable">$cur_dir</span>/<span class="token punctuation">..</span>/<span class="token punctuation">..</span> <span class="token punctuation">[</span>3<span class="token punctuation">]</span>  <span class="token punctuation">[</span>4<span class="token punctuation">]</span> <span class="token function">cd</span> <span class="token variable">$root_dir</span> <span class="token punctuation">[</span>5<span class="token punctuation">]</span>  <span class="token punctuation">[</span>6<span class="token punctuation">]</span> redo<span class="token operator">=</span>1 <span class="token punctuation">[</span>7<span class="token punctuation">]</span> data_root_dir<span class="token operator">=</span><span class="token string">"<span class="token variable">$HOME</span>/data/VOCdevkit"</span> <span class="token punctuation">[</span>8<span class="token punctuation">]</span> dataset_name<span class="token operator">=</span><span class="token string">"VOC0712"</span> <span class="token punctuation">[</span>9<span class="token punctuation">]</span> mapfile<span class="token operator">=</span><span class="token string">"<span class="token variable">$root_dir</span>/data/<span class="token variable">$dataset_name</span>/labelmap_voc.prototxt"</span></code></pre></li></ol><p>执行完毕后将会自动在<code>$data_root_dir</code>目录下生成<code>VOC0712</code>子目录，里边包含了从数据集VOC2007和VOC2012提取的图片和标记信息，并构建caffe能够高效读取的lmdb文件。<br><code>VOC0712</code>子目录结构如下图所示：<br><img src="/imgs/mssd-try/try1/lmdb_tree.png" width="350">       </p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p><strong>下载MobileNet-SSD源码：</strong>          </p><ul><li>直接用git克隆仓库到examples目录下     <pre class=" language-bash"><code class="language-bash">  <span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span>/examples  <span class="token function">git</span> clone https://github.com/chuanqi305/MobileNet-SSD.git</code></pre></li><li>或者到我的网盘上下载并解压到caffe的examples目录下：<a href="https://pan.baidu.com/s/1wR0iJcvTgT7c4vwJF1pVUQ#list/path=%2Fblog-share%2Fmobilenet_ssd" target="_blank" rel="noopener">blog-share/mobilenet_ssd/MobileNet-SSD.zip | 百度网盘</a>         </li></ul><p><strong>创建数据集软链接：</strong>       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> TRAINVAL_LMDB<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/VOC0712/lmdb/VOC0712_trainval_lmdb/<span class="token function">export</span> TEST_LMDB<span class="token operator">=</span><span class="token variable">$HOME</span>/data/VOCdevkit/VOC0712/lmdb/VOC0712_test_lmdb/<span class="token function">cd</span> <span class="token variable">$CAFFE_ROOT</span>/examples/MobileNet-SSD<span class="token function">ln</span> -s <span class="token variable">$TRAINVAL_LMDB</span> ./trainval_lmdb<span class="token function">ln</span> -s <span class="token variable">$TEST_LMDB</span> ./test_lmdb</code></pre><p><strong>把VOC的标签映射文件复制过来：</strong>        </p><pre class=" language-bash"><code class="language-bash"><span class="token function">cp</span> <span class="token variable">$CAFFE_ROOT</span>/data/VOC0712/labelmap_voc.prototxt <span class="token variable">$CAFFE_ROOT</span>/examples/MobileNet-SSD/labelmap.prototxt</code></pre><p><strong>生成模型文件：</strong>     </p><pre class=" language-bash"><code class="language-bash">./gen_model.sh 21</code></pre><p>这里21指的是VOC的21个类别（含负样本），生成的模型文件默认放置在<code>example</code>目录下；          </p><p><strong>如果需要修改训练参数和测试参数</strong>，可以分别修改目录下的<code>solver_train.protxt</code>和<code>sovler_test.protxt</code>文件，<br>默认使用<code>example</code>目录下的训练模型和测试模型；     </p><p><strong>如果需要指定GPU和初始化权重</strong>，可以修改目录下的<code>train.sh</code>或<code>test.sh</code>文件，以<code>train.sh</code>为例：       </p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span><span class="token keyword">if</span> <span class="token operator">!</span> <span class="token function">test</span> -f example/MobileNetSSD_train.prototxt <span class="token punctuation">;</span><span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"error: example/MobileNetSSD_train.prototxt does not exist."</span>    <span class="token keyword">echo</span> <span class="token string">"please use the gen_model.sh to generate your own model."</span>        <span class="token keyword">exit</span> 1<span class="token keyword">fi</span><span class="token function">mkdir</span> -p snapshot<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/build/tools/caffe train -solver<span class="token operator">=</span><span class="token string">"solver_train.prototxt"</span> \-weights<span class="token operator">=</span><span class="token string">"mobilenet_iter_73000.caffemodel"</span> \-gpu 0 </code></pre><p><code>weights</code>参数指定初始化的权重文件，这里用了chuanqi305预训练迭代了73000次的模型；<br><code>gpu</code>参数指定使用的gpu，多个gpu可以用逗号隔开；<br>除此之外，如果需要继续之前中断的训练，还可以指定<code>snapshot</code>参数，<br>比如我想从最近的快照继续训练，可以这样修改<code>train.sh</code>——      </p><pre class=" language-bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>latest<span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token function">ls</span> -t snapshot/*.caffemodel <span class="token operator">|</span> <span class="token function">head</span> -n 1<span class="token variable">)</span></span><span class="token keyword">if</span> <span class="token operator">!</span> <span class="token function">test</span> -f example/MobileNetSSD_train.prototxt <span class="token punctuation">;</span><span class="token keyword">then</span>    <span class="token keyword">echo</span> <span class="token string">"error: example/MobileNetSSD_train.prototxt does not exist."</span>    <span class="token keyword">echo</span> <span class="token string">"please use the gen_model.sh to generate your own model."</span>        <span class="token keyword">exit</span> 1<span class="token keyword">fi</span><span class="token function">mkdir</span> -p snapshot<span class="token punctuation">..</span>/<span class="token punctuation">..</span>/build/tools/caffe train -solver<span class="token operator">=</span><span class="token string">"solver_train.prototxt"</span> \-snapshot<span class="token operator">=</span><span class="token variable">$latest</span> \-gpu 0 </code></pre><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p><strong>合并BN层：</strong><br>训练后会在<code>snapshot</code>目录下产生一个相应的<code>caffemodel</code>文件；<br>按实际情况修改<code>merge_bn.py</code>文件并执行：      </p><pre class=" language-python"><code class="language-python"><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np  <span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token keyword">import</span> sys<span class="token punctuation">,</span>os  <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">]</span> caffe_root <span class="token operator">=</span> <span class="token string">'/your/caffe/root/path/'</span><span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">]</span> sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> caffe_root <span class="token operator">+</span> <span class="token string">'python'</span><span class="token punctuation">)</span>  <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">]</span> <span class="token keyword">import</span> caffe  <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">]</span> <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">]</span> train_proto <span class="token operator">=</span> <span class="token string">'example/MobileNetSSD_train.prototxt'</span>       <span class="token comment" spellcheck="true"># 训练时所用的模型文件</span><span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">]</span> train_model <span class="token operator">=</span> <span class="token string">'mobilenet_iter_73000.caffemodel'</span>           <span class="token comment" spellcheck="true"># 训练后产生的caffemodel文件</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> deploy_proto <span class="token operator">=</span> <span class="token string">'example/MobileNetSSD_deploy.prototxt'</span>     <span class="token comment" spellcheck="true"># 部署时所要用的模型文件（去掉BN层）</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span> save_model <span class="token operator">=</span> <span class="token string">'MobileNetSSD_deploy.caffemodel'</span>             <span class="token comment" spellcheck="true"># 最终生成的caffemodel文件（合并BN层参数）</span></code></pre><p>生成的合并BN层后的caffemodel就在MobileNet-SSD项目的根目录下；<br>编辑<code>example/MobileNetSSD_deploy.prototxt</code>修改输入层，即把</p><pre><code>input: &quot;data&quot;input_shape {    dim: 1    dim: 3    dim: 300    dim: 300}</code></pre><p>改为     </p><pre><code>layer {    name: &quot;input&quot;    type: &quot;Input&quot;    top: &quot;data&quot;    input_param {        shape {            dim: 1            dim: 3            dim: 300            dim: 300        }    }}</code></pre><p>把 <code>example/MobileNetSSD_deploy.prototxt</code> 和 <code>MobileNetSSD_deploy.caffemodel</code> 拷贝到<strong>Tengine</strong>平台的<code>models</code>目录下，此时运行<code>mobilenet_ssd/MSSD</code>用的就是新训练好的模型啦！      </p><hr><p>本文简单介绍了如何用chuanqi305的MobileNet-SSD训练出自己的网络。<br>下一篇文章《<a href="/2018/08/24/mssd-try2/">基于MobileNet-SSD的目标检测Demo（一） | Hey~YaHei!</a>》将继续尝试根据实际情况删减多余类别进行训练。还可以注意到，        </p><ul><li>chuanqi305的MobileNet-SSD模型除了基础网络部分之外依旧保守的使用了Standard Conv，可以尝试将这一部分也改造为Depthwise Conv；      </li><li>同时，MobileNet-SSD使用带group的caffe原生Conv来进行Depthwise Conv操作，这是非常低效率的，下篇文章还将进一步比较Depthwise Conv和带group的原生Conv的效率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNet-SSD网络解析</title>
      <link href="/2018/08/08/MobileNets-SSD/"/>
      <url>/2018/08/08/MobileNets-SSD/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=28941713&auto=0&height=32"></iframe>     <p>上一篇文章《<a href="/2018/08/06/SSD/#网络结构">SSD框架解析 - 网络结构| Hey~YaHei!</a>》和上上篇文章《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》我们分别解析了SSD目标检测框架和MobileNet v1分类模型。<br>在本文中将会把两者综合起来，一起分析<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合得到MobileNet-SSD网络的。         </p><hr><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>参照 <a href="https://github.com/chuanqi305/MobileNet-SSD/tree/master/template" target="_blank" rel="noopener">MobileNet-SSD(chuanqi305)的caffe模型（prototxt文件） | github</a>，绘制出MobileNet-SSD的整体结构如下（忽略一些参数细节）：<br><img src="/imgs/MobileNet-SSD/mobilenet-ssd.jpg" alt="mobilenet-ssd">    </p><p>图片中从上到下分别是MobileNet v1模型（统一输入大小为300x300）、chuanqi305的Mobilenet-SSD网络、VGG16-SSD网络。且默认都是用3x3大小的卷积核，除了MobileNet-SSD的Conv14_1、Conv15_1、Conv16_1、Conv17_1和VGG16-SSD的Conv8_1、Conv9_1、Conv10_1、Conv11_1用的是1x1大小的卷积核。<br>图中每个立方体代表对应层的<strong>输出</strong>特征图；        </p><ul><li>首先观察基础网络部分<br>  MobileNet-SSD从Conv0到Conv13的配置与MobileNet v1模型是完全一致的，相当于只是去掉MobileNet v1最后的全局平均池化、全连接层和Softmax层；     </li><li>再看SSD部分       <ul><li>在VGG16-SSD的方案中，用Conv6和Conv7分别替代了原VGG16的FC6和FC7；       </li><li>MobileNet-SSD和VGG16-SSD都是从六个不同尺度的特征图上提取特征来做Detections，它们的大小为：      <pre><code>  MobileNet-SSD   |   VGG16-SSD  ----------------+-----------------  19 x 19 x  512  |   38 x 38 x  512  10 x 10 x 1024  |   19 x 19 x 1024   5 x  5 x  512  |   10 x 10 x  512   3 x  3 x  256  |   5  x  5 x  256   2 x  2 x  256  |   3  x  3 x  256   1 x  1 x  128  |   1  x  1 x  128</code></pre><ul><li>从通道数量上看，两者是完全一致的</li><li>从特征图分辨率上看，MobileNet-SSD都只有VGG16-SSD的一半     <ul><li><strong>这意味着什么？</strong><br>  打个比方，假设对于那个分辨率最大的特征图，都能用4x4的感受野检测出一只猫，如下图所示，黑色是头，红色是身体，棕色是腿，黄色是尾巴。<br>  <center><img src="/imgs/MobileNet-SSD/cat.png" width="200"></center><br>  那用MobileNet-SSD可以检测出占原图$\frac{4}{19} \approx 0.211$大小的猫，而VGG16-SSD却可以检测出占原图$\frac{4}{38} \approx 0.105$大小的猫；       </li><li><strong>那为什么MobileNet-SSD为什么不和VGG16-SSD一样，从38x38分辨率的特征图开始做Detections呢？</strong><br>  回到上一篇博文《<a href="/2018/08/06/SSD/#网络结构">SSD框架解析 - 网络结构| Hey~YaHei!</a>》，VGG16是从Conv4_3也就是第10层卷积层取出38x38分辨率的特征图；<br>  再观察一下MobileNet v1-300的模型，想要取出38x38分辨率的特征图，最深也只能从Conv5也就是第6层卷积层取出，这个位置比较浅，实在很难保证网络提取出了足够有用的特征可以使用；         </li><li><strong>那可以通过增加最初输入图片的分辨率来解决这个问题吗？</strong><br>  倒也可以，比如把输入图片大小扩大到512x512，那么Conv11的输出就变为32x32，按上上一点的描述，可以检测出占原图$\frac{4}{32} = 0.125$大小的猫；<br>  但要付出相应的代价，仅考虑基础网络部分（Conv0到Conv13），参数数量和乘加运算量均提高为原来的 $(\frac{512}{300})^2 \approx 2.913$ 倍（不考虑padding的影响，计算方式可以参考《<a href="/2018/08/05/MobileNets_v1/#效率比较">MobileNets v1模型解析 - 效率比较 | Hey~YaHei!</a>》），MobileNet本身小模型的低参数量、低运算量优势变得不再明显。           </li></ul></li></ul></li><li>还有一个小细节，观察特征图到Detections的路径<br>  VGG16-SSD中用的都是3x3大小的卷积核，缺省框数量依次是<font color="red">4</font>、6、6、6、<font color="red">4</font>、<font color="red">4</font>；<br>  MobileNet-SSD中用的都是1x1大小的卷积核，缺省框数量依次是<font color="red">3</font>、6、6、6、<font color="red">6</font>、<font color="red">6</font>；<br>  <em>这一部分的改动不是很能理解，3x3卷积改1x1卷积可能是实践中发现改动后效果差不多但可以减少运算量；缺省框数量改动的原因就不得而知了~</em>      </li></ul></li></ul><h3 id="BN层合并"><a href="#BN层合并" class="headerlink" title="BN层合并"></a>BN层合并</h3><p>对比chuanqi305的 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/template/MobileNetSSD_train_template.prototxt" target="_blank" rel="noopener">train模型</a> 和 <a href="https://github.com/chuanqi305/MobileNet-SSD/blob/master/template/MobileNetSSD_deploy_template.prototxt" target="_blank" rel="noopener">deploy模型</a> 还能发现一件有趣的事情——<br><strong>deploy模型中的BN层和scale层都不见啦！！！</strong><br>BN层是这样随随便便就能丢弃的么？没道理啊！         </p><p>几经辗转，查阅资料之后发现，原来BN层是可以合并进前一层的卷积层或全连接层的，而且这还有利于减少预测用时。<br>参考《<a href="http://machinethink.net/blog/object-detection-with-yolo/#converting-to-metal" target="_blank" rel="noopener">Real-time object detection with YOLO - Converting to Metal</a>》    </p><p>合并的原理：卷积层、全连接层和BN层都是纯粹的线性转换。       </p><p>数学推导也很简单：<br><em>假设图片为 $x$ ，卷积层权重为 $w$ 。</em><br>那么对于卷积运算有，<br>$$ conv[j] = x[i]w[0] + x[i+1]w[1] + x[i+2]w[2] + … + x[i+k]w[k] + b $$<br>BN层运算为，<br>$$ bn[j] = \frac{\gamma (conv[j] - mean)}{\sqrt{variance}} + \beta = \frac{\gamma \cdot conv[j]}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} + \beta $$<br>代入$conv[j]$变为，<br>$$ bn[j] = x[i] \frac{\gamma \cdot w[0]}{\sqrt{variance}} + x[i+1] \frac{\gamma \cdot w[1]}{\sqrt{variance}} + … + x[i+k] \frac{\gamma \cdot w[k]}{\sqrt{variance}} + \frac{\gamma \cdot b}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} + \beta $$<br>两式对比可以得到，<br>$$ w_{new} = \frac{\gamma \cdot w}{\sqrt{variance}} $$<br>$$ b_{new} = \beta + \frac{\gamma \cdot b}{\sqrt{variance}} - \frac{\gamma \cdot mean}{\sqrt{variance}} = \beta + \frac{\gamma (b-mean)}{\sqrt{variance}} $$<br>注意，其中 $\gamma$、$mean$、$variance$、$\beta$ 都是训练出来的量，在预测阶段相当于一个常量。       </p><p>原文摘录如下：    </p><center><img src="/imgs/MobileNet-SSD/Converting to Metal.png" alt="Converting to Metal"></center>    <hr><p>本文介绍了chuanqi305的MobileNet-SSD网络是如何组成的以及实用的MergeBN技术，在下一篇博文中我们将尝试用该网络进行训练并部署在RK3399的Tengine平台上，并且进一步对该网络进行改进以满足我们实际场景的需要。        </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>SSD框架解析</title>
      <link href="/2018/08/06/SSD/"/>
      <url>/2018/08/06/SSD/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=38689090&auto=0&height=32"></iframe>       <p>上一篇文章《<a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>》介绍了MobileNets v1的核心思想和网络结构，本文将解析MobileNet SSD网络的另外一部分——SSD框架。SSD的主要贡献，一方面在于从多个不同尺度的特征图获取特征信息进而预测目标的位置和类别，使网络同时对输入图片上的大小物体都比较敏感；另一方面在于其训练技巧值得借鉴，这在论文中有一定的阐述，更加详细的训练技巧可以结合作者开源的代码学习。         </p><hr><p>论文：《<a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector(2016)</a>》      </p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>与分析MobileNets v1模型不同，分析框架我们先从整体入手。          </p><p><img src="/imgs/SSD/SSD_architecture.png" alt="SSD architeture">     </p><ul><li>用预训练好的分类网络作为特征提取器（论文里使用的是VGG16）<br>  VGG16原模型如下图所示：<br>  <img src="/imgs/SSD/vgg16_raw.png" alt="vgg16"><br>  由于SSD论文里输入是 $300 \times 300$，我们重写一下VGG16模型的各层输出大小：<br>  <img src="/imgs/SSD/vgg16_300.png" alt="vgg16">     </li><li>论文中，SSD丢掉了VGG16最后的全局池化和全连接层（FC6和FC7）<br>  并且分别用 $3 \times 3 \times 1024$ 的卷积层Conv6和 $1 \times 1 \times 1024$ 的卷积层Conv7替代FC6和FC7作基础分类网络的最终特征抽取。         </li><li>随后是一系列不同尺度的卷积层在不同尺度上做特征提取      </li><li>融合不同尺度特征信息<br>  分别用卷积操作从 $38 \times 38$ 的Conv4_3、$19 \times 19$ 的Conv7、$10 \times 10$ 的Conv8_2、$5 \times 5$ 的Conv9_2、$3 \times 3$ 的Conv10_2、$1 \times 1$ 的Conv11_2抽取特征（直接回归出后述的预测框的位置以及各分类的置信度），各自Flatten之后拼接成“长条”状特征向量。       </li><li>非极大值抑制（Non-Maximum Suppression，NMS）      <ul><li>从置信度最高的框开始，如果其他预测框和该框的jaccard重叠率超过阈值，则丢弃     </li><li>从剩下的框找到置信度最高的框，如果其他预测框和该框的jaccard重叠率超过阈值，则丢弃     </li><li>……重复直到遍历所有的框</li></ul></li></ul><h3 id="感受野和缺省框"><a href="#感受野和缺省框" class="headerlink" title="感受野和缺省框"></a>感受野和缺省框</h3><center><img src="/imgs/SSD/bounding box.png" alt="bounding box"></center>       <h4 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h4><p>卷积是对特定小区域的特征提取，比如一张 $300 \times 300$ 的原图经过一定卷积操作之后得到 $8 \times 8$ 的特征图，特征图上的每个“像素点”其实对应原图的一个“感受野”，在这里一个“感受野”的大小为 $\frac{300}{8} \times \frac{300}{8}$ 也即 $37.5 \times 37.5$ （实际上不可能有$.5$，是$37$还是$38$要看卷积过程是否padding）。      </p><p>换句话说，特征图左上角的一个感受野其实是原图左上角一个 $37.5 \times 37.5$ 大小区域的一个特征抽取（或者说是这个区域的一个抽象化，融合了这个区域的信息）。     </p><h4 id="缺省框"><a href="#缺省框" class="headerlink" title="缺省框"></a>缺省框</h4><p><em>论文里称为default box，而源码里称为prior box.</em>          </p><p>在SSD中，为每个向Detection贡献特征的特征图上的每个感受野分配若干不同大小和长宽比（aspect ratio）的绑定框（Bounding Box），比如在作者开源的源码中，这些绑定框是这样生成的——<br>参考<a href="https://github.com/weiliu89/caffe/blob/ssd/src/caffe/layers/prior_box_layer.cpp" target="_blank" rel="noopener">SSD - PriorBoxLayer源码 | github</a><br>（其中，$D_{min}、x_{offset}、y_{offset}、AR、D_{max}$ 都是用户指定的超参数，而且 $D_{max}$ 是可选的）</p><ol><li>首先有一个最小的绑定框，其尺寸为 $D_{min} \times D_{min}$，绑定框中心相对感受野中心偏移 $(x_{offset}, y_{offset})$；       </li><li>根据用户指定的长宽比列表 $AR$ 生成若干绑定框<br> 对于某一特定长宽比 $AR_i$，生成尺寸分别为 $\frac{D_{min}}{\sqrt{AR_i}} \times D_{min}\sqrt{AR_i}$ 和 $D_{min}\sqrt{AR_i} \times \frac{D_{min}}{\sqrt{AR_i}}$ 的绑定框，其框中心同1；      </li><li>最后还生成一个尺寸为 $D_{max} \times D_{max}$ 的绑定框，其框中心同1；     </li></ol><p>事实上网络输出的预测框位置参数就是这每一个缺省框的中心点偏移量和长、宽偏移量（每个框共计4个参数）！<br><em>对于一个方框其实有两种不同的表示方法，都是四元组，即（中心点x坐标，中心点y坐标，长度，宽度）或（左上角x坐标，左下角x坐标，右下角x坐标，右下角y坐标）</em>    </p><h4 id="D-min-和-D-max-的设置建议"><a href="#D-min-和-D-max-的设置建议" class="headerlink" title="$D_{min}$ 和 $D_{max}$ 的设置建议"></a>$D_{min}$ 和 $D_{max}$ 的设置建议</h4><p>论文中也给出了超参数 $D_{min}$ 和 $D_{max}$ 的设置建议——      </p><ul><li>首先确定框的最大最小归一化尺寸 $scale_{min}$ 、 $scale_{max}$<br>  $$ scale = \frac{BoxSize}{ImageSize} $$<br>  比如按论文的设计 $300 \times 300$ 的输入图片，设 $scale_{min} = 0.2、scale_{max} = 0.9$，那框的最大尺寸就是 $270 \times 270$，最小尺寸为 $60 \times 60$；     </li><li>然后最浅层的特征图对应的bbox设 $\text{min_size} = scale_{min} \times ImageSize$；        </li><li>随后按等差数量设置各特征对应的bbox的 min_size，而 max_size 可以取下一层的 min_size（论文中默认不设置max_size）；      </li></ul><p>写成数学公式为：<br>$$s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1} (k-1), k \in [1,m]$$<br>其中，<br>$s_k$，是第k个特征图的 $min_size$ 参数；<br>$s_{min}$，是设计好的最小归一化尺寸，即前述 $scale_{min}$；<br>$s_{max}$，是设计好的最大归一化尺寸，即前述 $scale_{max}$；<br>$m$，是特征图的总数；<br><em>注意：上述“特征图”指的是对Detection有贡献的特征图，序号从浅层到深层递增</em>        </p><p>回过头再看看网络结构，注意从不同尺度融合特征信息的那一部分，用的都是一个 $3 \times 3$ 的卷积核，输出通道要么是 num_class + 4 的四倍要么是六倍，这里 num_class 就是最终分类的数量，数值 $4$ 其实指的是预测框的位置参数的数量，而四倍或六倍指的是特征图上每个感受野对应的绑定框数量。<br>按照论文，浅层的分辨率比较高，所以使用了 $AR=\{1,2,3\}$ 的分辨率组合，对于 $AR_i = 2$ 和 $AR_i = 3$ 分别会生成两个绑定框，而 $AR_i = 1$ 除了本身之外还会额外产生一个归一化尺寸为 $s’_k = \sqrt{s_k s_{k+1}}$ 的绑定框；而浅层分辨率较低，2或3的长宽比其实没太大区别或必要，所以只取了 $AR=\{1,2\}$。       </p><p>源码prototxt参数示例：       </p><pre><code>layer {  name: &quot;conv11_mbox_priorbox&quot;  type: &quot;PriorBox&quot;  bottom: &quot;conv11&quot;  # bottom[0]：特征向量  bottom: &quot;data&quot;    # bottom[1]：原始输入（主要提供宽、高参数）  top: &quot;conv11_mbox_priorbox&quot;  prior_box_param {    min_size: 30.0      # 最小的基本Box大小（可根据论文计算）    aspect_ratio: 2.0   # 所要用的长宽比（除了1.0）    flip: true          # 是否以0.5的概率翻转    clip: false         # 是否截断Box（若截断表示不允许Box超出图片范围）    variance: 0.1       # xmin的偏差因子    variance: 0.1       # ymin的偏差因子    variance: 0.2       # xmax的偏差因子    variance: 0.2       # ymax的偏差因子    offset: 0.5         # Box中心相对于感受野中心的x、y偏移量  }}</code></pre><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><h4 id="框配对（Matching）"><a href="#框配对（Matching）" class="headerlink" title="框配对（Matching）"></a>框配对（Matching）</h4><p>前边讲到了感受野和缺省框，我们现在看看这些缺省框是如何跟实际标注的真实框（Ground Truth, GT）配对起来的。        </p><p>介绍一个重叠率计算公式，jaccard重叠率：<br>$$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|} $$</p><p>对于每个GT，      </p><ul><li>首先，在所有缺省框中挑选出jaccard重叠率最高的一个作为配对正样本，确保每个GT都有唯一一个配对的缺省框；       </li><li>然后，用其他缺省框挨个与GT计算jaccard，超过阈值（论文中设定为0.5）的都作为配对正样本；       </li><li>其他未配对的都作为负样本，但显然数量上负样本远多于正样本，这会导致网络过于重视负样本而loss不稳定；<br>  所以论文采取了Hard Negative Mining的策略，训练时按照分类的置信度为各个负样本框排序；<br>  挑选置信度高的一批作为实际训练的负样本，同时控制负样本的数量大概为正样本的三倍左右      </li></ul><h4 id="损失函数（MultiBoxLoss）"><a href="#损失函数（MultiBoxLoss）" class="headerlink" title="损失函数（MultiBoxLoss）"></a>损失函数（MultiBoxLoss）</h4><p>损失函数由位置损失 $L_{loc}$ 和分类损失 $L_{conf}$ 加权求和获得；<br>完整公式如下：<br>$$L(x,c,l,g) = \frac{1}{N} (L_{conf}(x,c) + \alpha L_{loc}(x,l,g))$$<br>其中，<br>$N$，是匹配正样本的总量（如果N=0，则令L=0）；<br>$x$、$c$，分别是分类的指示量和置信度；<br>$l$、$g$，分别是预测框和GT框；<br>$\alpha$，是位置损失的权重。        </p><p>位置损失是预测框l和真实框g之间的Smooth L1损失，<br>$$ L_{loc} = \sum^N_{i \in Pos} \sum_{m \in {cx, cy, w, h} x^k_{ij}} smooth_{L1} (l^m_i - \hat{g}_j^m) $$<br>$$ \hat{g}^{cx}_j = \frac{g^{cx}_j - d^{cx}_i}{d_i^w}，\hat{g}^{cy}_j = \frac{g^{cy}_j - d^{cy}_i}{d_i^h}，\hat{g}^w_j = log(\frac{g_j^w}{d_i^w})，\hat{g}^h_j = log(\frac{g_j^h}{d_i^h}) $$<br>其中，<br>$x^k_{ij}$，是指示量，当第i个匹配框和分类p的第j个真实框配对时值为1，否则为0；<br>$cx$、$cy$、$w$、$h$，分别是框的中心点x坐标、中心点y坐标，宽度、高度；<br>$d$，是绑定框（网络本身预设的绑定框）；<br>$l$，是预测框（网络输出的加上预测偏移量的框）；<br>$g$，是GT框（数据集标注的真实框）。     </p><p>分类损失是分类置信度之间的softmax损失，<br>$$ L_{conf}(x,c) = -\sum^N_{i \in Pos} x^p_{ij} log(\hat{c}^p_i) - \sum_{i \in Neg}log(\hat{c}^0_i) $$<br>$$ \hat{c}^p_i = \frac{exp(c^p_i)}{\sum_p exp(c_i^p)} $$        </p><p>源码prototxt参数示例：     </p><pre><code>layer {  name: &quot;mbox_loss&quot;  type: &quot;MultiBoxLoss&quot;  bottom: &quot;mbox_loc&quot;        # 用于预测位置的特征向量  bottom: &quot;mbox_conf&quot;       # 用于预测分类的特征向量  bottom: &quot;mbox_priorbox&quot;   # 若干PriorBox的输出连接  bottom: &quot;label&quot;           # 训练用的标签  top: &quot;mbox_loss&quot;  include {    phase: TRAIN  }  propagate_down: true      # bottom[0] : mbox_loc，需要训练（反向传播）  propagate_down: true      # bottom[1] : mbox_conf，需要训练  propagate_down: false     # bottom[2] : mbox_priorbox，不需要训练  propagate_down: false     # bottom[3] : label，不需要训练  # 损失计算参数组  loss_param {    normalization: VALID    # 损失的归一化方式                            #    FULL：除以batch_size                            #    VALID：除以有效的数量（排除ignore_label参数指定的标签）                            #    NONE  }  # multibox损失参数组  multibox_loss_param {    loc_loss_type: SMOOTH_L1    # 位置预测的损失函数    conf_loss_type: SOFTMAX     # 分类预测的损失函数    loc_weight: 1.0             # loc_loss的权重，论文中的alpha    num_classes: 12             # 输出类别    share_location: true        # 是否让所有的预测框共享参数    match_type: PER_PREDICTION  #     overlap_threshold: 0.5      # 重叠阈值（训练时超过该阈值的Box作为正样本）    use_prior_for_matching: true    # 是否使用先验框（即前边是否有PriorBox）    background_label_id: 0      # 背景（background）标签的id值（与labelmap.prototxt文件匹配）    use_difficult_gt: true      # 是否使用difficult的Ground Truth（？）    neg_pos_ratio: 3.0          # 负样本:正样本 的比例    neg_overlap: 0.5            # 负样本阈值（低于该阈值的Box作为负样本）    code_type: CENTER_SIZE      # bounding box的编码方式    ignore_cross_boundary_bbox: false   #     mining_type: MAX_NEGATIVE   # 挖掘类型                                #   NONE：什么都不用，会发生正负样本步均衡                                #   MAX_NEGATIVE：为负样本排序，选择分类得分最高的一批作为训练的负样本                                #   HARD_EXAMPLE：选择基于“在线硬示例挖掘的基于训练区域的对象探测器”的硬实例（？）  }}</code></pre><h4 id="数据增强（Data-Augmentation）"><a href="#数据增强（Data-Augmentation）" class="headerlink" title="数据增强（Data Augmentation）"></a>数据增强（Data Augmentation）</h4><p>目标检测任务和图片分类任务不同，      </p><ul><li>图片分类任务可以随意对分类对象（也即整张图片）作伸缩变换       </li><li>目标检测任务只能对整张图片作伸缩变换，却不能对分类对象直接作伸缩变换      </li></ul><p>基本的数据增强，每张图片在输入会等概率地选取以下一种变换：      </p><ol><li>原始图片      </li><li>随机裁剪    </li><li>带jaccard重叠率约束的随机裁剪（论文给出的是0.1, 0.3, 0.5, 0.7, 0.9五种重叠率设置）        </li></ol><p>随机裁剪是有条件的，限制最低归一化尺寸（论文中为0.1），以及长宽比（论文中为$[1/2,2]$）；<br>实际操作中是先生成一系列满足条件的框，在从中挑选若干框来裁剪图像输入训练。         </p><p>仔细思考一下会发现，上述的数据增强方式只能对某个目标进行放大（zoom in）操作，这会导致小目标数据集缺失。<br>于是论文中在执行上述数据增强前又增加了一步缩小（zoom out）的增强措施——      </p><ol><li>首先生成一张画布，画布的长宽是原图片的1-4倍（随机数）；        </li><li>将原图放在画布的随机位置上；     </li><li>……接下来再送给前述的基本数据增强步骤       </li></ol><p><em>据论文介绍，该额外操作为mAP提升了两到三个百分点。</em>       </p><p>对于画布，从源码<a href="https://github.com/weiliu89/caffe/blob/ssd/src/caffe/data_transformer.cpp#L489" target="_blank" rel="noopener">data_transform.cpp/ExpandImage()函数（第489行） | github</a>上看，是用空格符初始化了一个字符串缓冲区来作为画布，这似乎意味着是用RGB(32,32,32)的固定颜色填充了画布（空格符的assic码为32）；<br>这种做法有些残暴，或许用原图的平均值、或者一个随机值来作固定颜色填充效果会更好一些；<br>甚至，用实际图片作为背景图可能会取得更好的效果，具体结论还有待实验。      </p><p>源码prototxt参数示例：     </p><pre><code>layer {  name: &quot;data&quot;  type: &quot;AnnotatedData&quot;  top: &quot;data&quot;  top: &quot;label&quot;  include {    phase: TRAIN  }  # 图像转换参数组  transform_param {    scale: 0.007843     # 归一化，1/127.5    mirror: true        # 0.5的概率镜像翻转    mean_value: 127.5   # 去均值（R通道）    mean_value: 127.5   # 去均值（G通道）    mean_value: 127.5   # 去均值（B通道）    # 缩放参数组    resize_param {      prob: 1.0                 # 缩放操作的概率      resize_mode: WARP         # 缩放模式                                #   WARP：放大或缩小以适应(width, height)                                #   FIT_SMALL_SIZE：                                #   FIT_LARGE_SIZE_AND_PAD：      height: 300               # 缩放后的高      width: 300                # 缩放后的宽      # 插值模式同opencv      interp_mode: LINEAR       # 线性      interp_mode: AREA         # 像素区域重采样      interp_mode: NEAREST      # 最近邻      interp_mode: CUBIC        # 三次样条      interp_mode: LANCZOS4     # Lanczos    }    emit_constraint {      emit_type: CENTER    }    # 色彩扭曲参数组    distort_param {      brightness_prob: 0.5      brightness_delta: 32.0      contrast_prob: 0.5      contrast_lower: 0.5      contrast_upper: 1.5      hue_prob: 0.5      hue_delta: 18.0      saturation_prob: 0.5      saturation_lower: 0.5      saturation_upper: 1.5      random_order_prob: 0.0    }    # 图像扩展参数组（zoom-out）    expand_param {      prob: 0.5                 # 概率      max_expand_ratio: 4.0     # 扩大倍数（4x4）    }  }  # 数据源参数组  data_param {    source: &quot;trainval_lmdb/&quot;    # 源文件    batch_size: 24           backend: LMDB               # 数据源类型  }  # AnnotatedData参数组  annotated_data_param {    # 采样器1：原图    batch_sampler {      max_sample: 1      max_trials: 1    }    # 采样器2：随机抠图（overlap=0.1）    batch_sampler {      sampler {        min_scale: 0.15         # 最小尺寸（原图的0.15倍）        max_scale: 1.0          # 最大尺寸        min_aspect_ratio: 0.5   # 最小长宽比        max_aspect_ratio: 2.0   # 最大长宽比      }      sample_constraint {        min_jaccard_overlap: 0.1    # 最小JACCARD_OVERLAP（重叠率）      }      max_sample: 1     # 最大采样数量      max_trials: 50    # 最大尝试数量（产生50个随机框，然后再筛选出1个输出）    }    # [略]采样器3：随机抠图（overlap=0.3）    # [略]采样器4：随机抠图（overlap=0.5）    # [略]采样器5：随机抠图（overlap=0.7）    # [略]采样器6：随机抠图（overlap=0.9）    # [略]采样器7：随机抠图（overlap=1.0）    label_map_file: &quot;labelmap.prototxt&quot;     # 标签映射文件  }}</code></pre><hr><p>本文介绍分析了SSD框架的核心思路和训练技巧，加上上一篇文章对MobileNet模型的解析，基本的前置理论准备完毕。在下一篇博文中，将分析<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合起来的——《<a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>》。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MobileNets v1模型解析</title>
      <link href="/2018/08/05/MobileNets_v1/"/>
      <url>/2018/08/05/MobileNets_v1/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=488388731&auto=0&height=32"></iframe><br>上一篇文章《<a href="/2018/08/04/RK3399-Tengine">RK3399上Tengine平台搭建 | Hey~YaHei!</a>》中在RK3399上搭建了Tengine平台并试运行了MobileNet SSD网络，本文将为你解析MobileNets v1的实现思路。<br><strong><em>下边分解过程是按自己理解画的图，如果理解有误欢迎指正~</em></strong>       </p><hr><p>论文：《<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(2017)</a>》   </p><h3 id="深度向卷积分解（Depthwise-Separable-Convolution）"><a href="#深度向卷积分解（Depthwise-Separable-Convolution）" class="headerlink" title="深度向卷积分解（Depthwise Separable Convolution）"></a>深度向卷积分解（Depthwise Separable Convolution）</h3><h4 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h4><p>将普通卷积的过程分解为“滤波”和“组合”两个阶段——    </p><center><img src="/imgs/MobileNet/traditional conv1.jpg" width="500"></center>   <p>如上图，<br><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><ol><li>①阶段为“滤波”阶段，$N$ 个卷积核分别作用在图 $I$ 的每个通道上提取特征，最终输出 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图；      </li><li>②阶段为“组合”阶段，$N$ 张特征图堆叠组合起来，得到一张 $N$ 通道的特征图 $O$     </li></ol><p>更详细地，①过程还能进一步分解——<br><img src="/imgs/MobileNet/traditional conv2.jpg" alt="traditional conv2">   </p><p>如上图，将①阶段进一步细分为 Ⅰ 到 Ⅳ 四个子阶段，     </p><ol><li>Ⅰ 阶段，将原图 $I$ 按通道分离成 $\{ I_1, I_2, …, I_M \}$ 的 $M$ 张单通道图；</li><li>Ⅱ 阶段，用M个卷积核 $K_m$ 对各个单通道图提取特征，分别得到一张大小为 $D_O \times D_O$ 的单通道特征图；</li><li>Ⅲ 阶段，对 Ⅱ 阶段输出的 $M$ 张单通道特征图按通道堆叠起来然后“拍扁”（沿通道方向作加法操作）,得到原图在该卷积核作用下的最终输出 $O_n$；</li><li>Ⅳ 阶段，用另外 $N-1$ 个卷积核重复 Ⅱ 、 Ⅲ 阶段，得到 $N$ 张大小为 $D_O \times D_O$ 的单通道特征图   </li></ol><p>可以看到，传统的卷积中用 $N$ 个不同的卷积核不厌其烦地对原图进行特征提取来得到 $N$ 通道的输出，这其中必定从原图中提取到了大量的重复特征。有没有可能<strong>只用单个卷积核来做特征提取，最后依旧能输出多通道的特征图</strong>呢？这就是深度向卷积分解的核心思想。<br>观察①阶段中的第三个子阶段，该阶段将多张单通道特征图按通道堆叠起来之后“拍扁”，如果去掉这个“拍扁”的过程，其实就可以提取得到一张 $M$ 通道的特征图啦，再经过一个 $M$ 维空间到 $N$ 维空间的线性映射，就能够和普通的卷积操作一样得到一张 $N$ 通道的特征图 $O$。完整的卷积过程如下图所示——<br><img src="/imgs/MobileNet/depthwise conv.jpg" alt="depthwise conv">   </p><ol><li><strong>滤波Depthwise Convolution</strong><br>①、②阶段与传统卷积①阶段的前两个阶段完全相同，③阶段比传统卷积①阶段的第三个阶段少了一个“拍扁”的过程，直接堆叠形成一张 $M$ 通道的特征图；       </li><li><strong>组合Pointwise Convolution</strong><br>④阶段用 $N$ 个 $1 \times 1$ 卷积核将特征图从 $M$ 维空间线性映射到 $N$ 维空间上</li></ol><h4 id="效率比较"><a href="#效率比较" class="headerlink" title="效率比较"></a>效率比较</h4><p><em>假设 $M$ 通道输入图 $I$ 大小为 $D_I \times D_I$，经过一个核大小 $D_K \times D_K$ 的卷积层，最终输出一张大小为 $D_O \times D_O$ 的 $N$ 特征图 $O$</em>         </p><p>对于普通的卷积操作，        </p><ul><li>输出 $N$ 通道特征图需要 $N$ 个卷积核，故参数数量为 $M \times N \times D_K \times D_K$；     </li><li>一个 $D_K \times D_K$ 的卷积核在原图的某个位置的某个通道上需要进行 $D_K \times D_K$ 次乘加操作，输出特征图大小为 $D_O \times D_O$，原图通道数量为 $M$，共有 $N$ 个不同的卷积核，故乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M \times N$           </li></ul><p>对于深度向分解后的卷积操作，       </p><ul><li>特征提取只使用了一个 $D_K \times D_K$ 的卷积核，组合过程为了作线性映射用了 $N$ 个 $1 \times 1$ 的卷积核，故参数数量为 $M \times N + M \times D_K \times D_K$；       </li><li>特征提取过程只有一个卷积核，所以该过程乘加操作数量为 $D_K \times D_K \times D_O \times D_O \times M$。同样的，组合过程容易算得需要 $D_O \times D_O \times M \times N$ 次乘加操作。故乘加操作的总数量为 $D_O \times D_O \times M \times ( D_K \times D_K + N)$             </li></ul><p>总的来说，参数数量和乘加操作的运算量均下降为原来的 $\frac{1}{D_K^2} + \frac{1}{N}$，通常使用 $3 \times 3$ 的卷积核，也就是下降为原来的九分之一到八分之一左右。而从论文的实验部分来看，准确率也只有极小的下降。    </p><center><img src="/imgs/MobileNet/dw_vs_full.png" alt="dw_vs_full"></center>    <h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><center><img src="/imgs/MobileNet/v1_body_architecture.png" alt="v1_body_architecture"></center>    </p><ul><li>第一层使用普通的卷积层，最前端的特征提取非常重要，宁可存在重复的特征信息，也不该放掉；     </li><li>随后是一系列的深度向分解卷积层，用于逐层次提取特征，用步长不为1的卷积替代池化做降采样，同时整体也满足通道加深，特征图分辨率降低的CNN一般特点；        </li><li>最后也是常规的全局平均池化、全连接、Softmax；      </li><li>每次卷积操作之后都紧跟一个BN层（在预测阶段可以被合并）和一个RELU层；      <center><img src="/imgs/MobileNet/dw_vs_full_train.png" alt="dw_vs_full_train"></center>    </li><li>而且，深度向分解的卷积中绝大多数参数和运算都集中在 $1 \times 1$ 的pointwise卷积运算当中，这种运算恰恰是能够被 <code>GEneral Matrix Multiply(GEMM)</code> 函数高度优化的（具体参见《<a href="/2019/02/28/bag-of-tricks2/#高效的1x1卷积">深度学习小技巧（二）：模型微调 - 高效的1x1卷积 | Hey~YaHei!</a>》）；    <center><img src="/imgs/MobileNet/resource.png" alt="resource"></center>    </li><li>论文中还提到两个压缩模型的因子，分别用于输入图片分辨率收缩和特征通道数量收缩来进一步精简模型；    </li></ul><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul><li>小模型，不容易过拟合，不需要太多正则化策略和数据增强策略     </li><li>参数数量少，可以为Depthwise Convolution加入一个很小的权重衰减甚至不需要权重衰减         </li></ul><hr><p>本文介绍了MobileNets v1的主要思想——Depthwise Separable Convolution以及网络的完整结构，其实这一思想也不是MobileNet首创，在MobileNets v1之前Xception就已经提出这种思路。此外，MobileNets v1还是只是一个传统的结构，而且没有像Xception一样去RELU来避免卷积后通道下降非线性单元对特征信息造成的损失，在今年Google新发的MobileNets v2就分析和缓解这一问题，并且引入了类似ResNet的shortcut设计。     </p><p>但，我们还是先一步步解剖好chuanqi305的MobileNets-SSD网络，所以暂时不就MobileNets v2的设计展开讨论，下篇文章将继续讨论<strong>目标检测框架SSD</strong>的设计——《<a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>》。      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>RK3399上Tengine平台搭建</title>
      <link href="/2018/08/04/RK3399-Tengine/"/>
      <url>/2018/08/04/RK3399-Tengine/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=640866&auto=0&height=32"></iframe><h3 id="Tengine-amp-RK3399介绍"><a href="#Tengine-amp-RK3399介绍" class="headerlink" title="Tengine&amp;RK3399介绍"></a>Tengine&amp;RK3399介绍</h3><h4 id="Tengine"><a href="#Tengine" class="headerlink" title="Tengine"></a>Tengine</h4><p><a href="https://github.com/OAID/Tengine" target="_blank" rel="noopener">OADI/Tengine | github</a>       </p><blockquote><p>Tengine 是OPEN AI LAB 为嵌入式设备开发的一个轻量级、高性能并且模块化的引擎。<br>Tengine在嵌入式设备上支持CPU，GPU，DLA/NPU，DSP异构计算的计算框架，实现异构计算的调度器，基于ARM平台的高效的计算库实现，针对特定硬件平台的性能优化，动态规划计算图的内存使用，提供对于网络远端AI计算能力的访问支持，支持多级别并行，整个系统模块可拆卸，基于事件驱动的计算模型，吸取已有AI计算框架的优点，设计全新的计算图表示。    </p></blockquote><h4 id="RK3399"><a href="#RK3399" class="headerlink" title="RK3399"></a>RK3399</h4><p><a href="https://store.t-firefly.com/goods.php?id=44" target="_blank" rel="noopener">Firefly-RK3399 | Firefly</a><br><a href="http://www.t-firefly.com/doc/download/page/id/3.html" target="_blank" rel="noopener">Firefly-RK3399资料下载 | Firefly</a>     </p><blockquote><p>作为Firefly新一代的顶级开源平台，Firefly-RK3399采用了六核64位“服务器级”处理器Rockchip RK3399，拥有2GB/4GB DDR3和16G/32GB eMMC, 并新增DP 1.2、PCIe 2.1 M.2、Type-C、USB3.0 HOST等高性能数据传输和显示接口。Firefly-RK3399强大的性能配置将给VR、全景拍摄、视觉识别、服务器、3D等前沿技术带来里程碑的变革。</p></blockquote><h3 id="RK3399系统烧录"><a href="#RK3399系统烧录" class="headerlink" title="RK3399系统烧录"></a>RK3399系统烧录</h3><p>系统烧录是玩开发板重要的一步，学会如何为开发板烧录系统，就可以无所畏惧地瞎捣鼓——玩坏了大不了就重刷系统！<br>参考<a href="http://dev.t-firefly.com/thread-12613-1-1.html" target="_blank" rel="noopener">RK3399资料 | Firefly论坛</a>      </p><ol><li>下载烧录工具和系统镜像<br> <a href="https://pan.baidu.com/s/1i4AhyJV#list/path=%2F" target="_blank" rel="noopener">烧录工具下载地址 | 百度云</a><br> <a href="https://pan.baidu.com/s/1hsOCOJU#list/path=%2F" target="_blank" rel="noopener">系统镜像下载地址 | 百度云</a><br> 系统镜像选择<strong>Firefly-RK3399-ubuntu16.04-20180416112819</strong>，下载下来是一个tar压缩包，解压后得到一个img镜像文件；<br> 烧录工具的压缩包解压后包含一个<code>AndroidTool</code>的烧录工具以及一个<code>DriverAssitant</code>驱动程序；      </li><li>按照USB驱动<br> 解压<code>DriverAssitant_v4.5</code>的压缩包，运行其中的<code>Driverinstall.exe</code>程序，点击“驱动安装”，按照步骤安装即可； <center><img src="/imgs/RK3399-Tengine/系统烧录-驱动安装.png" alt="系统烧录-驱动安装"></center>      </li><li>使RK3399进入升级模式<br> 用USB线连接PC和RK3399，Type-A端接PC，Type-C端接RK3399；<br> RK3399断电，按住RECOVERY键并接上电源（或在通电情况下，按住RECOVERY然后轻按RESET重启），保持两三秒后松开RECOVERY键，此时启动PC的设备管理器（快捷键Win+X，可以找到设备管理器入口），如果看到多出一个<strong>Class for rockusb devices</strong>设备说明RK3399成功进入升级模式     </li><li>系统烧录<br> 运行<code>AndroidTool.exe</code>，切换到“升级固件”选项卡，点击“固件”并选择下载的镜像文件（扩展名为<code>.img</code>），然后点击“升级”开始烧录，右边的log会输出相关的信息，直到“下载固件成功”以及“重启设备成功”说明成功完成烧录。<br> <img src="/imgs/RK3399-Tengine/镜像烧录成功.png" alt="镜像烧录成功">     </li></ol><h3 id="RK3399远程访问"><a href="#RK3399远程访问" class="headerlink" title="RK3399远程访问"></a>RK3399远程访问</h3><p>有时候专门为RK3399外接显示器和键鼠不大方便，我们可以通过ssh或vnc来远程访问；<br>首先让RK3399连接上网络（有线或无线），然后快捷键<code>ctrl</code>+<code>alt</code>+<code>t</code>呼出终端，输入指令<code>ifconfig</code>查看当前的网络配置——       </p><p><center><img src="/imgs/RK3399-Tengine/ifconfig.png" alt="ifconfig"></center><br>其中<code>eth0</code>和<code>wlan0</code>分别是有线和无线网络的配置信息，我这里连接的是无线网，可以看到<code>wlan0</code>下有一项<code>inet addr</code>，这是设备在无线网络上的ip地址，把后边这串地址<code>192.168.50.176</code>记下来待会用得上。（如果你接的是有线网络，那么也可以在<code>eth0</code>下找到相应的<code>inet addr</code>地址）<br>推荐一个非常实用的免费远程连接工具：<a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener">MobaXterm</a>     </p><h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><p>烧录的系统镜像本身自带一个ssh服务器<code>openssh-server</code>，不需要我们额外安装。直接打开MobaXterm，点击左上角的Session<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh1.png" alt="Mobaxterm_ssh1">    </p><p>按照下图进行配置——<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh2.png" alt="Mobaxterm_ssh2">       </p><p>配置完就可以通过远程连接到RK3399的终端上——<br><img src="/imgs/RK3399-Tengine/Mobaxterm_ssh3.png" alt="Mobaxterm_ssh3">      </p><p>既可以直接在PC上远程执行指令，也可以方便地在PC和RK3399之间传输文件。     </p><h4 id="vnc"><a href="#vnc" class="headerlink" title="vnc"></a>vnc</h4><p>ssh只能连接到RK3399上的纯文本模式的终端，如果你需要进一步控制RK3399的界面，可以额外安装vnc服务；<br>打开终端，刷新apt源：     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> update</code></pre><p>安装x11vnc：       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> x11vnc</code></pre><p>为vnc服务生成密码（按照提示输入密码，并写入文件）：         </p><pre class=" language-bash"><code class="language-bash">x11vnc -storepasswd</code></pre><p>添加服务：     </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> vim /lib/systemd/system/x11vnc.service</code></pre><p>为x11vnc.service添加以下内容然后保存：</p><pre><code>[Unit]Description=Start x11vnc at startup.After=multi-user.target[Service]Type=simpleExecStart=/usr/bin/x11vnc -auth guess -once -loop -noxdamage -repeat -rfbauth /home/firefly/.vnc/passwd -rfbport 5900 -shared[Install]WantedBy=multi-user.target</code></pre><p>加载服务：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl daemon-reload</code></pre><p>启动服务：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">service</span> x11vnc start</code></pre><p>设置开机自启动：       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl <span class="token function">enable</span> x11vnc.service</code></pre><p>这样一来RK3399上的vnc服务就设置完毕，接下来直接用<code>MobaXterm</code>远程控制桌面；<br>和ssd一样点击左上角的Session选项，切换到vnc选项卡，如下图配置：<br><img src="/imgs/RK3399-Tengine/Mobaxterm_vnc.png" alt="Mobaxterm_vnc"><br>配置完毕后双击并输入刚刚在RK3399上设置的密码就可以远程控制桌面~~        </p><h3 id="安装Tengine"><a href="#安装Tengine" class="headerlink" title="安装Tengine"></a>安装Tengine</h3><p>RK3399的基本环境安顿好之后，接下来可以开始搭建Tengine的环境。       </p><ol><li>安装git      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token function">git</span></code></pre></li><li>用git下载源码     <pre class=" language-bash"><code class="language-bash"> <span class="token function">git</span> clone https://github.com/OAID/tengine</code></pre></li><li>安装编译源码时需要依赖的包      <pre class=" language-bash"><code class="language-bash"> <span class="token function">sudo</span> apt <span class="token function">install</span> libprotobuf-dev protobuf-compiler libboost-all-dev libgoogle-glog-dev libopenblas-dev libopencv-dev</code></pre></li><li>进入Tengine目录，复制编译的配置文件      <pre class=" language-bash"><code class="language-bash"> <span class="token function">cd</span> ~/tengine <span class="token function">cp</span> makefile.config.example makefile.config</code></pre></li><li>编辑<code>makefile.config</code>文件（如果不需要修改配置，可以直接忽略这一步）      <pre class=" language-bash"><code class="language-bash"> vim makefile.config</code></pre> <em>后续需要用到MobileNet SSD网络，其中包含维度交换的<code>Permute</code>层，该层是ACL暂时不支持的，所以这里暂时不建议开启ACL支持</em>     </li><li>编译     <pre class=" language-bash"><code class="language-bash"> <span class="token function">make</span> <span class="token function">make</span> <span class="token function">install</span></code></pre><!--7. 配置相关环境       ```bash sudo mkdir -p /usr/local/AID/Tengine sudo cp -rpf ~/Tengine/install/* /usr/local/AID/Tengine wget ftp://ftp.openailab.net.cn/tools/script/gen-pkg-config-pc.sh chmod +x ./gen-pkg-config-pc.sh sudo ./gen-pkg-config-pc.sh ```--></li></ol><h3 id="小试牛刀：运行Tengine自带的Demo"><a href="#小试牛刀：运行Tengine自带的Demo" class="headerlink" title="小试牛刀：运行Tengine自带的Demo"></a>小试牛刀：运行Tengine自带的Demo</h3><p>Tengine配置完毕，接下来我们试着运行Tengine自带的几个Demo。       </p><h4 id="分类网络SqueezeNet和MobileNet"><a href="#分类网络SqueezeNet和MobileNet" class="headerlink" title="分类网络SqueezeNet和MobileNet"></a>分类网络SqueezeNet和MobileNet</h4><ol><li>运行SqueezeNet<br> <code>./build/tests/bin/bench_sqz -r1</code>——（-r1 代表重复次数）       </li><li>运行MobileNet<br> <code>./build/tests/bin/bench_mobilenet -r1</code>——（-r1 代表重复次数）     </li></ol><p>运行后即可在终端看到输出结果。         </p><h4 id="目标检测网络MobileNet-SSD"><a href="#目标检测网络MobileNet-SSD" class="headerlink" title="目标检测网络MobileNet SSD"></a>目标检测网络MobileNet SSD</h4><p><a href="https://github.com/OAID/Tengine/tree/master/examples/mobilenet_ssd" target="_blank" rel="noopener">Mobilenet_SSD implementation with Tengine | github</a></p><p>在<code>example</code>目录下有一个<code>mobilenet_ssd</code>的子目录，一般情况下在目录执行      </p><pre class=" language-bash"><code class="language-bash">cmake <span class="token keyword">.</span><span class="token function">make</span></code></pre><p>就可以编译目录下的程序，然而……<br><img src="/imgs/RK3399-Tengine/tengine_cmake_not_found.png" alt="tengine_cmake_not_found"><br>好吧，烧录的系统上没有<code>cmake</code>，安装一下：    </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cmake</code></pre><p>不过make的时候又报了错——<br><img src="/imgs/RK3399-Tengine/tengine_headers_not_found.png" alt="tengine_headers_not_found"><br>看起来是找不到tengine的头文件，打开<code>CMakeLists.txt</code>文件瞧瞧，开头部分是这样的——     </p><pre class=" language-bash"><code class="language-bash">cmake_minimum_required <span class="token punctuation">(</span>VERSION 2.8<span class="token punctuation">)</span>project<span class="token punctuation">(</span>MSSD<span class="token punctuation">)</span>set<span class="token punctuation">(</span> INSTALL_DIR <span class="token variable">${TENGINE_DIR}</span>/install/<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_LIBS tengine<span class="token punctuation">)</span><span class="token punctuation">..</span>.</code></pre><p>好像这里引用了一个变量<code>TENGINE_DIR</code>但却没有提前指定，我们给它设置一下，变为——    </p><pre class=" language-bash"><code class="language-bash">cmake_minimum_required <span class="token punctuation">(</span>VERSION 2.8<span class="token punctuation">)</span>project<span class="token punctuation">(</span>MSSD<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_DIR /home/firefly/Tengine <span class="token punctuation">)</span>set<span class="token punctuation">(</span> INSTALL_DIR <span class="token variable">${TENGINE_DIR}</span>/install/<span class="token punctuation">)</span>set<span class="token punctuation">(</span> TENGINE_LIBS tengine<span class="token punctuation">)</span><span class="token punctuation">..</span>.</code></pre><p>再make一下，头文件是找到了，但printf好像有点问题——<br><img src="/imgs/RK3399-Tengine/tengine_printf.png" alt="tengine_printf"><br>打开源代码<code>mssd.cpp</code>，添加头文件<br><code>#include &lt;stdio.h&gt;</code><br>搜索一下<code>prinf</code>，如果<code>printf</code>前有<code>std::</code>就去掉（也就是把<code>std::printf</code>替换为<code>printf</code>），保存后再make一下……诶！通过了~~     </p><p>运行一下<br><code>./MSSD</code>     </p><p><img src="/imgs/RK3399-Tengine/tengine_model_not_found.png" alt="tengine_model_not_found"><br>ummmm没有模型文件，下载一个！<br>Tengine提供了一些训练好的模型——<a href="https://pan.baidu.com/s/1LXZ8vOdyOo50IXS0CUPp8g#list/path=%2F" target="_blank" rel="noopener">Tengine_models | 百度云（提取码：57vb）</a><br>找到<code>mobilenet_ssd</code>文件夹把其中的<code>MobileNetSSD_deploy.prototxt</code>和<code>MobileNetSSD_deploy.caffemodel</code>下载下来放到<code>./models</code>目录下就行，再运行一下<code>./MSSD</code>——<br><img src="/imgs/RK3399-Tengine/tengine_mssd.png" alt="tengine_mssd"><br>没报错，有结果<del>，好了，收工</del>！     </p><p>等等，这些输出什么意思呢？      </p><ul><li>从prototxt文件里读出模型<br>  <code>proto file not specified,using /home/firefly/Tengine/models/MobileNetSSD_deploy.prototxt by default</code></li><li>从caffemodel文件里读出模型参数<br>  <code>model file not specified,using /home/firefly/Tengine/models/MobileNetSSD_deploy.caffemodel by default</code></li><li>读一张<code>ssd_dog.jpg</code>的文件作为输入<br>  <code>image file not specified,using /home/firefly/Tengine/tests/images/ssd_dog.jpg by default</code><br>  这张图片长这样：<br>  <img src="/imgs/RK3399-Tengine/tengine_mssd_input.jpg" alt="tengine_mssd_input">     </li><li>检测出了三个物体：     <pre><code>  repeat 1 times, avg time per run is 161.088 ms  detect ruesult num: 3  dog     :100%  BOX:( 138.529 , 209.238 ),( 324.026 , 541.275 )  car     :100%  BOX:( 466.138 , 72.3095 ),( 688.261 , 171.256 )  bicycle :99%  BOX:( 106.674 , 140.974 ),( 573.514 , 415.127 )</code></pre>  分别是狗、小车、自行车，用时161.088ms       </li><li>最后图片输出到了<code>save.jpg</code><br>  <code>[DETECTED IMAGE SAVED]: save.jpg</code><br>  这张图长这样：<br>  <img src="/imgs/RK3399-Tengine/tengine_mssd_output.jpg" alt="tengine_mssd_output">     </li></ul><p>啊就输入一张图片，输出检测好框好图片的结果。好没意思~改成动态检测的吧！<br>以下是修改后的源码，改动也不大，就是调用摄像头获取图片，处理完之后再输出显示（在RK3399上FPS大概为5-6）。      </p><pre class=" language-cpp"><code class="language-cpp"><span class="token comment" spellcheck="true">/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * License); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * *   http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, * software distributed under the License is distributed on an * AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY * KIND, either express or implied.  See the License for the * specific language governing permissions and limitations * under the License. */</span><span class="token comment" spellcheck="true">/* * Copyright (c) 2018, Open AI Lab * Author: chunyinglv@openailab.com */</span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;iomanip></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;string></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/imgproc/imgproc.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"opencv2/highgui/highgui.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"tengine_c_api.h"</span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">"common.hpp"</span></span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_PROTO "models/MobileNetSSD_deploy.prototxt"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_MODEL "models/MobileNetSSD_deploy.caffemodel"</span><span class="token macro property">#<span class="token directive keyword">define</span> DEF_IMAGE "tests/images/ssd_dog.jpg"</span><span class="token keyword">struct</span> Box<span class="token punctuation">{</span>    <span class="token keyword">float</span> x0<span class="token punctuation">;</span>    <span class="token keyword">float</span> y0<span class="token punctuation">;</span>    <span class="token keyword">float</span> x1<span class="token punctuation">;</span>    <span class="token keyword">float</span> y1<span class="token punctuation">;</span>    <span class="token keyword">int</span> class_idx<span class="token punctuation">;</span>    <span class="token keyword">float</span> score<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// void get_input_data_ssd(std::string&amp; image_file, float* input_data, int img_h,  int img_w)</span><span class="token keyword">void</span> <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> input_data<span class="token punctuation">,</span> <span class="token keyword">int</span> img_h<span class="token punctuation">,</span>  <span class="token keyword">int</span> img_w<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// cv::Mat img = cv::imread(image_file);</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// std::cerr &lt;&lt; "Failed to read image file " &lt;&lt; image_file &lt;&lt; ".\n";</span>        std<span class="token operator">::</span>cerr <span class="token operator">&lt;&lt;</span> <span class="token string">"Failed to read image from camera.\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    cv<span class="token operator">::</span><span class="token function">resize</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    img<span class="token punctuation">.</span><span class="token function">convertTo</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> CV_32FC3<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>img_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span>img<span class="token punctuation">.</span>data<span class="token punctuation">;</span>    <span class="token keyword">int</span> hw <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w<span class="token punctuation">;</span>    <span class="token keyword">float</span> mean<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">,</span><span class="token number">127.5</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> h <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> h <span class="token operator">&lt;</span> img_h<span class="token punctuation">;</span> h<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> w <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> w <span class="token operator">&lt;</span> img_w<span class="token punctuation">;</span> w<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> <span class="token number">3</span><span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span>            <span class="token punctuation">{</span>                input_data<span class="token punctuation">[</span>c <span class="token operator">*</span> hw <span class="token operator">+</span> h <span class="token operator">*</span> img_w <span class="token operator">+</span> w<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.007843</span><span class="token operator">*</span> <span class="token punctuation">(</span><span class="token operator">*</span>img_data <span class="token operator">-</span> mean<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                img_data<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// void post_process_ssd(std::string&amp; image_file,float threshold,float* outdata,int num,std::string&amp; save_name)</span><span class="token keyword">void</span> <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>cv<span class="token operator">::</span>Mat img<span class="token punctuation">,</span> <span class="token keyword">float</span> threshold<span class="token punctuation">,</span><span class="token keyword">float</span><span class="token operator">*</span> outdata<span class="token punctuation">,</span><span class="token keyword">int</span> num<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> class_names<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"background"</span><span class="token punctuation">,</span>                            <span class="token string">"aeroplane"</span><span class="token punctuation">,</span> <span class="token string">"bicycle"</span><span class="token punctuation">,</span> <span class="token string">"bird"</span><span class="token punctuation">,</span> <span class="token string">"boat"</span><span class="token punctuation">,</span>                            <span class="token string">"bottle"</span><span class="token punctuation">,</span> <span class="token string">"bus"</span><span class="token punctuation">,</span> <span class="token string">"car"</span><span class="token punctuation">,</span> <span class="token string">"cat"</span><span class="token punctuation">,</span> <span class="token string">"chair"</span><span class="token punctuation">,</span>                            <span class="token string">"cow"</span><span class="token punctuation">,</span> <span class="token string">"diningtable"</span><span class="token punctuation">,</span> <span class="token string">"dog"</span><span class="token punctuation">,</span> <span class="token string">"horse"</span><span class="token punctuation">,</span>                            <span class="token string">"motorbike"</span><span class="token punctuation">,</span> <span class="token string">"person"</span><span class="token punctuation">,</span> <span class="token string">"pottedplant"</span><span class="token punctuation">,</span>                            <span class="token string">"sheep"</span><span class="token punctuation">,</span> <span class="token string">"sofa"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"tvmonitor"</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// cv::Mat img = cv::imread(image_file);</span>    <span class="token keyword">int</span> raw_h <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>height<span class="token punctuation">;</span>    <span class="token keyword">int</span> raw_w <span class="token operator">=</span> img<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>width<span class="token punctuation">;</span>    std<span class="token operator">::</span>vector<span class="token operator">&lt;</span>Box<span class="token operator">></span> boxes<span class="token punctuation">;</span>    <span class="token keyword">int</span> line_width<span class="token operator">=</span>raw_w<span class="token operator">*</span><span class="token number">0.002</span><span class="token punctuation">;</span>    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"detect ruesult num: %d \n"</span><span class="token punctuation">,</span>num<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>num<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>=</span>threshold<span class="token punctuation">)</span>        <span class="token punctuation">{</span>            Box box<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>class_idx<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>score<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y0<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>x1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_w<span class="token punctuation">;</span>            box<span class="token punctuation">.</span>y1<span class="token operator">=</span>outdata<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">*</span>raw_h<span class="token punctuation">;</span>            boxes<span class="token punctuation">.</span><span class="token function">push_back</span><span class="token punctuation">(</span>box<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%s\t:%.0f%%\n"</span><span class="token punctuation">,</span> class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> box<span class="token punctuation">.</span>score <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"BOX:( %g , %g ),( %g , %g )\n"</span><span class="token punctuation">,</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>x1<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y1<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        outdata<span class="token operator">+</span><span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span>boxes<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        Box box<span class="token operator">=</span>boxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x1<span class="token operator">-</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>y1<span class="token operator">-</span>box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>line_width<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>ostringstream score_str<span class="token punctuation">;</span>        score_str<span class="token operator">&lt;&lt;</span>box<span class="token punctuation">.</span>score<span class="token punctuation">;</span>        std<span class="token operator">::</span>string label <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">string</span><span class="token punctuation">(</span>class_names<span class="token punctuation">[</span>box<span class="token punctuation">.</span>class_idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">": "</span> <span class="token operator">+</span> score_str<span class="token punctuation">.</span><span class="token function">str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> baseLine <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span>Size label_size <span class="token operator">=</span> cv<span class="token operator">::</span><span class="token function">getTextSize</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>baseLine<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">rectangle</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Rect</span><span class="token punctuation">(</span>cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span>box<span class="token punctuation">.</span>y0<span class="token operator">-</span> label_size<span class="token punctuation">.</span>height<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  cv<span class="token operator">::</span><span class="token function">Size</span><span class="token punctuation">(</span>label_size<span class="token punctuation">.</span>width<span class="token punctuation">,</span> label_size<span class="token punctuation">.</span>height <span class="token operator">+</span> baseLine<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CV_FILLED<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">putText</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> label<span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Point</span><span class="token punctuation">(</span>box<span class="token punctuation">.</span>x0<span class="token punctuation">,</span> box<span class="token punctuation">.</span>y0<span class="token punctuation">)</span><span class="token punctuation">,</span>                    cv<span class="token operator">::</span>FONT_HERSHEY_SIMPLEX<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> cv<span class="token operator">::</span><span class="token function">Scalar</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// cv::imwrite(save_name,img);</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"======================================\n";</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"[DETECTED IMAGE SAVED]:\t"&lt;&lt; save_name&lt;&lt;"\n";</span>    <span class="token comment" spellcheck="true">// std::cout&lt;&lt;"======================================\n";</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">const</span> std<span class="token operator">::</span>string root_path <span class="token operator">=</span> <span class="token function">get_root_path</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>string proto_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string model_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string image_file<span class="token punctuation">;</span>    std<span class="token operator">::</span>string save_name<span class="token operator">=</span><span class="token string">"save.jpg"</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> res<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span> <span class="token punctuation">(</span> res<span class="token operator">=</span><span class="token function">getopt</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span>argv<span class="token punctuation">,</span><span class="token string">"p:m:i:h"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">switch</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token keyword">case</span> <span class="token string">'p'</span><span class="token operator">:</span>                proto_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'m'</span><span class="token operator">:</span>                model_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'i'</span><span class="token operator">:</span>                image_file<span class="token operator">=</span>optarg<span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token keyword">case</span> <span class="token string">'h'</span><span class="token operator">:</span>                std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"[Usage]: "</span> <span class="token operator">&lt;&lt;</span> argv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" [-h]\n"</span>                          <span class="token operator">&lt;&lt;</span> <span class="token string">"   [-p proto_file] [-m model_file] [-i image_file]\n"</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>            <span class="token keyword">default</span><span class="token operator">:</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>model_name <span class="token operator">=</span> <span class="token string">"mssd_300"</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>proto_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        proto_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_PROTO<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"proto file not specified,using "</span><span class="token operator">&lt;&lt;</span>proto_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>model_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        model_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_MODEL<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"model file not specified,using "</span><span class="token operator">&lt;&lt;</span>model_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>image_file<span class="token punctuation">.</span><span class="token function">empty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        image_file <span class="token operator">=</span> root_path <span class="token operator">+</span> DEF_IMAGE<span class="token punctuation">;</span>        std<span class="token operator">::</span>cout<span class="token operator">&lt;&lt;</span> <span class="token string">"image file not specified,using "</span><span class="token operator">&lt;&lt;</span>image_file<span class="token operator">&lt;&lt;</span> <span class="token string">" by default\n"</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// init tengine</span>    <span class="token function">init_tengine_library</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">request_tengine_version</span><span class="token punctuation">(</span><span class="token string">"0.1"</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">load_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> <span class="token string">"caffe"</span><span class="token punctuation">,</span> proto_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">.</span><span class="token function">c_str</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"load model done!\n"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// create graph</span>    graph_t graph <span class="token operator">=</span> <span class="token function">create_runtime_graph</span><span class="token punctuation">(</span><span class="token string">"graph"</span><span class="token punctuation">,</span> model_name<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_graph_valid</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"create graph0 failed\n"</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment" spellcheck="true">// input</span>    <span class="token keyword">int</span> img_h <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_w <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> img_size <span class="token operator">=</span> img_h <span class="token operator">*</span> img_w <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>input_data <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">*</span> img_size<span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span>VideoCapture <span class="token function">capture</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_WIDTH<span class="token punctuation">,</span> <span class="token number">1920</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    capture<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>CV_CAP_PROP_FRAME_HEIGHT<span class="token punctuation">,</span> <span class="token number">1080</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    cv<span class="token operator">::</span>Mat frame<span class="token punctuation">;</span>    <span class="token keyword">int</span> node_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> tensor_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>    tensor_t input_tensor <span class="token operator">=</span> <span class="token function">get_graph_input_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> node_idx<span class="token punctuation">,</span> tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">check_tensor_valid</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Get input node failed : node_idx: %d, tensor_idx: %d\n"</span><span class="token punctuation">,</span>node_idx<span class="token punctuation">,</span>tensor_idx<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">int</span> dims<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token function">set_tensor_shape</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> dims<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">prerun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> repeat_count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>repeat <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">"REPEAT_COUNT"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>repeat<span class="token punctuation">)</span>        repeat_count <span class="token operator">=</span> std<span class="token operator">::</span><span class="token function">strtoul</span><span class="token punctuation">(</span>repeat<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">float</span> <span class="token operator">*</span>outdata<span class="token punctuation">;</span>    <span class="token keyword">int</span> out_dim<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">struct</span> timeval t0<span class="token punctuation">,</span> t1<span class="token punctuation">;</span>        <span class="token keyword">float</span> total_time <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>f<span class="token punctuation">;</span>        capture <span class="token operator">>></span> frame<span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> repeat_count<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>        <span class="token punctuation">{</span>            <span class="token function">get_input_data_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span>  img_w<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t0<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">set_tensor_buffer</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> input_data<span class="token punctuation">,</span> img_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">run_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t1<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">float</span> mytime <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>t1<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t1<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span>t0<span class="token punctuation">.</span>tv_sec <span class="token operator">*</span> <span class="token number">1000000</span> <span class="token operator">+</span> t0<span class="token punctuation">.</span>tv_usec<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">1000</span><span class="token punctuation">;</span>            total_time <span class="token operator">+</span><span class="token operator">=</span> mytime<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"--------------------------------------\n"</span><span class="token punctuation">;</span>        std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"repeat "</span> <span class="token operator">&lt;&lt;</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" times, avg time per run is "</span> <span class="token operator">&lt;&lt;</span> total_time <span class="token operator">/</span> repeat_count <span class="token operator">&lt;&lt;</span> <span class="token string">" ms\n"</span><span class="token punctuation">;</span>        tensor_t out_tensor <span class="token operator">=</span> <span class="token function">get_graph_output_tensor</span><span class="token punctuation">(</span>graph<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//"detection_out");</span>        <span class="token function">get_tensor_shape</span><span class="token punctuation">(</span> out_tensor<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        outdata <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">get_tensor_buffer</span><span class="token punctuation">(</span>out_tensor<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> num<span class="token operator">=</span>out_dim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">float</span> show_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">;</span>        <span class="token function">post_process_ssd</span><span class="token punctuation">(</span>frame<span class="token punctuation">,</span> show_threshold<span class="token punctuation">,</span> outdata<span class="token punctuation">,</span> num<span class="token punctuation">)</span><span class="token punctuation">;</span>        cv<span class="token operator">::</span><span class="token function">imshow</span><span class="token punctuation">(</span><span class="token string">"MSSD"</span><span class="token punctuation">,</span> frame<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> cv<span class="token operator">::</span><span class="token function">waitKey</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'q'</span> <span class="token punctuation">)</span>            <span class="token keyword">break</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token function">postrun_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">free</span><span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">destroy_runtime_graph</span><span class="token punctuation">(</span>graph<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">remove_model</span><span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>报错，<br><img src="/imgs/RK3399-Tengine/opengl_error.png" alt="opengl_error"><br>烧录的系统没带opengl，没法调用opencv的imshow，树莓派也有一样的问题，安装 <code>libgl1-mesa-dri</code> 然后重启板子就能解决。       </p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> libgl1-mesa-dri<span class="token function">sudo</span> <span class="token function">reboot</span></code></pre><hr><p>本篇文章中我们在RK3399上搭建了Tengine平台并试运行了MobileNet SSD网络，接下来我们将细致解析MobileNets分类网络和SSD目标检测框架，最后进一步解析源码作者<strong><a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="noopener">chuanqi305</a></strong>是如何把MobileNets和SSD结合起来的。      </p><ul><li><a href="/2018/08/05/MobileNets_v1">MobileNets v1模型解析 | Hey~YaHei!</a>       </li><li><a href="/2018/08/06/SSD">SSD框架解析 | Hey~YaHei!</a>      </li><li><a href="/2018/08/08/MobileNets-SSD/">MobileNet-SSD网络解析 | Hey~YaHei!</a>     </li></ul><p>随后还将结合实际的使用场景，尝试对MobileNet-SSD的网络结构以及训练参数细节进行分析优化~      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tengine </tag>
            
            <tag> RK3399 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>自编码器</title>
      <link href="/2018/04/25/autoencoder/"/>
      <url>/2018/04/25/autoencoder/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=489970553&auto=0&height=32"></iframe><br>BGM：<strong>《末日时在做什么？有没有空？可以来拯救吗？》第九话插入曲</strong><br>这番名字真是狗血，但剧情和音乐出奇的不错       </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap15<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><p>自编码器工作方式非常简单，就是学习如何模仿输入来产生输出；<br>我们会采取各种约束（比如限制输出大小、加噪等）来避免自编码器纯粹地把输入作为输出，从而得到一个高效的数据表示方式；<br>简而言之，自编码器通过尝试学习某些约束下的特征函数来产生输入的编码，也即一种高效的数据表示；<br>自编码器可以无监督学习输入数据的编码方式、降低数据维度、作为特征检测器、作为生成模型等等……      </p><h3 id="数据的高效表示"><a href="#数据的高效表示" class="headerlink" title="数据的高效表示"></a>数据的高效表示</h3><p>论文 <a href="https://www.sciencedirect.com/science/article/pii/0010028573900042" target="_blank" rel="noopener">Perception in chess(1973)</a> 研究了记忆、概念、模式匹配之间的联系；     </p><p>自编码器可以分为Encoder和Decoder两部分，<br>Encoder也称识别网络，用于将输入转换为某种内部表示；<br>Decoder也称生成网络，用于将内部表示转换成输出；<br>架构跟MLP一样，不过他的<strong>输出神经元数与输入数相等</strong>，<strong>中间层的神经元数小于输入数</strong>；<br>也就是说，中间层的输出必定是输入的一个不完全表示，我们的目的就在于训练出一个输出与输入相近的网络——<br><strong>可以理解为Encoder是对输入的一个有损压缩，Decoder对其进行解压，我们要训练一个损耗率尽可能小的网络</strong><br><img src="/Handson-ML/15Simple_Autoencoder.png" alt="15Simple_Autoencoder">       </p><h3 id="简单的线性自编码器"><a href="#简单的线性自编码器" class="headerlink" title="简单的线性自编码器"></a>简单的线性自编码器</h3><p>无非线性激活，MSE损失函数，可以实现一个PCA；       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> fully_connected<span class="token comment" spellcheck="true"># 三维输入编码为二维表示</span>n_inputs <span class="token operator">=</span> <span class="token number">3</span>n_hidden <span class="token operator">=</span> <span class="token number">2</span>n_outputs <span class="token operator">=</span> n_inputslearning_rate <span class="token operator">=</span> <span class="token number">0.01</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>hidden <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 纯线性，无激活</span>outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># MSE</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>reconstruction_loss<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>X_train<span class="token punctuation">,</span> X_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>     <span class="token comment" spellcheck="true"># 载入数据集</span>n_iterations <span class="token operator">=</span> <span class="token number">1000</span>codings <span class="token operator">=</span> hidden     <span class="token comment" spellcheck="true"># 自编码器的目的是获取编码，也即中间层的输出</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        training_op<span class="token punctuation">.</span>run<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_train<span class="token punctuation">}</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 无监督</span>    codings_val <span class="token operator">=</span> codings<span class="token punctuation">.</span>eval<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_test<span class="token punctuation">}</span><span class="token punctuation">)</span></code></pre><h3 id="深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）"><a href="#深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）" class="headerlink" title="深层自编码器（Deep autoencoders, or Stacked autoencoders）"></a>深层自编码器（Deep autoencoders, or Stacked autoencoders）</h3><p>深层自编码器为中心对称的“三明治”结构，最中间的一层产生实际的编码，称编码层（Coding Layer），如——<br><img src="/Handson-ML/15Stacked_Autoencoder.png" alt="15Stacked_Autoencoder">       </p><p>深度学习的训练技术（如<a href="/2018/04/08/梯度消失与梯度爆炸">解决梯度爆炸与梯度消失的技术</a>、<a href="/2018/04/09/复用预训练层">复用预训练层</a>、<a href="/2018/04/10/优化器">优化器</a>、<a href="/2018/04/11/正则化技术">正则化技术</a>等）依旧适用；<br>与之前讲述的网络的区别在于，自编码器没有标注，是无监督学习     </p><h3 id="权重共享"><a href="#权重共享" class="headerlink" title="权重共享"></a>权重共享</h3><p>由于结构是中心对称的，Encoder和Decoder可以直接共享权重，但tensorflow中的 <code>fully_connected()</code> 没法共享权重，所以需要手动书写全连接层——    </p><pre class=" language-python"><code class="language-python">activation <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>eluregularizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>l2_regularizer<span class="token punctuation">(</span>l2_reg<span class="token punctuation">)</span>initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>weights1_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_inputs<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">]</span><span class="token punctuation">)</span>weights2_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">]</span><span class="token punctuation">)</span>weights1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights1_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights1"</span><span class="token punctuation">)</span>weights2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights2_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights2"</span><span class="token punctuation">)</span>weights3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights2<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights3"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>weights4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights1<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights4"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>biases1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden1<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases1"</span><span class="token punctuation">)</span>biases2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden2<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases2"</span><span class="token punctuation">)</span>biases3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden3<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases3"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>biases4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases4"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>hidden1 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> weights1<span class="token punctuation">)</span> <span class="token operator">+</span> biases1<span class="token punctuation">)</span>hidden2 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>hidden3 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> weights3<span class="token punctuation">)</span> <span class="token operator">+</span> biases3<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden3<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 只对weights1和weights2施加正则化</span>loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> reg_lossoptimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="逐层训练"><a href="#逐层训练" class="headerlink" title="逐层训练"></a>逐层训练</h3><p>逐层训练然后将各层堆叠起来，要比直接训练一个堆叠好的自编码器要快得多；<br>比如训练一个三层的自编码器：     </p><ol><li>只保留第一隐藏层进行训练    </li><li>叠加第二、第三隐藏层进行训练（<em>这里第一隐藏层和第三隐藏层的权重不共享？</em>）       </li><li>各层训练完毕，叠加起来得到一个完整的自编码器       </li></ol><p><img src="/Handson-ML/15Training_One_Autoencoder_at_A_Time.png" alt="15Training_One_Autoencoder_at_A_Time">     </p><p>可以按照这个描述，分别构造多个计算图来进行训练；<br>更巧妙的方式是添加一些操作在同一张计算图中区分训练阶段——<br><img src="/Handson-ML/15A_Single_Graph_To_Train_A_Stacked_Autoencoder.png" alt="15A_Single_Graph_To_Train_A_Stacked_Autoencoder">     </p><ol><li>中间部分是一个完整的自编码器   </li><li>左侧是第一训练阶段，旁路了第二和第三隐藏层，<code>Phase 1 Outputs</code> 跟 完整模型中的 <code>Outputs</code> 是共享参数的；<br> 这一阶段目标是使得最终输出与输入接近，训练 <code>Hidden 1</code> 和 <code>Outputs</code> 的权重      </li><li>右侧是第二训练阶段，旁路了输出层，第一隐藏层参数固定，只训练第二和第三隐藏层<br> 这一阶段目标是使得 <code>Hidden 3</code> 的输出与 <code>Hidden 1</code> 的输出接近，训练 <code>Hidden 2</code> 和 <code>Hidden 3</code> 的权重       </li></ol><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Build the whole stacked autoencoder normally.</span><span class="token comment" spellcheck="true"># In this example, the weights are not tied.</span><span class="token comment" spellcheck="true"># [...] </span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 阶段一的输出层，与完整模型的输出层共享参数</span>    phase1_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4    phase1_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>phase1_outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    phase1_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights4<span class="token punctuation">)</span>    phase1_loss <span class="token operator">=</span> phase1_reconstruction_loss <span class="token operator">+</span> phase1_reg_loss    phase1_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase1_loss<span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    phase2_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3 <span class="token operator">-</span> hidden1<span class="token punctuation">)</span><span class="token punctuation">)</span>    phase2_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights3<span class="token punctuation">)</span>    phase2_loss <span class="token operator">=</span> phase2_reconstruction_loss <span class="token operator">+</span> phase2_reg_loss    <span class="token comment" spellcheck="true"># 忽略weights1和biases1</span>    train_vars <span class="token operator">=</span> <span class="token punctuation">[</span>weights2<span class="token punctuation">,</span> biases2<span class="token punctuation">,</span> weights3<span class="token punctuation">,</span> biases3<span class="token punctuation">]</span>    phase2_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase2_loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>train_vars<span class="token punctuation">)</span></code></pre><p>由于阶段一已经计算了 <code>hidden 1</code> 的输出，而且阶段二中 <code>hidden 1</code> 参数固定；<br>在内存足够的情况下可以先计算出整个batch上的输出并保留，以减少阶段二的训练时间；<br>这与<a href="/2018/04/09/复用预训练层/#加速训练：复用frozen层的输出结果（牺牲空间）">复用预训练层 - 复用frozen层输出加速训练</a>类似      </p><h4 id="重构效果可视化"><a href="#重构效果可视化" class="headerlink" title="重构效果可视化"></a>重构效果可视化</h4><p>直接显示压缩再解压后的结果，与输入进行直观的比较进行判断；<br>可以初步判断重构的效果       </p><h4 id="特征可视化"><a href="#特征可视化" class="headerlink" title="特征可视化"></a>特征可视化</h4><p>对于高层神经元，尤其是最后一个隐藏层的神经元，可以直接观察特定的输入时哪些神经元激活程度比较高；<br>但底层神经元关注的是更抽象、更小的特征，是我们无法直接理解的特征；   </p><ol><li>用权重分布图来观察每一个神经元的关注点<br> 比如观察第一隐藏层前五个神经元的关注点：     <pre class=" language-python"><code class="language-python"> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># train autoencoder</span>     weights1_val <span class="token operator">=</span> weights1<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>     plot_image<span class="token punctuation">(</span>weights1_val<span class="token punctuation">.</span>T<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre> <img src="/Handson-ML/15Visualzing_The_Feature.png" alt="15Visualzing_The_Feature"><br> <em>【越是关注的像素点，权重越大，在灰度图中就越亮】</em><br> 可以看到前四个神经元关注的都是某些局部的小区域，第五个神经元则似乎更关注竖直的笔画       </li><li>随机输入一个图像，然后用反向传播不断改变图像以最大化某个神经元的激活程度<br> 经过一定的迭代次数之后，图像将被扭曲为明显带有该神经元所关注特征的图像       </li><li>如果用自编码器用于某些任务（如分类任务）的前期无监督预训练<br> 那么可以直接通过观察这些任务的最终表现，来判断自编码器的效果      </li></ol><h3 id="用自编码器作无监督预训练"><a href="#用自编码器作无监督预训练" class="headerlink" title="用自编码器作无监督预训练"></a>用自编码器作无监督预训练</h3><p><a href="/2018/04/09/复用预训练层/#无监督预训练">《Handson-ML》笔记 - 无监督预训练</a><br>论文：<a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep" target="_blank" rel="noopener">Greedy Layer-Wise Training of Deep Networks(2006)</a>   </p><p><img src="/Handson-ML/15Unsupervised_Pretraining.png" alt="15Unsupervised_Pretraining">       </p><h3 id="不同约束下的自编码器"><a href="#不同约束下的自编码器" class="headerlink" title="不同约束下的自编码器"></a>不同约束下的自编码器</h3><p>添加约束，避免自编码器纯粹地把输入作为输出，从而得到一个更加高效的数据表示方式       </p><h4 id="降噪自编码器（Denoising-Autoencoders）"><a href="#降噪自编码器（Denoising-Autoencoders）" class="headerlink" title="降噪自编码器（Denoising Autoencoders）"></a>降噪自编码器（Denoising Autoencoders）</h4><p>论文：<br><a href="https://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf" target="_blank" rel="noopener">Extracting and Composing Robust Features with Denoising Autoencoders(2008)</a> 提出自编码器可以用于特征提取；<br><a href="http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf" target="_blank" rel="noopener">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(2010)</a> 提出降噪自编码器；      </p><p>降噪自编码器通过<strong>对输入加入高斯噪声</strong>或者<strong>在输入之后紧接一个dropout层</strong>，可以有效的避免输入噪声对模型的影响；<br><img src="/Handson-ML/15Denoising_Autoencoders.png" alt="15Denoising_Autoencoders">        </p><p>具体实现：<br>【高斯噪声方案】      </p><pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>X_noisy <span class="token operator">=</span> X <span class="token operator">+</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 其他层</span></code></pre><p>【dropout层方案】    </p><pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.7</span>is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder_with_default<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>X_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 其他层</span></code></pre><h4 id="稀疏自编码器（Sparse-Autoencoders）"><a href="#稀疏自编码器（Sparse-Autoencoders）" class="headerlink" title="稀疏自编码器（Sparse Autoencoders）"></a>稀疏自编码器（Sparse Autoencoders）</h4><p>基本思想是为损失函数添加适当的稀疏度损失项，使得对于每个输入，编码层只有一小部分神经元被显著激活；<br>这使得编码器可以更好地提取出特征；<br><em>如果只让你用一句话描述自己的想法，那你可能会深思熟虑如何更好的表达</em>      </p><p>在每次迭代前，都必须先评估编码层的实际稀疏程度——计算编码层中活跃神经元的平均数量；<br>为了获取比较准确的平均数量，batch的大小一定不能太小；       </p><p>接下来我们为损失函数添加一个关于神经元活跃程度的惩罚项，比如用MSE；<br>但更好的方式是用具有更大梯度的<strong>KL散度（Kullback–Leibler divergence）</strong>；        </p><p><img src="/Handson-ML/15Sparsity loss.png" alt="15Sparsity loss">     </p><p>对于两个离散概率分布P和Q，KL散度表示为——<br>$$ D_{KL}(P||Q) = \sum_i P(i) log \frac{P(i)}{Q(i)} $$       </p><p>具体到稀疏自编码器上，对于评估的激活概率q和目标激活概率p，KL散度为（激活/不激活 是二项分布的）——<br>$$ D_{KL}(p||q) = p log \frac{p}{q} + (1-p) log \frac{1-p}{1-q} $$       </p><p>一旦计算出编码层上每个神经元的稀疏度损失，就可以把他们都加和到损失函数上进行训练；<br>为了权衡稀疏度损失和重构损失的重要性，可以为加和的稀疏度损失额外添加一个数值合适的权重超参数进行训练；       </p><p>具体实现：    </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算KL散度</span><span class="token keyword">def</span> <span class="token function">kl_divergence</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> p <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p <span class="token operator">/</span> q<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span>learning_rate <span class="token operator">=</span> <span class="token number">0.01</span>sparsity_target <span class="token operator">=</span> <span class="token number">0.1</span>sparsity_weight <span class="token operator">=</span> <span class="token number">0.2</span><span class="token comment" spellcheck="true"># [...] # Build a normal autoencoder (in this example the coding layer is hidden1)</span><span class="token comment" spellcheck="true"># 注意：编码层的激活程度必须是在(0,1)区间的</span><span class="token comment" spellcheck="true">#    比如可以用sigmoid函数强制激活程度归一化为(0,1)区间的数值</span><span class="token comment" spellcheck="true">#    hidden1 = tf.nn.sigmoid(tf.matmul(X, weights1) + biases1)</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>hidden1_mean <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 计算整个batch上的平均值</span>sparsity_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kl_divergence<span class="token punctuation">(</span>sparsity_target<span class="token punctuation">,</span> hidden1_mean<span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算稀疏度损失总和</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算重构损失（MSE）</span>loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> sparsity_weight <span class="token operator">*</span> sparsity_loss    <span class="token comment" spellcheck="true"># 计算全局损失</span>training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span></code></pre><p>把重构损失 <code>reconstruction_loss</code> 的计算改为交叉熵可以加速收敛，但要注意交叉熵要求输入归一化，因此——     </p><pre class=" language-python"><code class="language-python">logits <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 训练时outputs不是必要的，只是为了看到重构结果才计算outputs</span>reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span> <span class="token punctuation">)</span></code></pre><h4 id="变分自编码器（Variational-Autoencoders）"><a href="#变分自编码器（Variational-Autoencoders）" class="headerlink" title="变分自编码器（Variational Autoencoders）"></a>变分自编码器（Variational Autoencoders）</h4><p>论文：<a href="https://arxiv.org/pdf/1312.6114v10.pdf" target="_blank" rel="noopener">Auto-Encoding Variational Bayes(2014)</a>     </p><p>主要特点：     </p><ol><li>概率自编码器，输出是有一定偶然性的，即使是训练完之后      </li><li>生成自编码器，能够生成类似他们在训练集上采样的实例     </li></ol><p>这跟RBMs有些类似，但变分自编码器更加容易训练而且采样更快！     </p><p>其结构如下所示，编码层不再直接输出编码，而是在 $\mu$ 附近随机采样——<br><img src="/Handson-ML/15Variational_Autoencoder.png" alt="15Variational_Autoencoder">    </p><p>比如下图的输入数据，编码层将在以 $\mu$ 为中心，半径为 $\sigma$ 的范围内随机采样作为编码结果；<br><img src="/Handson-ML/15Variational_Autoencoder_Instance.png" alt="15Variational_Autoencoder_Instance">    </p><p>损失函数分为两个部分：    </p><ol><li>重构损失    </li><li><p>隐藏损失：即编码层上的在高斯分布下的损失（这一部分用高斯分布的目标分布和实际分布的KL散度来表示）<br> 高斯分布的噪声使传输给编码层的信息数量受限，迫使网络学习一些有意义的特征；<br> 这在数学计算上复杂了不少，不过可以用下列这个式子进行简化——<br> $$ L_l = \frac{1}{2} \sum \sigma^2 + \mu^2 - 1 - log(eps + \sigma^2) $$<br> 通常取 $eps=10^{-10}$，用于防止 $log(0)$ 的情况出现；<br> 有一种变种的损失函数——<br> $$ L_l = \frac{1}{2} \sum e^\gamma + \mu^2 - 1 - \gamma $$<br> 其中 $\gamma = log(\sigma^2)$，即 $\sigma = e^{\gamma / 2}$；<br> 该变种使得不同尺度下的 $\sigma$ 更容易被捕获，从而加速收敛；        </p><pre class=" language-python"><code class="language-python"> <span class="token comment" spellcheck="true"># [...] 超参数</span> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>framework<span class="token punctuation">.</span>arg_scope<span class="token punctuation">(</span>         <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>         activation_fn<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu<span class="token punctuation">,</span>         weights_initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># [...] 前层，hidden3为编码层</span>     hidden3_mean <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 从hidden2习得平均值</span>     hidden3_gamma <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 从hidden2习得gamma（与标准差相关）</span>     hidden3_sigma <span class="token operator">=</span> tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> hidden3_gamma<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算标准差sigma</span>     noise <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>hidden3_sigma<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 高斯分布噪声</span>     hidden3 <span class="token operator">=</span> hidden3_mean <span class="token operator">+</span> hidden3_sigma <span class="token operator">*</span> noise  <span class="token comment" spellcheck="true"># 在以平均值为中心，标准差为半径的范围内随机采样</span>     <span class="token comment" spellcheck="true"># [...] 后层</span>     logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden5<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 用于计算损失函数</span>     outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 实际输出</span> <span class="token comment" spellcheck="true"># 计算重构损失</span> reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>                         tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算隐藏损失</span> latent_loss <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>                         tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>hidden3_gamma<span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3_mean<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> hidden3_gamma<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 计算全局损失</span> cost <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> latent_loss <span class="token comment" spellcheck="true"># [...] 优化器等训练过程略</span></code></pre></li></ol><h4 id="其他自编码器"><a href="#其他自编码器" class="headerlink" title="其他自编码器"></a>其他自编码器</h4><ul><li>Contractive autoencoder(CAE)<br>  论文：<a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" target="_blank" rel="noopener">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction(2011)</a><br>  关于输入的编码上的导数比较小，使得两个相似的输入得到相似的编码         </li><li>Stacked convolutioncal autoencoders<br>  论文：<a href="http://people.idsia.ch/~ciresan/data/icann2011.pdf" target="_blank" rel="noopener">Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction(2011)</a><br>  通过用卷积层来重构图像的方式来提取图像特征     </li><li>Generative stochastic network(GSN)<br>  论文：<a href="https://arxiv.org/pdf/1503.05571v2.pdf" target="_blank" rel="noopener">GSNs: Generative Stochastic Networks(2015)</a><br>  降噪自编码器的一般化，能够生成数据       </li><li>Winner-take-all(WTA) autoencoder<br>  论文：<a href="https://arxiv.org/pdf/1409.2752v2.pdf" target="_blank" rel="noopener">Winner-Take-All Autoencoders(2015)</a><br>  训练过程中，计算时只保留激活程度前k%的神经元的激活程度，其余都置为0；<br>  这将稀疏化编码，类似的WTA方法也可以应用于生成稀疏化的卷积自编码器；      </li><li>Adversarial autoencoders<br>  论文：<a href="https://arxiv.org/pdf/1511.05644v2.pdf" target="_blank" rel="noopener">Adversarial Autoencoders(2016)</a><br>  分为两个网络，一个训练来重构输入的数据，与此同时另外一个训练来找到前者重构效果不好的输入数据；<br>  以此来迫使前者学习出鲁棒性比较好的编码方式      </li></ul>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>循环神经网络RNN</title>
      <link href="/2018/04/22/RNN/"/>
      <url>/2018/04/22/RNN/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=30431340&auto=0&height=32"></iframe><br>BGM：<strong>《Aldnoah Zero》ED1</strong><br>歌词混好几种语言       </p><hr><p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap14<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p><blockquote><p>【占坑待填】<br>Handson ML对RNN的介绍比较简略，先占着坑          </p></blockquote><h3 id="Basic-RNNs的tensorflow实现"><a href="#Basic-RNNs的tensorflow实现" class="headerlink" title="Basic RNNs的tensorflow实现"></a>Basic RNNs的tensorflow实现</h3><ul><li><a href="/Handson-ML/14.1Static_Unrolling_Through_Time(Plain).html">Jupyter - Static Unrolling Through Time(Plain)</a><br>  底层操作实现的静态展开</li><li><a href="/Handson-ML/14.2Unrolling_Through_Time(High-level_API).html">Jupyter - Unrolling Through Time(High-level_API)</a><br>  高层操作实现的静态展开和动态展开          </li><li><a href="/Handson-ML/14.3Handling_Variable_Length_Sequences.html">Jupyter - Handling Variable Length Sequences</a><br>  处理变长的输入输出，另外还有一种办法，参见<a href="#机器翻译（Encoder-Decoder）">5.2 机器翻译（Encoder-Decoder）</a>       </li></ul><h3 id="训练RNN（BackproPagation-Through-Time-BPTT）"><a href="#训练RNN（BackproPagation-Through-Time-BPTT）" class="headerlink" title="训练RNN（BackproPagation Through Time, BPTT）"></a>训练RNN（BackproPagation Through Time, BPTT）</h3><p>一般将RNN按时间展开，然后使用常规的反向传播进行训练；<br>如下图所示：      </p><p><img src="/Handson-ML/14BPTT.png" alt="BPTT">        </p><p>注意这里损失函数 <code>C(·)</code> 是由最后几个输出计算得到的，而不是最后一个输出！      </p><h4 id="序列分类器"><a href="#序列分类器" class="headerlink" title="序列分类器"></a>序列分类器</h4><p>类似CNN做MNIST分类器，RNN也可以实现手写数字的识别；<br>按照《Handson-ML》，准确率也可以达到98%以上      </p><h4 id="预测时间序列"><a href="#预测时间序列" class="headerlink" title="预测时间序列"></a>预测时间序列</h4><p>比如股价预测——<br>随机截取一些连续的20个时间点的股价作为mini-batch，并且右移1个时间点作为标注；<br>即训练一个预测下一个时间点的股价的模型；<br><img src="/Handson-ML/14Stock_Price_Prediction.png" alt="14Stock_Price_Prediction">       </p><p>按前述直接用动态展开的BasicRNNCell组成的RNN网络——    </p><pre class=" language-python"><code class="language-python">n_steps <span class="token operator">=</span> <span class="token number">20</span>n_inputs <span class="token operator">=</span> <span class="token number">1</span>n_neurons <span class="token operator">=</span> <span class="token number">100</span>n_outputs <span class="token operator">=</span> <span class="token number">1</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>但是此时的网络输出outputs是一个包含100元素（<code>n_neurons=100</code>）的向量，<br>而我们需要的只是一个元素，即使下一时刻的预测值，最简单的思路是对cell进行包装：<br>其实就是在BasicRNNCell之后再加一个fully connected层（无激活函数）<br><img src="/Handson-ML/14OutputProjectionWrapper.png" alt="OutputProjectionWrapper">           </p><p>具体实现——         </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [...]</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>OutputProjectionWrapper<span class="token punctuation">(</span>basic_cell<span class="token punctuation">,</span> output_size<span class="token operator">=</span>n_outputs<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>此时的outputs就是一个单元素的输出啦~<br>尽管OutputProjectionWrapper可以解决这一问题，但每一步的神经元输出都需要过一层FC，效率不是很高；<br>更好的办法是（只需要过一次FC）：      </p><ol><li>将RNN的输出从 <code>[batch_size, n_steps, n_neurons]</code> 重整为 <code>[batch_size * n_steps, n_neurons]</code>；        </li><li>再通过一个fully connected整合成 <code>[batch_size * n_steps, n_outputs]</code> 大小的输出；        </li><li>最后展开成 <code>[batch_size, n_steps, n_outputs]</code> 大小的最终输出<br><img src="/Handson-ML/14More_Efficient_Method.png" alt="14More_Efficient_Method">       </li></ol><p>具体实现——      </p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># [...]</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>basic_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将RNN的输出从 [batch_size, n_steps, n_neurons] 重整为 [batch_size * n_steps, n_neurons]</span>stacked_rnn_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>rnn_outputs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_neurons<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 通过一个fully connected整合成 [batch_size * n_steps, n_outputs] 大小的输出</span>stacked_outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>stacked_rnn_outputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 最后展开成 [batch_size, n_steps, n_outputs] 大小的最终输出</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>stacked_outputs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h3><p>直接堆叠RNN就可以得到一个深层的RNN——<br><img src="/Handson-ML/14Deep_RNN.png" alt="14Deep_RNN">         </p><p>具体实现——<br>借助 <code>tf.contrib.rnn.MultiRNNCell()</code> 可以将多个RNN堆叠起来       </p><pre class=" language-python"><code class="language-python">n_neurons <span class="token operator">=</span> <span class="token number">100</span>n_layers <span class="token operator">=</span> <span class="token number">3</span>basic_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span>multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>basic_cell<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span>outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>此时 <code>states</code> 是一个元组，每个元素都是每一层对应的大小为 <code>[batch_size, n_neurons]</code> 的Tensor；<br>如果为 <code>MultiRNNCell</code> 指定参数 <code>state_is_tuple=False</code> ，那么 <code>states</code> 就只是一个 <code>[batch_size, n_layers * n_neurons]</code> 大小的Tensor；      </p><h4 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h4><p><em>……暂略……</em>    </p><h4 id="使用Dropout"><a href="#使用Dropout" class="headerlink" title="使用Dropout"></a>使用Dropout</h4><p>如果只是在RNN层之前或者之后，可以直接添加Dropout层【可以参见<a href="/2018/04/11/正则化技术/#dropout">正则化技术 - Dropout</a>】；<br>但如果是在RNN层与RNN层之间添加Dropout，就必须使用 <code>DropoutWrapper</code>：        </p><pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.5</span>cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用DropoutWrapper包装BasicRNNCell</span><span class="token comment" spellcheck="true"># ... input_keep_prob参数是输入前dropout层的keep_prob（不指定则步添加dropout）</span><span class="token comment" spellcheck="true"># ... 相应的还有output_keep_prob参数</span>cell_drop <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 直接堆叠若干个RNN层，层与层之间不能直接使用Dropout层</span>multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>cell_drop<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span>rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre><p>然而头疼的是，<code>DropoutWrapper</code> 并不支持 <code>is_training</code> 参数，也就是说，它在预测是依旧存在dropout层；<br>解决方法有两种——      </p><ol><li>自己重写一个支持 <code>is_training</code> 参数的 <code>DropoutWrapper</code> 类（<em>什么鬼设定</em>）</li><li><p>针对训练和预测构建不同的计算图，具体如下：        </p><pre class=" language-python"><code class="language-python"> <span class="token keyword">import</span> sys is_training <span class="token operator">=</span> <span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"train"</span><span class="token punctuation">)</span> X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span> y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_steps<span class="token punctuation">,</span> n_outputs<span class="token punctuation">]</span><span class="token punctuation">)</span> cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>BasicRNNCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">)</span> <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># 如果是训练，则用DropoutWrapper包装；如果是预测就拉倒</span>     cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>DropoutWrapper<span class="token punctuation">(</span>cell<span class="token punctuation">,</span> input_keep_prob<span class="token operator">=</span>keep_prob<span class="token punctuation">)</span> multi_layer_cell <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>MultiRNNCell<span class="token punctuation">(</span><span class="token punctuation">[</span>cell<span class="token punctuation">]</span> <span class="token operator">*</span> n_layers<span class="token punctuation">)</span> rnn_outputs<span class="token punctuation">,</span> states <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>dynamic_rnn<span class="token punctuation">(</span>multi_layer_cell<span class="token punctuation">,</span> X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [...] build the rest of the graph</span> init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span> saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>     <span class="token keyword">if</span> is_training<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># 如果是训练，那就执行初始化器并进行训练</span>         init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>         <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true"># [...] # train the model</span>             save_path <span class="token operator">=</span> saver<span class="token punctuation">.</span>save<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">"/tmp/my_model.ckpt"</span><span class="token punctuation">)</span>     <span class="token keyword">else</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 如果是预测，那就直接载入模型并使用</span>         saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span>sess<span class="token punctuation">,</span> <span class="token string">"/tmp/my_model.ckpt"</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># [...] # use the model</span></code></pre></li></ol><h4 id="长期依赖问题"><a href="#长期依赖问题" class="headerlink" title="长期依赖问题"></a>长期依赖问题</h4><p><a href="/2018/04/08/梯度消失与梯度爆炸/">梯度消失与梯度爆炸</a> 所述技术对RNN也是有效的；<br>但是使用这些技术之后，随着时间步增多，训练将变得十分缓慢；<br>最简单粗暴的解决方法是<strong>缩短时间步</strong>，但如此一来RNN忽略比较遥远的过去，也即存在长期依赖的问题；<br>为了解决这一问题，LSTM出现啦！       </p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>论文：<br>【起源】：<a href="https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw" target="_blank" rel="noopener">Long Short-Term Memory(2006)</a><br>【改进】：       </p><ol><li><a href="https://arxiv.org/pdf/1402.1128.pdf" target="_blank" rel="noopener">LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION(2014)</a>        </li><li><a href="https://arxiv.org/pdf/1409.2329v5.pdf" target="_blank" rel="noopener">RECURRENT NEURAL NETWORK REGULARIZATION(2015)</a>       </li><li><a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Recurrent Nets that Time and Count(2000)</a> 【提出peephole connection】      </li><li><a href="https://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)</a> 【提出GRU和Encoder-Decoder架构】<br>其他：<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks(2015)</a>      </li></ol><p>tensorflow中的使用：<br>直接将先前的 <code>tf.contrib.rnn.BasicRNNCell</code> 替换为 <code>tf.contrib.rnn.BasicLSTMCell</code> 即可；<br>区别在于，<code>BasicLSTMCell</code> 的states包含两个向量，如果要合并在一起的话可以指定参数 <code>state_is_tuple=False</code>；<br>如果要使用变种的LSTM，则使用 <code>tf.contrib.rnn.LSTMCell</code>：<br>比如使用带peephole connection的LSTM，则指定参数 <code>use_peepholes=True</code>          </p><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>LSTMCell<span class="token punctuation">(</span>num_units<span class="token operator">=</span>n_neurons<span class="token punctuation">,</span> use_peepholes<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks(2015)</a> 一问介绍了RNN的一些应用；   </p><p>接下来只介绍RNN在自然语言处理（NLP）上的两个应用</p><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>参考：</p><ol><li><a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">Tensorflow Tutorial - Word2Vec</a> 或 中文的<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/word2vec.html" target="_blank" rel="noopener">字词的向量表示</a>   </li><li><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" target="_blank" rel="noopener">Deep Learning, NLP, and Representations(2014)</a>      </li><li><a href="http://ruder.io/word-embeddings-2017/" target="_blank" rel="noopener">Word Embeddings in 2017: Trends and future directions(2017)</a>  </li></ol><p>词表示的方式：       </p><ol><li>独热码：稀疏表示    </li><li>顺序编码：稠密表示          </li><li>词嵌入：前两者的折中，而且可以通过训练，使得两个词的距离表示它们的相似程度      </li></ol><p>tensorflow实现：    </p><pre class=" language-python"><code class="language-python">vocabulary_size <span class="token operator">=</span> <span class="token number">50000</span>embedding_size <span class="token operator">=</span> <span class="token number">150</span><span class="token comment" spellcheck="true"># 创建一个待训练的词向量变量</span>embeddings <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>random_uniform<span class="token punctuation">(</span><span class="token punctuation">[</span>vocabulary_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 占位符，输入单词的顺序编码</span>train_inputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 用embedding_lookup获取对应单词的顺序编码对应的词向量表示</span>embed <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>embedding_lookup<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> train_inputs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># [...] 投喂语料库训练出合适的词向量</span><span class="token comment" spellcheck="true"># 投喂前需要预处理语料库，比如将一些未知的单词都置为&lt;UNK>、将链接都置为&lt;URL></span></code></pre><p>当然，也可以直接下载已经训练好的词向量载入给 <code>embedding</code> 然后直接使用；      </p><h4 id="机器翻译（Encoder-Decoder）"><a href="#机器翻译（Encoder-Decoder）" class="headerlink" title="机器翻译（Encoder-Decoder）"></a>机器翻译（Encoder-Decoder）</h4><p>参考：<a href="https://www.tensorflow.org/tutorials/seq2seq" target="_blank" rel="noopener">Tensorflow Tutorial - Seq2Seq</a> 【<a href="https://github.com/google/seq2seq" target="_blank" rel="noopener">代码</a>】    </p><p>训练时：<br><img src="/Handson-ML/14Translate.png" alt="14Translate">    </p><ul><li>源语句投喂给Encoder，注意应该使第一个单词最先进入Decoder      </li><li>目标语句投喂给Decoder     </li><li>Decoder对每个时间步产生一个评分并交由Softmax层得到单词的输出概率      </li></ul><p>预测时：<br><img src="/Handson-ML/14Translate_Inference.png" alt="14Translate_Inference">     </p><p>Google - Seq2Seq项目的特别之处：     </p><ol><li>独特的处理变长序列的方式<br> 前述提到用 <strong>sequence_length参数+静态展开</strong> 或 <strong>动态展开</strong> 的方式来处理变长序列；<br> 而Seq2Seq采用另一种方式——<br> 先将语句根据不同长度段分别放入不同的桶中（比如有接收长度为1~6的语句桶、接收长度为7~12的语句桶）；<br> 再用符号 <code>&lt;pad&gt;</code> 将桶内的语句填充到同一规格（比如<code>I drink milk &lt;eos&gt;</code>被填充为<code>I drink milk &lt;eos&gt; &lt;pad&gt; &lt;pad&gt;</code>）；<br> （注意，源语句在前面填充符号，目标语句在末尾填充符号）<br> 然后用一个 <code>target_weights</code> 向量表征每个单词的权重（比如<code>I drink milk &lt;eos&gt; &lt;pad&gt; &lt;pad&gt;</code>的权重为<code>[1,1,1,1,0,0]</code>）；<br> 这样，当损失函数与 <code>target_weights</code> 向量相乘时，<code>&lt;pad&gt;</code> 就被忽略了；      </li><li>使用<strong>Sampled Softmax</strong>技术<br> 论文：<a href="https://arxiv.org/pdf/1412.2007v2.pdf" target="_blank" rel="noopener">On Using Very Large Target Vocabulary for Neural Machine Translation</a><br> 当输出字典非常大（比如50000）时，Decoder将产生50000维的向量，使得计算softmax函数时变得非常复杂；<br> 为了避免这个问题，可以使Decoder输出小得多的向量（如1000维），然后用采样技术来评估损失；<br> 这在tensorflow中可以借助函数 <code>sampled_softmax_loss()</code> 实现    </li><li>使用注意力机制<br> 论文：<a href="https://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a>    </li><li>Seq2Seq使用了 <code>tf.nn.legacy_seq2seq</code> 模块，模块中包含了各种各样的Encoder-Decoder模型<br> 比如 <code>embedding_rnn_seq2seq()</code> 函数创建一个与前述机器翻译训练时的图中相同的模型       </li></ol>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络CNN</title>
      <link href="/2018/04/18/CNN/"/>
      <url>/2018/04/18/CNN/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=458725210&auto=0&height=32"></iframe><br>BGM：<strong>《四月是你的谎言》ED2</strong>        </p><hr><p>参考：     </p><ol><li>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)</a>》Chap13<br> <a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </li><li>《<a href="https://pan.baidu.com/s/1GF4yLtVphlwFS9by00Ns1Q" target="_blank" rel="noopener">卷积神经网络——深度学习实践手册(2017.05)</a>》      </li><li>《<a href="https://book.douban.com/subject/27087503/" target="_blank" rel="noopener">Deep Learning 深度学习(2017)</a>》        </li></ol><h3 id="CNN原理"><a href="#CNN原理" class="headerlink" title="CNN原理"></a>CNN原理</h3><p>卷积神经网络主要由<strong>卷积层+激活函数+池化层</strong>组成，并且在最后用全连接层输出——<br><img src="/Handson-ML/12CNN.png" alt="12CNN">     </p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p><a href="/ML-Andrew/ML-Andrew-notes3.html#反向传播算法?_blank">机器学习-吴恩达/3 非线性分类器——神经网络/反向传播算法 | Hey~YaHei!</a><br>论文：<a href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf" target="_blank" rel="noopener">Learning representation by back-propagating errors(1986)</a>   </p><p>前向传播：<br>$$ z_i = \omega_i^T x_{i}  $$<br>其中，<br>$z_i$ 为第i层的损失，即 $z_i = Cost(x_{i+1}, y)$；<br>$x_i$ 为第i层的输入；<br>$\omega_i$ 为第i层的参数；<br>$x_0$ 为原始输入    </p><p>根据链式法则，$z_i$ 对参数 $\omega_i$ 和输入 $x_i$ 求偏导——<br>$$ \frac{\partial z_i}{\partial \omega_i} = \frac{\partial z_i}{\partial x_{i+1}} \frac{\partial x_{i+1}}{\partial \omega_i} $$<br>$$ \frac{\partial z_i}{\partial x_i} = \frac{\partial z_i}{\partial x_{i+1}} \frac{\partial x_{i+1}}{\partial x_i} $$<br>其中，<br>由于 $x_{i+1}$ 由 $x_i$ 经过 $\omega_i$ 的作用得到，则 $\frac{\partial x_{i+1}}{\partial \omega_i}$ 和 $\frac{\partial x_{i+1}}{\partial x_i}$ 可以直接求得；<br>剩下的部分 $\frac{\partial z_i}{\partial x_{i+1}}$ 是由后一层计算得到；    </p><p>总的来说，误差由后往前传播，<br>$\frac{\partial z_i}{\partial \omega_i}$ 用于梯度下降，如 $\omega_i \gets \omega_i - \eta \frac{\partial z}{\partial \omega_i}$；<br>$\frac{\partial z_i}{\partial x_i}$ 用于前层 $\frac{\partial z_{i-1}}{\partial \omega_{i-1}}$ 的计算     </p><p>这里对不同参数的求导操作非常繁琐，Theano、Tensorflow都采用符号微分的方法进行自动求导（编译时就计算得到导数的数学表示）；<br>具体可以参见“花书”《Deep Learning 深度学习》（中文版）P126-139       </p><h4 id="卷积层（Conv）"><a href="#卷积层（Conv）" class="headerlink" title="卷积层（Conv）"></a>卷积层（Conv）</h4><p>卷积层并不是使用严格数学意义的卷积运算，而是使用保留卷积性质但抛弃可交换性的互相关函数；<br><em>卷积运算具有可交换性，这在数学证明上是很有用的，但在神经网络的应用中却是一个比较鸡肋的性质</em><br>卷积操作选用一定大小的卷积核（下图黄色区域）在原始数据上移动，与重合部分数据做乘和运算；<br>依次类推，最终输出一张特征图（Feature Map）<br><img src="/Handson-ML/12Convolutional_Kernel.gif" alt="12Convolutional_Kernel">       </p><p>卷积核的作用相当一个滤波器，其参数是经过学习得到的，可以用于提取图片中的特征；<br>由于核参数是随机初始化的，所以它们很可能会提取出不同的特征；<br>由低层的卷积层提取简单特征，然后逐层堆叠卷积层，将简单特征逐渐抽象为更高层次的语义概念；       </p><p>大核的卷积层可以用多层的小核的卷积层实现；<br>比如用三层3x3卷积核的卷积层可以提取到一层7x7卷积核的卷积层一样的特征——<br><img src="/Handson-ML/12Multi_Conv_Layers.png" alt="12Multi_Conv_Layers"><br>而且，使用多层小核卷积层由以下优势：    </p><ol><li>减少参数<br> 7x7卷积核有 $7 \times 7 = 49$ 个参数，而三层3x3卷积核只有 $3 \times 3 \times 3 = 27$ 个参数      </li><li>增加网络深度<br> 增加网络容量和复杂度    </li></ol><p>卷积操作的变体：     </p><ol><li>扩大原有卷积核在前层的感受野<br> 论文：<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable Convolutional Networks(2017)</a>     </li><li>感受野形状可变（而不再是简单的矩形区域）<br> 论文：<a href="https://arxiv.org/pdf/1511.07122.pdf" target="_blank" rel="noopener">Multi-Scale Context Aggregation by Dilated Convolutions(2016)</a>      </li></ol><h4 id="池化层（Pool）"><a href="#池化层（Pool）" class="headerlink" title="池化层（Pool）"></a>池化层（Pool）</h4><p>池化操作与卷积操作类似，但池化层是<strong>不需要参数</strong>的；<br>选用一定大小的池化核在原始数据上移动，与重合部分数据做一定的聚合运算（取均值、取最值、按一定概率随机取值等）；<br>依次类推，最终输出一张特征图；       </p><p>详见 《<a href="/2018/05/07/漫谈池化层?_blank">漫谈池化层 | Hey~YaHei!</a>》     </p><h4 id="全连接层（Fully-Connection）"><a href="#全连接层（Fully-Connection）" class="headerlink" title="全连接层（Fully Connection）"></a>全连接层（Fully Connection）</h4><p>参考：<a href="https://www.zhihu.com/question/41037974" target="_blank" rel="noopener">全连接层的作用是什么？——魏秀参的回答 | 知乎</a>     </p><p>全连接层在CNN中起“分类器”作用，将卷积层、池化层、激活函数学到的特征表示映射到样本的标记空间；<br>由于最后一层卷积层输出一个若干个二维数据（总体为三维），所以输入FC前通常需要将其展平（Flatten）为一维；       </p><p>实际上，全连接层可以用卷积操作代为实现：      </p><ol><li>如果FC前为FC，则该FC可以转换成1 x 1的卷积     </li><li>如果FC前为卷积层，则该FC可以转换为H x W的卷积（H、W为前层输出的高、宽）       </li></ol><p>由于全连接层参数冗余，一些网络如ResNet、GoogLeNet等采用全局平均池化（GAP）取代FC来融合学到的深度特征；    </p><p>近期研究也发现，FC可以在模型表示能力迁移过程中（尤其是原任务与目标任务差异较大时）充当“防火墙”，保证模型表示能力的迁移；     </p><h3 id="tensorflow实现"><a href="#tensorflow实现" class="headerlink" title="tensorflow实现"></a>tensorflow实现</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>借助 <code>tf.nn.conv2d()</code> ——            </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_sample_images<span class="token comment" spellcheck="true"># 读入一些图片</span>dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>load_sample_images<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>batch_size<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 创建两个三维filter</span><span class="token comment" spellcheck="true"># ... 大小7*7，通道数由图片决定</span><span class="token comment" spellcheck="true"># ... 一个水平filter，一个垂直filter（只有某一行或列为1）</span>filters_test <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>filters_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># vertical line</span>filters_test<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment" spellcheck="true"># horizontal line</span><span class="token comment" spellcheck="true"># 图片占位符</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加卷积层</span><span class="token comment" spellcheck="true"># ... X为输入</span><span class="token comment" spellcheck="true"># ... filters是所用的一系列filter，列表最后一维是filter的索引</span><span class="token comment" spellcheck="true"># ... stride是步长，针对输入而言的，比如这里batch、行、列、通道的步长分别为1,2,2,1</span><span class="token comment" spellcheck="true"># ... padding为填充方式，SAME表示填零，VALID表示不填零（可能会舍弃结尾的部分元素）</span>convolution <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>X<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"SAME"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    output <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>convolution<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> dataset<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示第一张图片的第二个feature map</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>padding参数，SAME和VALID的区别——<br><img src="/Handson-ML/12Padding.png" alt="padding">          </p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>借助<code>tf.nn.max_pool()</code>、<code>tf.nn.avg_pool()</code> 等——       </p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_sample_images<span class="token comment" spellcheck="true"># 读入一些图片</span>dataset <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>load_sample_images<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>images<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>batch_size<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels <span class="token operator">=</span> dataset<span class="token punctuation">.</span>shape<span class="token comment" spellcheck="true"># 图片占位符</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width<span class="token punctuation">,</span> channels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 添加最大池化层</span><span class="token comment" spellcheck="true"># ... X为输入的数据</span><span class="token comment" spellcheck="true"># ... ksize为池化核大小，这里表示每次一张图片，池化核大小为2x2，池化核每次只应用于一个通道</span><span class="token comment" spellcheck="true"># ...... 注意：tensorflow不支持多对象池化，所以第一维必须是1</span><span class="token comment" spellcheck="true"># .......... 而且，只支持平面池化或者深度池化</span><span class="token comment" spellcheck="true"># .......... 也就是说，要么深度（通道）参数置为1，要么长宽都置为1</span><span class="token comment" spellcheck="true"># ... stride为步长，同卷积层，这里batch、行、列、通道的步长分别为1,2,2,1</span><span class="token comment" spellcheck="true"># ... padding为填充方式，同卷积层，这里表示不填充</span>max_pool <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>X<span class="token punctuation">,</span> ksize<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"VALID"</span><span class="token punctuation">)</span><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>    output <span class="token operator">=</span> sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>max_pool<span class="token punctuation">,</span> feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> dataset<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 显示第一张图片池化后的效果</span>plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="内存占用计算（以卷积层为例）"><a href="#内存占用计算（以卷积层为例）" class="headerlink" title="内存占用计算（以卷积层为例）"></a>内存占用计算（以卷积层为例）</h3><p>考虑 $N_f$ 个大小为 $m_f \times n_f \times c$ 的三维filter组成的卷积层，<br>假设输入数据（比如图片）大小为 $m_i \times n_i$，每个batch有 $N_i$ 个instances，<br>那么每个filter输出feature map的大小也为 $m_i \times n_i$；        </p><p>此时,<br>该卷积层包含参数数量为——<br>$$ N_p = (m_f \times n_f \times c \underbrace{+1}_{\text{偏置单元}}) \times N_f $$<br>该卷积层输出的feature maps的变量总数为——<br>$$ N_v = m_i \times n_i \times N_i $$<br>该卷积层每一趟要执行的运算次数为——<br>$$ N_o = \underbrace{m_i \times n_i \times N_i}_{\text{feature maps的变量总数}} \times \underbrace{m_f \times n_f \times c}_{\text{卷积核大小}} $$        </p><p>比如，200个大小为 $5 \times 5 \times 3$ 的filter组成的卷积层，输入大小为 $150 \times 100$ 的三通道图片；<br>那么该层参数数量为 $ (5 \times 5 \times 3 + 1) \times 200 = 15200 $ ；<br>如果每个batch大小为1，使用float32存储变量，那么需要占用内存 $200 \times 150 \times 100 \times 32 = 96,000,000bits$ （约11.4MB）；<br>需要进行 $200 \times 150 \times 100 \times 5 \times 5 \times 3 = 225,000,000$ 次float乘法；      </p><p>解决内存溢出的办法：     </p><ol><li>加大步长达到降维的目的（使feature map比输入小）       </li><li>减少一些层      </li><li>使用占用空间更少的变量类型        </li><li>分布式运算</li></ol><h3 id="经典的CNN分类架构"><a href="#经典的CNN分类架构" class="headerlink" title="经典的CNN分类架构"></a>经典的CNN分类架构</h3><p>目前常见的CNN分类架构有LeNet-5、AlexNet(2012)、NIN(2014)、VGG-Nets(2015)、GoogLeNet(2015)、ResNet(2015)等；<br>详见 《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》     </p><h3 id="目标检测架构"><a href="#目标检测架构" class="headerlink" title="目标检测架构"></a>目标检测架构</h3><p>参见 <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741534&amp;idx=1&amp;sn=02dc164ffcedbf22124b97841ba67fe5&amp;chksm=871adf60b06d567690fa2328b161c012a464687768e50f812a51b5533a7d68b99af1cf8f02b8&amp;scene=0#rd" target="_blank" rel="noopener">从RCNN到SSD，这应该是最全的一份目标检测算法盘点 | 机器之心(2018)</a>【<a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9" target="_blank" rel="noopener">原文</a>】     </p><h3 id="CNN可视化"><a href="#CNN可视化" class="headerlink" title="CNN可视化"></a>CNN可视化</h3><p>论文：    </p><ol><li><a href="https://cs.nyu.edu/~fergus/drafts/deconv_iccv_names.pdf" target="_blank" rel="noopener">Adaptive Deconvolutional Networks for Mid and High Level Feature Learning(2011)</a> 提出反卷积技术     </li><li><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Visualizing and Understanding Convolutional Networks(2013)</a> 用反卷积技术实现CNN可视化（以AlexNet为例）    </li></ol><h3 id="网络压缩"><a href="#网络压缩" class="headerlink" title="网络压缩"></a>网络压缩</h3><p>深度神经网络面临严峻的<strong>过参数化（over-parameterization）</strong>问题，<br>如论文 <a href="http://export.arxiv.org/pdf/1306.0543" target="_blank" rel="noopener">Predicting Parameters in Deep Learning(2014)</a> 指出只给定很小一部分参数子集（约5%）就可以完整地重构剩余的参数；    </p><p>但事实上，参数的冗余在模型<strong>训练过程中</strong>是十分必要的，因为面临一个极其复杂的非凸优化问题，对现有基于梯度下降的优化算法而言，参数冗余保证了网络能够收敛到一个比较好的最优值。一定程度上，网络越深，参数越多，模型越复杂，最终效果也往往越好；     </p><p>压缩既指体积上的压缩，也指时间上的压缩。<br>绝大多数压缩算法旨在将一个庞大而复杂的<strong>预训练模型</strong>转化为一个精简的小模型；<br>按对网络结构的破坏程度分，可以分为前端压缩和后端压缩——     </p><ul><li><strong>前端压缩</strong><br>  不改变原网络结构，仅仅在原模型基础上减少网络层数或滤波器个数，可以完美适配现有的深度学习库；<br>  主要包括知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝等     </li><li><strong>后端压缩</strong><br>  尽可能减少模型大小，对原始网络造成极大程度的改造（往往不可逆），必须开发相应配套的运行库甚至专门的硬件设备；<br>  主要包括低秩近似、未加限制的剪枝、参数量化、二值网络等    </li><li>前端压缩和后端压缩是互补的关系<br>  通过相互结合，将前端压缩和后端压缩级联起来，可以在最大程度上减少模型复杂度       </li><li>此外，也有人试图设计更加紧凑的网络结构，对新的网络结构进行训练<br>  这也能减小模型复杂度，但不是严格意义上的网络压缩    </li></ul><!-- 妈耶！好难啊，先放一放#### 低秩近似       基本思想：      CNN的卷积操作由矩阵乘法完成，权重矩阵往往稠密巨大，带来计算和存储上的巨大开销；      可以将稠密的矩阵用若干个小规模矩阵近似重构出来，这类算法大多采用低秩近似的技术      对于权重矩阵 $W \in R^{m \times n}$ ，用若干低秩矩阵 $M_i$ 组合来进行表示——      $$ W = \sum^n_{i=1} \alpha_i M_i $$        其中，$M_i \in R^{m \times n}$ 且其秩为 $r_i << min(m,n)$；       并且可以对每个低秩矩阵进一步分解为小规模矩阵的乘积——       $$ M_i = G_i H_i^T $$        其中，$G_i \in R^{m \times r_i}$， $H_i \in R^{n \times r_i}$      当 $r_i$ 很小时，可以大幅度降低总体的存储和计算开销（以全连接层为例）——     * 原始权重矩阵 $W$          参数总数为 $nm$；         计算 $W^T X$          包含 $nm$ 次乘法 和 $n(m-1)$ 次加法；      * 低秩矩阵 $W_i$         参数总数为 $n(m r_i + n r_i + 1)$；        计算 $W^T X = \sum_{i=1}^n \alpha_i M_i^T X = \sum_{i=1}^n \alpha_i H_i G_i^T X$         包含 $r_i(m+n) + n$ 次乘法 和 $r_i((n-1)+(m-1)) + n$ 次加法；         *注意：先计算 $G_i^T X$，再计算 $H_i (G_i^T X)$*           这里看起来参数总数反而变多了（实际上后边有更简单的表示法），但在 $r_i << min(m, n)$ 时可以明显看到计算量减少了-->       <h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p>有效的数据扩充通过扩充训练样本数量，增加样本多样性，一方面可以避免过拟合，另一方面会给模型性能带来提升；       </p><h4 id="简单的数据扩充"><a href="#简单的数据扩充" class="headerlink" title="简单的数据扩充"></a>简单的数据扩充</h4><ul><li>图像水平翻转<br>  数据集扩充一倍    </li><li>随机抠取<br>  一般用较大的正方形（0.8~0.9倍的原图大小）在原图的随机位置抠取图像块；<br>  抠取次数决定数据集的扩充倍数；<br>  同时用设定好的比例抠取图像，避免了图像缩放带来的分辨率失真        </li><li>尺度变换<br>  将原图等比率缩放为原图的0.8、0.9、1.1、1.2、1.3等倍数；<br>  缩放次数决定数据集的扩充倍数；<br>  增加CNN在物体尺度上的鲁棒性       </li><li>旋转<br>  将原图旋转-30度、-15度、15度、30度等角度；<br>  旋转次数决定数据集的扩充倍数；<br>  增加CNN在方向上的鲁棒性        </li><li>色彩抖动<br>  在RGB颜色空间对色彩分布进行轻微扰动，在HSV颜色空间对图像饱和度、明度、色调进行轻微扰动；    </li></ul><p>实践中往往会在上述几种方式叠加使用；<br>相关实践代码可以参见：<a href="https://github.com/aleju/imgaug" target="_blank" rel="noopener">aleju/imgaug | github</a>，一个图像数据集的扩充python库     </p><h4 id="特殊的数据扩充"><a href="#特殊的数据扩充" class="headerlink" title="特殊的数据扩充"></a>特殊的数据扩充</h4><ul><li><p>Fancy PCA<br>  论文：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">Imagenet Classification with Deep Convolutional Neural Networks(2012)</a><br>  和<a href="/2018/05/02/经典的CNN分类架构/#AlexNet">AlexNet</a>网络一同提出；<br>  论文指出，Fancy PCA可以近似捕获自然图像的一个重要特性——<strong>物体特质与光照强度和颜色变化无关</strong>        </p><ol><li>对整个数据集的R、G、B进行PCA操作，得到特征向量 $p_i$ 和特征值 $\lambda_i$，其中 $i=1,2,3$；     </li><li>计算一组随机值$[p_1, p_2, p_3][\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T$，将其作为扰动加到原像素值中；<br> 其中，$\alpha_i$ 为0均值0.1标准差的高斯分布随机值      </li><li>每一个epoch之后，重新选取一次$\alpha_i$进行扰动       </li></ol></li><li><p>监督式数据扩充<br>  海康威视在2016ImageNet竞赛的场景分类任务中提出；<br>  在以物体为中心的图像分类任务中，随机抠取图像块可以取得比较好的效果；<br>  但对于依靠图像整体蕴含的高层语义的场景分类任务中，随机抠取图像块很可能会抠取到关联性比较差的结果（比如“海滩”中抠取到“树”和“天空”）；<br>  可以借助图像标记信息解决这一问题：    </p><ol><li>根据原数据训练一个分类的初始模型     </li><li>利用该模型对每张图生成激活图（activation map）或热力图（heat map）<br> 可以直接将最后一层卷积层特征按深度方向加和得到；<br> 也可以参照论文 <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">Learning Deep Features for Discriminative Localization</a> 生成分类激活图（class activation map）<br> 该热力图可以指示图像区域与场景标记之间的相关概率    </li><li>根据上述概率映射回原图选择较强相关的图像区域作为抠取的图像块</li></ol></li></ul><h3 id="图像预处理：中心式归一化"><a href="#图像预处理：中心式归一化" class="headerlink" title="图像预处理：中心式归一化"></a>图像预处理：中心式归一化</h3><p>在训练集上计算各通道的均值，然后对训练集、验证集、测试集上每一个像素点的各通道都减去该均值；<br>其原理在于，自然图像一般是一类平稳的数据分布，通过减均值操作可以移除图像的共性部分而凸显个体的差异；<br>比如下图通过减均值操作之后，可以发现背景部分被有效“移除”了，而只保留车、建筑等显著区域<br><img src="/Handson-ML/12Image_Preprocess.png" alt="12Image_Preprocess">     </p><h3 id="超参数设定"><a href="#超参数设定" class="headerlink" title="超参数设定"></a>超参数设定</h3><h4 id="输入数据像素大小"><a href="#输入数据像素大小" class="headerlink" title="输入数据像素大小"></a>输入数据像素大小</h4><p>CNN需要对输入的图像统一大小，通常为了便于GPU设备并行，都会统一将图像压缩为 $2^n$ 大小；<br>在设备、时间条件允许的情况下，一般分辨率高的数据有助于网络性能的提升，尤其是对基于注意力模型的网络；<br>一般CNN最后采用FC作为分类器，如果改变了原模型的图像分辨率，通常也需要重新设定FC输入的滤波器大小以及其他相关参数     </p><h4 id="卷积层参数"><a href="#卷积层参数" class="headerlink" title="卷积层参数"></a>卷积层参数</h4><p>包括卷积核大小、卷积步长、卷积核个数（即输出的特征图数量）；      </p><p>实践中通常采用3x3和5x5的小核，小的卷积核有以下作用：      </p><ol><li>增加模型复杂度，防止欠拟合    </li><li>减少参数数量     </li></ol><p>卷积操作可以选择性的搭配填充操作（padding），有以下作用：     </p><ol><li>充分利用和处理输入数据的边缘信息    </li><li>搭配合适的参数可以保持输入、输出大小不变，避免随着网络深度增加输入大小急剧减小<br> 对于fxf的卷积核、步长为1的卷积操作，在边缘各添加 $p=(f-1)/2$ 个像素可以维持输入输出大小不变       </li></ol><p>为了便于GPU设备方便存储，卷积核个数也即输出的特征图数量通常为 $2^n$ ；    </p><p>可参考：<a href="https://www.quora.com/How-can-I-decide-the-kernel-size-output-maps-and-layers-of-CNN?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa" target="_blank" rel="noopener">How can I decide the kernel size, output maps and layers of CNN? | Quora</a>  </p><p>论文：    </p><ol><li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">Efficient BackProp(1998)</a>     </li><li><a href="https://arxiv.org/pdf/1606.02228v2.pdf" target="_blank" rel="noopener">Systematic evaluation of CNN advances on the ImageNet(2017)</a> 比较了ILSVRC上各种技术、模块在不同参数下的表现<br> 对应github评估项目：<a href="https://github.com/ducha-aiki/caffenet-benchmark" target="_blank" rel="noopener">ducha-aiki/caffenet-benchmark | github</a>     </li></ol><p>通常，     </p><ul><li>网络越深越好，但这是以更大的数据集、学习任务复杂度增加为代价的；     </li><li>batch size设为几百，具体视任务而定，批大小约大计算资源的需求就越高，批大小不宜太小（这会导致估计产生较大的偏差）；     </li><li>一开始使用较少的特征图数量，然后逐渐增加并且一边观察误差的变化趋势；    </li><li>小核可以捕捉图像的细节，大核容易丢失图像的细微特征；    </li><li>可以参考类似任务的网络配置；     </li></ul><h4 id="池化层参数"><a href="#池化层参数" class="headerlink" title="池化层参数"></a>池化层参数</h4><ul><li>池化核一般比较小，如2x2、3x3等<br>  为了不丢弃过多输入而损失网络性能，很少使用超过3x3的池化核     </li><li>最常用的是2x2大小、2步长的池化操作<br>  此时输出缩小为原来的四分之一，也即丢弃了75%的响应值     </li></ul><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><p><strong>随机打乱训练数据</strong>：<br>信息论指出——    </p><blockquote><p>从相似的事件中学习总是比从相似事件中学习更具信息量    </p></blockquote><p>在每轮（epoch）训练中随机打乱训练数据，使得模型每次在不同批次看到“不同”的数据，<br>不仅可以提高模型收敛速度，还对模型的泛化性能有略微的提升      </p><p><strong>学习率设定</strong>：     </p><ul><li>初始学习率不宜过大，以0.01和0.001为宜；<br>  如果刚训练几步模型的loss就急剧上升，说明初始学习率过大       </li><li>使用一定的学习计划策略<br>  可参见 <a href="/2018/04/10/优化器/#学习计划（learning-schedules）">优化器/学习计划 | Hey~YaHei!</a>     </li><li>训练过程中观察学习曲线（loss随步数的变化）对学习率进行诊断<br>  <img src="/Handson-ML/learning_rate.png" alt="learning_rate">     </li></ul><p><strong>批规范化操作（BN操作）</strong>：<br>参见 <a href="/2018/04/08/梯度消失与梯度爆炸/#批量归一化（Batch-Normalization-BN）">梯度消失与梯度爆炸/批量归一化 | Hey~YaHei!</a>     </p><p><strong>优化器</strong>：<br>参见 <a href="/2018/04/10/优化器/">优化器 | Hey~YaHei!</a>     </p><p><strong>微调预训练好的神经网络</strong>：<br>用目标任务数据在预训练模型上继续进行训练过程；      </p><ul><li>网络已经在原始数据上收敛，微调时采用更小的学习率（一般在$10^{-4}$及其以下）    </li><li>CNN浅层有更强的泛化特征，深层对应更抽象的语义特征；<br>  微调时往往对前层更新的少，对深层更新的多，故可以设置不同的学习率；      </li><li>微调策略   <ul><li>数据较少且任务非常相似时，可仅微调最后的几层    </li><li>数据较多且任务相似时，可以微调更多甚至全部的网络层    </li><li>当数据较少、差异较大时，可以尝试微调，但不一定能成功<br>  这种情况下还可以借助<strong>部分原始数据与目标数据协同训练</strong>；<br>  论文：<a href="https://arxiv.org/pdf/1702.08690.pdf" target="_blank" rel="noopener">Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning(2017)</a><br>  在浅层特征空间选择目标数据的近邻作为协同训练的原始数据子集；<br>  微调阶段改造为多目标学习任务：将目标任务基于原始数据子集、将目标任务基于全部目标数据；     </li></ul></li></ul><hr><p>2018-05-02<br>将经典的CNN分类架构抽离出来作为单独一篇博文：<br>《<a href="/2018/05/02/经典的CNN分类架构?_blank">经典的CNN分类架构 | Hey~YaHei!</a>》     </p><hr><p>2018-05-07<br>将池化层原理部分抽离出来作为单独一篇博文：<br>详见 《<a href="/2018/05/07/漫谈池化层?_blank">漫谈池化层 | Hey~YaHei!</a>》         </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Handson-ML</title>
      <link href="/2018/04/08/Handson-ML/"/>
      <url>/2018/04/08/Handson-ML/</url>
      
        <content type="html"><![CDATA[<p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=34609001&auto=0&height=32"></iframe><br>BGM：<strong>《血界战线》ED</strong><br>中文名叫《方糖歌曲和苦味舞步》 好像也有叫 《甜蜜情歌和苦涩舞步》 的；<br>原版网易云没版权，这是双声道版，也还行~~这贝斯手很灵性emmm       </p><hr><p>《<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>》笔记<br>目前结合毕设，主要只看TensorFlow部分，也就是DL的部分——       </p><ul><li><a href="/Handson-ML/9up_and_running_with_tensorflow.html?_blank">09 Up and Running with Tensorflow</a><br>  tensorflow的基本使用     <ul><li><a href="/Handson-ML/9.1Linear_Regression(Normal_Equation).html?_blank">Jupyter - Linear_Regression(Normal_Equation)</a><br>  正规方程实现线性回归      </li><li><a href="/Handson-ML/9.2Linear_Regression(Gradient_Descent).html?_blank">Jupyter - Linear_Regression(Gradient_Descent)</a><br>  梯度下降实现线性回归</li></ul></li><li>10 Introduction to Artificial Neural Networks<br>  简单的神经网络      <ul><li><a href="/Handson-ML/10.1DNN_MNIST(High-level_API).html?_blank">Jupyter - DNN_MNIST(High-level_API)</a><br>  高层API操作实现DNN来完成手写体识别</li><li><a href="/Handson-ML/10.2DNN_MNIST(Plain).html?_blank">Jupyter - DNN_MNIST(Plain)</a><br>  底层操作实现DNN来完成手写体识别，宽度、深度、激活函数的选择      </li></ul></li><li>11 Training Deep Neural Nets<br>  深层神经网络训练中的问题与相关的解决技术<br>  常见的配置为：<br>  初始化（Initialization）：<strong>He Initialization</strong><br>  激活函数（Activation function）：<strong>ELU</strong><br>  归一化（Normalization）：<strong>Batch Normalization, BN</strong><br>  正则化（Regularization）：<strong>Dropout</strong><br>  优化器（Optimizer）：<strong>Adam</strong><br>  学习计划（Learning rate schedule）：<strong>无（Adam具备自适应学习率）</strong>          <ul><li><a href="/2018/04/08/梯度消失与梯度爆炸?_blank">Vanishing/Exploding Gradients Problem</a><br>  梯度爆炸与梯度消失：随机初始化、nonsaturating函数、批量归一化、梯度裁剪      </li><li><a href="/2018/04/09/复用预训练层?_blank">Reusing Pretrained Layers</a><br>  复用与训练层：加速训练过程、解决标注数据少等问题        </li><li><a href="/2018/04/10/优化器?_blank">Faster Optimizers</a><br>  优化器（Momentum, NAG, AdaGrad, RMSProp, Adam）等、训练稀疏模型、学习计划         </li><li><a href="/2018/04/11/正则化技术?_blank">Avoiding Overfitting Through Regularization</a><br>  正则化技术：提前终止、L1和L2范数惩罚、Dropout、最大范数、数据增强等         </li></ul></li><li>12 Distributing TensorFlow Across Devices and Servers<br>  暂略</li><li><a href="/2018/04/18/CNN?_blank">13 Convolutional Neural Networks</a><br>  CNN原理、CNN的tensorflow实现、CNN资源占用计算、几个常见的CNN典型框架      </li><li><a href="/2018/04/22/RNN?_blank">14 Recurrent Neural Networks</a><br>  <em>RNN原理【留坑待填】</em>、RNN的tensorflow实现、应用（词嵌入、机器翻译）         </li><li><a href="/2018/04/25/autoencoder?_blank">15 Autoencoders</a><br>  自编码器原理、自编码器的实现与训练、不同约束下的自编码器       </li><li>16 Reinforcement Learning<br>  暂略</li></ul><p>其他：<a href="/Handson-ML/CS20SI小记.pdf">CS20SI小记</a>         </p><hr><p>2018-04-25<br>呀嘿，真是高产的四月。<br>DL部分基本算是读完了吧，除了分布计算和强化学习的部分，先暂时搁着；<br>还有些地方尤其是原理的部分还需要填坑完善，一边读论文或者其他书籍一边慢慢填上这些坑吧~      </p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.1 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Handson-ML》笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World~</title>
      <link href="/2017/08/24/hello%20world/"/>
      <url>/2017/08/24/hello%20world/</url>
      
        <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=592080&auto=0&height=32"></iframe>     <p>BGM：<strong>《Code Geass反叛的鲁路修》S01E14、S02E03插入曲</strong>，也算是夏丽的角色曲吧。<br><!--S01E14插入是在鲁路修行动时无意中把夏丽父母卷入战争致死，夏丽在喜欢鲁路修、却又得知鲁路修即是zero也是害死自己父母的罪魁祸首，感到十分矛盾与痛苦。为此鲁路修不得已用gease抹除了夏丽这方面的记忆；        S02E03插入是在夏丽死的时候，夏丽因为gease被清除，回想起之前的事情，在矛盾与痛苦中被鲁路修的“弟弟”洛洛（也是个悲惨的角色）杀死，几乎也是这个时候，鲁路修彻底告别自己平凡的一面；         歌曲是夏丽对平凡的鲁鲁修的爱情的内心描述，这个回音般的音效还是很有特色的，跟专辑的名字《Angel Feather Voice》一样，有点像天使的声音。       --></p><hr><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前两年虚拟机崩了<br>博客上的数据全丢，而且没有备份，心疼<br>没有md文件的存根（只有少数几份在win上有备份），只剩下个编译后的网站——<a href="http://zkkzkk368.github.io?_blank" target="_blank" rel="noopener">Hey!YaHei~</a><br>最近没啥学习的动力，重操旧业把博客搭起来玩玩吧<br>这次依旧用<a href="https://hexo.io/?_blank" target="_blank" rel="noopener">hexo框架</a><br><a href="https://github.com/iissnan/hexo-theme-next?_blank" target="_blank" rel="noopener">NexT主题</a>也更换为<a href="https://github.com/viosey/hexo-theme-material?_blank" target="_blank" rel="noopener">Material主题</a>（这个主题看起来还不错）<br>前者其实更加简约，但它的首页感觉有些难受<br>额外搞了个<a href="https://github.com/ele828/hexo-prism-plugin?_blank" target="_blank" rel="noopener">prism</a>语法高亮插件，美滋滋……     </p><p>主要更学习笔记吧，其实自己的笔记别人未必看得懂<br>欢迎订阅RSS（以QQ邮箱为例）：<br><img src="/imgs/RSS_demo.png" alt="RSS_demo">     </p><p>兴许偶尔会写点别的~~<br>估计没人会来看，纯属自娱自乐    </p><h3 id="原博客目录"><a href="#原博客目录" class="headerlink" title="原博客目录"></a>原博客目录</h3><p>（带星号的表示新博客上也有一样的文章）    </p><ul><li>2014-10-29 <a href="http://zkkzkk368.github.io/2014/10/30/%E7%AC%AC%E4%B8%80%E4%BA%8C%E6%9C%9F%E4%BD%9C%E4%B8%9A%E6%80%BB%E7%BB%93?_blank" target="_blank" rel="noopener">*第一二期作业总结</a><br>  当初大一俱乐部的作业     </li><li>2015-02-05 <a href="http://zkkzkk368.github.io/2015/02/06/%E4%BF%AE%E4%BA%86%E4%B8%89%E5%A4%A9%E5%8D%8A%E7%94%B5%E8%84%91%E7%9A%84%E6%80%BB%E7%BB%93?_blank" target="_blank" rel="noopener">*居然……还我数据？！？！</a><br>  折腾linux的时候躺过的坑    </li><li>2015-03-23 <a href="http://zkkzkk368.github.io/2015/03/24/%E6%98%A5%E7%BA%B3%E5%90%8E%E7%AB%AF?_blank" target="_blank" rel="noopener">*俱乐部春纳网页后端小结</a><br>  大一俱乐部纳新网站制作小结    </li><li>2015-04-05 <a href="http://zkkzkk368.github.io/2015/04/06/fedora%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE?_blank" target="_blank" rel="noopener">fedora安装与配置</a><br>  闲着无聊把ubuntu换成了fedora     </li><li>2015-04-19 <a href="http://zkkzkk368.github.io/2015/04/20/Hexo%E7%9A%84%E4%BD%BF%E7%94%A8?_blank" target="_blank" rel="noopener">Hexo的使用</a><br>  嗯~第一次搭博客哈哈      </li><li>2015-04-22 <a href="http://zkkzkk368.github.io/2015/04/23/python%E7%88%AC%E8%99%AB%E5%88%9D%E6%8E%A2?_blank" target="_blank" rel="noopener">python爬虫初探</a><br>  第一次写爬虫    </li><li>2015-05-07 <a href="http://zkkzkk368.github.io/2015/05/08/C%E8%AF%AD%E8%A8%80%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%BC%96%E7%A8%8B%EF%BC%88WIN%EF%BC%89?_blank" target="_blank" rel="noopener">C语言图形化编程（WIN）</a><br>  C大程写大作业前的学习小结    </li><li>2015-05-10 <a href="http://zkkzkk368.github.io/2015/05/11/javascript%E7%AC%94%E8%AE%B0?_blank" target="_blank" rel="noopener">JS笔记</a><br>  闲来无事小补了一发js，后来写数据结构的bonus居然用上了   </li><li>2015-06-02 <a href="http://zkkzkk368.github.io/2015/06/03/%E7%AE%80%E5%8D%95%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%A7%A3%E6%9E%90%E5%99%A8?_blank" target="_blank" rel="noopener">简单的正则表达式解析器</a><br>  《代码之美》里一段有趣的代码    </li><li>2015-08-11 <a href="http://zkkzkk368.github.io/2015/08/12/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7?_blank" target="_blank" rel="noopener">《vim实用技巧》笔记</a><br>  已烂尾…    </li><li>2015-08-30 <a href="http://zkkzkk368.github.io/2015/08/31/%E3%80%8AC%E4%B8%93%E5%AE%B6%E7%BC%96%E7%A8%8B%E3%80%8B%E5%B0%8F%E8%AE%B0?_blank" target="_blank" rel="noopener">《C专家编程》小记</a><br>  二刷《C专家编程》，随手记了点东西   </li><li>2015-08-30 <a href="http://zkkzkk368.github.io/2015/08/31/%E6%8C%87%E9%92%88&amp;%E6%95%B0%E7%BB%84?_blank" target="_blank" rel="noopener">指针&amp;数组</a>        </li><li>2015-09-20 <a href="http://zkkzkk368.github.io/2015/09/20/Note_for_Python?_blank" target="_blank" rel="noopener">python笔记</a></li><li>2015-09-20 <a href="http://zkkzkk368.github.io/2015/09/20/Note_For_Linux?_blank" target="_blank" rel="noopener">《鸟哥的Linux私房菜》笔记</a></li><li>2015-10-17 <a href="http://zkkzkk368.github.io/2015/10/17/%E3%80%8AJava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E3%80%8B%E7%AC%94%E8%AE%B0?_blank" target="_blank" rel="noopener">《Java核心技术》笔记</a><br>  烂尾…      </li></ul><h3 id="更新日志"><a href="#更新日志" class="headerlink" title="更新日志"></a>更新日志</h3><h4 id="2017-07-24"><a href="#2017-07-24" class="headerlink" title="2017-07-24"></a>2017-07-24</h4><!--> 『情』束缚的不是『头脑』，而是『内心』。> ——《东京暗鸦》11卷5章2节，字野耕平    -->      <p>唉~     </p><h4 id="2017-08-27"><a href="#2017-08-27" class="headerlink" title="2017-08-27"></a>2017-08-27</h4><p>由于markdown与mathjax的语法冲突，mathjax的_时常被markdown错误的渲染<br>故修改<code>node_modules/marked/lib/marked.js</code>文件中的正则表达式来放弃markdown对_的渲染<br>参考：<a href="http://www.cnblogs.com/Ai-heng/p/7282110.html" target="_blank" rel="noopener">hexo博客MathJax公式渲染问题</a>    </p><h4 id="2018-04-25"><a href="#2018-04-25" class="headerlink" title="2018-04-25"></a>2018-04-25</h4><p>修改Material的模板文件<code>themes/material/layout/layout.ejs</code>——<br>在末尾追加添加相关js代码；<br>将mardown语法 <code>[text](url?_blank)</code> 转换成带属性 <code>target=&quot;_blank</code> 的 <code>&lt;a&gt;</code> 标签，<br>弥补markdown不支持“从新标签打开链接”的功能——     </p><pre class=" language-javascript"><code class="language-javascript"><span class="token operator">&lt;</span>script type<span class="token operator">=</span><span class="token string">"text/javascript"</span><span class="token operator">></span>    <span class="token keyword">var</span> aTagArr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">.</span>slice<span class="token punctuation">.</span><span class="token function">apply</span><span class="token punctuation">(</span>document<span class="token punctuation">.</span><span class="token function">getElementsByTagName</span><span class="token punctuation">(</span><span class="token string">"a"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    aTagArr<span class="token punctuation">.</span><span class="token function">forEach</span><span class="token punctuation">(</span><span class="token keyword">function</span> <span class="token punctuation">(</span>e<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span> e<span class="token punctuation">.</span>href<span class="token punctuation">.</span><span class="token function">lastIndexOf</span><span class="token punctuation">(</span><span class="token string">"_blank"</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token operator">-</span><span class="token number">1</span> <span class="token punctuation">)</span><span class="token punctuation">{</span>            e<span class="token punctuation">.</span>target <span class="token operator">=</span> <span class="token string">"_blank"</span><span class="token punctuation">;</span>            e<span class="token punctuation">.</span>href <span class="token operator">=</span> e<span class="token punctuation">.</span>href<span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">"?_blank"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token operator">&lt;</span><span class="token operator">/</span>script<span class="token operator">></span></code></pre><h4 id="2018-08-24"><a href="#2018-08-24" class="headerlink" title="2018-08-24"></a>2018-08-24</h4><ul><li>更新Material至1.5.6版本<br>  这个版本有点小bug，修复方式参见 <a href="https://github.com/viosey/hexo-theme-material/issues/686" target="_blank" rel="noopener">Issue #686 | github</a> 中michaelcai的回复；       </li><li>引入评论系统<br>  LiveRe评论系统，海外服务，可能加载会稍微慢点，偶尔也不大稳定（加载不出来），坐等LiveRe提供大陆服务吧       </li><li>更改字体源至ustc<br>  原先从google获取，有时候会加载不出来       </li><li>从bootcdn获取jquery和mathjax<br>  提高页面加载速度         </li></ul><h4 id="2019-03-06"><a href="#2019-03-06" class="headerlink" title="2019-03-06"></a>2019-03-06</h4><p>参考：<br>《<a href="https://blog.csdn.net/StaunchKai/article/details/82901437" target="_blank" rel="noopener">hexo 博客开启 https (SSL 证书) | CSDN, staunchKai</a>》<br>《<a href="https://www.jianshu.com/p/8046a12fec4a" target="_blank" rel="noopener">Hexo 博客 之 添加 https | 简书, evenyao</a>》       </p><ul><li>将<code>www.hey-yahei.cn</code>重定向到<code>hey-yahei.cn</code></li><li>新增子域名<code>yahei.zjuarm.club</code>（重定向至<code>hey-yahei.cn</code>）<br>  小彩蛋(?)：<a href="http://zjuarm.club" target="_blank" rel="noopener">http://zjuarm.club</a>           </li><li>增加ssl证书，并强制重定向至https</li></ul>]]></content>
      
      
      <categories>
          
          <category> 0 其他 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>QT学习之路</title>
      <link href="/2017/01/29/QT%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/"/>
      <url>/2017/01/29/QT%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br>做srtp的时候看的一波资料<br><a href="/others/QT学习之路.pdf">QT学习之路.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《C++编程思想》笔记</title>
      <link href="/2016/07/03/C++%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/"/>
      <url>/2016/07/03/C++%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<p><strong>从onenote上搬运过来的笔记</strong><br><a href="/others/C++编程思想.pdf">C++编程思想.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>python-sh</title>
      <link href="/2015/09/20/sh%E6%A8%A1%E5%9D%97/"/>
      <url>/2015/09/20/sh%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="sh模块"><a href="#sh模块" class="headerlink" title="sh模块"></a>sh模块</h2><p><a href="http://amoffat.github.io/sh/" target="_blank" rel="noopener">sh官方文档</a>  </p><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="直接使用命令对应的函数"><a href="#直接使用命令对应的函数" class="headerlink" title="直接使用命令对应的函数"></a>直接使用命令对应的函数</h4><p>如：<code>print(sh.ls(&quot;/&quot;))</code>  </p><ol><li>命令参数分别以函数参数的形式给出<br> 如：<code>tar(&quot;cvf&quot;, &quot;/tmp/test.tar&quot;, &quot;/my/home/directory/&quot;)</code><br> 即执行linux命令<code>tar -cvf /tmp/test.tar /my/home/directory/</code>   </li><li>命令名中如果出现横线<code>-</code>则其对应的函数名应改为下划线<code>_</code><br> 如：linux命令<code>google-chrome</code>对应函数<code>google_chrome</code>  </li></ol><h4 id="自定义命令函数"><a href="#自定义命令函数" class="headerlink" title="自定义命令函数"></a>自定义命令函数</h4><p>如：  </p><pre><code>    lscmd = sh.Command(&quot;/bin/ls -l&quot;)      lscmd(&quot;/&quot;)</code></pre><p>即将带参数linux命令<code>/bin/ls -l</code>“封装”成<code>lscmd()</code>  </p><h4 id="提供参数的两种形式"><a href="#提供参数的两种形式" class="headerlink" title="提供参数的两种形式"></a>提供参数的两种形式</h4><p>以linux命令<code>curl http://duckduckgo.com/ -o page.html --silent</code>为例  </p><ol><li>以关键词参数的形式给出<br> <code>sh.curl(&quot;http://duckduckgo.com/&quot;, o=&quot;page.html&quot;, silent=True)</code>  </li><li>以分割的字符串的形式给出<br> <code>sh.curl(&quot;http://duckduckgo.com/&quot;, &quot;-o&quot;, &quot;page.html&quot;, &quot;--silent&quot;)</code>  </li></ol><h4 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h4><p>命令默认前台运行（阻塞式，block），如果将命令转至后台（非阻塞式，doesn’t block），添加关键词参数<code>_bg=True</code>即可<br>如：<code>sh.sleep(3, _bg=True)</code>  </p><h4 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h4><p>使用linux中的管道命令，直接用函数的嵌套调用即可<br>如：<code>sh.wc( sh.ls(&quot;/etc&quot;, &quot;-l&quot;), &quot;-l&quot; )</code><br>即linux命令<code>ls /etc -l | wc -l</code>  </p><p>默认情况下，被传递的命令完成后才会传递给管道命令<br>如上例中<code>ls</code>完成后才会传递并执行<code>wc</code>命令   </p><p>给被传递的命令函数加入参数<code>_piped=True</code>后，可以使两个命令同时执行，被传递的命令不断得将新产生的返回信息传递给管道命令<br>如上例中不等<code>ls</code>完成即会传递并执行<code>wc</code>命令，<code>ls</code>不断产生新的信息并不断传递给<code>wc</code>命令   </p><h4 id="数据流重定向"><a href="#数据流重定向" class="headerlink" title="数据流重定向"></a>数据流重定向</h4><p>使用参数<code>_out={filename}</code>和<code>_err={filename}</code>分别将标准输出和错误输出重定向到指定文件中<br>如果文件已存在，默认将清空文件内容后重新写入<br>如：<code>sh.ls(_out=&quot;files.list&quot;)</code>  </p><h4 id="标准输入stdin"><a href="#标准输入stdin" class="headerlink" title="标准输入stdin"></a>标准输入stdin</h4><p>可以给一个命令函数提供标准输入<br>只需要提供参数<code>_in={stdin}</code>即可<br>如：<code>print( sh.cat(_in=&quot;hello&quot;) )</code><br>“stdin”不仅可以是字符串，还可以是文件、队列和任何可迭代对象（列表、集合、字典等）  </p><h4 id="子命令的两种形式"><a href="#子命令的两种形式" class="headerlink" title="子命令的两种形式"></a>子命令的两种形式</h4><p>以linux命令<code>sudo ls /root</code>为例  </p><ol><li>使用命令函数下的对应子函数<br> <code>sh.sudo.ls(&quot;/root&quot;)</code>  </li><li>将子命令作为参数给出<br> <code>sh.sudo( &quot;ls&quot;, &quot;/root&quot; )</code>  </li></ol><p><strong>注意：对于sudo命令，用户必须设置NOPASSWD选项使该用户在命令执行时无需再输入密码才能正常执行</strong>  </p><h4 id="命令回传值的接收与处理"><a href="#命令回传值的接收与处理" class="headerlink" title="命令回传值的接收与处理"></a>命令回传值的接收与处理</h4><p>命令函数的返回内容除了其应有的输出，还包含<code>exit_code</code>属性记录命令的回传值（一般正常执行的回传值为0）<br>如：<br><code>print(sh.ls(&quot;/root&quot;))</code>打印ls命令的输出<br><code>print(sh.ls(&quot;/root&quot;).exit_code)</code>打印ls命令的回传值   </p><ul><li>命令执行失败会引起python出现相应的异常，可以借助python的try/except机制捕获并处理异常<br>  回传值为<code>x</code>的命令错误会触发<code>ErrorReturnCode_x</code>的异常<br>  如：回传值为<code>2</code>的命令错误会触发<code>ErrorReturnCode_2</code>的异常  </li><li>有些命令即使正常执行也会报错，可以在调用相应的函数时给出<code>ok_code={ok_code_list}</code>参数来告知哪些回传值是正常的<br>  如：<code>sh.weird_program(_ok_code=[0,3,5])</code>  </li></ul><h4 id="通配符的使用"><a href="#通配符的使用" class="headerlink" title="通配符的使用"></a>通配符的使用</h4><p>将使用通配符的字符串用<code>sh.glob()</code>函数处理（<strong>注意不是<code>glob.glob()</code>函数</strong>）<br>如：<br><code>sh.ls( sh.glob(&quot;*.py&quot;) )</code><br>即linux命令<code>ls *.py</code>  </p><h3 id="进阶使用"><a href="#进阶使用" class="headerlink" title="进阶使用"></a>进阶使用</h3><p>暂略……</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.2 语言 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>《vim实用技巧》小记</title>
      <link href="/2015/08/11/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/"/>
      <url>/2015/08/11/vim%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<p>=。=最近在看《vim实用技巧》<br>对一些内容作点简单的记录，方便以后查阅  </p><h2 id="Vim解决问题的方式"><a href="#Vim解决问题的方式" class="headerlink" title="Vim解决问题的方式"></a>Vim解决问题的方式</h2><ul><li>多用<code>.</code>命令（一个微型宏）来重复一些简单的操作  </li><li>减少无关的移动，形成合理的撤销块  </li><li>善用复合命令减少操作<br>  <code>A</code>    == <code>$a</code>，插入在行末<br>  <code>C</code> == <code>c$</code>，替换行末字符并持续插入<br>  <code>s</code> == <code>cl</code>，替换当前字符并持续插入<br>  <code>S</code> == <code>^c</code>，替换前一字符并持续插入<br>  <code>I</code> == <code>^i</code>，从行首开始编辑<br>  <code>o</code> == <code>A&lt;CR&gt;</code>，在下方插入新行<br>  <code>O</code> == <code>ko</code>，在上方插入新行<br>  <code>……</code>  </li><li>尽可能使修改、移动变得可重复  </li><li>常用的重复与回退操作  <ul><li>一般的修改<code>{edit}</code>：<br>  <code>.</code>重复，<code>u</code>回退  </li><li>行内的查找<code>[f|F|t|T]{char}</code>：<br>  <code>;</code>重复，<code>,</code>回退  下</li><li>文档内的查找<code>[/|?]{pattern}&lt;CR&gt;</code>：<br>  <code>n</code>重复，<code>N</code>回退  </li><li>执行替换<code>:s/target/replacement</code>：<br>  <code>&amp;</code>重复，<code>u</code>回退  </li><li>执行一系列修改<code>qx{changes}q</code>：<br>  <code>@x</code>重复，<code>u</code>回退  </li></ul></li><li><code>.</code>范式<br>  用一个键移动，用另一个键执行的可重复修改操作  </li></ul><h2 id="普通模式"><a href="#普通模式" class="headerlink" title="普通模式"></a>普通模式</h2><ul><li>停顿思考时切换到普通模式</li><li>合理地切分撤销单元（模式间的切换）  </li><li>如果在插入模式中移动的光标，将会产生一个新的撤销单元  </li><li>删除单词<code>daw</code>，可以解读为”delete a word”，该命令可重复  </li><li>简单的算术运算<code>{num}&lt;C-a&gt;</code>加法，<code>{num}&lt;C-x&gt;</code>减法<br>  不需要把光标移动到数字上<br>  运算为num加减当前光标或光标以后的数字<br>  注意：默认情况下0开头的数字会被当作八进制，0x开头则为十六进制<br>  <em>可以通过<code>set nrformats=</code>关闭进制识别（都作为十进制数）</em>  </li><li>重复&amp;次数<br>  删除多个单词有两种风格——  <ol><li><code>dw</code>+<code>.</code>的重复风格<br> 常用风格，使用灵活，方便回退，无需数单词个数  </li><li><code>d2w</code>或<code>2dw</code>的次数风格<br> 多用于删除一整块词组，如<code>a couple of</code>;可以提高撤销单元的连贯性  </li></ol></li><li>操作 = 操作符 + 动作命令{motion}  <ul><li>操作符  <ul><li><code>c</code>：修改  </li><li><code>d</code>：删除</li><li><code>y</code>：复制到寄存器  </li><li><code>g~</code>：字母大小写反转  </li><li><code>gu</code>：字母转换为小写  </li><li><code>gU</code>：字母转换为大写  </li><li><code>&gt;</code>：增加缩进  </li><li><code>&lt;</code>：减小缩进  </li><li><code>=</code>：自动缩进</li><li><code>!</code>：用外部程序过滤{motion}跨越的行</li></ul></li><li>动作命令  <ul><li><code>[h|j|k|l]</code>：左下上右  </li><li><code>[-|+]</code>：上一行（下一行）的非空白字符</li><li><code>0</code>（数字）：行首（含空白）</li><li><code>^</code>：行首（不含空白），即本行第一个非空白字符  </li><li><code>$</code>：行末  </li><li><code>gg</code>：文首  </li><li><code>G</code>：文末  </li><li><code>[f|F|t|T]{char}</code>：右侧（左侧）的第一个char字符</li><li><code>[;|,]</code>：下一个（上一个）”f|F|t|T”的char字符</li><li><code>:{num}</code>：至第num行  </li><li><code>[w|b]</code>：后一个（前一个）或当前单词的头部（尾部）——含符号  </li><li><code>[W|B]</code>：后一个（前一个）或当前单词的头部（尾部）——跳过符号（仅字母和数字包括负号）</li><li><code>............................</code>  </li><li>动作命令可以待有修饰符前缀<ul><li><code>a</code>：即an，表示一个非空白对象  </li><li><code>i</code>：即inner，表示一个内含对象</li></ul></li></ul></li><li>当一个操作符被连续调用两次，就表示作用于当前行<br>  <em>特例：<code>g~~</code>、<code>guu</code>、<code>gUU</code></em>  </li></ul></li><li>操作符待决模式<br>  键入操作符后vim会进入操作符待决模式，此时vim会等待一个动作命令后执行操作<br>  在该模式下可以按<code>&lt;ESC&gt;</code>来退回普通模式<br>  因为该模式的存在，使得自定义操作符及动作指令能够存在  </li></ul><h2 id="插入模式"><a href="#插入模式" class="headerlink" title="插入模式"></a>插入模式</h2><ul><li>单词输入错误，应删除整个单词再重新输入<br>  <em>可以减少以后输入出错</em>  </li><li>删除  <ul><li><code>&lt;C-h&gt;</code>：删除前一个字符，相当于退格键  </li><li><code>&lt;C-w&gt;</code>：删除前一个单词  </li><li><code>&lt;C-u&gt;</code>：删除至行首  </li><li><strong>注意：只能删除本次插入模式下插入的内容</strong>  </li><li><em>这些命令不是插入模式独有，也不是vim独有，在命令行模式、shell中都能使用它们</em>  </li></ul></li><li>切换到普通模式使用<code>&lt;C-[&gt;</code>更加方便  </li><li>插入-普通模式：<code>&lt;C-o&gt;</code><br>  执行一个普通模式命令后，马上就可以返回到插入命令<br>  在插入模式下，通过<code>&lt;C-o&gt;zz</code>迅速把当前行移动到屏幕正中  </li><li>插入模式下的粘贴几个单词<br>  <code>&lt;C-r&gt;{register}</code>：将register号寄存器的内容粘贴到当前位置<br>  <em>普通模式下复制文本默认保存在0号寄存器中</em><br>  <strong>注意：当文本比较多的时候，应切换到普通模式下操作比较合适</strong>  </li><li>运算<br>  <code>&lt;C-r&gt;={expr}&lt;CR&gt;</code>：执行expr运算并且把结果插入到当前位置  </li><li>用字符编码插入字符  <ul><li>十进制ASCII字符：<code>&lt;C-v&gt;{code}</code><br>  只能插入三位十进制数的ASCII码，注意高位补0  </li><li>十六进制unicode字符：<code>&lt;C-v&gt;u{code}</code><br>  插入四位十六进制数的unicode码，注意高位补0  </li><li>通过二合字母插入字符<br>  <code>&lt;C-k&gt;{char1}{char2}</code><br>  二合字母集可以通过<code>:digraphs</code>查看<br>  <code>:digraph-table</code>可以获得二合字母集更详细的信息  </li></ul></li><li>查看当前字符的编码<br>  <code>ga</code>：屏幕下方会显示当前字符的十进制、十六进制、八进制编码信息  </li><li>虚拟替换模式<br>  <code>R</code>：替换模式，<code>&lt;Tab&gt;</code>会作为一个字符被替换<br>  <code>gR</code>：虚拟替换模式，<code>&lt;Tab&gt;</code>会作为多个字符（一般为八个）被逐个替换（直到末尾才替换掉<code>&lt;Tab&gt;</code>）<br>  <code>r</code>和<code>gr</code>同理～<br>  <strong>应尽量使用虚拟替换模式</strong>  </li></ul><h2 id="可视模式"><a href="#可视模式" class="headerlink" title="可视模式"></a>可视模式</h2><ul><li>三种可视模式  <ol><li><code>v</code>：操作字符文本  </li><li><code>V</code>：操作行文本  </li><li><code>&lt;C-v&gt;</code>：操作块文本  </li></ol></li><li>该模式与其他模式不同，是先选择文本后触发命令  </li><li>选择模式<code>&lt;C-g&gt;</code><br>  类似与windows的选择模式，输入的可见字符会替换掉选中的文本，之后vim进入插入模式<br>  这个模式只是为了迎合windows用户，应尽量少用  </li><li><code>gv</code>：重选上次的高亮选区  </li><li>该模式下的<code>.</code>命令有时候会出现异常<br>  <strong>所以应尽可能使用操作命令而不是可视命令</strong>  </li><li>有时候修改文本的范围很难用动作命令表达出来，这时候才用到可视命令  </li><li>修改多行文本时，只有第一行发生变化，只有当返回普通模式后，其他行才会发生变化  </li><li><code>&lt;C-v&gt;</code>不仅可以选中矩形的区域，还可以选中长短不一的块<br>  当在该模式下选中行末部分，可以实现长短不一的块的选择  </li></ul><h2 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h2><ul><li>三种形式的命令行模式  <ol><li>按下<code>:</code>的Ex命令  </li><li>按下<code>/</code>的查找命令  </li><li>用<code>&lt;C-r&gt;=</code>访问表达式寄存器  </li></ol></li><li>指定命令作用范围<ul><li>指定行<br>  <code>:{number}....</code><br>  只包含数字的Ex命令表示跳转<br>  <strong>可以使用特殊符号<code>$</code>表示最后一行，<code>%</code>表示当前文件的所有行</strong>  </li><li>用地址来指定一个范围<br>  <code>:{start},{end}....</code>，执行命令后光标将跳转到end行  </li><li>用高亮选取指定范围<br>  先用可视模式<code>v</code>,<code>V</code>,<code>&lt;C-v&gt;</code>选取高亮区<br>  再按下<code>:</code>，此时命令行会自动填充成<code>:&#39;&lt;,&#39;&gt;</code>，表示作用在高亮区上  </li><li>用模式指定<br>  <code>:/{pattern1},/{pattern2}/....</code><br>  例如：<code>:/&lt;html&gt;/,/&lt;\/html&gt;/</code>指定了html内的所有内容（包含<code>&lt;html&gt;</code>和<code>&lt;/html&gt;</code>）<br>  <em>注意：斜杠<code>/</code>有特殊含义，需要用反斜杠<code>\</code>转义</em>  </li><li>地址偏移<br>  直接对地址进行<code>+</code>或<code>-</code>运算<br>  如：<code>:/&lt;html&gt;/+1,/&lt;\/html&gt;/-1</code>则排除了<code>&lt;html&gt;</code>和<code>&lt;/html&gt;</code>而只包含了其中间的内容<br>  <strong>如果不加数字，默认偏移量为1</strong>  </li><li>特殊符号总结  <ul><li><code>1</code>：第一行  </li><li><code>0</code>：虚拟行，位于第一行上方，常用于插入到首行等功能</li><li><code>$</code>：最后一行  </li><li><code>.</code>：当前行  </li><li><code>%</code>：整个文件，相当于<code>:1,$</code>  </li><li><code>&#39;m</code>：包含位置标记m的行  </li><li><code>&#39;&lt;</code>和<code>&#39;&gt;</code>：高亮选取的起始和结束行  </li></ul></li></ul></li><li>复制命令：<code>:t</code>或<code>:co</code>或<code>:copy</code><br>  <code>:{range}t {address}</code><br>  与普通模式下的<code>y</code>命令不同，该命令不把文本保存到寄存器  </li><li>移动命令：<code>:move</code>或<code>:m</code><br>  <code>:{range}m {address}</code>  </li><li>在指定范围上执行普通模式命令<br>  <code>:{range}normal {commands}</code>，常用于多行的处理<br>  常用的命令如下：  <ul><li><code>:{range}normal .</code>：对多行重复同一操作   </li><li><code>:{range}normal A;</code>：对多行末尾补上分号  </li><li><code>:{range}normal i//</code>：将多行内容注释掉  </li></ul></li><li>重复上一条命令：<code>@:</code>  </li><li>使用<code>&lt;C-o&gt;</code>进入插入-普通模式时，命令记录会跳转到上一条命令，可以借此进行命令记录的跳转</li><li>自动补全  <ul><li><code>&lt;Tab&gt;</code>自动补全命令<br>  多次按<code>&lt;Tab&gt;</code>会正向遍历补全列表的内容  </li><li><code>&lt;S-Tab&gt;</code>反向遍历补全列表，<em>S表示<code>&lt;shift&gt;</code>按键</em>  </li><li><code>&lt;C-d&gt;</code>显示可用的补全列表  </li><li>自定义补全行为  <ul><li>bash shell形式：<code>set wildmode=longest,list</code>  </li><li>zsh形式：<code>set wildmenu</code>和<code>set wildmode=full</code>  <ul><li><code>wildmenu</code>为补全导航列表  </li></ul></li></ul></li></ul></li><li>将当前单词插入到命令行中<br>  常用于替换命令和查看帮助文档<br>  在命令行模式下，<code>&lt;C-r&gt;&lt;C-w&gt;</code>会复制当前单词到命令行中  </li><li>回溯历史命令  <ul><li>回溯所有命令<br>  在<code>:</code>下保持提示符为空按<code>&lt;up&gt;</code>(<code>&lt;C-p&gt;</code>)和<code>&lt;down&gt;</code>(<code>&lt;C-n&gt;</code>)  <ul><li><code>&lt;C-p&gt;`</code><c-n><code>和</code><up><code></code><down><code>的区别    前者方便，但不能过滤命令    解决方法：    在</code>.vim<code>文件中创建映射项</code>cnoremap <c-p> <up><code></code>cnoremap <c-n> <down>`  </down></c-n></up></c-p></down></up></c-n></li></ul></li><li>过滤回溯的命令<br>  如<code>:help&lt;up&gt;</code>回溯以”help”开头的命令  </li><li>命令历史容量设置<br>  缺省只有20<br>  推荐<code>set history=200</code>  </li><li>除了Ex命令，查找命令也会记录下来并保存在另一个文件中   </li></ul></li><li>一次性执行多条命令<br>  用<code>|</code>隔开命令即可  </li><li>命令行窗口<br>  像一个常规的vim缓冲区，其中每一行是命令历史中的一个条目<br>  可以在该窗口下修改命令历史记录，对之前使用过的命令做调整后方便重复利用<br>  <em>当打开命令行窗口时，它始终拥有焦点，除非关闭它，否则无法切换到其他窗口</em>  <ul><li><code>q:</code>：打开Ex命令的命令行窗口   </li><li><code>q/</code>：打开查找命令的命令行窗口  </li><li><code>&lt;C-f&gt;</code>：从命令行模式切换到命令行窗口（起始命令行模式下输入的内容仍然保留下来）  </li></ul></li><li>运行shell命令<br>  <code>!{command}</code>  <ul><li><code>%</code>代表当前文件名  </li><li><code>:shell</code>可以打开一个交互式的shell会话（通过<code>exit</code>退出）<br>  更好的方法是用<code>&lt;C-z&gt;</code>挂起vim所属进程，用<code>fp</code>唤醒挂起的作业<br>  <em>shell下的<code>jobs</code>命令可以查看当前的作业列表</em>  </li></ul></li><li>大量读取或写入命令输入输出<br>  <code>:read !{cmd}</code>将命令输出读取到当前缓冲区中<br>  <code>:write !{cmd}</code>：将当前缓冲区内容作为命令的标准输入   </li><li>借助shell命令过滤指定范围<br>  <code>:{range}!{filter}</code><br>  如用sort命令，<code>:2,$!sort -t &#39;,&#39; -k 2</code>表示第2行到最后一行之间，以逗号为分隔符的第二字段排序  </li><li>常见的和操作缓冲区文本的Ex命令  <ul><li><code>:{range}d {x}</code>：删除指定内容（并保存到寄存器x中）</li><li><code>:{range}y {x}</code>：复制指定内容到寄存器x中  </li><li><code>:{line}put {x}</code>：在指定行后粘贴寄存器x中的内容   </li><li><code>:{range}t {address}</code>：复制指定内容到address处  </li><li><code>:{range}m {address}</code>：移动指定内容到address处  </li><li><code>:{range}join</code>：连接指定行内容  </li><li><code>:{range}normal {commands}</code>：对指定范围执行普通模式命令  </li><li><code>:{range}s/{pattern}/{string}/{flag}</code>：替换  </li><li><code>:{range}global/{pattern}/{cmd}</code>：对指定范围内匹配pattern的所有行执行cmd命令（Ex）  </li></ul></li></ul><h2 id="管理多个文件"><a href="#管理多个文件" class="headerlink" title="管理多个文件"></a>管理多个文件</h2><h3 id="缓冲区列表"><a href="#缓冲区列表" class="headerlink" title="缓冲区列表"></a>缓冲区列表</h3><p>文件读取后在内存缓冲区中  </p><ul><li><code>:ls</code>命令可以列出所有被载入到内存中的缓冲区列表   <ul><li>每个条目开头的数字为系统自动分配而<strong>不可改变</strong>的缓冲区编号</li><li><code>%</code>：当前窗口中可见的缓冲区   </li><li><code>#</code>：轮换文件<br>  <strong>按<code>&lt;C-^&gt;</code>可以在两个文件之间快速轮换</strong>  </li></ul></li><li>缓冲区切换<ul><li>遍历缓冲区列表：  <ul><li>正向移动：<code>:bn</code>或<code>:bnext</code>  </li><li>反向移动：<code>:bp</code>或<code>:bprevious</code>  </li></ul></li><li><code>:buffer {N}</code>：跳转到指定编号的缓冲区  </li><li><code>:buffer {bufname}</code>：跳转到可以被bufname唯一标识的缓冲区（若多个缓冲区具有同一标识，可以通过<code>&lt;Tab&gt;</code>选择）  </li></ul></li><li><code>:bufdo {commands}</code>：对所有缓冲区执行Ex命令  </li><li><p>创建快速遍历缓冲区列表的键盘映射  </p><pre><code>  nnoremap &lt;silent&gt; [b :bp&lt;CR&gt;    nnoremap &lt;silent&gt; ]b :bn&lt;CR&gt;    nnoremap &lt;silent&gt; [B :bfirst&lt;CR&gt;    nnoremap &lt;silent&gt; ]B :blast&lt;CR&gt;  </code></pre></li><li><p>删除<code>:bd</code>或<code>:bdelect</code><br>  <code>:bd {N1,N2,N3....}</code>：删除列出的缓冲区<br>  <code>:{N,M} bd</code>：删除连续的缓冲区  </p></li></ul><h3 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h3><p>对一批文件进行分组，其<strong>文件顺序可调整*</strong>   </p><ul><li>初始的参数列表为启动时vim的文件列表   </li><li><code>:args</code>：查看参数列表<br>  其中<code>[]</code>表明了当前的活动文件  </li><li><code>:args {arglist}</code>：填充参数列表<br>  arglist可以是文件名、通配符、shell命令的输出结果等   <ul><li>用文件名指定文件<br>  <code>:args {file1,file2,....}</code>  </li><li>用Glob模式指定文件<br>  <code>*</code>：表示0到无穷多个字符，但不包含子目录<br>  <code>**</code>：表示0到无穷多个字符，包含子目录  </li><li>用反引号结构指定文件<br>  将shell命令用反引号括起来，其输出将填充到相应的位置<br>  如：<code>:args \</code>cat .chapters``  </li></ul></li><li>隐藏缓冲区<br>  缓冲区列表中，<code>+</code>代表缓冲区被修改过，此时切换缓冲区会弹出错误信息，可以用感叹号强制切换，此时列表中被标记为<code>a</code>的为活动缓冲区（active），被标记为<code>h</code>的为隐藏活动区（hidden）  <ul><li>处理隐藏缓冲区  <ul><li><code>:w</code>或<code>:write</code>：写入磁盘  </li><li><code>:e</code>或<code>:edit</code>：从磁盘中读入到缓冲区（回滚修改操作）  </li><li><code>:qa</code>或<code>:qall</code>：关闭所有窗口，放弃所有修改  </li><li><code>:wa</code>或<code>:wall</code>：全部写入磁盘   </li></ul></li><li>用<code>:argdo</code>或<code>:bufdo</code>修改一组缓冲区<br>  前提：<strong>打开<code>hidden</code>选项</strong><br>  打开后，对已修改的缓冲区执行<code>:next</code>,<code>:bnext</code>,<code>cnext</code>等命令无需再加感叹号   </li></ul></li></ul><h3 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h3><ul><li>分割  <ul><li><code>&lt;C-w&gt;s</code>：水平切分（同高度），新窗口仍为当前缓冲区   </li><li><code>&lt;C-w&gt;v</code>：垂直切分（同宽度），新窗口仍未当前缓冲区  </li><li><code>:sp {file}</code>或<code>:split {file}</code>：水平切分，新窗口为file内容   </li><li><code>:vsp {file}</code>或<code>:vsplit {file}</code>：垂直切分，新窗口为file内容  </li></ul></li><li>切换  <ul><li><code>&lt;C-w&gt;w</code>：轮换   </li><li><code>&lt;C-w&gt;{h|j|k|l}</code>：切换到左（下上右）侧的窗口  </li><li>若为GVIM，可以直接点击对用窗口进行切换    </li></ul></li><li>关闭  <ul><li><code>:clo</code>或<code>:close</code>或<code>&lt;C-w&gt;c</code>：关闭当前窗口   </li><li><code>:on</code>或<code>:only</code>或<code>&lt;C-w&gt;o</code>：关闭其他重口   </li></ul></li><li>改变窗口大小和重排列  <ul><li><code>&lt;C-w&gt;=</code>：使所有窗口等宽、等高  </li><li><code>&lt;C-w&gt;_</code>：使当前窗口高度最大化  </li><li><code>&lt;C-w&gt;|</code>：使当前窗口宽度最大化   </li><li><code>{N}&lt;C-w&gt;_</code>：使当前窗口高度调整为N行  </li><li><code>{N}&lt;C-w&gt;|</code>：使当前窗口宽度调整为N列  </li><li>若为GVIM，可以直接用鼠标拖动窗口的分界线   </li></ul></li></ul><h3 id="标签页"><a href="#标签页" class="headerlink" title="标签页"></a>标签页</h3><p>对窗口进行分组   </p><ul><li><code>:lcd {path}</code>：设置工作路径<br>  限定标签页的工程范围  <ul><li>只能改变当前窗口，而不是当前标签页  </li><li>如果要改变标签页的所有窗口，应用<code>:windo lcd {path}</code>   </li></ul></li><li>打开和关闭      <ul><li><code>:tabe {file}</code>或<code>tabedit {file}</code>：在新标签页中打开file  </li><li><code>&lt;C-w&gt;T</code>：将当前窗口移动到一个新标签页   </li><li><code>:tabc</code>或<code>:tabclos</code>：关闭当前标签页及其所有窗口   </li><li><code>:tabo</code>或<code>:tabonly</code>：关闭其他标签页及其所有窗口   </li></ul></li><li>切换  <ul><li>标签页从1开始编号   </li><li><code>:tabn {N}</code>或<code>:tabnext {N}</code>或<code>{N}gt</code>：切换到N号标签页   </li><li><code>:tabn</code>或<code>:tabnext</code>或<code>gt</code>：切换到下一标签页  </li><li><code>:tabp</code>或<code>:tabprevious</code>或<code>gT</code>：切换到上一标签页  </li></ul></li><li>重排列  <ul><li><code>:tabmove {N}</code>：将当前标签页移动到N号标签页之后  <ul><li><code>:tabmove 0</code>：表示移动到开头   </li><li><code>:tabmove</code>：表示移动到末尾   </li></ul></li><li>若为GVIM，可以直接用鼠标拖曳</li></ul></li></ul><h2 id="打开及保存文件"><a href="#打开及保存文件" class="headerlink" title="打开及保存文件"></a>打开及保存文件</h2><p>……待施工……</p>]]></content>
      
      
      <categories>
          
          <category> 1 计算机 </category>
          
          <category> -- 1.3 Linux </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
