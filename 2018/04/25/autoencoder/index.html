<!DOCTYPE html>
<html style="display: none;" lang="zh">
    <head>
    <meta charset="utf-8">
    <!--
        © Material Theme
        https://github.com/viosey/hexo-theme-material
        Version: 1.5.6 -->
    <script>
        window.materialVersion = "1.5.6"
        // Delete localstorage with these tags
        window.oldVersion = [
            'codestartv1',
            '1.3.4',
            '1.4.0',
            '1.4.0b1',
            '1.5.0',
            '1.5.2',
            '1.5.5'
        ]
    </script>

    <!-- dns prefetch -->
    <meta http-equiv="x-dns-prefetch-control" content="on">






    <link rel="dns-prefetch" href="https://cdn-city.livere.com"/>






    <link rel="dns-prefetch" href="https://fonts.proxy.ustclug.org"/>





    <!-- Meta & Info -->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!-- Title -->
    
    <title>
        
            自编码器 | 
        
        Hey~YaHei!
    </title>

    <!-- Favicons -->
    <link rel="icon shortcut" type="image/ico" href="/img/favicon.png">
    <link rel="icon" href="/img/favicon.png">

    <meta name="format-detection" content="telephone=no"/>
    <meta name="description" itemprop="description" content="">
    <meta name="keywords" content=",《Handson-ML》笔记">
    <meta name="theme-color" content="#0097A7">

    <!-- Disable Fucking Bloody Baidu Tranformation -->
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <!--[if lte IE 9]>
        <link rel="stylesheet" href="/css/ie-blocker.css">

        
            <script src="/js/ie-blocker.zhCN.js"></script>
        
    <![endif]-->

    <!-- Import lsloader -->
    <script>(function(){window.lsloader={jsRunSequence:[],jsnamemap:{},cssnamemap:{}};lsloader.removeLS=function(a){try{localStorage.removeItem(a)}catch(b){}};lsloader.setLS=function(a,c){try{localStorage.setItem(a,c)}catch(b){}};lsloader.getLS=function(a){var c="";try{c=localStorage.getItem(a)}catch(b){c=""}return c};versionString="/*"+(window.materialVersion||"unknownVersion")+"*/";lsloader.clean=function(){try{var b=[];for(var a=0;a<localStorage.length;a++){b.push(localStorage.key(a))}b.forEach(function(e){var f=lsloader.getLS(e);if(window.oldVersion){var d=window.oldVersion.reduce(function(g,h){return g||f.indexOf("/*"+h+"*/")!==-1},false);if(d){lsloader.removeLS(e)}}})}catch(c){}};lsloader.clean();lsloader.load=function(f,a,b,d){if(typeof b==="boolean"){d=b;b=undefined}d=d||false;b=b||function(){};var e;e=this.getLS(f);if(e&&e.indexOf(versionString)===-1){this.removeLS(f);this.requestResource(f,a,b,d);return}if(e){var c=e.split(versionString)[0];if(c!=a){console.log("reload:"+a);this.removeLS(f);this.requestResource(f,a,b,d);return}e=e.split(versionString)[1];if(d){this.jsRunSequence.push({name:f,code:e});this.runjs(a,f,e)}else{document.getElementById(f).appendChild(document.createTextNode(e));b()}}else{this.requestResource(f,a,b,d)}};lsloader.requestResource=function(b,e,a,c){var d=this;if(c){this.iojs(e,b,function(h,f,g){d.setLS(f,h+versionString+g);d.runjs(h,f,g)})}else{this.iocss(e,b,function(f){document.getElementById(b).appendChild(document.createTextNode(f));d.setLS(b,e+versionString+f)},a)}};lsloader.iojs=function(d,b,g){var a=this;a.jsRunSequence.push({name:b,code:""});try{var f=new XMLHttpRequest();f.open("get",d,true);f.onreadystatechange=function(){if(f.readyState==4){if((f.status>=200&&f.status<300)||f.status==304){if(f.response!=""){g(d,b,f.response);return}}a.jsfallback(d,b)}};f.send(null)}catch(c){a.jsfallback(d,b)}};lsloader.iocss=function(f,c,h,a){var b=this;try{var g=new XMLHttpRequest();g.open("get",f,true);g.onreadystatechange=function(){if(g.readyState==4){if((g.status>=200&&g.status<300)||g.status==304){if(g.response!=""){h(g.response);a();return}}b.cssfallback(f,c,a)}};g.send(null)}catch(d){b.cssfallback(f,c,a)}};lsloader.iofonts=function(f,c,h,a){var b=this;try{var g=new XMLHttpRequest();g.open("get",f,true);g.onreadystatechange=function(){if(g.readyState==4){if((g.status>=200&&g.status<300)||g.status==304){if(g.response!=""){h(g.response);a();return}}b.cssfallback(f,c,a)}};g.send(null)}catch(d){b.cssfallback(f,c,a)}};lsloader.runjs=function(f,c,e){if(!!c&&!!e){for(var b in this.jsRunSequence){if(this.jsRunSequence[b].name==c){this.jsRunSequence[b].code=e}}}if(!!this.jsRunSequence[0]&&!!this.jsRunSequence[0].code&&this.jsRunSequence[0].status!="failed"){var a=document.createElement("script");a.appendChild(document.createTextNode(this.jsRunSequence[0].code));a.type="text/javascript";document.getElementsByTagName("head")[0].appendChild(a);this.jsRunSequence.shift();if(this.jsRunSequence.length>0){this.runjs()}}else{if(!!this.jsRunSequence[0]&&this.jsRunSequence[0].status=="failed"){var d=this;var a=document.createElement("script");a.src=this.jsRunSequence[0].path;a.type="text/javascript";this.jsRunSequence[0].status="loading";a.onload=function(){d.jsRunSequence.shift();if(d.jsRunSequence.length>0){d.runjs()}};document.body.appendChild(a)}}};lsloader.tagLoad=function(b,a){this.jsRunSequence.push({name:a,code:"",path:b,status:"failed"});this.runjs()};lsloader.jsfallback=function(c,b){if(!!this.jsnamemap[b]){return}else{this.jsnamemap[b]=b}for(var a in this.jsRunSequence){if(this.jsRunSequence[a].name==b){this.jsRunSequence[a].code="";this.jsRunSequence[a].status="failed";this.jsRunSequence[a].path=c}}this.runjs()};lsloader.cssfallback=function(e,c,b){if(!!this.cssnamemap[c]){return}else{this.cssnamemap[c]=1}var d=document.createElement("link");d.type="text/css";d.href=e;d.rel="stylesheet";d.onload=d.onerror=b;var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(d,a)};lsloader.runInlineScript=function(c,b){var a=document.getElementById(b).innerText;this.jsRunSequence.push({name:c,code:a});this.runjs()}})();</script>

    <!-- Import queue -->
    <script>function Queue(){this.dataStore=[];this.offer=b;this.poll=d;this.execNext=a;this.debug=false;this.startDebug=c;function b(e){if(this.debug){console.log("Offered a Queued Function.")}if(typeof e==="function"){this.dataStore.push(e)}else{console.log("You must offer a function.")}}function d(){if(this.debug){console.log("Polled a Queued Function.")}return this.dataStore.shift()}function a(){var e=this.poll();if(e!==undefined){if(this.debug){console.log("Run a Queued Function.")}e()}}function c(){this.debug=true}}var queue=new Queue();</script>

    <!-- Import CSS -->
    
        <style id="material_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_css","/css/material.min.css?Z7a72R1E4SxzBKR/WGctOA==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>
        <style id="style_css"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("style_css","/css/style.min.css?NKhlKQkXw/c66TR5p4wO+w==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>

        

    

    

    <!-- Config CSS -->

<!-- Other Styles -->
<style>
  body, html {
    font-family: Roboto, "Helvetica Neue", Helvetica, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "微软雅黑", Arial, sans-serif;
    overflow-x: hidden !important;
  }
  
  code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  a {
    color: #00838F;
  }

  .mdl-card__media,
  #search-label,
  #search-form-label:after,
  #scheme-Paradox .hot_tags-count,
  #scheme-Paradox .sidebar_archives-count,
  #scheme-Paradox .sidebar-colored .sidebar-header,
  #scheme-Paradox .sidebar-colored .sidebar-badge{
    background-color: #0097A7 !important;
  }

  /* Sidebar User Drop Down Menu Text Color */
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:hover,
  #scheme-Paradox .sidebar-colored .sidebar-nav>.dropdown>.dropdown-menu>li>a:focus {
    color: #0097A7 !important;
  }

  #post_entry-right-info,
  .sidebar-colored .sidebar-nav li:hover > a,
  .sidebar-colored .sidebar-nav li:hover > a i,
  .sidebar-colored .sidebar-nav li > a:hover,
  .sidebar-colored .sidebar-nav li > a:hover i,
  .sidebar-colored .sidebar-nav li > a:focus i,
  .sidebar-colored .sidebar-nav > .open > a,
  .sidebar-colored .sidebar-nav > .open > a:hover,
  .sidebar-colored .sidebar-nav > .open > a:focus,
  #ds-reset #ds-ctx .ds-ctx-entry .ds-ctx-head a {
    color: #0097A7 !important;
  }

  .toTop {
    background: #757575 !important;
  }

  .material-layout .material-post>.material-nav,
  .material-layout .material-index>.material-nav,
  .material-nav a {
    color: #757575;
  }

  #scheme-Paradox .MD-burger-layer {
    background-color: #757575;
  }

  #scheme-Paradox #post-toc-trigger-btn {
    color: #757575;
  }

  .post-toc a:hover {
    color: #00838F;
    text-decoration: underline;
  }

</style>


<!-- Theme Background Related-->

    <style>
      body{
        background-color: #F5F5F5;
      }

      /* blog_info bottom background */
      #scheme-Paradox .material-layout .something-else .mdl-card__supporting-text{
        background-color: #fff;
      }
    </style>




<!-- Fade Effect -->

    <style>
      .fade {
        transition: all 800ms linear;
        -webkit-transform: translate3d(0,0,0);
        -moz-transform: translate3d(0,0,0);
        -ms-transform: translate3d(0,0,0);
        -o-transform: translate3d(0,0,0);
        transform: translate3d(0,0,0);
        opacity: 1;
      }

      .fade.out{
        opacity: 0;
      }
    </style>


<!-- Import Font -->
<!-- Import Roboto -->

    <link href="https://fonts.proxy.ustclug.org/css?family=Roboto:300,400,500" rel="stylesheet">


<!-- Import Material Icons -->


    <style id="material_icons"></style><script>if(typeof window.lsLoadCSSMaxNums === "undefined")window.lsLoadCSSMaxNums = 0;window.lsLoadCSSMaxNums++;lsloader.load("material_icons","/css/material-icons.css?pqhB/Rd/ab0H2+kZp0RDmw==",function(){if(typeof window.lsLoadCSSNums === "undefined")window.lsLoadCSSNums = 0;window.lsLoadCSSNums++;if(window.lsLoadCSSNums == window.lsLoadCSSMaxNums)document.documentElement.style.display="";}, false)</script>




    <!-- Import jQuery -->
    
        <script>lsloader.load("jq_js","https://cdn.bootcss.com/jquery/2.2.0/jquery.min.js", true)</script>
    

    <!-- WebAPP Icons -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="application-name" content="Hey~YaHei!">
    <meta name="msapplication-starturl" content="https://hey-yahei.cn/2018/04/25/autoencoder/">
    <meta name="msapplication-navbutton-color" content="#0097A7">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-title" content="Hey~YaHei!">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon" href="/img/favicon.png">

    <!-- Site Verification -->
    <meta name="google-site-verification" content="9RYmZ6xORQYm4BISDRGPi57Oj0NzoKAN-U9ZR6UxSxY" />
    <meta name="baidu-site-verification" content="xij8ev3DGx" />

    <!-- RSS -->
    
        
            <link rel=alternate type="application/rss+xml" href="/rss2.xml">
        
    

    <!-- The Open Graph protocol -->
    <meta property="og:url" content="https://hey-yahei.cn/2018/04/25/autoencoder/">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="自编码器 | Hey~YaHei!">
    <meta property="og:image" content="/img/favicon.png">
    <meta property="og:description" content="">
    <meta property="og:article:tag" content="《Handson-ML》笔记"> 

    
        <meta property="article:published_time" content="Wed Apr 25 2018 00:00:00 GMT+0800">
        <meta property="article:modified_time" content="Fri Aug 24 2018 12:31:59 GMT+0800">
    

    <!-- The Twitter Card protocol -->
    <meta name="twitter:card" content="summary_large_image">

    <!-- Add canonical link for SEO -->
    
        <link rel="canonical" href="https://hey-yahei.cn/2018/04/25/autoencoder/index.html" />
    

    <!-- Structured-data for SEO -->
    
        


<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": "https://hey-yahei.cn/2018/04/25/autoencoder/index.html",
    "headline": "自编码器",
    "datePublished": "Wed Apr 25 2018 00:00:00 GMT+0800",
    "dateModified": "Fri Aug 24 2018 12:31:59 GMT+0800",
    "author": {
        "@type": "Person",
        "name": "YaHei",
        "image": {
            "@type": "ImageObject",
            "url": "/imgs/avatar2.jpg"
        },
        "description": "Hey!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "Hey~YaHei!",
        "logo": {
            "@type":"ImageObject",
            "url": "/img/favicon.png"
        }
    },
    "keywords": ",《Handson-ML》笔记",
    "description": "",
}
</script>


    

    <!-- Analytics -->
    
    
    

    <!-- Custom Head -->
    

<link rel="stylesheet" href="/css/prism.css" type="text/css"></head>


    
        <body id="scheme-Paradox" class="lazy">
            <div class="material-layout  mdl-js-layout has-drawer is-upgraded">
                

                <!-- Main Container -->
                <main class="material-layout__content" id="main">

                    <!-- Top Anchor -->
                    <div id="top"></div>

                    
                        <!-- Hamburger Button -->
                        <button class="MD-burger-icon sidebar-toggle">
                            <span id="MD-burger-id" class="MD-burger-layer"></span>
                        </button>
                    

                    <!-- Post TOC -->

    
    <!-- Back Button -->
    <!--
    <div class="material-back" id="backhome-div" tabindex="0">
        <a class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon"
           href="#" onclick="window.history.back();return false;"
           target="_self"
           role="button"
           data-upgraded=",MaterialButton,MaterialRipple">
            <i class="material-icons" role="presentation">arrow_back</i>
            <span class="mdl-button__ripple-container">
                <span class="mdl-ripple"></span>
            </span>
        </a>
    </div>
    -->


    <!-- Left aligned menu below button -->
    
    
    <button id="post-toc-trigger-btn"
        class="mdl-button mdl-js-button mdl-button--icon">
        <i class="material-icons">format_list_numbered</i>
    </button>

    <ul class="post-toc-wrap mdl-menu mdl-menu--bottom-left mdl-js-menu mdl-js-ripple-effect" for="post-toc-trigger-btn" style="max-height:80vh; overflow-y:scroll;">
        <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#数据的高效表示"><span class="post-toc-number">1.</span> <span class="post-toc-text">数据的高效表示</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#简单的线性自编码器"><span class="post-toc-number">2.</span> <span class="post-toc-text">简单的线性自编码器</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）"><span class="post-toc-number">3.</span> <span class="post-toc-text">深层自编码器（Deep autoencoders, or Stacked autoencoders）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#权重共享"><span class="post-toc-number">4.</span> <span class="post-toc-text">权重共享</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#逐层训练"><span class="post-toc-number">5.</span> <span class="post-toc-text">逐层训练</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#重构效果可视化"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">重构效果可视化</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#特征可视化"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">特征可视化</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#用自编码器作无监督预训练"><span class="post-toc-number">6.</span> <span class="post-toc-text">用自编码器作无监督预训练</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#不同约束下的自编码器"><span class="post-toc-number">7.</span> <span class="post-toc-text">不同约束下的自编码器</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#降噪自编码器（Denoising-Autoencoders）"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">降噪自编码器（Denoising Autoencoders）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#稀疏自编码器（Sparse-Autoencoders）"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">稀疏自编码器（Sparse Autoencoders）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#变分自编码器（Variational-Autoencoders）"><span class="post-toc-number">7.3.</span> <span class="post-toc-text">变分自编码器（Variational Autoencoders）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#其他自编码器"><span class="post-toc-number">7.4.</span> <span class="post-toc-text">其他自编码器</span></a></li></ol></li></ol>
    </ul>
    




<!-- Layouts -->

    <!-- Post Module -->
    <div class="material-post_container">

        <div class="material-post mdl-grid">
            <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">

                <!-- Post Header(Thumbnail & Title) -->
                
    <!-- Paradox Post Header -->
    
        
            <!-- Random Thumbnail -->
            <div class="post_thumbnail-random mdl-card__media mdl-color-text--grey-50">
            <script type="text/ls-javascript" id="post-thumbnail-script">
    var randomNum = Math.floor(Math.random() * 19 + 1);

    $('.post_thumbnail-random').attr('data-original', '/img/random/material-' + randomNum + '.png');
    $('.post_thumbnail-random').addClass('lazy');
</script>

        
    
            <p class="article-headline-p">
                自编码器
            </p>
        </div>





                
                    <!-- Paradox Post Info -->
                    <div class="mdl-color-text--grey-700 mdl-card__supporting-text meta">

    <!-- Author Avatar -->
    <div id="author-avatar">
        <img src="/imgs/avatar2.jpg" width="44px" height="44px" alt="Author Avatar"/>
    </div>
    <!-- Author Name & Date -->
    <div>
        <strong>YaHei</strong>
        <span>4月 25, 2018</span>
    </div>

    <div class="section-spacer"></div>

    <!-- Favorite -->
    <!--
        <button id="article-functions-like-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon btn-like">
            <i class="material-icons" role="presentation">favorite</i>
            <span class="visuallyhidden">favorites</span>
        </button>
    -->

    <!-- Qrcode -->
    

    <!-- Tags (bookmark) -->
    
    <button id="article-functions-viewtags-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
        <i class="material-icons" role="presentation">bookmark</i>
        <span class="visuallyhidden">bookmark</span>
    </button>
    <ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-functions-viewtags-button">
        <li class="mdl-menu__item">
        <a class="post_tag-link" href="/tags/《Handson-ML》笔记/">《Handson-ML》笔记</a>
    </ul>
    

    <!-- Share -->
    
        <button id="article-fuctions-share-button" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon">
    <i class="material-icons" role="presentation">share</i>
    <span class="visuallyhidden">share</span>
</button>
<ul class="mdl-menu mdl-menu--bottom-right mdl-js-menu mdl-js-ripple-effect" for="article-fuctions-share-button">
    

    

    <!-- Share Weibo -->
    
        <a class="post_share-link" href="http://service.weibo.com/share/share.php?appkey=&title=自编码器&url=https://hey-yahei.cn/2018/04/25/autoencoder/index.html&pic=https://hey-yahei.cn/img/favicon.png&searchPic=false&style=simple" target="_blank">
            <li class="mdl-menu__item">
                分享到微博
            </li>
        </a>
    

    <!-- Share Twitter -->
    

    <!-- Share Facebook -->
    

    <!-- Share Google+ -->
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    
        <a class="post_share-link" href="http://connect.qq.com/widget/shareqq/index.html?site=Hey~YaHei!&title=自编码器&summary=&pics=https://hey-yahei.cn/img/favicon.png&url=https://hey-yahei.cn/2018/04/25/autoencoder/index.html" target="_blank">
            <li class="mdl-menu__item">
                分享到 QQ
            </li>
        </a>
    

    <!-- Share Telegram -->
    
</ul>

    
</div>

                

                <!-- Post Content -->
                <div id="post-content" class="mdl-color-text--grey-700 mdl-card__supporting-text fade out">
    
        <p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=489970553&auto=0&height=32"></iframe><br>BGM：<strong>《末日时在做什么？有没有空？可以来拯救吗？》第九话插入曲</strong><br>这番名字真是狗血，但剧情和音乐出奇的不错       </p>
<hr>
<p>参考<a href="https://book.douban.com/subject/26840215/" target="_blank" rel="noopener">《Hands-On Machine Learning with Scikit-Learn and TensorFlow(2017)》</a>Chap15<br><a href="/2018/04/08/Handson-ML/">《Hands-On Machine Learning with Scikit-Learn and TensorFlow》笔记</a>        </p>
<p>自编码器工作方式非常简单，就是学习如何模仿输入来产生输出；<br>我们会采取各种约束（比如限制输出大小、加噪等）来避免自编码器纯粹地把输入作为输出，从而得到一个高效的数据表示方式；<br>简而言之，自编码器通过尝试学习某些约束下的特征函数来产生输入的编码，也即一种高效的数据表示；<br>自编码器可以无监督学习输入数据的编码方式、降低数据维度、作为特征检测器、作为生成模型等等……      </p>
<h3 id="数据的高效表示"><a href="#数据的高效表示" class="headerlink" title="数据的高效表示"></a>数据的高效表示</h3><p>论文 <a href="https://www.sciencedirect.com/science/article/pii/0010028573900042" target="_blank" rel="noopener">Perception in chess(1973)</a> 研究了记忆、概念、模式匹配之间的联系；     </p>
<p>自编码器可以分为Encoder和Decoder两部分，<br>Encoder也称识别网络，用于将输入转换为某种内部表示；<br>Decoder也称生成网络，用于将内部表示转换成输出；<br>架构跟MLP一样，不过他的<strong>输出神经元数与输入数相等</strong>，<strong>中间层的神经元数小于输入数</strong>；<br>也就是说，中间层的输出必定是输入的一个不完全表示，我们的目的就在于训练出一个输出与输入相近的网络——<br><strong>可以理解为Encoder是对输入的一个有损压缩，Decoder对其进行解压，我们要训练一个损耗率尽可能小的网络</strong><br><img src="/Handson-ML/15Simple_Autoencoder.png" alt="15Simple_Autoencoder">       </p>
<h3 id="简单的线性自编码器"><a href="#简单的线性自编码器" class="headerlink" title="简单的线性自编码器"></a>简单的线性自编码器</h3><p>无非线性激活，MSE损失函数，可以实现一个PCA；       </p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers <span class="token keyword">import</span> fully_connected

<span class="token comment" spellcheck="true"># 三维输入编码为二维表示</span>
n_inputs <span class="token operator">=</span> <span class="token number">3</span>
n_hidden <span class="token operator">=</span> <span class="token number">2</span>
n_outputs <span class="token operator">=</span> n_inputs

learning_rate <span class="token operator">=</span> <span class="token number">0.01</span>

X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>
hidden <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 纯线性，无激活</span>
outputs <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>

reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># MSE</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>
training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>reconstruction_loss<span class="token punctuation">)</span>

init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>

X_train<span class="token punctuation">,</span> X_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span>     <span class="token comment" spellcheck="true"># 载入数据集</span>

n_iterations <span class="token operator">=</span> <span class="token number">1000</span>
codings <span class="token operator">=</span> hidden     <span class="token comment" spellcheck="true"># 自编码器的目的是获取编码，也即中间层的输出</span>

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
    init<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
        training_op<span class="token punctuation">.</span>run<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_train<span class="token punctuation">}</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 无监督</span>
    codings_val <span class="token operator">=</span> codings<span class="token punctuation">.</span>eval<span class="token punctuation">(</span>feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>X<span class="token punctuation">:</span> X_test<span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre>
<h3 id="深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）"><a href="#深层自编码器（Deep-autoencoders-or-Stacked-autoencoders）" class="headerlink" title="深层自编码器（Deep autoencoders, or Stacked autoencoders）"></a>深层自编码器（Deep autoencoders, or Stacked autoencoders）</h3><p>深层自编码器为中心对称的“三明治”结构，最中间的一层产生实际的编码，称编码层（Coding Layer），如——<br><img src="/Handson-ML/15Stacked_Autoencoder.png" alt="15Stacked_Autoencoder">       </p>
<p>深度学习的训练技术（如<a href="/2018/04/08/梯度消失与梯度爆炸">解决梯度爆炸与梯度消失的技术</a>、<a href="/2018/04/09/复用预训练层">复用预训练层</a>、<a href="/2018/04/10/优化器">优化器</a>、<a href="/2018/04/11/正则化技术">正则化技术</a>等）依旧适用；<br>与之前讲述的网络的区别在于，自编码器没有标注，是无监督学习     </p>
<h3 id="权重共享"><a href="#权重共享" class="headerlink" title="权重共享"></a>权重共享</h3><p>由于结构是中心对称的，Encoder和Decoder可以直接共享权重，但tensorflow中的 <code>fully_connected()</code> 没法共享权重，所以需要手动书写全连接层——    </p>
<pre class=" language-python"><code class="language-python">activation <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu
regularizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>l2_regularizer<span class="token punctuation">(</span>l2_reg<span class="token punctuation">)</span>
initializer <span class="token operator">=</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>

X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>

weights1_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_inputs<span class="token punctuation">,</span> n_hidden1<span class="token punctuation">]</span><span class="token punctuation">)</span>
weights2_init <span class="token operator">=</span> initializer<span class="token punctuation">(</span><span class="token punctuation">[</span>n_hidden1<span class="token punctuation">,</span> n_hidden2<span class="token punctuation">]</span><span class="token punctuation">)</span>

weights1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights1_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights1"</span><span class="token punctuation">)</span>
weights2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>weights2_init<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights2"</span><span class="token punctuation">)</span>
weights3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights2<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights3"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>
weights4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>weights1<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"weights4"</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># Decoder，共享Encoder的权重</span>

biases1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden1<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases1"</span><span class="token punctuation">)</span>
biases2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden2<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases2"</span><span class="token punctuation">)</span>
biases3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_hidden3<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases3"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>
biases4 <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_outputs<span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"biases4"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Decoder，但偏置还是独立的</span>

hidden1 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> weights1<span class="token punctuation">)</span> <span class="token operator">+</span> biases1<span class="token punctuation">)</span>
hidden2 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>
hidden3 <span class="token operator">=</span> activation<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> weights3<span class="token punctuation">)</span> <span class="token operator">+</span> biases3<span class="token punctuation">)</span>
outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden3<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4

reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>
reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 只对weights1和weights2施加正则化</span>
loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> reg_loss

optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>
training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

init <span class="token operator">=</span> tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<h3 id="逐层训练"><a href="#逐层训练" class="headerlink" title="逐层训练"></a>逐层训练</h3><p>逐层训练然后将各层堆叠起来，要比直接训练一个堆叠好的自编码器要快得多；<br>比如训练一个三层的自编码器：     </p>
<ol>
<li>只保留第一隐藏层进行训练    </li>
<li>叠加第二、第三隐藏层进行训练（<em>这里第一隐藏层和第三隐藏层的权重不共享？</em>）       </li>
<li>各层训练完毕，叠加起来得到一个完整的自编码器       </li>
</ol>
<p><img src="/Handson-ML/15Training_One_Autoencoder_at_A_Time.png" alt="15Training_One_Autoencoder_at_A_Time">     </p>
<p>可以按照这个描述，分别构造多个计算图来进行训练；<br>更巧妙的方式是添加一些操作在同一张计算图中区分训练阶段——<br><img src="/Handson-ML/15A_Single_Graph_To_Train_A_Stacked_Autoencoder.png" alt="15A_Single_Graph_To_Train_A_Stacked_Autoencoder">     </p>
<ol>
<li>中间部分是一个完整的自编码器   </li>
<li>左侧是第一训练阶段，旁路了第二和第三隐藏层，<code>Phase 1 Outputs</code> 跟 完整模型中的 <code>Outputs</code> 是共享参数的；<br> 这一阶段目标是使得最终输出与输入接近，训练 <code>Hidden 1</code> 和 <code>Outputs</code> 的权重      </li>
<li>右侧是第二训练阶段，旁路了输出层，第一隐藏层参数固定，只训练第二和第三隐藏层<br> 这一阶段目标是使得 <code>Hidden 3</code> 的输出与 <code>Hidden 1</code> 的输出接近，训练 <code>Hidden 2</code> 和 <code>Hidden 3</code> 的权重       </li>
</ol>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Build the whole stacked autoencoder normally.</span>
<span class="token comment" spellcheck="true"># In this example, the weights are not tied.</span>

<span class="token comment" spellcheck="true"># [...] </span>

optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase1"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 阶段一的输出层，与完整模型的输出层共享参数</span>
    phase1_outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights4<span class="token punctuation">)</span> <span class="token operator">+</span> biases4

    phase1_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>phase1_outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>
    phase1_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights1<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights4<span class="token punctuation">)</span>
    phase1_loss <span class="token operator">=</span> phase1_reconstruction_loss <span class="token operator">+</span> phase1_reg_loss

    phase1_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase1_loss<span class="token punctuation">)</span>

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>name_scope<span class="token punctuation">(</span><span class="token string">"phase2"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    phase2_reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3 <span class="token operator">-</span> hidden1<span class="token punctuation">)</span><span class="token punctuation">)</span>
    phase2_reg_loss <span class="token operator">=</span> regularizer<span class="token punctuation">(</span>weights2<span class="token punctuation">)</span> <span class="token operator">+</span> regularizer<span class="token punctuation">(</span>weights3<span class="token punctuation">)</span>
    phase2_loss <span class="token operator">=</span> phase2_reconstruction_loss <span class="token operator">+</span> phase2_reg_loss

    <span class="token comment" spellcheck="true"># 忽略weights1和biases1</span>
    train_vars <span class="token operator">=</span> <span class="token punctuation">[</span>weights2<span class="token punctuation">,</span> biases2<span class="token punctuation">,</span> weights3<span class="token punctuation">,</span> biases3<span class="token punctuation">]</span>
    phase2_training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>phase2_loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>train_vars<span class="token punctuation">)</span>
</code></pre>
<p>由于阶段一已经计算了 <code>hidden 1</code> 的输出，而且阶段二中 <code>hidden 1</code> 参数固定；<br>在内存足够的情况下可以先计算出整个batch上的输出并保留，以减少阶段二的训练时间；<br>这与<a href="/2018/04/09/复用预训练层/#加速训练：复用frozen层的输出结果（牺牲空间）">复用预训练层 - 复用frozen层输出加速训练</a>类似      </p>
<h4 id="重构效果可视化"><a href="#重构效果可视化" class="headerlink" title="重构效果可视化"></a>重构效果可视化</h4><p>直接显示压缩再解压后的结果，与输入进行直观的比较进行判断；<br>可以初步判断重构的效果       </p>
<h4 id="特征可视化"><a href="#特征可视化" class="headerlink" title="特征可视化"></a>特征可视化</h4><p>对于高层神经元，尤其是最后一个隐藏层的神经元，可以直接观察特定的输入时哪些神经元激活程度比较高；<br>但底层神经元关注的是更抽象、更小的特征，是我们无法直接理解的特征；   </p>
<ol>
<li>用权重分布图来观察每一个神经元的关注点<br> 比如观察第一隐藏层前五个神经元的关注点：     <pre class=" language-python"><code class="language-python"> <span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
     <span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># train autoencoder</span>
     weights1_val <span class="token operator">=</span> weights1<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>
     plot_image<span class="token punctuation">(</span>weights1_val<span class="token punctuation">.</span>T<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
 <img src="/Handson-ML/15Visualzing_The_Feature.png" alt="15Visualzing_The_Feature"><br> <em>【越是关注的像素点，权重越大，在灰度图中就越亮】</em><br> 可以看到前四个神经元关注的都是某些局部的小区域，第五个神经元则似乎更关注竖直的笔画       </li>
<li>随机输入一个图像，然后用反向传播不断改变图像以最大化某个神经元的激活程度<br> 经过一定的迭代次数之后，图像将被扭曲为明显带有该神经元所关注特征的图像       </li>
<li>如果用自编码器用于某些任务（如分类任务）的前期无监督预训练<br> 那么可以直接通过观察这些任务的最终表现，来判断自编码器的效果      </li>
</ol>
<h3 id="用自编码器作无监督预训练"><a href="#用自编码器作无监督预训练" class="headerlink" title="用自编码器作无监督预训练"></a>用自编码器作无监督预训练</h3><p><a href="/2018/04/09/复用预训练层/#无监督预训练">《Handson-ML》笔记 - 无监督预训练</a><br>论文：<a href="http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep" target="_blank" rel="noopener">Greedy Layer-Wise Training of Deep Networks(2006)</a>   </p>
<p><img src="/Handson-ML/15Unsupervised_Pretraining.png" alt="15Unsupervised_Pretraining">       </p>
<h3 id="不同约束下的自编码器"><a href="#不同约束下的自编码器" class="headerlink" title="不同约束下的自编码器"></a>不同约束下的自编码器</h3><p>添加约束，避免自编码器纯粹地把输入作为输出，从而得到一个更加高效的数据表示方式       </p>
<h4 id="降噪自编码器（Denoising-Autoencoders）"><a href="#降噪自编码器（Denoising-Autoencoders）" class="headerlink" title="降噪自编码器（Denoising Autoencoders）"></a>降噪自编码器（Denoising Autoencoders）</h4><p>论文：<br><a href="https://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf" target="_blank" rel="noopener">Extracting and Composing Robust Features with Denoising Autoencoders(2008)</a> 提出自编码器可以用于特征提取；<br><a href="http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf" target="_blank" rel="noopener">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion(2010)</a> 提出降噪自编码器；      </p>
<p>降噪自编码器通过<strong>对输入加入高斯噪声</strong>或者<strong>在输入之后紧接一个dropout层</strong>，可以有效的避免输入噪声对模型的影响；<br><img src="/Handson-ML/15Denoising_Autoencoders.png" alt="15Denoising_Autoencoders">        </p>
<p>具体实现：<br>【高斯噪声方案】      </p>
<pre class=" language-python"><code class="language-python">X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>
X_noisy <span class="token operator">=</span> X <span class="token operator">+</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [...] 其他层</span>
</code></pre>
<p>【dropout层方案】    </p>
<pre class=" language-python"><code class="language-python">keep_prob <span class="token operator">=</span> <span class="token number">0.7</span>

is_training <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder_with_default<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'is_training'</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>None<span class="token punctuation">,</span> n_inputs<span class="token punctuation">]</span><span class="token punctuation">)</span>
X_drop <span class="token operator">=</span> dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> keep_prob<span class="token punctuation">,</span> is_training<span class="token operator">=</span>is_training<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># [...] 其他层</span>
</code></pre>
<h4 id="稀疏自编码器（Sparse-Autoencoders）"><a href="#稀疏自编码器（Sparse-Autoencoders）" class="headerlink" title="稀疏自编码器（Sparse Autoencoders）"></a>稀疏自编码器（Sparse Autoencoders）</h4><p>基本思想是为损失函数添加适当的稀疏度损失项，使得对于每个输入，编码层只有一小部分神经元被显著激活；<br>这使得编码器可以更好地提取出特征；<br><em>如果只让你用一句话描述自己的想法，那你可能会深思熟虑如何更好的表达</em>      </p>
<p>在每次迭代前，都必须先评估编码层的实际稀疏程度——计算编码层中活跃神经元的平均数量；<br>为了获取比较准确的平均数量，batch的大小一定不能太小；       </p>
<p>接下来我们为损失函数添加一个关于神经元活跃程度的惩罚项，比如用MSE；<br>但更好的方式是用具有更大梯度的<strong>KL散度（Kullback–Leibler divergence）</strong>；        </p>
<p><img src="/Handson-ML/15Sparsity loss.png" alt="15Sparsity loss">     </p>
<p>对于两个离散概率分布P和Q，KL散度表示为——<br>$$ D_{KL}(P||Q) = \sum_i P(i) log \frac{P(i)}{Q(i)} $$       </p>
<p>具体到稀疏自编码器上，对于评估的激活概率q和目标激活概率p，KL散度为（激活/不激活 是二项分布的）——<br>$$ D_{KL}(p||q) = p log \frac{p}{q} + (1-p) log \frac{1-p}{1-q} $$       </p>
<p>一旦计算出编码层上每个神经元的稀疏度损失，就可以把他们都加和到损失函数上进行训练；<br>为了权衡稀疏度损失和重构损失的重要性，可以为加和的稀疏度损失额外添加一个数值合适的权重超参数进行训练；       </p>
<p>具体实现：    </p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算KL散度</span>
<span class="token keyword">def</span> <span class="token function">kl_divergence</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> q<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> p <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p <span class="token operator">/</span> q<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> p<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> q<span class="token punctuation">)</span><span class="token punctuation">)</span>

learning_rate <span class="token operator">=</span> <span class="token number">0.01</span>
sparsity_target <span class="token operator">=</span> <span class="token number">0.1</span>
sparsity_weight <span class="token operator">=</span> <span class="token number">0.2</span>

<span class="token comment" spellcheck="true"># [...] # Build a normal autoencoder (in this example the coding layer is hidden1)</span>
<span class="token comment" spellcheck="true"># 注意：编码层的激活程度必须是在(0,1)区间的</span>
<span class="token comment" spellcheck="true">#    比如可以用sigmoid函数强制激活程度归一化为(0,1)区间的数值</span>
<span class="token comment" spellcheck="true">#    hidden1 = tf.nn.sigmoid(tf.matmul(X, weights1) + biases1)</span>

optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>learning_rate<span class="token punctuation">)</span>
hidden1_mean <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 计算整个batch上的平均值</span>
sparsity_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kl_divergence<span class="token punctuation">(</span>sparsity_target<span class="token punctuation">,</span> hidden1_mean<span class="token punctuation">)</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算稀疏度损失总和</span>
reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>outputs <span class="token operator">-</span> X<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算重构损失（MSE）</span>
loss <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> sparsity_weight <span class="token operator">*</span> sparsity_loss    <span class="token comment" spellcheck="true"># 计算全局损失</span>

training_op <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
</code></pre>
<p>把重构损失 <code>reconstruction_loss</code> 的计算改为交叉熵可以加速收敛，但要注意交叉熵要求输入归一化，因此——     </p>
<pre class=" language-python"><code class="language-python">logits <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden1<span class="token punctuation">,</span> weights2<span class="token punctuation">)</span> <span class="token operator">+</span> biases2<span class="token punctuation">)</span>
outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 训练时outputs不是必要的，只是为了看到重构结果才计算outputs</span>
reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span> <span class="token punctuation">)</span>
</code></pre>
<h4 id="变分自编码器（Variational-Autoencoders）"><a href="#变分自编码器（Variational-Autoencoders）" class="headerlink" title="变分自编码器（Variational Autoencoders）"></a>变分自编码器（Variational Autoencoders）</h4><p>论文：<a href="https://arxiv.org/pdf/1312.6114v10.pdf" target="_blank" rel="noopener">Auto-Encoding Variational Bayes(2014)</a>     </p>
<p>主要特点：     </p>
<ol>
<li>概率自编码器，输出是有一定偶然性的，即使是训练完之后      </li>
<li>生成自编码器，能够生成类似他们在训练集上采样的实例     </li>
</ol>
<p>这跟RBMs有些类似，但变分自编码器更加容易训练而且采样更快！     </p>
<p>其结构如下所示，编码层不再直接输出编码，而是在 $\mu$ 附近随机采样——<br><img src="/Handson-ML/15Variational_Autoencoder.png" alt="15Variational_Autoencoder">    </p>
<p>比如下图的输入数据，编码层将在以 $\mu$ 为中心，半径为 $\sigma$ 的范围内随机采样作为编码结果；<br><img src="/Handson-ML/15Variational_Autoencoder_Instance.png" alt="15Variational_Autoencoder_Instance">    </p>
<p>损失函数分为两个部分：    </p>
<ol>
<li>重构损失    </li>
<li><p>隐藏损失：即编码层上的在高斯分布下的损失（这一部分用高斯分布的目标分布和实际分布的KL散度来表示）<br> 高斯分布的噪声使传输给编码层的信息数量受限，迫使网络学习一些有意义的特征；<br> 这在数学计算上复杂了不少，不过可以用下列这个式子进行简化——<br> $$ L_l = \frac{1}{2} \sum \sigma^2 + \mu^2 - 1 - log(eps + \sigma^2) $$<br> 通常取 $eps=10^{-10}$，用于防止 $log(0)$ 的情况出现；<br> 有一种变种的损失函数——<br> $$ L_l = \frac{1}{2} \sum e^\gamma + \mu^2 - 1 - \gamma $$<br> 其中 $\gamma = log(\sigma^2)$，即 $\sigma = e^{\gamma / 2}$；<br> 该变种使得不同尺度下的 $\sigma$ 更容易被捕获，从而加速收敛；        </p>
<pre class=" language-python"><code class="language-python"> <span class="token comment" spellcheck="true"># [...] 超参数</span>

 <span class="token keyword">with</span> tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>framework<span class="token punctuation">.</span>arg_scope<span class="token punctuation">(</span>
         <span class="token punctuation">[</span>fully_connected<span class="token punctuation">]</span><span class="token punctuation">,</span>
         activation_fn<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>elu<span class="token punctuation">,</span>
         weights_initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>variance_scaling_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token comment" spellcheck="true"># [...] 前层，hidden3为编码层</span>
     hidden3_mean <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 从hidden2习得平均值</span>
     hidden3_gamma <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden2<span class="token punctuation">,</span> n_hidden3<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 从hidden2习得gamma（与标准差相关）</span>
     hidden3_sigma <span class="token operator">=</span> tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> hidden3_gamma<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 计算标准差sigma</span>
     noise <span class="token operator">=</span> tf<span class="token punctuation">.</span>random_normal<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>hidden3_sigma<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 高斯分布噪声</span>
     hidden3 <span class="token operator">=</span> hidden3_mean <span class="token operator">+</span> hidden3_sigma <span class="token operator">*</span> noise  <span class="token comment" spellcheck="true"># 在以平均值为中心，标准差为半径的范围内随机采样</span>
     <span class="token comment" spellcheck="true"># [...] 后层</span>
     logits <span class="token operator">=</span> fully_connected<span class="token punctuation">(</span>hidden5<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> activation_fn<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 用于计算损失函数</span>
     outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 实际输出</span>

 <span class="token comment" spellcheck="true"># 计算重构损失</span>
 reconstruction_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>
                         tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>labels<span class="token operator">=</span>X<span class="token punctuation">,</span> logits<span class="token operator">=</span>logits<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token comment" spellcheck="true"># 计算隐藏损失</span>
 latent_loss <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>
                         tf<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>hidden3_gamma<span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>square<span class="token punctuation">(</span>hidden3_mean<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">-</span> hidden3_gamma<span class="token punctuation">)</span>
 <span class="token comment" spellcheck="true"># 计算全局损失</span>
 cost <span class="token operator">=</span> reconstruction_loss <span class="token operator">+</span> latent_loss

 <span class="token comment" spellcheck="true"># [...] 优化器等训练过程略</span>
</code></pre>
</li>
</ol>
<h4 id="其他自编码器"><a href="#其他自编码器" class="headerlink" title="其他自编码器"></a>其他自编码器</h4><ul>
<li>Contractive autoencoder(CAE)<br>  论文：<a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" target="_blank" rel="noopener">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction(2011)</a><br>  关于输入的编码上的导数比较小，使得两个相似的输入得到相似的编码         </li>
<li>Stacked convolutioncal autoencoders<br>  论文：<a href="http://people.idsia.ch/~ciresan/data/icann2011.pdf" target="_blank" rel="noopener">Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction(2011)</a><br>  通过用卷积层来重构图像的方式来提取图像特征     </li>
<li>Generative stochastic network(GSN)<br>  论文：<a href="https://arxiv.org/pdf/1503.05571v2.pdf" target="_blank" rel="noopener">GSNs: Generative Stochastic Networks(2015)</a><br>  降噪自编码器的一般化，能够生成数据       </li>
<li>Winner-take-all(WTA) autoencoder<br>  论文：<a href="https://arxiv.org/pdf/1409.2752v2.pdf" target="_blank" rel="noopener">Winner-Take-All Autoencoders(2015)</a><br>  训练过程中，计算时只保留激活程度前k%的神经元的激活程度，其余都置为0；<br>  这将稀疏化编码，类似的WTA方法也可以应用于生成稀疏化的卷积自编码器；      </li>
<li>Adversarial autoencoders<br>  论文：<a href="https://arxiv.org/pdf/1511.05644v2.pdf" target="_blank" rel="noopener">Adversarial Autoencoders(2016)</a><br>  分为两个网络，一个训练来重构输入的数据，与此同时另外一个训练来找到前者重构效果不好的输入数据；<br>  以此来迫使前者学习出鲁棒性比较好的编码方式      </li>
</ul>

        
    

    
</div>


                

                <!-- Post Comments -->
                
                    
    <!-- 使用 来必力 -->
<div id="livere-comment">
    <div id="lv-container" data-id="city" data-uid="MTAyMC8zOTE5OS8xNTcyNg==">
	<script type="text/ls-javascript" id="livere-comment-js">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];
       if (typeof LivereTower === 'function') { return; }
       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;
       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
</div>
</div>
<style>
    #livere-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>

                
            </div>

            <!-- Post Prev & Next Nav -->
            <nav class="material-nav mdl-color-text--grey-50 mdl-cell mdl-cell--12-col">
    <!-- Prev Nav -->
    
        <a href="/2018/05/02/经典的CNN分类架构/" id="post_nav-newer" class="prev-content">
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_back</i>
            </button>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            新篇
        </a>
    

    <!-- Section Spacer -->
    <div class="section-spacer"></div>

    <!-- Next Nav -->
    
        <a href="/2018/04/22/RNN/" id="post_nav-older" class="next-content">
            旧篇
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <button class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon mdl-color--white mdl-color-text--grey-900" role="presentation">
                <i class="material-icons">arrow_forward</i>
            </button>
        </a>
    
</nav>

        </div>
    </div>



                    
                        <!-- Overlay For Active Sidebar -->
<div class="sidebar-overlay"></div>

<!-- Material sidebar -->
<aside id="sidebar" class="sidebar sidebar-colored sidebar-fixed-left" role="navigation">
    <div id="sidebar-main">
        <!-- Sidebar Header -->
        <div class="sidebar-header header-cover" style="background-image: url(/img/sidebar_header.png);">
    <!-- Top bar -->
    <div class="top-bar"></div>

    <!-- Sidebar toggle button -->
    <button type="button" class="sidebar-toggle mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--icon" style="display: initial;" data-upgraded=",MaterialButton,MaterialRipple">
        <i class="material-icons">clear_all</i>
        <span class="mdl-button__ripple-container">
            <span class="mdl-ripple">
            </span>
        </span>
    </button>

    <!-- Sidebar Avatar -->
    <div class="sidebar-image">
        <img src="/imgs/avatar2.jpg" alt="YaHei's avatar">
    </div>

    <!-- Sidebar Email -->
    <a data-toggle="dropdown" class="sidebar-brand" href="#settings-dropdown">
        hey.yahei.zk@gamil.com
        <b class="caret"></b>
    </a>
</div>


        <!-- Sidebar Navigation  -->
        <ul class="nav sidebar-nav">
    <!-- User dropdown  -->
    <li class="dropdown">
        <ul id="settings-dropdown" class="dropdown-menu">
            
                <li>
                    <a href="mailto: hey.yahei.zk@gamil.com" target="_blank" title="Email Me">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">email</i>
                        
                        Email Me
                    </a>
                </li>
            
                <li>
                    <a href="http://music.163.com/#/user/home?id=421946266" target="_blank" title="Netease Cloud Music">
                        
                            <i class="material-icons sidebar-material-icons sidebar-indent-left1pc-element">music_note</i>
                        
                        Netease Cloud Music
                    </a>
                </li>
            
        </ul>
    </li>

    <!-- Homepage -->
    
        <li id="sidebar-first-li">
            <a href="/">
                
                    <i class="material-icons sidebar-material-icons">home</i>
                
                主页
            </a>
        </li>
        
    

    <!-- Archives  -->
    

    <!-- Categories  -->
    

    <!-- Pages  -->
    
        <li>
            <a href="/tags" title="标签云">
                
                    <i class="material-icons sidebar-material-icons">cloud</i>
                
                标签云
            </a>
        </li>
        
    
        <li>
            <a href="/timeline" title="时间轴">
                
                    <i class="material-icons sidebar-material-icons">event_note</i>
                
                时间轴
            </a>
        </li>
        
    

    <!-- Article Number  -->
    
        <li>
            <a href="/archives">
                文章总数
                <span class="sidebar-badge">50</span>
            </a>
        </li>
        
    
</ul>


        <!-- Sidebar Footer -->
        <!--
I'm glad you use this theme, the development is no so easy, I hope you can keep the copyright, I will thank you so much.
If you still want to delete the copyrights, could you still retain the first one? Which namely "Theme Material"
It will not impact the appearance and can give developers a lot of support :)

很高兴您使用并喜欢该主题，开发不易 十分谢谢与希望您可以保留一下版权声明。
如果您仍然想删除的话 能否只保留第一项呢？即 "Theme Material"
它不会影响美观并可以给开发者很大的支持和动力。 :)
-->

<!-- Sidebar Divider -->

    <div class="sidebar-divider"></div>


<!-- Theme Material -->

    <a href="https://github.com/viosey/hexo-theme-material"  class="sidebar-footer-text-a" target="_blank">
        <div class="sidebar-text mdl-button mdl-js-button mdl-js-ripple-effect sidebar-footer-text-div" data-upgraded=",MaterialButton,MaterialRipple">
            主题 - Material
            <span class="sidebar-badge badge-circle">i</span>
        </div>
    </a>


<!-- Help & Support -->
<!--

-->

<!-- Feedback -->
<!--

-->

<!-- About Theme -->
<!--

-->

    </div>

    <!-- Sidebar Image -->
    

</aside>

                    

                    
                        <!-- Footer Top Button -->
                        <div id="back-to-top" class="toTop-wrap">
    <a href="#top" class="toTop">
        <i class="material-icons footer_top-i">expand_less</i>
    </a>
</div>

                    

                    <!--Footer-->
<footer class="mdl-mini-footer" id="bottom">
    
        <!-- Paradox Footer Left Section -->
        <div class="mdl-mini-footer--left-section sns-list">
    <!-- Twitter -->
    

    <!-- Facebook -->
    

    <!-- Google + -->
    

    <!-- Weibo -->
    

    <!-- Instagram -->
    

    <!-- Tumblr -->
    

    <!-- Github -->
    
        <a href="https://github.com/hey-yahei" target="_blank">
            <button class="mdl-mini-footer--social-btn social-btn footer-sns-github">
                <span class="visuallyhidden">Github</span>
            </button><!--
     --></a>
    

    <!-- LinkedIn -->
    

    <!-- Zhihu -->
    

    <!-- Bilibili -->
    

    <!-- Telegram -->
    

    <!-- V2EX -->
    

    <!-- Segmentfault -->
    
</div>


        <!--Copyright-->
        <div id="copyright">
            Copyright&nbsp;©&nbsp;<span year></span>&nbsp;Hey~YaHei!
            
                <br>
                
                    <a href="http://www.miitbeian.gov.cn" rel="nofollow">浙ICP备17042598号-1</a>
                
            
        </div>

        <!-- Paradox Footer Right Section -->

        <!--
        I am glad you use this theme, the development is no so easy, I hope you can keep the copyright.
        It will not impact the appearance and can give developers a lot of support :)

        很高兴您使用该主题，开发不易，希望您可以保留一下版权声明。
        它不会影响美观并可以给开发者很大的支持。 :)
        -->

        <div class="mdl-mini-footer--right-section">
            <div>
                <div class="footer-develop-div">Powered by <a href="https://hexo.io" target="_blank" class="footer-develop-a">Hexo</a></div>
                <div class="footer-develop-div">Theme - <a href="https://github.com/viosey/hexo-theme-material" target="_blank" class="footer-develop-a">Material</a></div>
            </div>
        </div>
    
</footer>


                    <!-- Import JS File -->

    <script>lsloader.load("lazyload_js","/js/lazyload.min.js?1BcfzuNXqV+ntF6gq+5X3Q==", true)</script>



    <script>lsloader.load("js_js","/js/js.min.js?Bn9UzEm8RrBSxqyZB0zPjA==", true)</script>



    <script>lsloader.load("np_js","/js/nprogress.js?pl3Qhb9lvqR1FlyLUna1Yw==", true)</script>


<script type="text/ls-javascript" id="NProgress-script">
    NProgress.configure({
        showSpinner: true
    });
    NProgress.start();
    $('#nprogress .bar').css({
        'background': '#29d'
    });
    $('#nprogress .peg').css({
        'box-shadow': '0 0 10px #29d, 0 0 15px #29d'
    });
    $('#nprogress .spinner-icon').css({
        'border-top-color': '#29d',
        'border-left-color': '#29d'
    });
    setTimeout(function() {
        NProgress.done();
        $('.fade').removeClass('out');
    }, 800);
</script>



    
        <script>lsloader.load("sm_js","/js/smoothscroll.js?lOy/ACj5suSNi7ZVFVbpFQ==", true)</script>
    







   





<!-- UC Browser Compatible -->
<script>
	var agent = navigator.userAgent.toLowerCase();
	if(agent.indexOf('ucbrowser')>0) {
		document.write('<link rel="stylesheet" href="/css/uc.css">');
	   alert('由于 UC 浏览器使用极旧的内核，而本网站使用了一些新的特性。\n为了您能更好的浏览，推荐使用 Chrome 或 Firefox 浏览器。');
	}
</script>

<!-- Import prettify js  -->



<!-- Window Load -->
<!-- add class for prettify -->
<script type="text/ls-javascript" id="window-load">
    $(window).on('load', function() {
        // Post_Toc parent position fixed
        $('.post-toc-wrap').parent('.mdl-menu__container').css('position', 'fixed');
    });

    
    
</script>

<!-- MathJax Load-->

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

    <script src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>




<!-- Bing Background -->


<script type="text/ls-javascript" id="lazy-load">
    // Offer LazyLoad
    queue.offer(function(){
        $('.lazy').lazyload({
            effect : 'show'
        });
    });

    // Start Queue
    $(document).ready(function(){
        setInterval(function(){
            queue.execNext();
        },200);
    });
</script>

<!-- Custom Footer -->



<script>
    var copyrightNow = new Date().getFullYear();
    var textContent = document.querySelector('span[year]')

    copyrightSince = 2017;
    if (copyrightSince === copyrightNow||copyrightSince === 0000) {
        textContent.textContent = copyrightNow
    } else {
        textContent.textContent = copyrightSince + ' - ' + copyrightNow
    }

    (function(){
        var scriptList = document.querySelectorAll('script[type="text/ls-javascript"]')

        for (var i = 0; i < scriptList.length; ++i) {
            var item = scriptList[i];
            lsloader.runInlineScript(item.id,item.id);
        }
    })()
console.log('\n %c © Material Theme | Version: 1.5.6 | https://github.com/viosey/hexo-theme-material %c \n', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-left-radius:5px;border-bottom-left-radius:5px;', 'color:#455a64;background:#e0e0e0;padding:5px 0;border-top-right-radius:5px;border-bottom-right-radius:5px;');
</script>

                </main>
            </div>
        </body>
    
</html>

<script type="text/javascript">
    var aTagArr = [].slice.apply(document.getElementsByTagName("a"));
    aTagArr.forEach(function (e, i) {
        if( e.href.lastIndexOf("_blank") > -1 ){
            e.target = "_blank";
            e.href = e.href.replace("?_blank", "");
        }
    });
</script>